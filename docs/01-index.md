<details>
<summary>100. HW3 (presented by 謝宏祺, 朱柏澄, 蔡昀達)</summary><br>

<a href="https://www.youtube.com/watch?v=TR937eL1WLc" target="_blank">
    <img src="https://img.youtube.com/vi/TR937eL1WLc/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>99. HW1 (presented by 劉俊緯, 吳宗翰)</summary><br>

<a href="https://www.youtube.com/watch?v=Lobg0qVR-y0" target="_blank">
    <img src="https://img.youtube.com/vi/Lobg0qVR-y0/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>98. HW1 (presented by 毛弘仁, 王克安, 林哲賢)</summary><br>

<a href="https://www.youtube.com/watch?v=95hyyAMJieU" target="_blank">
    <img src="https://img.youtube.com/vi/95hyyAMJieU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>97. HW1 (presented by 黃文璁,蔡昕宇,許傑盛)</summary><br>

<a href="https://www.youtube.com/watch?v=LGAMeOgAwU4" target="_blank">
    <img src="https://img.youtube.com/vi/LGAMeOgAwU4/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>96. DRL Lecture 6: Actor-Critic</summary><br>

<a href="https://www.youtube.com/watch?v=j82QLgfhFiY" target="_blank">
    <img src="https://img.youtube.com/vi/j82QLgfhFiY/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>95. DRL Lecture 7: Sparse Reward</summary><br>

<a href="https://www.youtube.com/watch?v=-5cCWhu0OaM" target="_blank">
    <img src="https://img.youtube.com/vi/-5cCWhu0OaM/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>94. DRL Lecture 5: Q-learning (Continuous Action)</summary><br>

<a href="https://www.youtube.com/watch?v=tnPVcec22cg" target="_blank">
    <img src="https://img.youtube.com/vi/tnPVcec22cg/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>93. DRL Lecture 8: Imitation Learning</summary><br>

<a href="https://www.youtube.com/watch?v=rl_ozvqQUU8" target="_blank">
    <img src="https://img.youtube.com/vi/rl_ozvqQUU8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>92. DRL Lecture 2:  Proximal Policy Optimization (PPO)</summary><br>

<a href="https://www.youtube.com/watch?v=OAKAZhFmYoI" target="_blank">
    <img src="https://img.youtube.com/vi/OAKAZhFmYoI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>91. DRL Lecture 1: Policy Gradient (Review)</summary><br>

<a href="https://www.youtube.com/watch?v=z95ZYgPgXOY" target="_blank">
    <img src="https://img.youtube.com/vi/z95ZYgPgXOY/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>90. DRL Lecture 3: Q-learning (Basic Idea)</summary><br>

<a href="https://www.youtube.com/watch?v=o_g9JUMw1Oc" target="_blank">
    <img src="https://img.youtube.com/vi/o_g9JUMw1Oc/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>89. DRL Lecture 4: Q-learning (Advanced Tips)</summary><br>

<a href="https://www.youtube.com/watch?v=2-zGCx4iv_k" target="_blank">
    <img src="https://img.youtube.com/vi/2-zGCx4iv_k/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>88. GAN Lecture 10 (2018): Evaluation & Concluding Remarks</summary><br>

<a href="https://www.youtube.com/watch?v=IB_ADssBomk" target="_blank">
    <img src="https://img.youtube.com/vi/IB_ADssBomk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>87. GAN Lecture 8 (2018): Photo Editing</summary><br>

<a href="https://www.youtube.com/watch?v=Lhs_Kphd0jg" target="_blank">
    <img src="https://img.youtube.com/vi/Lhs_Kphd0jg/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>86. GAN Lecture 9 (2018): Sequence Generation</summary><br>

<a href="https://www.youtube.com/watch?v=Xb1x4ZgV6iM" target="_blank">
    <img src="https://img.youtube.com/vi/Xb1x4ZgV6iM/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>85. GAN Lecture 6 (2018): WGAN, EBGAN</summary><br>

<a href="https://www.youtube.com/watch?v=3JP-xuBJsyc" target="_blank">
    <img src="https://img.youtube.com/vi/3JP-xuBJsyc/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>84. GAN Lecture 3 (2018): Unsupervised Conditional Generation</summary><br>

<a href="https://www.youtube.com/watch?v=-3LgL3NXLtI" target="_blank">
    <img src="https://img.youtube.com/vi/-3LgL3NXLtI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>83. GAN Lecture 7 (2018): Info GAN, VAE-GAN, BiGAN</summary><br>

<a href="https://www.youtube.com/watch?v=sU5CG8Z0zgw" target="_blank">
    <img src="https://img.youtube.com/vi/sU5CG8Z0zgw/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>82. GAN Lecture 1 (2018): Introduction</summary><br>

<a href="https://www.youtube.com/watch?v=DQNNMiAP5lw" target="_blank">
    <img src="https://img.youtube.com/vi/DQNNMiAP5lw/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>81. GAN Lecture 2 (2018): Conditional Generation</summary><br>

<a href="https://www.youtube.com/watch?v=LpyL4nZSuqU" target="_blank">
    <img src="https://img.youtube.com/vi/LpyL4nZSuqU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>80. GAN Lecture 5 (2018): General Framework</summary><br>

<a href="https://www.youtube.com/watch?v=av1bqilLsyQ" target="_blank">
    <img src="https://img.youtube.com/vi/av1bqilLsyQ/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>79. GAN Lecture 4 (2018): Basic Theory</summary><br>

<a href="https://www.youtube.com/watch?v=DMA4MrNieWo" target="_blank">
    <img src="https://img.youtube.com/vi/DMA4MrNieWo/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>78. Deep Learning Theory 3-2: Indicator of Generalization</summary><br>

<a href="https://www.youtube.com/watch?v=pivB5jEBOQw" target="_blank">
    <img src="https://img.youtube.com/vi/pivB5jEBOQw/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>77. Deep Learning Theory 3-1: Generalization Capability of Deep Learning</summary><br>

<a href="https://www.youtube.com/watch?v=9dtxv4HLq_8" target="_blank">
    <img src="https://img.youtube.com/vi/9dtxv4HLq_8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>76. Deep Learning Theory 2-5: Geometry of Loss Surfaces (Empirical)</summary><br>

<a href="https://www.youtube.com/watch?v=XysGHdNOTbg" target="_blank">
    <img src="https://img.youtube.com/vi/XysGHdNOTbg/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>75. Deep Learning Theory 2-2: Deep Linear Network</summary><br>

<a href="https://www.youtube.com/watch?v=0O6nYRC7GeY" target="_blank">
    <img src="https://img.youtube.com/vi/0O6nYRC7GeY/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>74. Deep Learning Theory 2-4: Geometry of Loss Surfaces (Conjecture)</summary><br>

<a href="https://www.youtube.com/watch?v=_VuWvQUMQVk" target="_blank">
    <img src="https://img.youtube.com/vi/_VuWvQUMQVk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>73. Deep Learning Theory 2-3: Does Deep Network have Local Minima?</summary><br>

<a href="https://www.youtube.com/watch?v=NmelPQkUark" target="_blank">
    <img src="https://img.youtube.com/vi/NmelPQkUark/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>72. Deep Learning Theory 2-1: When Gradient is Zero</summary><br>

<a href="https://www.youtube.com/watch?v=XSdkBG6Vvr0" target="_blank">
    <img src="https://img.youtube.com/vi/XSdkBG6Vvr0/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>71. Deep Learning Theory 1-3: Is Deep better than Shallow?</summary><br>

<a href="https://www.youtube.com/watch?v=qpuLxXrHQB4" target="_blank">
    <img src="https://img.youtube.com/vi/qpuLxXrHQB4/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>70. Deep Learning Theory 1-2: Potential of Deep</summary><br>

<a href="https://www.youtube.com/watch?v=FN8jclCrqY0" target="_blank">
    <img src="https://img.youtube.com/vi/FN8jclCrqY0/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>69. Deep Learning Theory 1-1: Can shallow network fit any function?</summary><br>

<a href="https://www.youtube.com/watch?v=KKT2VkTdFyc" target="_blank">
    <img src="https://img.youtube.com/vi/KKT2VkTdFyc/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>68. ML Lecture 23-3: Reinforcement Learning (including Q-learning)</summary><br>

<a href="https://www.youtube.com/watch?v=2-JNBzCq77c" target="_blank">
    <img src="https://img.youtube.com/vi/2-JNBzCq77c/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>67. ML Lecture 23-2: Policy Gradient (Supplementary Explanation)</summary><br>

<a href="https://www.youtube.com/watch?v=y8UPGr36ccI" target="_blank">
    <img src="https://img.youtube.com/vi/y8UPGr36ccI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>66. GAN Lecture 4 (2017):  From A to Z</summary><br>

<a href="https://www.youtube.com/watch?v=dFwesaqC_Wo" target="_blank">
    <img src="https://img.youtube.com/vi/dFwesaqC_Wo/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>65. GAN Lecture 3 (2017): Improving Sequence Generation by GAN</summary><br>

<a href="https://www.youtube.com/watch?v=Adi54-wp8Qk" target="_blank">
    <img src="https://img.youtube.com/vi/Adi54-wp8Qk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>64. GAN Lecture 2 (2017): CycleGAN</summary><br>

<a href="https://www.youtube.com/watch?v=9N_uOIPghuo" target="_blank">
    <img src="https://img.youtube.com/vi/9N_uOIPghuo/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>63. GAN Lecture 1 (2017): Introduction of Generative Adversarial Network (GAN)</summary><br>

<a href="https://www.youtube.com/watch?v=G0dZc-8yIjE" target="_blank">
    <img src="https://img.youtube.com/vi/G0dZc-8yIjE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>62. SELU</summary><br>

<a href="https://www.youtube.com/watch?v=1WPjVpwJ88I" target="_blank">
    <img src="https://img.youtube.com/vi/1WPjVpwJ88I/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>61. Tuning Hyperparameters</summary><br>

<a href="https://www.youtube.com/watch?v=kyX29rUntjM" target="_blank">
    <img src="https://img.youtube.com/vi/kyX29rUntjM/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>60. Interesting things about deep learning</summary><br>

<a href="https://www.youtube.com/watch?v=1KElr75pHdQ" target="_blank">
    <img src="https://img.youtube.com/vi/1KElr75pHdQ/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>59. Batch Normalization</summary><br>

<a href="https://www.youtube.com/watch?v=BZh1ltr5Rkg" target="_blank">
    <img src="https://img.youtube.com/vi/BZh1ltr5Rkg/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>58. ML Lecture 9-3: Fizz Buzz in Tensorflow (sequel)</summary><br>

<a href="https://www.youtube.com/watch?v=F1vek6ULo9w" target="_blank">
    <img src="https://img.youtube.com/vi/F1vek6ULo9w/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>57. Pointer Network</summary><br>

<a href="https://www.youtube.com/watch?v=VdOyqNQ9aww" target="_blank">
    <img src="https://img.youtube.com/vi/VdOyqNQ9aww/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>56. ML Lecture 22: Ensemble</summary><br>

<a href="https://www.youtube.com/watch?v=tH9FH1DH5n0" target="_blank">
    <img src="https://img.youtube.com/vi/tH9FH1DH5n0/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>55. Gated RNN and Sequence Generation (Recorded at Fall, 2017)</summary><br>

<a href="https://www.youtube.com/watch?v=T8mGfIy9dWM" target="_blank">
    <img src="https://img.youtube.com/vi/T8mGfIy9dWM/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>54. [2017-10-20] ML Lecture 9-2: Keras Demo 2</summary><br>

<a href="https://www.youtube.com/watch?v=Ky1ku1miDow" target="_blank">
    <img src="https://img.youtube.com/vi/Ky1ku1miDow/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

### 核心主題：人工智慧模型在MNIST數據集上的訓練與優化

---

#### 主要觀念：
1. **模型結構**：使用簡單的神經網路結構（如多層感知機）來處理MNIST手寫數字分類任務。
2. **數據特性**：MNIST數據集具有平衡且清潔的特徵，適合用於示範基本的人工智慧技術。
3. **性能指標**：模型在訓練集和測試集上的正確率用於評估其泛化能力。

---

#### 問題原因：
1. **過度擬合（Overfitting）**：模型在訓練數據上表現極佳，但在測試數據上性能大幅下降。
2. **不平衡的訓練與測試性能**：訓練集和測試集之間存在性能 mismatch，表明模型缺乏泛化能力。

---

#### 解決方法：
1. **正規化技術（Dropout）**：
   - 在隱藏層後加入_dropout_層，以降低過度擬合的可能性。
   - Dropout rate設置為0.7，用於限制_neurons_的相關性並提升模型的泛化能力。

2. **優化算法（AdamOptimizer）**：
   - 使用Adam優化器加速訓練過程，提高學習效率。
   - 相對於常規SGD，Adam在訓練初期階段顯著提升了性能。

3. **數據增強（加入Noise）**：
   - 在測試數據上故意添加隨機噪聲，用於模擬真實世界中的數據不確定性。
   - 通過此方法評估模型的魯棒性。

---

#### 優化方式：
1. **學習率調整**：在訓練過程中動態調整 learning rate，以平衡收斂速度和穩定性。
2. **	layer size 設計**：適當增加隱藏層大小，提升模型 capacity。
3. **批量規範化（Batch Normalization）**：在某些情況下可進一步優化模型性能。

---

#### 結論：
1. 適當引入_dropout_技術可以有效降低過度擬合，但會稍微影響訓練過程中的性能。
2. 使用Adam優化器顯著提升了training efficiency和model generalizability。
3. 模型在加入_noise_後的測試數據上表現有所提升，但仍需進一步優化以達到更好的 robustness。

---

#### 總結：
本文通過實驗展示了多種常見的人工智慧技術在MNIST分類任務中的應用效果。結果表明，結合_dropout_、AdamOptimizer和數據增強等方法可以有效改善模型性能，但依然需要根據具體任務需求進一步調優。
</details>

<details>
<summary>53. [2017-10-19] ML Lecture 8-2: Keras 2.0</summary><br>

<a href="https://www.youtube.com/watch?v=5BJDJd-dzzg" target="_blank">
    <img src="https://img.youtube.com/vi/5BJDJd-dzzg/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

### 小節歸納

#### 核心主題
- 神經網路在手寫數字辨識中的應用。
- 使用 Keras 搭建和訓練神經網路模型。

#### 主要觀念
1. **模型結構**：
   - 使用多層感知機（MLP）架構，包含輸入層、隱藏層和輸出層。
   - 輸入層接受 784 維向量（28x28 圖像），輸出層有 10 個神經元對應 0-9 的數字。

2. **訓練過程**：
   - 使用訓練數據（training data）進行模型訓練。
   - 訓練目標是使模型學習到將 입력向量映射到正確的數字標籤。

3. **數據表示**：
   - 輸入數據為一維向量，大小為 (number of examples, 784)。
   - 標籤數據為獨熱編碼（one-hot encoding），大小為 (number of examples, 10)，每個樣本只有一個位置為 1，對應其數字類別。

#### 問題原因
- 手寫數字辨識需要模型學習複雜的圖像特徵。
- 需要確保模型能夠有效地從數據中提取這些特徵並分類。

#### 解決方法
1. **模型設計**：
   - 選擇合適的神經網路架構，如多層感知機（MLP）。
   - 使用適當的激活函數（如ReLU）和損失函數（如交叉熵損失）。

2. **數據預處理**：
   - 將圖像展平為一維向量。
   - 將標籤轉換為獨熱編碼格式以適應模型輸出。

3. **訓練與評估**：
   - 使用訓練數據進行模型訓練，並定期評估模型在測試數據上的表現。
   - 通過損失值和 accuracy 等指標來衡量模型性能。

#### 優化方式
1. **超參數調優**：
   - 選擇合適的學習率（learning rate）。
   - 調整神經網路層數和 neurons 數量以優化模型性能。

2. **正則化技術**：
   - 使用正規化或dropout來防止過度擬合。

3. **數據增強**：
   - 對訓練數據進行 augmentation（如旋轉、翻轉）以增加數據多樣性，提升模型泛化能力。

#### 結論
- 通過適當的模型設計和數據處理，神經網路能夠有效完成手寫數字辨識任務。
- 模型在訓練後可以儲存起來，用於實際應用中的預測。
- 使用evaluation和predict功能可分別評估模型性能和進行線上預測。
</details>

<details>
<summary>52. [2017-10-15] ML Lecture 8-3: Keras Demo</summary><br>

<a href="https://www.youtube.com/watch?v=L8unuZNpWw8" target="_blank">
    <img src="https://img.youtube.com/vi/L8unuZNpWw8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 文章整理：人工智慧深度學習入門與挑戰

## 核心主題
- **深度學習入門**：文章主要介紹了深度學習的基本概念和應用，特別是通過手寫數字辨識的簡單案例來展示深度學習的實際操作。
- **Keras框架簡介**：使用Keras框架搭建神經網路模型，強調其易用性和快速開發的特點。

## 主要觀念
1. **深度學習的核心價值**：
   - 深度學習適合處理大型數據集和複雜模式識別任務。
   - 通過多層神經網路結構，能夠自動提取高級特徵。

2. **Keras框架優勢**：
   - 簡單易用：提供高級API，降低深度學習門檻。
   - 快速開發：適合快速原型設計和 experimentation。

3. **手寫數字辨識案例**：
   - 使用MNIST數據集進行訓練和評估。
   - 展示了模型搭建、訓練和評價的基本流程。

## 問題原因
1. **初學者常見挑戰**：
   - 模型表現不佳：最初模型的驗證accuracy僅為10%，接近隨機猜測水平。
   - 網路結構設計不合理：初始模型深度不足，層數過少。
   - 調參困難：學習率、:hidden_units等超參數選擇不當。

2. **常見錯誤與局限**：
   - 忽略正則化：導致模型過擬合或欠擬合。
   - 網路結構設計不合理：層數不足，影響模型表達能力。
   - 評估方法不科學：未使用交叉驗證等更可靠的評估值。

## 解決方法
1. **改善網路結構**：
   - 增加隱藏層數，提升模型深度。
   - 使用Batch Normalization等技術優化訓練過程。

2. **合理調參**：
   - 選擇適當的學習率和(hidden_units)。
   - 引入Dropout等正則化方法防止過擬合。

3. **科學評估**：
   - 使用交叉驗證等更可靠的模型評估值。
   - 仔細分析損失函數變化，確保模型訓練效果。

## 結論
1. **深度學習的潛力與挑戰**：
   - 深度學習在模式識別領域具有巨大潛力。
   - 初學者需要克服技術門檻和實踐經驗不足等困難。

2. **持續改進建議**：
   - 開始於簡單案例，逐步掌握核心概念。
   - 多進行 experimentation，累積調參經驗。
   - 學習更先進的模型架構和訓練技巧。

3. **未來學習方向**：
   - 探索更複雜的模型結構，如卷積神經網路（CNN）。
   - 學習深度學習的理論基礎，如Optimizer、Activation Function等。
   - 練習實際項目，提升問題分析和解決能力。
</details>

<details>
<summary>51. ML Lecture 0-2: Why we need to learn machine learning?</summary><br>

<a href="https://www.youtube.com/watch?v=On1N8u1z2Ng" target="_blank">
    <img src="https://img.youtube.com/vi/On1N8u1z2Ng/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

