<details>
<summary>201. Network Compression (5/6)</summary><br>

<a href="https://www.youtube.com/watch?v=L0TOXlNpCJ8" target="_blank">
    <img src="https://img.youtube.com/vi/L0TOXlNpCJ8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>202. Network Compression (6/6)</summary><br>

<a href="https://www.youtube.com/watch?v=f0rOMyZSZi4" target="_blank">
    <img src="https://img.youtube.com/vi/f0rOMyZSZi4/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>203. GAN (Quick Review)</summary><br>

<a href="https://www.youtube.com/watch?v=ufcKFjdpT98" target="_blank">
    <img src="https://img.youtube.com/vi/ufcKFjdpT98/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>204. Transformer</summary><br>

<a href="https://www.youtube.com/watch?v=ugWDIIOHtPA" target="_blank">
    <img src="https://img.youtube.com/vi/ugWDIIOHtPA/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>205. Meta Learning - Train+Test as RNN</summary><br>

<a href="https://www.youtube.com/watch?v=ePimv_k-H24" target="_blank">
    <img src="https://img.youtube.com/vi/ePimv_k-H24/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>206. ELMO, BERT, GPT</summary><br>

<a href="https://www.youtube.com/watch?v=UYPa347-DdE" target="_blank">
    <img src="https://img.youtube.com/vi/UYPa347-DdE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>207. Flow-based  Generative Model</summary><br>

<a href="https://www.youtube.com/watch?v=uXY18nzdSsM" target="_blank">
    <img src="https://img.youtube.com/vi/uXY18nzdSsM/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>208. Machine Learning (2020): Course Introduction</summary><br>

<a href="https://www.youtube.com/watch?v=c9TwBeWAj_U" target="_blank">
    <img src="https://img.youtube.com/vi/c9TwBeWAj_U/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>209. [DLHLP 2020] Speech Recognition (1/7) - Overview</summary><br>

<a href="https://www.youtube.com/watch?v=AIKu43goh-8" target="_blank">
    <img src="https://img.youtube.com/vi/AIKu43goh-8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>210. [DLHLP 2020] Deep Learning for Human Language Processing (Course Overview)</summary><br>

<a href="https://www.youtube.com/watch?v=nER51ZyJaCQ" target="_blank">
    <img src="https://img.youtube.com/vi/nER51ZyJaCQ/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>211. [DLHLP 2020] Speech Recognition (2/7) - Listen, Attend, Spell</summary><br>

<a href="https://www.youtube.com/watch?v=BdUeBa6NbXA" target="_blank">
    <img src="https://img.youtube.com/vi/BdUeBa6NbXA/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>212. [DLHLP 2020] Speech Recognition (3/7) - CTC, RNN-T and more</summary><br>

<a href="https://www.youtube.com/watch?v=CGuLuBaLIeI" target="_blank">
    <img src="https://img.youtube.com/vi/CGuLuBaLIeI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>213. [DLHLP 2020] Speech Recognition (4/7) - HMM (optional)</summary><br>

<a href="https://www.youtube.com/watch?v=XWTGY_PNABo" target="_blank">
    <img src="https://img.youtube.com/vi/XWTGY_PNABo/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>214. [DLHLP 2020] Speech Recognition (5/7) - Alignment of HMM, CTC and RNN-T (optional)</summary><br>

<a href="https://www.youtube.com/watch?v=5SSVra6IJY4" target="_blank">
    <img src="https://img.youtube.com/vi/5SSVra6IJY4/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>215. [DLHLP 2020] Speech Recognition (6/7) - RNN-T Training  (optional)</summary><br>

<a href="https://www.youtube.com/watch?v=L519dCHUCog" target="_blank">
    <img src="https://img.youtube.com/vi/L519dCHUCog/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>216. [DLHLP 2020] Speech Recognition (7/7) - Language Modeling</summary><br>

<a href="https://www.youtube.com/watch?v=dymfkWtVUdo" target="_blank">
    <img src="https://img.youtube.com/vi/dymfkWtVUdo/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>217. [DLHLP 2020] Voice Conversion (1/2) - Feature Disentangle</summary><br>

<a href="https://www.youtube.com/watch?v=Jj6blc8UijY" target="_blank">
    <img src="https://img.youtube.com/vi/Jj6blc8UijY/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>218. [2020-03-26] [TA 補充課] Graph Neural Network (1/2) (由助教姜成翰同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=eybCCtNKwzA" target="_blank">
    <img src="https://img.youtube.com/vi/eybCCtNKwzA/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

### 本文重點整理

#### 核心主題
文章主要探討圖神經網路（Graph Neural Networks, GNNs）中 aggregation 操作的核心思想及其不同實現方法。並分析了各種 aggregation 技術的優缺點以及適用場景。

---

#### 主要觀念
1. **Aggregation 操作的重要性**  
   Aggregation 是 GNN 中用於將鄰居節點的特徵信息聚合起來，以更新當前節點表示的核心操作。
   
2. **常見的 Aggregation 方法**  
   - **Mean Pooling**：簡單平均鄰居特徵。  
   - **Max Pooling**：取鄰居特徵的最大值。  
   - **Sum Pooling**：將鄰居特徵相加。  
   - **Attention Mechanism**：基於注意力機制的加權聚合。  

3. **Recent Developments**  
   最近的研究（如 Graph Isomorphism Network,GIN）提出，使用合適的 aggregation 方法可以顯著提升模型性能。

---

#### 問題原因
1. **Mean Pooling 的缺點**  
   - 無法區分具有相同鄰居特徵但結構不同的圖。  

2. **Max Pooling 的缺點**  
   - 可能忽略較小但重要的特徵值，導致信息丟失。  

3. **傳統 Aggregation 方法的局限性**  
   - 離散的聚合方式可能無法充分捕捉到圖結構中的細緻變化。

---

#### 解決方法
1. **GIN 的創新**  
   GIN 提出使用 aggregation 操作後再加上一個 multi-layer perceptron (MLP)，以學習更加豐富的表徵信息。具體公式如下：  
   $$ h_v^{(k+1)} = \text{AGGREGATE}(\{h_u^{(k)}\}_{u \in \mathcal{N}(v)}, h_v^{(k)}) $$  

2. **注意力機制的優化**  
   - 基於鄰居特徵計算注意力權重，實現動態聚合。  
   - 公式：  
     $$ \alpha_{uv} = \text{softmax}(e(h_u, h_v)) $$  
     $$ h_v^{(k+1)} = \sum_{u \in \mathcal{N}(v)} \alpha_{uv} h_u^{(k)} $$  

3. **Sum Pooling 的優勢**  
   - 使用鄰居特徵的簡單相加，避免平均和最大操作的局限性。  

---

#### 結論
1. **GIN 的理論意義**  
   GIN 提供了 aggregation 操作的理論基礎，證明了合適的聚合方式可以顯著提升模型在圖結構數據上的表現。

2. **未來研究方向**  
   - 探索更加高效的注意力機制。  
   - 研究不同聚合方法在特定應用場景下的最佳匹配。  

3. **實踐建議**  
   - 在實際應用中，根據具體任務需求選擇合適的 aggregation 方法。  
   - GIN 提供了一種簡單而有效的聚合方式，值得進一步探索和適用。

---

以上為文章的主要內容整理，涵蓋了核心思想、主要觀念、問題分析及解決方案等關鍵點。
</details>

<details>
<summary>219. [2020-03-26] [TA 補充課] Graph Neural Network (2/2) (由助教姜成翰同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=M9ht8vsVEw8" target="_blank">
    <img src="https://img.youtube.com/vi/M9ht8vsVEw8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

### 核心主題
- **圖神經網路（Graph Neural Networks, GNNs）**：研究如何將圖結構數據引入深度學習模型中，進行各種任務如分類、聚類等。

### 主要觀念
1. **圖神經網路的基本概念**：
   - **圖結構數據**：由節點和邊組成的數據結構，反映實體之間的關係。
   - **圖神經網路的目的**：將非結構化或結構化的圖數據轉換為可學習的表示。

2. **GNN的主要方法**：
   - **基於空間的方法**（Spatial-based）：如GAT（Graph Attention Networks），直接在圖的節點上進行操作。
   - **基於光譜的方法**（Spectral-based）：如GCN（Graph Convolutional Networks），通過傅裏葉變換將圖數據轉化為頻域進行處理。

3. **主要任務**：
   - **監督分類（Supervised Classification）**：在部分或完全標註的數據上訓練模型。
   - **半監督分類（Semi-supervised Classification）**：利用少量標註數據和大量未標註數據進行學習。
   - **圖生成（Graph Generation）**：使用VAE、GAN等方法從頭生成符合特定分布的圖數據。

### 問題原因
1. **GCN的局限性**：
   - **疊代深度增加的性能下降**：隨著模型深度的增加，性能反而惡化。
   - **過平滑現象（Over-smoothing）**：多次聚合操作後，節點表示趨於相似，導致信息喪失。

2. **GNN訓練中的挑戰**：
   - **圖數據的多樣性**：不同類型的圖數據可能需要不同的模型結構。
   - **計算複雜度高**：處理大型圖數據時，計算資源需求較高。

### 解決方法
1. **GCN性能提升**：
   - **DropEdge技術**：在聚合鄰居節點特徵時，隨機丟失一些邊，防止過平滑現象。
   
2. **模型優化**：
   - **使用注意力機制（Attention Mechanism）**：如GAT，根據不同鄰居的重要性分配權重。
   - **引入殘差連接（Residual Connections）**：在深度網絡中，將某層的輸出直接傳遞到淺層，防止梯度消失。

3. **圖生成模型改進**：
   - **Auto-regressive模型**：逐步生成節點和邊，提高生成的可控性。
   - **混合 GenerationTypechniques**：結合VAE和GAN優勢，提升生成數據的多樣性和真實性。

### 結論
- **GNN的研究意義**：作為一種有效的處理結構化數據的方法，在社交網絡、生物信息學等領域有廣泛應用。
- **未來研究方向**：
   - 探索更高效的圖聚合操作。
   - 研究圖神經網路與其他深度學習技術（如Transformer）的結合。
   - 開發適合大規模圖數據的並行計算方法。

### 其他
- **教育意義**：了解GNN的基本原理和應用場景，對於處理複雜實體關係問題具有重要啟示。
</details>

<details>
<summary>220. [DLHLP 2020] Voice Conversion (2/2) - CycleGAN and StarGAN</summary><br>

<a href="https://www.youtube.com/watch?v=JUWVuF2ucTk" target="_blank">
    <img src="https://img.youtube.com/vi/JUWVuF2ucTk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>221. [DLHLP 2020] Speech Separation (1/2) - Deep Clustering, PIT</summary><br>

<a href="https://www.youtube.com/watch?v=tovg5ZxNgIo" target="_blank">
    <img src="https://img.youtube.com/vi/tovg5ZxNgIo/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>222. PyTorch Tutorial (由助教劉記良同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=kQeezFrNoOg" target="_blank">
    <img src="https://img.youtube.com/vi/kQeezFrNoOg/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>223. [2020-04-09] [TA 補充課] Optimization for Deep Learning (1/2) (由助教簡仲明同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=4pUmZ8hXlHM" target="_blank">
    <img src="https://img.youtube.com/vi/4pUmZ8hXlHM/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

### 文章整理：SGD 收斂性改進方法研究

#### 1. 核心主題
- **研究目標**：探討隨機梯度下降（Stochastic Gradient Descent, SGD）算法在深度學習中的收斂性問題，提出改進方法以提升其性能和效率。

#### 2. 主要觀念
- **SGD的基本原理**：SGD是一種常用優化算法，通過隨機採樣訓練數據來更新模型參數，具有計算效率高、內存佔用低的優點。
- **SGD的局限性**：
  - 學習率（Learning Rate, LR）的選擇對收斂速度和結果影響顯著。
  - 易陷入局部最優解，缺乏探索能力。
  - 在複雜的優化landscape中表現不穩定。

#### 3. 問題原因
- **學習率選擇不當**：過大的學習率可能導致模型發散，過小的學習率則會降低收斂速度。
- **優化landscape的複雜性**：深度神經網絡的損失函數 landscape 存在多個局部最優解和鞍點，SGD容易陷入其中。
- **缺乏動態調整機制**：固定學習率無法適應不同階段的優化需求。

#### 4. 解決方法
- **自適應學習率方法**：
  - **Adam優化器**：通過計算梯度的一階矩和二階矩估計來動態調整學習率，具有良好的收斂性和穩定性。
  - **Adagrad**：根據參數梯度的歷史信息自適應地調整學習率。
  - **RMSprop**：基於梯度的平方平均值來調整學習率。

- **周期性學習率方法**：
  - **Cyclical Learning Rate (CLR)**：通過周期性地增加和減少學習率，幫助模型在局部最優解之間進行探索。
  - **One-Cycle Learning Rate**：在一個周期內先增大後減小學習率，以實現快速收斂。

- **學習率範圍測試（LR Range Test）**：
  - 通過實驗確定合適的學習率範圍，避免手動調參的盲目性。

#### 5. 優化方式
- **動態調整機制**：引入自適應算法，根據梯度信息動態調節學習率。
- **探索與收斂結合**：利用周期性變化的學習率，在局部最優解附近進行細緻搜索的同時，保持一定的探索能力。
- **預熱階段（Warm-Up）**：在訓練初期逐漸增加學習率，幫助模型平穩進入優化狀態。

#### 6. 結論
- **研究意義**：改進的SGD算法能夠顯著提升深度神經網絡的訓練效率和收斂質量。
- **未來方向**：
  - 結合多種優化方法，進一步提高算法的通用性和魯棒性。
  - 探索更高效的自適應學習率調整策略。
</details>

<details>
<summary>224. [TA 補充課] Optimization for Deep Learning (2/2) (由助教簡仲明同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=e03YKGHXnL8" target="_blank">
    <img src="https://img.youtube.com/vi/e03YKGHXnL8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>225. [DLHLP 2020] Speech Synthesis (1/2) - Tacotron</summary><br>

<a href="https://www.youtube.com/watch?v=DMxKeHW8KdM" target="_blank">
    <img src="https://img.youtube.com/vi/DMxKeHW8KdM/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>226. [DLHLP 2020] Speech Separation (2/2) - TasNet</summary><br>

<a href="https://www.youtube.com/watch?v=G0O1A7lONSY" target="_blank">
    <img src="https://img.youtube.com/vi/G0O1A7lONSY/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>227. [DLHLP 2020] Speaker Verification</summary><br>

<a href="https://www.youtube.com/watch?v=z3yvxvyP-lE" target="_blank">
    <img src="https://img.youtube.com/vi/z3yvxvyP-lE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>228. [TA 補充課] More about Explainable AI (由助教楊書文同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=LsdiOt0wiWM" target="_blank">
    <img src="https://img.youtube.com/vi/LsdiOt0wiWM/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>229. [DLHLP 2020] Speech Synthesis (2/2) - More than Tacotron</summary><br>

<a href="https://www.youtube.com/watch?v=Eau1Fr2x86Y" target="_blank">
    <img src="https://img.youtube.com/vi/Eau1Fr2x86Y/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>230. [TA 補充課] More about Adversarial Attack (2/2) (由助教黃冠博同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=PaHhMlxFPyU" target="_blank">
    <img src="https://img.youtube.com/vi/PaHhMlxFPyU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>231. [TA 補充課] More about Adversarial Attack (1/2) (由助教黃冠博同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=tfpKIZIWidA" target="_blank">
    <img src="https://img.youtube.com/vi/tfpKIZIWidA/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>232. [DLHLP 2020] Vocoder (由助教許博竣同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=6g2aPc0ol2Y" target="_blank">
    <img src="https://img.youtube.com/vi/6g2aPc0ol2Y/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>233. [DLHLP 2020] Overview of NLP Tasks</summary><br>

<a href="https://www.youtube.com/watch?v=tFBrqPPxWzE" target="_blank">
    <img src="https://img.youtube.com/vi/tFBrqPPxWzE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>234. [DLHLP 2020] BERT and its family - Introduction and Fine-tune</summary><br>

<a href="https://www.youtube.com/watch?v=1_gRK9EIQpc" target="_blank">
    <img src="https://img.youtube.com/vi/1_gRK9EIQpc/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>235. [ICASSP 2020] TOWARDS UNSUPERVISED SPEECH RECOGNITION AND SYNTHESIS (Speaker: Tao Tu)</summary><br>

<a href="https://www.youtube.com/watch?v=cnZdfLSqwiE" target="_blank">
    <img src="https://img.youtube.com/vi/cnZdfLSqwiE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>236. [ICASSP 2020] META LEARNING FOR END-TO-END LOW-RESOURCE SPEECH RECOGNITION (Speaker: Jui-Yang Hsu)</summary><br>

<a href="https://www.youtube.com/watch?v=goav0eXKPwg" target="_blank">
    <img src="https://img.youtube.com/vi/goav0eXKPwg/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>237. [ICASSP 2020] WHAT DOES A NETWORK LAYER HEAR? (Speaker: Chung-Yi Li)</summary><br>

<a href="https://www.youtube.com/watch?v=6gtn7H-pWr8" target="_blank">
    <img src="https://img.youtube.com/vi/6gtn7H-pWr8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>238. [ICASSP 2020] ONE-SHOT VOICE CONVERSION BY VECTOR QUANTIZATION (Speaker: Da-Yi Wu)</summary><br>

<a href="https://www.youtube.com/watch?v=W3t8FHgV90M" target="_blank">
    <img src="https://img.youtube.com/vi/W3t8FHgV90M/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>239. [ICASSP 2020] MOCKINGJAY: UNSUPERVISED SPEECH REPRESENTATION LEARNING (Speaker: Andy T. Liu)</summary><br>

<a href="https://www.youtube.com/watch?v=JlOSyRNFjOw" target="_blank">
    <img src="https://img.youtube.com/vi/JlOSyRNFjOw/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>240. [ICASSP 2020] Defense against adversarial attacks on spoofing countermeasures (Speaker: Haibin Wu)</summary><br>

<a href="https://www.youtube.com/watch?v=sKwz5GvxGgI" target="_blank">
    <img src="https://img.youtube.com/vi/sKwz5GvxGgI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>241. [ICASSP 2020] ASR WITH WORD EMBEDDING REGULARIZATION AND FUSED DECODING (Speaker: Alexander H. Liu)</summary><br>

<a href="https://www.youtube.com/watch?v=1j46kdawA4Q" target="_blank">
    <img src="https://img.youtube.com/vi/1j46kdawA4Q/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>242. [ICASSP 2020] INTERRUPTED AND CASCADED PIT FOR SPEECH SEPARATION (Speaker: Gene-Ping Yang)</summary><br>

<a href="https://www.youtube.com/watch?v=RUhkc6ihyYI" target="_blank">
    <img src="https://img.youtube.com/vi/RUhkc6ihyYI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>243. [ICASSP 2020] TRAINING CODE-SWITCHING LANGUAGE MODEL WITH MONOLINGUAL DATA (Speaker: Shun-Po Chuang)</summary><br>

<a href="https://www.youtube.com/watch?v=qf0j0A0-SVM" target="_blank">
    <img src="https://img.youtube.com/vi/qf0j0A0-SVM/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>244. [DLHLP 2020] BERT and its family - ELMo, BERT, GPT, XLNet, MASS, BART, UniLM, ELECTRA, and more</summary><br>

<a href="https://www.youtube.com/watch?v=Bywo7m6ySlk" target="_blank">
    <img src="https://img.youtube.com/vi/Bywo7m6ySlk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>245. [TA 補充課] Transformer and its variant (由助教紀伯翰同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=lluMBz5AoOg" target="_blank">
    <img src="https://img.youtube.com/vi/lluMBz5AoOg/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>246. [ICLR 2020] Language Modeling for Lifelong Language Learning (Speaker: Fan-Keng Sun)</summary><br>

<a href="https://www.youtube.com/watch?v=cW04Sb02lU4" target="_blank">
    <img src="https://img.youtube.com/vi/cW04Sb02lU4/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>247. [DLHLP 2020] Non-Autoregressive Sequence Generation (由助教莊永松同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=jvyKmU4OM3c" target="_blank">
    <img src="https://img.youtube.com/vi/jvyKmU4OM3c/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>248. [TA 補充課] Network Compression (2/2): Network Pruning (由助教劉俊緯同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=w6gdV2_PtsE" target="_blank">
    <img src="https://img.youtube.com/vi/w6gdV2_PtsE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>249. [TA 補充課] Network Compression (1/2): Knowledge Distillation (由助教劉俊緯同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=9CCn9uPfJ64" target="_blank">
    <img src="https://img.youtube.com/vi/9CCn9uPfJ64/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>250. [DLHLP 2020] Text Style Transfer and Unsupervised Summarization/Translation/Speech Recognition</summary><br>

<a href="https://www.youtube.com/watch?v=WROBoprE0js" target="_blank">
    <img src="https://img.youtube.com/vi/WROBoprE0js/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

