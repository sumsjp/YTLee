<details>
<summary>201. Network Compression (5/6)</summary><br>

<a href="https://www.youtube.com/watch?v=L0TOXlNpCJ8" target="_blank">
    <img src="https://img.youtube.com/vi/L0TOXlNpCJ8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# Network Compression (5/6)


</details>

<details>
<summary>202. Network Compression (6/6)</summary><br>

<a href="https://www.youtube.com/watch?v=f0rOMyZSZi4" target="_blank">
    <img src="https://img.youtube.com/vi/f0rOMyZSZi4/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# Network Compression (6/6)


</details>

<details>
<summary>203. GAN (Quick Review)</summary><br>

<a href="https://www.youtube.com/watch?v=ufcKFjdpT98" target="_blank">
    <img src="https://img.youtube.com/vi/ufcKFjdpT98/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# GAN (Quick Review)


</details>

<details>
<summary>204. Transformer</summary><br>

<a href="https://www.youtube.com/watch?v=ugWDIIOHtPA" target="_blank">
    <img src="https://img.youtube.com/vi/ugWDIIOHtPA/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# Transformer


</details>

<details>
<summary>205. Meta Learning - Train+Test as RNN</summary><br>

<a href="https://www.youtube.com/watch?v=ePimv_k-H24" target="_blank">
    <img src="https://img.youtube.com/vi/ePimv_k-H24/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# Meta Learning - Train+Test as RNN


</details>

<details>
<summary>206. ELMO, BERT, GPT</summary><br>

<a href="https://www.youtube.com/watch?v=UYPa347-DdE" target="_blank">
    <img src="https://img.youtube.com/vi/UYPa347-DdE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# ELMO, BERT, GPT


</details>

<details>
<summary>207. Flow-based  Generative Model</summary><br>

<a href="https://www.youtube.com/watch?v=uXY18nzdSsM" target="_blank">
    <img src="https://img.youtube.com/vi/uXY18nzdSsM/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# Flow-based  Generative Model


</details>

<details>
<summary>208. Machine Learning (2020): Course Introduction</summary><br>

<a href="https://www.youtube.com/watch?v=c9TwBeWAj_U" target="_blank">
    <img src="https://img.youtube.com/vi/c9TwBeWAj_U/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# Machine Learning (2020): Course Introduction


</details>

<details>
<summary>209. [DLHLP 2020] Speech Recognition (1/7) - Overview</summary><br>

<a href="https://www.youtube.com/watch?v=AIKu43goh-8" target="_blank">
    <img src="https://img.youtube.com/vi/AIKu43goh-8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [DLHLP 2020] Speech Recognition (1/7) - Overview


</details>

<details>
<summary>210. [DLHLP 2020] Deep Learning for Human Language Processing (Course Overview)</summary><br>

<a href="https://www.youtube.com/watch?v=nER51ZyJaCQ" target="_blank">
    <img src="https://img.youtube.com/vi/nER51ZyJaCQ/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [DLHLP 2020] Deep Learning for Human Language Processing (Course Overview)


</details>

<details>
<summary>211. [DLHLP 2020] Speech Recognition (2/7) - Listen, Attend, Spell</summary><br>

<a href="https://www.youtube.com/watch?v=BdUeBa6NbXA" target="_blank">
    <img src="https://img.youtube.com/vi/BdUeBa6NbXA/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [DLHLP 2020] Speech Recognition (2/7) - Listen, Attend, Spell


</details>

<details>
<summary>212. [DLHLP 2020] Speech Recognition (3/7) - CTC, RNN-T and more</summary><br>

<a href="https://www.youtube.com/watch?v=CGuLuBaLIeI" target="_blank">
    <img src="https://img.youtube.com/vi/CGuLuBaLIeI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [DLHLP 2020] Speech Recognition (3/7) - CTC, RNN-T and more


</details>

<details>
<summary>213. [DLHLP 2020] Speech Recognition (4/7) - HMM (optional)</summary><br>

<a href="https://www.youtube.com/watch?v=XWTGY_PNABo" target="_blank">
    <img src="https://img.youtube.com/vi/XWTGY_PNABo/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [DLHLP 2020] Speech Recognition (4/7) - HMM (optional)


</details>

<details>
<summary>214. [DLHLP 2020] Speech Recognition (5/7) - Alignment of HMM, CTC and RNN-T (optional)</summary><br>

<a href="https://www.youtube.com/watch?v=5SSVra6IJY4" target="_blank">
    <img src="https://img.youtube.com/vi/5SSVra6IJY4/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [DLHLP 2020] Speech Recognition (5/7) - Alignment of HMM, CTC and RNN-T (optional)


</details>

<details>
<summary>215. [DLHLP 2020] Speech Recognition (6/7) - RNN-T Training  (optional)</summary><br>

<a href="https://www.youtube.com/watch?v=L519dCHUCog" target="_blank">
    <img src="https://img.youtube.com/vi/L519dCHUCog/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [DLHLP 2020] Speech Recognition (6/7) - RNN-T Training  (optional)


</details>

<details>
<summary>216. [DLHLP 2020] Speech Recognition (7/7) - Language Modeling</summary><br>

<a href="https://www.youtube.com/watch?v=dymfkWtVUdo" target="_blank">
    <img src="https://img.youtube.com/vi/dymfkWtVUdo/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [DLHLP 2020] Speech Recognition (7/7) - Language Modeling


</details>

<details>
<summary>217. [DLHLP 2020] Voice Conversion (1/2) - Feature Disentangle</summary><br>

<a href="https://www.youtube.com/watch?v=Jj6blc8UijY" target="_blank">
    <img src="https://img.youtube.com/vi/Jj6blc8UijY/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [DLHLP 2020] Voice Conversion (1/2) - Feature Disentangle


</details>

<details>
<summary>218. [2020-03-26] [TA 補充課] Graph Neural Network (1/2) (由助教姜成翰同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=eybCCtNKwzA" target="_blank">
    <img src="https://img.youtube.com/vi/eybCCtNKwzA/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [TA 補充課] Graph Neural Network (1/2) (由助教姜成翰同學講授)

### 本文重點整理

#### 核心主題
文章主要探討圖神經網路（Graph Neural Networks, GNNs）中 aggregation 操作的核心思想及其不同實現方法。並分析了各種 aggregation 技術的優缺點以及適用場景。

---

#### 主要觀念
1. **Aggregation 操作的重要性**  
   Aggregation 是 GNN 中用於將鄰居節點的特徵信息聚合起來，以更新當前節點表示的核心操作。
   
2. **常見的 Aggregation 方法**  
   - **Mean Pooling**：簡單平均鄰居特徵。  
   - **Max Pooling**：取鄰居特徵的最大值。  
   - **Sum Pooling**：將鄰居特徵相加。  
   - **Attention Mechanism**：基於注意力機制的加權聚合。  

3. **Recent Developments**  
   最近的研究（如 Graph Isomorphism Network,GIN）提出，使用合適的 aggregation 方法可以顯著提升模型性能。

---

#### 問題原因
1. **Mean Pooling 的缺點**  
   - 無法區分具有相同鄰居特徵但結構不同的圖。  

2. **Max Pooling 的缺點**  
   - 可能忽略較小但重要的特徵值，導致信息丟失。  

3. **傳統 Aggregation 方法的局限性**  
   - 離散的聚合方式可能無法充分捕捉到圖結構中的細緻變化。

---

#### 解決方法
1. **GIN 的創新**  
   GIN 提出使用 aggregation 操作後再加上一個 multi-layer perceptron (MLP)，以學習更加豐富的表徵信息。具體公式如下：  
   $$ h_v^{(k+1)} = \text{AGGREGATE}(\{h_u^{(k)}\}_{u \in \mathcal{N}(v)}, h_v^{(k)}) $$  

2. **注意力機制的優化**  
   - 基於鄰居特徵計算注意力權重，實現動態聚合。  
   - 公式：  
     $$ \alpha_{uv} = \text{softmax}(e(h_u, h_v)) $$  
     $$ h_v^{(k+1)} = \sum_{u \in \mathcal{N}(v)} \alpha_{uv} h_u^{(k)} $$  

3. **Sum Pooling 的優勢**  
   - 使用鄰居特徵的簡單相加，避免平均和最大操作的局限性。  

---

#### 結論
1. **GIN 的理論意義**  
   GIN 提供了 aggregation 操作的理論基礎，證明了合適的聚合方式可以顯著提升模型在圖結構數據上的表現。

2. **未來研究方向**  
   - 探索更加高效的注意力機制。  
   - 研究不同聚合方法在特定應用場景下的最佳匹配。  

3. **實踐建議**  
   - 在實際應用中，根據具體任務需求選擇合適的 aggregation 方法。  
   - GIN 提供了一種簡單而有效的聚合方式，值得進一步探索和適用。

---

以上為文章的主要內容整理，涵蓋了核心思想、主要觀念、問題分析及解決方案等關鍵點。
</details>

<details>
<summary>219. [2020-03-26] [TA 補充課] Graph Neural Network (2/2) (由助教姜成翰同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=M9ht8vsVEw8" target="_blank">
    <img src="https://img.youtube.com/vi/M9ht8vsVEw8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [TA 補充課] Graph Neural Network (2/2) (由助教姜成翰同學講授)

### 核心主題
- **圖神經網路（Graph Neural Networks, GNNs）**：研究如何將圖結構數據引入深度學習模型中，進行各種任務如分類、聚類等。

### 主要觀念
1. **圖神經網路的基本概念**：
   - **圖結構數據**：由節點和邊組成的數據結構，反映實體之間的關係。
   - **圖神經網路的目的**：將非結構化或結構化的圖數據轉換為可學習的表示。

2. **GNN的主要方法**：
   - **基於空間的方法**（Spatial-based）：如GAT（Graph Attention Networks），直接在圖的節點上進行操作。
   - **基於光譜的方法**（Spectral-based）：如GCN（Graph Convolutional Networks），通過傅裏葉變換將圖數據轉化為頻域進行處理。

3. **主要任務**：
   - **監督分類（Supervised Classification）**：在部分或完全標註的數據上訓練模型。
   - **半監督分類（Semi-supervised Classification）**：利用少量標註數據和大量未標註數據進行學習。
   - **圖生成（Graph Generation）**：使用VAE、GAN等方法從頭生成符合特定分布的圖數據。

### 問題原因
1. **GCN的局限性**：
   - **疊代深度增加的性能下降**：隨著模型深度的增加，性能反而惡化。
   - **過平滑現象（Over-smoothing）**：多次聚合操作後，節點表示趨於相似，導致信息喪失。

2. **GNN訓練中的挑戰**：
   - **圖數據的多樣性**：不同類型的圖數據可能需要不同的模型結構。
   - **計算複雜度高**：處理大型圖數據時，計算資源需求較高。

### 解決方法
1. **GCN性能提升**：
   - **DropEdge技術**：在聚合鄰居節點特徵時，隨機丟失一些邊，防止過平滑現象。
   
2. **模型優化**：
   - **使用注意力機制（Attention Mechanism）**：如GAT，根據不同鄰居的重要性分配權重。
   - **引入殘差連接（Residual Connections）**：在深度網絡中，將某層的輸出直接傳遞到淺層，防止梯度消失。

3. **圖生成模型改進**：
   - **Auto-regressive模型**：逐步生成節點和邊，提高生成的可控性。
   - **混合 GenerationTypechniques**：結合VAE和GAN優勢，提升生成數據的多樣性和真實性。

### 結論
- **GNN的研究意義**：作為一種有效的處理結構化數據的方法，在社交網絡、生物信息學等領域有廣泛應用。
- **未來研究方向**：
   - 探索更高效的圖聚合操作。
   - 研究圖神經網路與其他深度學習技術（如Transformer）的結合。
   - 開發適合大規模圖數據的並行計算方法。

### 其他
- **教育意義**：了解GNN的基本原理和應用場景，對於處理複雜實體關係問題具有重要啟示。
</details>

<details>
<summary>220. [DLHLP 2020] Voice Conversion (2/2) - CycleGAN and StarGAN</summary><br>

<a href="https://www.youtube.com/watch?v=JUWVuF2ucTk" target="_blank">
    <img src="https://img.youtube.com/vi/JUWVuF2ucTk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [DLHLP 2020] Voice Conversion (2/2) - CycleGAN and StarGAN


</details>

<details>
<summary>221. [DLHLP 2020] Speech Separation (1/2) - Deep Clustering, PIT</summary><br>

<a href="https://www.youtube.com/watch?v=tovg5ZxNgIo" target="_blank">
    <img src="https://img.youtube.com/vi/tovg5ZxNgIo/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [DLHLP 2020] Speech Separation (1/2) - Deep Clustering, PIT


</details>

<details>
<summary>222. PyTorch Tutorial (由助教劉記良同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=kQeezFrNoOg" target="_blank">
    <img src="https://img.youtube.com/vi/kQeezFrNoOg/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# PyTorch Tutorial (由助教劉記良同學講授)


</details>

<details>
<summary>223. [2020-04-09] [TA 補充課] Optimization for Deep Learning (1/2) (由助教簡仲明同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=4pUmZ8hXlHM" target="_blank">
    <img src="https://img.youtube.com/vi/4pUmZ8hXlHM/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [TA 補充課] Optimization for Deep Learning (1/2) (由助教簡仲明同學講授)

### 文章整理：SGD 收斂性改進方法研究

#### 1. 核心主題
- **研究目標**：探討隨機梯度下降（Stochastic Gradient Descent, SGD）算法在深度學習中的收斂性問題，提出改進方法以提升其性能和效率。

#### 2. 主要觀念
- **SGD的基本原理**：SGD是一種常用優化算法，通過隨機採樣訓練數據來更新模型參數，具有計算效率高、內存佔用低的優點。
- **SGD的局限性**：
  - 學習率（Learning Rate, LR）的選擇對收斂速度和結果影響顯著。
  - 易陷入局部最優解，缺乏探索能力。
  - 在複雜的優化landscape中表現不穩定。

#### 3. 問題原因
- **學習率選擇不當**：過大的學習率可能導致模型發散，過小的學習率則會降低收斂速度。
- **優化landscape的複雜性**：深度神經網絡的損失函數 landscape 存在多個局部最優解和鞍點，SGD容易陷入其中。
- **缺乏動態調整機制**：固定學習率無法適應不同階段的優化需求。

#### 4. 解決方法
- **自適應學習率方法**：
  - **Adam優化器**：通過計算梯度的一階矩和二階矩估計來動態調整學習率，具有良好的收斂性和穩定性。
  - **Adagrad**：根據參數梯度的歷史信息自適應地調整學習率。
  - **RMSprop**：基於梯度的平方平均值來調整學習率。

- **周期性學習率方法**：
  - **Cyclical Learning Rate (CLR)**：通過周期性地增加和減少學習率，幫助模型在局部最優解之間進行探索。
  - **One-Cycle Learning Rate**：在一個周期內先增大後減小學習率，以實現快速收斂。

- **學習率範圍測試（LR Range Test）**：
  - 通過實驗確定合適的學習率範圍，避免手動調參的盲目性。

#### 5. 優化方式
- **動態調整機制**：引入自適應算法，根據梯度信息動態調節學習率。
- **探索與收斂結合**：利用周期性變化的學習率，在局部最優解附近進行細緻搜索的同時，保持一定的探索能力。
- **預熱階段（Warm-Up）**：在訓練初期逐漸增加學習率，幫助模型平穩進入優化狀態。

#### 6. 結論
- **研究意義**：改進的SGD算法能夠顯著提升深度神經網絡的訓練效率和收斂質量。
- **未來方向**：
  - 結合多種優化方法，進一步提高算法的通用性和魯棒性。
  - 探索更高效的自適應學習率調整策略。
</details>

<details>
<summary>224. [2020-04-09] [TA 補充課] Optimization for Deep Learning (2/2) (由助教簡仲明同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=e03YKGHXnL8" target="_blank">
    <img src="https://img.youtube.com/vi/e03YKGHXnL8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [TA 補充課] Optimization for Deep Learning (2/2) (由助教簡仲明同學講授)

### 小節一：核心主題
- 探討深度學習中優化器（Optimizer）的選擇與應用。
- 分析不同優化器在不同任務中的表現及其適用場景。

### 小節二：主要觀念
1. **SGD 與 Momentum SGD (SGDM)**：
   - SGD 是最基本的優化器，但收斂速度較慢且容易陷入鞍點。
   - Momentum SGD 引入動量機制，加速收斂並改善梯度下降的穩定性。

2. **Adam 優化器**：
   - 結合了 AdaGrad 和 RMSprop 的優點，自適應調整學習率。
   - 在大多數深度學習任務中表現優異，尤其在 NLP 領域。

3. **AdamW**：
   - Adam 的改進版本，通過引入權重衰減機制提升模型的泛化能力。

4. **Lookahead 策略**：
   - 一種優化器封裝策略，結合其他優化器（如 SGD 或 Adam）進一步提升性能。

### 小節三：問題原因
1. **SGD 的局限性**：
   - 收斂速度慢，容易陷入局部最優。
   - 對初始學習率敏感，難以處理複雜的損失函數 landscapes。

2. **Adam 的潛在問題**：
   - 在某些 CV 任務中可能不如 SGDM 穩定。
   - 可能導致模型在測試集上的表現不佳（generalization gap）。

3. **數據與架構的影響**：
   - 數據質量問題或網絡架構不合理可能導致優化器選擇無法解決的根本性問題。

### 小節四：解決方法
1. **選擇合適的優化器**：
   - CV 任務優先考慮 SGDM。
   - NLP 和生成模型推薦使用 Adam 或 AdamW。

2. **調整學習率與權重衰減**：
   - 使用適當的學習率調度策略（如餘弦退火）。
   - 在 AdamW 等優化器中引入權重衰減以提升泛化能力。

3. **結合 Lookahead 策略**：
   - 將 Lookahead 與其他優化器結合，進一步優化訓練效果。

4. **多嘗試與調整**：
   - 根據具體任務需求，多次實驗並調整優化器參數和策略。

### 小節五：優化方式
1. **學習率調度（Learning Rate Scheduling）**：
   - 使用如餘弦退火等方法動態調整學習率，加速收斂並提升模型性能。

2. **權重衰減（Weight Decay）**：
   - 在優化器中引入 L2 正則化，防止過擬合，提升泛化能力。

3. **動量機制（Momentum）**：
   - 通過引入動量，加速梯度下降過程，避免陷入鞍點。

4. **自適應學習率調整（Adaptive Learning Rate）**：
   - Adam 等優化器通過自適應調整學習率，提升訓練效率和穩定性。

### 小節六：結論
- 沒有一款 optimizer 是萬能的，選擇合適的優化器需根據具體任務需求。
- SGD 和 SGDM 在 CV 任務中表現較好，而 Adam 和 AdamW 更適合 NLP 和生成模型。
- Lookahead 策略可作爲提升訓練效果的輔助手段。
- 雖然優化器的選擇對性能有一定影響，但數據質量和網絡架構才是模型性能的關鍵因素。
</details>

<details>
<summary>225. [DLHLP 2020] Speech Synthesis (1/2) - Tacotron</summary><br>

<a href="https://www.youtube.com/watch?v=DMxKeHW8KdM" target="_blank">
    <img src="https://img.youtube.com/vi/DMxKeHW8KdM/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [DLHLP 2020] Speech Synthesis (1/2) - Tacotron


</details>

<details>
<summary>226. [DLHLP 2020] Speech Separation (2/2) - TasNet</summary><br>

<a href="https://www.youtube.com/watch?v=G0O1A7lONSY" target="_blank">
    <img src="https://img.youtube.com/vi/G0O1A7lONSY/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [DLHLP 2020] Speech Separation (2/2) - TasNet


</details>

<details>
<summary>227. [DLHLP 2020] Speaker Verification</summary><br>

<a href="https://www.youtube.com/watch?v=z3yvxvyP-lE" target="_blank">
    <img src="https://img.youtube.com/vi/z3yvxvyP-lE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [DLHLP 2020] Speaker Verification


</details>

<details>
<summary>228. [TA 補充課] More about Explainable AI (由助教楊書文同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=LsdiOt0wiWM" target="_blank">
    <img src="https://img.youtube.com/vi/LsdiOt0wiWM/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [TA 補充課] More about Explainable AI (由助教楊書文同學講授)


</details>

<details>
<summary>229. [DLHLP 2020] Speech Synthesis (2/2) - More than Tacotron</summary><br>

<a href="https://www.youtube.com/watch?v=Eau1Fr2x86Y" target="_blank">
    <img src="https://img.youtube.com/vi/Eau1Fr2x86Y/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [DLHLP 2020] Speech Synthesis (2/2) - More than Tacotron


</details>

<details>
<summary>230. [2020-04-20] [TA 補充課] More about Adversarial Attack (2/2) (由助教黃冠博同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=PaHhMlxFPyU" target="_blank">
    <img src="https://img.youtube.com/vi/PaHhMlxFPyU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [TA 補充課] More about Adversarial Attack (2/2) (由助教黃冠博同學講授)

# 文章重點整理

## 核心主題
文章主要探討了深度學習模型在音視覺領域中的安全問題，並介紹了針對音視覺模型的攻擊方法。這些攻擊包括影像攻擊（如One Pixel Attack）和音訊攻擊（如Hidden Voice Attack），旨在揭示模型的脆弱性。

---

## 主要觀念

### 1. 影像攻擊
- **核心思想**：通過少量修改（如一像素更改）來擾亂深度學習模型的判斷。
- **技術實現**：
  - 使用差分進化算法搜索最有效的攻擊像素。
  - 確定攻擊像素後，計算其RGB值以最大化模型誤判機率。

### 2. 音訊攻擊
- **核心思想**：在音訊中植入幹擾信號（如高頻正弦波），使模型無法準確識別內容。
- **技術實現**：
  - **Time Domain Inversion**：反轉音訊的時域特性。
  - **Random Phase Generation**：隨機修改音訊的相位。
  - **High Frequency Addition**：添加高頻正弦波擾動。
  - **Time Scaling**：改變音訊的速度，同時保持採樣率恆定。

---

## 問題原因
- 深度學習模型對輸入數據的高度敏感性導致其易受攻擊。
- 音視覺模型在處理結構化數據時的脆弱性為攻擊提供了可乘之機。

---

## 解決方法
- **影晌_Attack**：
  - 差分進化算法用於搜索最小幹擾下的最大影響像素。
  - 確定攻擊像素後，計算其RGB值以最大化模型誤判機率。

- **音訊_Attack**：
  - 添加高頻正弦波或修改音訊特性（如相位、速度）來擾亂模型。
  - 確保添加的幹擾在預處理階段被濾除，使模型保持穩定性。

---

## 優化方式
- **影晌_Attack**：
  - 開發更高效的搜索算法以降低計算成本。
  - 提高攻擊策略的通用性，使其適用於不同類型的深度學習模型。

- **音訊_Attack**：
  - 研究更隱蔽的幹擾方式，使其不易被人類感知。
  - 警告：濫用此技術可能對用戶體驗造成影響，需注意倫理問題。

---

## 結論
文章展示了深度學習模型在音視覺領域中的脆弱性，並提出了多種攻擊方法。這些方法可幫助研究者理解模型的局限性，從而在未來的研究中進一步改進模型的安全性和 robustness。
</details>

<details>
<summary>231. [2020-04-20] [TA 補充課] More about Adversarial Attack (1/2) (由助教黃冠博同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=tfpKIZIWidA" target="_blank">
    <img src="https://img.youtube.com/vi/tfpKIZIWidA/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [TA 補充課] More about Adversarial Attack (1/2) (由助教黃冠博同學講授)

### 小節一：核心主題  
- 文章介紹了差分進化（Differential Evolution, DE）算法及其在單像素攻擊（One Pixel Attack）中的應用。  
- 重點討論了DE算法的基本原理、實現步驟以及其在圖像對抗攻擊中的具體應用。

### 小節二：主要觀念  
1. **差分進化算法**  
   - DE是一種基於羣體的優化算法，通過迭代搜索來找到最優解。  
   - 主要步驟包括初始化種羣、變異、交叉和選擇。  
2. **單像素攻擊**  
   - 一種圖像對抗攻擊方法，僅修改一個像素即可實現對目標模型的攻擊。  
   - 攻擊目標是欺騙分類器，使其誤判圖像類別。

### 小節三：問題原因  
1. **算法適用性問題**  
   - DE算法最初設計用於連續空間優化，需適應其在離散和高維空間中的應用。  
2. **單像素攻擊的限制**  
   - 僅修改一個像素，增加了搜索空間的難度，且可能影響攻擊的成功率。  
3. **圖片大小的影響**  
   - 圖片越大，搜索空間越大，DE算法在固定迭代次數下成功率下降。

### 小節四：解決方法  
1. **適應DE算法到單像素攻擊**  
   - 將DE應用於五維向量（x, y, r, g, b），分別表示攻擊的像素坐標及顏色值。  
2. **優化搜索策略**  
   - 通過調整變異和交叉參數，提高種羣多樣性。  
3. **平衡圖片大小與資源**  
   - 在大圖片中增加迭代次數或候選數量，以提高攻擊成功率。

### 小節五：優化方式  
1. **參數調整**  
   - 調整DE算法的變異因子和交叉概率，以適應不同問題。  
2. **局部搜索增強**  
   - 結合其他優化方法（如梯度下降）進行局部精煉，提高解的質量。  
3. **資源分配策略**  
   - 根據圖片大小動態調整候選數量或迭代次數，確保攻擊效率。

### 小節六：結論  
- 差分進化算法在單像素攻擊中表現出良好的效果，但需根據具體問題進行參數和策略的優化。  
- 圖片大小對攻擊成功率有顯著影響，需通過增加資源投入來應對大尺寸圖片的挑戰。  
- 未來研究可進一步探索DE與其他優化方法的結合，提升攻擊效率和成功率。
</details>

<details>
<summary>232. [DLHLP 2020] Vocoder (由助教許博竣同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=6g2aPc0ol2Y" target="_blank">
    <img src="https://img.youtube.com/vi/6g2aPc0ol2Y/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [DLHLP 2020] Vocoder (由助教許博竣同學講授)


</details>

<details>
<summary>233. [DLHLP 2020] Overview of NLP Tasks</summary><br>

<a href="https://www.youtube.com/watch?v=tFBrqPPxWzE" target="_blank">
    <img src="https://img.youtube.com/vi/tFBrqPPxWzE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [DLHLP 2020] Overview of NLP Tasks


</details>

<details>
<summary>234. [DLHLP 2020] BERT and its family - Introduction and Fine-tune</summary><br>

<a href="https://www.youtube.com/watch?v=1_gRK9EIQpc" target="_blank">
    <img src="https://img.youtube.com/vi/1_gRK9EIQpc/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [DLHLP 2020] BERT and its family - Introduction and Fine-tune


</details>

<details>
<summary>235. [2020-05-03] [ICASSP 2020] TOWARDS UNSUPERVISED SPEECH RECOGNITION AND SYNTHESIS (Speaker: Tao Tu)</summary><br>

<a href="https://www.youtube.com/watch?v=cnZdfLSqwiE" target="_blank">
    <img src="https://img.youtube.com/vi/cnZdfLSqwiE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ICASSP 2020] TOWARDS UNSUPERVISED SPEECH RECOGNITION AND SYNTHESIS (Speaker: Tao Tu)

### 文章整理：基於共享表徵的語音自監督學習框架

---

#### 核心主題  
本文提出了一種基於共享表徵的語音自我監督學習架構，旨在 simultaneou 語音重建、語音識別和文本到語音合成（TTS）。該方法利用少量帶標籤數據和多部分訓練目標來提升模型性能。

---

#### 主要觀念  
1. **共享表徵架構**：(encoder 和 decoder 共享同一對稱表示，橋接重建、識別和合成模塊。)  
2. **三重_training 設計**：
   - **重建部分**：無需標籤數據，學習語音表示。
   - **識別部分**：使用CTC損失，將表徵與音素對齊。
   - **合成部分**：使用少量標籤數據،訓練.decoder 將離散表徵轉換為語音。  
3. **多模塊協作**：重建、識別和合成模塊通過共享表徵實現相互提升。

---

#### 問題分析  
1. **數據匱乏問題**：文本到語音合成傳統方法依賴大量帶標籤數據，限制了實用性。  
2. **孤立學習問題**：傳統模型的重建、識別和合成模塊缺乏共享表徵，導致性能瓶頸。

---

#### 解決方案  
1. **提出共享表徵架構**：(encoder 和 decoder 共享同一對稱表示，實現多模塊協作。)  
2. **三重_training 框架**：
   - 利用無標籤數據進行語音重建。
   - 使用少量帶標籤數據訓練語音識別和合成模塊。
3. **橋接重建與合成**：通過共享表徵，將語音重建與文本到語音合成有機結合。

---

#### 優化方式  
1. **自監督學習**：利用無標籤數據進行語音重建，降低對帶標籤數據的依賴。  
2. **多任務學習**：三重訓練目標（重建、識別、合成）共同提升模型性能。  
3. **共享表徵設計**：通過(encoder-decoder)共用表徵，實現模塊之間的相互增益。

---

#### 結論與實驗結果  
1. **表示能力**：
   - t-SNE 證據表示在 IPA 元音圖上與 linguistic 知識一致。
2. **語音識別性能**：
   - 在不同標籤數據量下，模型性能超越基準模型。
3. **語音合成質量**：
   - 使用少量標籤數據（20 分鐘），生成語音的MLS指標接近上限。
4. **實時演示效果**：
   - 模型在語音識別和文本到語音合成任務中表現出色，與真值高度一致。

---

#### 未來方向  
1. **擴展數據集**：進一步驗證模型在多說話人或多語言環境下的性能。  
2. **優化架構設計**：探索更高效的共享表徵結構。  
3. **提升合成質量**：研究如何利用更多語音特徵（如情感、語調）進一步提高合成效果。

---

### 總結  
本文提出了一種基於共享表徵的語音自我監督學習框架，同時實現語音重建、識別和合成。通過三重_training 和模塊協作，該方法在數據匱乏的情況下取得了優異的性能，為自監督學習在語音領域的應用提供了新思路。
</details>

<details>
<summary>236. [2020-05-03] [ICASSP 2020] META LEARNING FOR END-TO-END LOW-RESOURCE SPEECH RECOGNITION (Speaker: Jui-Yang Hsu)</summary><br>

<a href="https://www.youtube.com/watch?v=goav0eXKPwg" target="_blank">
    <img src="https://img.youtube.com/vi/goav0eXKPwg/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ICASSP 2020] META LEARNING FOR END-TO-END LOW-RESOURCE SPEECH RECOGNITION (Speaker: Jui-Yang Hsu)

### 文章整理與分析

---

#### **1. 核心主題**
本文圍繞**多語種自動語音識別（ASR）模型的跨語言遷移學習**展開研究，重點探討如何利用來源語言數據提升目標語言的性能，特別是在資源受限的情況下。

---

#### **2. 主要觀念**
- **多語種ASR模型**：通過訓練一個能夠處理多種語言的模型，實現跨語言遷移學習。
- **SOURCE LANGUAGE AND TARGET LANGUAGE PAIRS (SLTPs)**：來源語言和目標語言對的研究是核心，用於驗證不同語言之間的遷移效果。
- **LIMITED LANGUAGE PACK (LLP) 和 FULL LANGUAGE PACK (FLP)**：研究中使用了兩種數據集，分別代表資源受限和資源充足的場景。

---

#### **3. 問題與原因分析**
- **問題**：在資源匱乏的情況下（如目標語言只有少量數據），ASR模型的性能受限。
- **原因**：
  - 少數語言的數據不足，導致模型訓練效果差。
  - 多語種模型的遷移能力未被充分驗證。

---

#### **4. 解決方法**
- **多語種訓練策略**：利用來源語言（如 Bengali, Tagalog, Zulu）的豐富數據，訓練一個多語種ASR模型，然後將其遷移到目標語言。
- **META-SOCKET TRAINING APPROACH (MSTA)**：一種改進的訓練方法，考慮到適應過程中的潛在優化，提升遷移效果。
- **跨語言遷移學習**：通過來源語言和目標language pairs的研究，驗證模型的遷移能力。

---

#### **5. 優化方式**
- **數據資源利用**：
  - 使用FULL LANGUAGE PACK (FLP) 和LIMITED LANGUAGE PACK (LLP) 過多語言對進行訓練。
  - 對LLP進行交叉驗證，提升模型的泛化能力。
- **性能評估指標**：
  - 使用字符錯誤率（Character Error Rate, CER）作為主要評估指標。
  - 測試遷移學習的效果，降低過擬合風險。

---

#### **6. 結論**
- **實驗結果**：
  - META-SOCKET TRAINING APPROACH (MSTA) 的性能優於隨機初始化（Random Initialization），字符錯誤率更低。
  - 對來源語言和目標語言對的遷移效果進行了多種測試，結果一致顯示MSTA的效果更佳。
- **未來工作**：
  - 延伸研究更多語言對，提升模型的 robustness。
  - 探索更多應用場景，如文本到語音合成（Text-to-Speech, TTS）等。

---

#### **7. 總結**
本文提出了一種基於多語種ASR模型的跨語言遷移學習方法，並通過實驗驗證了其有效性。研究結果表明，META-SOCKET TRAINING APPROACH (MSTA) 能顯著提升目標語言的性能，特別是在資源匱乏的情況下。此方法具有廣泛的應用潛力，可進一步優化以應對更多多樣化的語言場景。
</details>

<details>
<summary>237. [2020-05-03] [ICASSP 2020] WHAT DOES A NETWORK LAYER HEAR? (Speaker: Chung-Yi Li)</summary><br>

<a href="https://www.youtube.com/watch?v=6gtn7H-pWr8" target="_blank">
    <img src="https://img.youtube.com/vi/6gtn7H-pWr8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ICASSP 2020] WHAT DOES A NETWORK LAYER HEAR? (Speaker: Chung-Yi Li)

# 文章重點整理

## 核心主題
- 提出一種分析語音恢復（Speech Recognition, SR）的新方法，通過探查模型的隱藏狀態來了解模型行為。
- 方法具有模型agnostic性，適用於不同類型的模型架構。

## 主要觀念
1. **問題焦點**：語音恢復模型在處理Noise corrupted audio時的能力與限制，特別是對話音特徵（如Prosody和韻律）的影響。
2. **探查隱藏狀態的作用**：通過分析不同層次的隱藏狀態，揭示模型如何逐層提取和消除信息，特別是語音特徵和Noise。

## 啟發與方法
- 使用生成的語音片段來模擬真實場景，並利用Speaker Verification指標（如ER）來衡量模型性能。
- 通過STOI（Short Time Objective Intelligibility）評估恢復語音的可懂度，量化 distortion 的影響。

## 問題原因
1. **語音特徵的逐層消除**：模型在深度學習過程中，Prosody和韻律等語音特徵逐漸被削弱或移除。
2. **Noise對Baseline模型的幹擾**：在低信噪比（SNR）環境下，基線模型無法有效抑制Noise，導致語音恢復效果差。

## 解決方法與優化
1. **提出新分析框架**：
   - 採用探查隱藏狀態的方式，提供直觀的模型行為洞察。
   - 方法具備通用性，可廣泛應用於不同模型架構。
2. **改進語音恢復能力**：
   - 開發Noise-Robust SR模型，提升在 noisy 環境中的性能。
   - 通過STOI測量揭示modelo的局限性，為未來研究提供方向。

## 測試與結果
1. **語音特徵保留度**：
   - CN部分：主要負責頻譜提取，保留語音信息，但未顯著影響Prosody和韻律。
   - TCN層：Prosody逐漸被削弱，導致Speaker Verification指標ER值上升。
2. **Noise抑制能力**：
   - Noise-Robust模型在低SNR條件下表現 superior，Noise被有效消除。
   - 基線模型在高Noise幹擾下性能下降明顯。

## 結論
- 提出的探查隱藏狀態方法為理解SR模型提供新視角，具有重要研究價值。
- Noise-Robust SR模型在實際應用中展現出更好的 robustness 和性能。
- 未來研究可進一步優化模型架構，提升語音特徵保留和Noise抑制能力。
</details>

<details>
<summary>238. [2020-05-03] [ICASSP 2020] ONE-SHOT VOICE CONVERSION BY VECTOR QUANTIZATION (Speaker: Da-Yi Wu)</summary><br>

<a href="https://www.youtube.com/watch?v=W3t8FHgV90M" target="_blank">
    <img src="https://img.youtube.com/vi/W3t8FHgV90M/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ICASSP 2020] ONE-SHOT VOICE CONVERSION BY VECTOR QUANTIZATION (Speaker: Da-Yi Wu)

### 文章整理（正式學術用語）

---

#### **核心主題**
- 探討基於語音轉換（Voice Conversion, VCC）的任務中，內容編碼（Content Code）與說話人編碼（Speaker Code）之間信息分離的技術。
- 研究如何通過模型設計和優化實現語音特徵的解耦。

---

#### **主要觀念**
1. 語音轉換任務的核心挑戰在於實現內容信息和說話人信息的有效分離。
2. 內容編碼應僅包含與語音內容相關的信息，而說話人編碼應僅反映說話人的特徵。
3. 提出一種基於自動編碼器（Autoencoder）的模型框架，通過引入內部分離機制（Internal Disentanglement）來實現特徵解耦。

---

#### **問題原因**
- 常見的語音轉換模型中，內容編碼和說話人編碼之間可能存在信息混雜現象，導致語音轉換結果的質量不穩定。
- 傳統的自動編碼器在特徵分離方面的能力有限，難以有效區分內容和說話人的信息。

---

#### **解決方法**
1. 引入內部分離機制（Internal Disentanglement）：
   - 在自動編碼器中，通過將編碼層分爲內容編碼和說話人編碼兩部分，實現對語音特徵的初步分離。
2. 使用向量量化（Vector Quantization, VQ）技術對內容編碼進行進一步優化：
   - 通過引入VQ模塊，降低內容編碼的空間相關性，增強其表徵能力。

---

#### **優化方式**
1. 在訓練過程中最小化重構損失，提升模型的語音生成質量。
2. 通過實驗驗證不同參數設置（如向量量化碼本大小）對模型性能的影響。
3. 對比分析模型在內容編碼和說話人編碼上的分離效果：
   - 使用分類任務評估說話人信息的純淨度。
   - 檢查內容編碼中是否存在說話人信息的混雜。

---

#### **實驗結果**
1. 在包含400位說話人的測試集上，模型表現出良好的特徵分離能力：
   - 內容編碼和說話人編碼在分類任務中的準確率接近50%，表明兩者的信息分離較爲理想。
2. 向量量化碼本大小對模型性能的影響顯著：
   - 當碼本大小爲256時，模型的重建質量達到最佳狀態。
3. 通過主觀聽覺評估，轉換後的語音質量得到提升，表現出較高的自然度和可懂度。

---

#### **結論**
- 內部分離機制結合向量量化技術能夠有效實現內容信息和說話人信息的解耦。
- 優化的內容編碼設計顯著提升了語音轉換模型的性能和穩定性。
- 模型在保持較低複雜度的同時，實現了高質量的語音轉換效果。

---

### 總結
本文提出了一種基於內部分離機制的語音轉換方法，通過結合向量量化技術，有效解決了內容信息與說話人信息混雜的問題。實驗結果表明，該模型在特徵分離和語音生成質量方面均表現出色，爲語音轉換任務提供了新的研究方向。
</details>

<details>
<summary>239. [2020-05-03] [ICASSP 2020] MOCKINGJAY: UNSUPERVISED SPEECH REPRESENTATION LEARNING (Speaker: Andy T. Liu)</summary><br>

<a href="https://www.youtube.com/watch?v=JlOSyRNFjOw" target="_blank">
    <img src="https://img.youtube.com/vi/JlOSyRNFjOw/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ICASSP 2020] MOCKINGJAY: UNSUPERVISED SPEECH REPRESENTATION LEARNING (Speaker: Andy T. Liu)

### 1. 核心主題  
   - 探討自監督學習（Self-Supervised Learning, SSL）在低資源語音任務中的有效性與優勢。  
   - 提出了一種新的語音預訓練方法，名爲 **Mockingjay**，用於提升有監督任務的性能。  

### 2. 主要觀念  
   - 自監督學習通過利用未標記數據進行預訓練，減少對標註數據的依賴，在低資源場景中具有重要價值。  
   - Mockingjay 方法結合了對比學習和掩蔽預測任務，以增強語音表示的學習能力。  

### 3. 問題原因  
   - 在低資源環境下（即標註數據有限），傳統的監督學習方法性能受限。  
   - 現有自監督方法在某些特定任務上的泛化能力不足，難以在不同數據集之間有效遷移。  

### 4. 解決方法  
   - **Mockingjay 預訓練框架**：  
     a. 使用對比學習（Contrastive Learning）來增強語音特徵的辨別能力。  
     b. 引入掩蔽預測任務（Masked Prediction Task），通過掩蓋部分語音信號，進一步提升模型對語音內容的理解能力。  
   - **多任務聯合優化**：在預訓練過程中同時優化多個相關任務，以提高模型的泛化性能。  

### 5. 優化方式  
   - **加權求和（Weighted Sum）**：通過學習各層隱藏狀態的權重，整合多層特徵表示，進一步提升下遊任務的性能。  
   - **微調策略（Fine-Tuning Strategies）**：  
     a. **全模型微調（Full Model Fine-Tuning, FT2）**：在保持預訓練權重的基礎上，對整個模型進行微調以適應特定任務需求。  
     b. **部分微調（Partial Fine-Tuning）**：僅對下遊任務相關模塊進行調整，減少計算開銷。  

### 6. 結論  
   - Mockingjay 方法在三個下遊任務（音素分類、說話人識別和情感分類）中均表現出色：  
     a. 基礎模型性能優於傳統的Mel特徵表示。  
     b. 大型模型通過加權求和進一步提升性能，最高實現5.75%的性能增益。  
   - 在低資源場景下（標註數據僅爲10%），基礎微調模型（Base FT2）表現最優，甚至超越使用全部標註數據的傳統Mel特徵方法。  
   - 驗證了自監督預訓練在提升有監督任務性能方面的有效性，尤其是在標註數據稀缺的情況下。
</details>

<details>
<summary>240. [2020-05-04] [ICASSP 2020] Defense against adversarial attacks on spoofing countermeasures (Speaker: Haibin Wu)</summary><br>

<a href="https://www.youtube.com/watch?v=sKwz5GvxGgI" target="_blank">
    <img src="https://img.youtube.com/vi/sKwz5GvxGgI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ICASSP 2020] Defense against adversarial attacks on spoofing countermeasures (Speaker: Haibin Wu)

### 核心主題  
- **自動說話人驗證（ASV）**：Automatic Speaker Verification 在生物特徵識別中的重要性及其高性能系統仍存在的漏洞，特別是針對spoofing攻擊的防禦需求。  

---

### 主要觀念  
1. **Spoofing 攻擊的危害**：  
   - Spoofing 音頻（如聲音重放、文本到語音生成）對 ASV 系統造成威脅，需建立有效的反spoofing模型以保護 ASV。  

2. **反spoofing 模型的脆弱性**：  
   - 即便高性能的反spoofing模型，也容易受到 adversarial examples 的攻擊，導致性能大幅下降。  

3. **Adversarial 攻擊的影響**：  
   - Adversarial 攻擊能生成看似無害但可 fools 模型的音頻，這類攻擊在 subjective 試驗中通常無法被人類察覺。  

---

### 問題原因  
1. **模型的脆弱性**：  
   - 現有反spoofing模型缺乏對 adversarial 攻擊的魯棒性，容易被攻擊者利用。  

2. **數據分布的限制**：  
   - 基於特定訓練數據集（如ESB Spoof 2019）訓練的模型，可能無法有效防禦未見過的 adversarial 攻擊。  

---

### 解決方法  
1. ** spatial smoothing 過濾器**：  
   - 使用不同類型的 spatial filters （均值、中位數、高斯濾波器）對音頻 spectogram 進行處理，消除 adversarial perturbations 的影響。  

2. **Hydra Stereo 訓練法則**：  
   - 通過 iterative 的訓練步驟（生成 adversarial examples 並反向傳導錯誤），修復模型的脆弱點，增強其魯棒性。  

---

### 優化方式  
1. **濾波器選擇**：  
   - 中位數和均值濾波器在提升模型性能方面表現優越，而高斯濾波器可能降低性能，需謹慎使用。  

2. **混合防禦策略**：  
   - 結合 spatial smoothing 和 adversarial training，可顯著提升模型的 robustness（如 testing accuracy 提升至 92.4%）。  

---

### 結論  
- 雖然 adversarial 攻擊具有高度隱蔽性且難以檢測，但通過結合patial.filters 和 adversarial training 等方法，可顯著增強反spoofing模型的魯棒性。未來工作可進一步探索其他防禦策略，以應對更多樣化和複雜的攻擊方式。
</details>

<details>
<summary>241. [2020-05-04] [ICASSP 2020] ASR WITH WORD EMBEDDING REGULARIZATION AND FUSED DECODING (Speaker: Alexander H. Liu)</summary><br>

<a href="https://www.youtube.com/watch?v=1j46kdawA4Q" target="_blank">
    <img src="https://img.youtube.com/vi/1j46kdawA4Q/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ICASSP 2020] ASR WITH WORD EMBEDDING REGULARIZATION AND FUSED DECODING (Speaker: Alexander H. Liu)

### 核心主題  
- 提出一種新型的正規化方法（Word Embedding Regularization）來改進序列到序列的自動語音辨識（ASR）模型。

### 主要觀念  
1. **_SEQUENCE TO SEQUENCE ASR**：基於序列到序列架構，利用編碼器和解碼器進行語音轉錄。  
2. **WORD EMBEDDINGS**：使用詞嵌入作為目標，以捕獲語言的語義信息。  
3. **UNPAIRED DATA**：在低資源情況下，利用未配對的文字數據來提升模型性能。  

### 問題原因  
1. **DATA SCARCITY**：在低資源設置中，可用的配對數據（audio-text pairs）量有限，限制了模型的訓練效果。  
2. **LANGUAGE MODEL LIMITATIONS**：傳統語言模型在某些情況下無法充分提升ASR性能。  

### 解決方法  
1. **WORD EMBEDDING REGULARIZATION**：在解碼器中引入詞嵌入正規化項，將未配對的文字數據融入模型訓練。  
2. **FUSION DECODING**：結合相似性基於的分佈和語言模型，在解碼過程中進一步提升性能。  

### 優化方式  
1. **LIGHTWEIGHT APPROACH**：提出的方法計算開銷小，可輕易整合到現有架構中。  
2. **COMPATIBILITY**：與其他解碼技術兼容，可疊加使用以獲得最佳效果。  

### 結論  
1. **IMPROVEMENT RESULTS**：在高資源和低資源設置下，提出的方法均顯著降低了WER（Word Error Rate）。  
2. **DEPENDENCE ON WORD EMBEDDINGS**：詞嵌入算法的選擇對性能有顯著影響，使用BERT等高性能詞嵌入算法可進一步提升效果。  
3. **STACKABLE NATURE**：方法可與其他技術結合使用，實現累積性能提升。  

### 總結  
本文提出了一種新型的正規化方法，充分利用未配對的文字數據來改進序列到序列ASR模型，在低資源和高資源設置下均展示了顯著的效果提升，且具備輕量化和兼容性的優勢。
</details>

<details>
<summary>242. [2020-05-04] [ICASSP 2020] INTERRUPTED AND CASCADED PIT FOR SPEECH SEPARATION (Speaker: Gene-Ping Yang)</summary><br>

<a href="https://www.youtube.com/watch?v=RUhkc6ihyYI" target="_blank">
    <img src="https://img.youtube.com/vi/RUhkc6ihyYI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ICASSP 2020] INTERRUPTED AND CASCADED PIT FOR SPEECH SEPARATION (Speaker: Gene-Ping Yang)

### 文章整理：Interrupted and Cascade Permutation Invariant Training for Speech Separation

---

#### 一、核心主題  
- 探討語音分離任務中基於排列不變性訓練（Permutation Invariant Training, PIT）的方法及其改進方案。

---

#### 二、主要觀念  
1. **排列不變性訓練的基本原理**：  
   - 在語音混合數據中，模型輸出的兩個聲道需要與真實的地面.truth進行匹配。  
   - 由於存在多種可能的對應關係（綠色和棕色），PIT通過最小化所有排列的可能性來訓練模型。

2. **存在的問題**：  
   - PIT方法在初始化階段容易陷入局部最優，導致性能不穩定。  
   - 模型在第一階段的隨機初始化可能導致不良的學習方向。

3. **提出的方法**：  
   - **階段一**：使用原始PIT進行訓練，獲取初步的標籤分配（label assignment）。  
   - **階段二**：基於階段一的結果，固定標籤分配並進一步優化模型。  
   - **階段三**：在階段二的基礎上，再次啟用PIT訓練，以提升最終性能。

---

#### 三、問題原因  
- 隨機初始化模型參數可能導致PIT訓練初期的不穩定性和次優解。  
- 初期的標籤分配（label assignment）可能無法充分反映數據的真實結構，影響模型的學習效果。

---

#### 四、解決方法  
1. **固定標籤分配訓練**：  
   - 在第一階段使用PIT獲得初步的標籤分配後，在第二階段固定該分配並進行進一步訓練。  
   - 通過固定標籤分配，避免了初始化階段的不穩定性，使模型朝著更好的方向優化。

2. **分段訓練策略**：  
   - 分為三階段：  
     1. 使用原始PIT進行初步訓練。  
     2. 基於第一階段的結果，固定標籤分配並進行第二階段訓練。  
     3. 再次啟用PIT訓練，進一步提升性能。

---

#### 五、優化方式  
- 在第二階段固定標籤分配後，模型參數已朝著更好的方向優化。  
- 第三階段再啟用PIT訓練時，初始模型參數更穩定，導致標籤分配的.switch率顯著降低，性能提升。

---

#### 六、實驗結果  
1. **基於固定標籤分配的訓練效果**：  
   - 使用不同L值（如L=10, L=80）進行第二階段訓練，最佳模型在L=80時取得17.66 dB的SDR性能。  

2. **與基線對比**：  
   - 與PIT基線相比，固定標籤分配後的模型性能提升約1.54 dB。

3. **最終性能**：  
   - 第三階段訓練後，模型在驗證集上的SDR性能達到17.99 dB，在測試集上也取得顯著提升。

---

#### 七、結論  
- 提出了一種分段訓練方法，通過固定標籤分配優化模型初始化問題。  
- 該方法可顯著提高語音分離性能，SDR增益約1.54 dB。  
- 本研究的方法具有普適性，可用於其他語音分離模型的改進。

--- 

此整理結構清晰地總結了文章的核心內容、問題、解決方案及實驗結果，並以正式的學術用語進行表達。
</details>

<details>
<summary>243. [2020-05-04] [ICASSP 2020] TRAINING CODE-SWITCHING LANGUAGE MODEL WITH MONOLINGUAL DATA (Speaker: Shun-Po Chuang)</summary><br>

<a href="https://www.youtube.com/watch?v=qf0j0A0-SVM" target="_blank">
    <img src="https://img.youtube.com/vi/qf0j0A0-SVM/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ICASSP 2020] TRAINING CODE-SWITCHING LANGUAGE MODEL WITH MONOLINGUAL DATA (Speaker: Shun-Po Chuang)

### 核心主題  
- 探討如何利用單語數據訓練多語言模型（Coastal Region Language Model, CRLM）。  
- 研究代碼切換任務中的數據不足問題，並提出解決方案。  

---

### 主要觀念  
1. **代碼切換任務的挑戰**：  
   - 代碼切換是指在單一文本中交替使用多種語言的現象，常見於多語社區。  
   - 數據不足是代碼切換任務（如機器翻譯、語言識別）的主要瓶頸。  

2. **單語數據的優勢與局限性**：  
   - 單語數據廣泛可用，但直接用於多語言模型訓練效果有限。  
   - 需要通過技術創新充分利用單語數據提升模型性能。  

---

### 問題原因  
- 傳統方法在利用單語數據進行多語言建模時面臨以下挑戰：  
  1. **信息不足**：單語數據缺乏跨語言的上下文關係，難以捕捉代碼切換模式。  
  2. **模型偏差**：僅依賴單語數據可能導致模型對某一種語言過於偏好，忽視其他語言的特徵。  

---

### 解決方法  
1. **僞代碼切換數據生成**：  
   - 利用句法和語義相似性規則，從單語數據中自動生成僞代碼切換句子。  
   - 示例：將英文句子「今天天氣真好」轉換爲「Today 天氣真好」。  

2. **正則化約束**：  
   - 引入對稱KL散度作爲正則化項，約束投影矩陣的分布差異，減少語言間偏差。  

3. **歸一化技術**：  
   - 對隱藏層表示進行L2歸一化處理，增強模型對不同語言特徵的區分能力。  

---

### 優化方式  
1. **實驗驗證**：  
   - 使用混合語料庫（單語文本 + 僞代碼切換文本）進行訓練，顯著降低困惑度（Perplexity）。  
   - 結果顯示，結合正則化約束和歸一化技術的模型性能最優。  

2. **可視化分析**：  
   - 通過PCA降維技術，展示投影矩陣在二維空間中的分布變化。  
     - 基線模型：語言間特徵可分但距離較遠。  
     - 使用僞代碼切換數據訓練：語言間距離縮小且無過度重疊。  
     - 應用對稱KL散度約束：語言特徵完全重疊，表徵能力顯著增強。  

3. **語義對齊實驗**：  
   - 測試模型對跨語言語義相似性任務的性能（如檢索同義詞）。  
   - 實驗表明，應用正則化和歸一化技術後，語義映射精度提升明顯。  

---

### 結論  
- 提出了一種基於單語數據的多語言建模新方法，有效解決了代碼切換任務中的數據不足問題。  
- 通過僞代碼切換數據生成、正則化約束和歸一化優化，顯著提升了模型在代碼切換場景下的表現。  
- 實驗結果表明，結合多種優化技術的模型性能最優，爲跨語言自然語言處理提供了新的研究方向。  

---

### 英文摘要  
This paper addresses the challenge of training a multi-language model using monolingual data, particularly focusing on code-switching tasks. We identify the limitations of conventional methods in leveraging monolingual data and propose innovative solutions to enhance model performance. Key contributions include:  
1. A novel approach to generating pseudo-code-switching sentences from monolingual data using syntactic and semantic similarity rules.  
2. Regularization techniques, including symmetric KL divergence constraints, to reduce language biases.  
3. Normalization methods to improve the distinguishability of cross-language features.  

Extensive experiments demonstrate that our proposed methods significantly reduce perplexity and enhance semantic mapping accuracy. The results highlight the potential of monolingual data in multi-language modeling and provide new insights for code-switching natural language processing research.
</details>

<details>
<summary>244. [DLHLP 2020] BERT and its family - ELMo, BERT, GPT, XLNet, MASS, BART, UniLM, ELECTRA, and more</summary><br>

<a href="https://www.youtube.com/watch?v=Bywo7m6ySlk" target="_blank">
    <img src="https://img.youtube.com/vi/Bywo7m6ySlk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [DLHLP 2020] BERT and its family - ELMo, BERT, GPT, XLNet, MASS, BART, UniLM, ELECTRA, and more


</details>

<details>
<summary>245. [2020-05-07] [TA 補充課] Transformer and its variant (由助教紀伯翰同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=lluMBz5AoOg" target="_blank">
    <img src="https://img.youtube.com/vi/lluMBz5AoOg/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [TA 補充課] Transformer and its variant (由助教紀伯翰同學講授)

### 文章重點整理

#### 核心主題
- **Reformer 模型**：介紹了一種新型Transformer架構，旨在改進計算效率和記憶體使用。
- **BERT模型變體**：討論了BERT的兩種主要實現方式及其差異。
- **Style GAN在動漫生成中的應用**：展示了一種GAN在生成高質量動漫圖像方面的突破。

#### 主要觀念
1. **Reformer模型的核心思想**
   - 通過分桶注意力（Bucketing Attention）降低計算複雜度。
   - 強調可逆層（Reversible Layer）以節省記憶體和計算資源。

2. **BERT模型的實現差異**
   - 基於Transformer架構的不同變體在實踐中的表現和優化。

3. **Style GAN的能力展示**
   - 在生成高質量動漫圖像方面的成功案例及其藝術價值。

#### 問題原因
- **計算複雜度**：傳統Transformer的注意力機制導致O(n²)時間複雜度，限制了大規模數據處理。
- **記憶體需求**：深度模型的訓練需要存儲多層激活值，增加了記憶體開銷。

#### 解決方法
1. **Reformer的.bucketing Attention**
   - 將序列分桶，每個元素只與其桶內及其他有限桶內的元素交互。
   - 複雜度從O(n²)降至O(n log n)，節省計算資源。

2. **Reversible Layer**
   - 通過殘差連接和可逆性質，只需存儲當前層激活值即可反向傳播。
   - 顯著降低記憶體需求，適合大規模模型訓練。

3. **BERT的優化策略**
   - 使用更深的網絡結構和有效的注意力機制來提升性能。
   - 適用不同數據集時進行參數調整以平衡效果和效率。

#### 總結與主旨
- **主旨**：介紹Reformer模型在Transformer架構中的創新，並展示BERT和Style GAN在自然語言處理和生成模型領域的最新進展。
- **總結**：
  - Reformer通過分桶注意力和可逆層設計，有效降低了計算複雜度和記憶體需求，適合大規模數據處理。
  - BERT展示了深度Transformer模型在自然語言理解中的強大能力，而Style GAN則在生成模型領域開闢了新的可能性。

#### 各段落之間的對應性
- 第一部分（Reformer）介紹了新穎的架構設計及其優勢，為後續BERT和GAN的討論奠定了技術背景。
- 第二部分（BERT）展示了如何將Transformer應用於具體任務，並探討其優化策略。
- 第三部分（Style GAN）則轉向生成模型，展示人工智能在藝術領域的潛力。

---

### 文章主旨與總結
本文主要圍繞三項重要的人工智能技術展開：Reformer模型、BERT及其變體，以及Style GAN。作者通過詳細介紹每種技術的核心思想、實現方法及實際應用，強調了這些技術在提升計算效率和生成質量方面的突破。文章最後以一個具體案例展示了GAN在藝術生成中的成功，進一步凸顯了人工智能的多樣性與潛力。

---

### 總結
本文圍繞Reformer模型、BERT變體和Style GAN展開，介紹了各項技術的核心思想、問題來源、解決方案及實際效果。文章結構清晰，從計算效率到具體應用均有涉獵，最後以一個藝術生成案例點明人工智能的創新能力。
</details>

<details>
<summary>246. [2020-05-07] [ICLR 2020] Language Modeling for Lifelong Language Learning (Speaker: Fan-Keng Sun)</summary><br>

<a href="https://www.youtube.com/watch?v=cW04Sb02lU4" target="_blank">
    <img src="https://img.youtube.com/vi/cW04Sb02lU4/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ICLR 2020] Language Modeling for Lifelong Language Learning (Speaker: Fan-Keng Sun)

### 核心主題
- **終身語言學習（Lifelong Language Learning）**：研究模型在學習多個NLP任務時，能夠逐步掌握而不導致災難性忘記的能力。
- **Lemmo 方法**：提出了一種基於語言模型的輕量級方法，用於解決終身學習中的災難性 forgetting 問題。


### 主要觀念
1. **終身學習的挑戰**：傳統機器學習算法在學習多個任務時易受災難性忘記影響。
2. **NLP 任務範圍**：涵蓋情感分類、語義角色標注、目標導向對話、問題解答和語義解析等多種任務。


### 問題原因
- **災難性 Forgetting**：模型在學習新任務後，舊任務的知識_recall 效果差。
- **數據生成需求**：傳統數據庫方法需要額外的數據生成模型，增加複雜度。


### 解決方法
1. **Lemmo 方法**：利用 Squad 格式轉換將多樣化的輸入輸出對統一為上下文、問題和答案格式。
2. **單一語言模型**: 一個模型用於任務解讀和假數據生成，降低複雜度。


### 優化方式
1. **任務特定令牌**：引入任務-specific令牌，在假數據生成時提高模型的目標識別能力。
2. **Squad 格式轉換**：將不同任務的數據格式化為統一的上下文、問題和答案格式，便於模型理解和生成。


### 結論
- **實驗結果**：Lemmo 方法在三個數據集上的表現優於先前的最先進方法，並接近多任務學習的上限。
- **未來方向**：進一步研究如何通過任務-specific令牌提高模型性能，特別是 задач數量增加時。
</details>

<details>
<summary>247. [DLHLP 2020] Non-Autoregressive Sequence Generation (由助教莊永松同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=jvyKmU4OM3c" target="_blank">
    <img src="https://img.youtube.com/vi/jvyKmU4OM3c/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [DLHLP 2020] Non-Autoregressive Sequence Generation (由助教莊永松同學講授)


</details>

<details>
<summary>248. [TA 補充課] Network Compression (2/2): Network Pruning (由助教劉俊緯同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=w6gdV2_PtsE" target="_blank">
    <img src="https://img.youtube.com/vi/w6gdV2_PtsE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [TA 補充課] Network Compression (2/2): Network Pruning (由助教劉俊緯同學講授)


</details>

<details>
<summary>249. [TA 補充課] Network Compression (1/2): Knowledge Distillation (由助教劉俊緯同學講授)</summary><br>

<a href="https://www.youtube.com/watch?v=9CCn9uPfJ64" target="_blank">
    <img src="https://img.youtube.com/vi/9CCn9uPfJ64/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [TA 補充課] Network Compression (1/2): Knowledge Distillation (由助教劉俊緯同學講授)


</details>

<details>
<summary>250. [DLHLP 2020] Text Style Transfer and Unsupervised Summarization/Translation/Speech Recognition</summary><br>

<a href="https://www.youtube.com/watch?v=WROBoprE0js" target="_blank">
    <img src="https://img.youtube.com/vi/WROBoprE0js/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [DLHLP 2020] Text Style Transfer and Unsupervised Summarization/Translation/Speech Recognition


</details>

