<details>
<summary>301. 【機器學習2021】類神經網路訓練不起來怎麼辦 (四)：損失函數 (Loss) 也可能有影響</summary><br>

<a href="https://www.youtube.com/watch?v=O2VkP8dJ5FE" target="_blank">
    <img src="https://img.youtube.com/vi/O2VkP8dJ5FE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】類神經網路訓練不起來怎麼辦 (四)：損失函數 (Loss) 也可能有影響


</details>

<details>
<summary>302. 【機器學習2021】類神經網路訓練不起來怎麼辦 (三)：自動調整學習速率 (Learning Rate)</summary><br>

<a href="https://www.youtube.com/watch?v=HYUXEeh3kwY" target="_blank">
    <img src="https://img.youtube.com/vi/HYUXEeh3kwY/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】類神經網路訓練不起來怎麼辦 (三)：自動調整學習速率 (Learning Rate)


</details>

<details>
<summary>303. 【機器學習2021】自注意力機制 (Self-attention) (上)</summary><br>

<a href="https://www.youtube.com/watch?v=hYdO9CscNes" target="_blank">
    <img src="https://img.youtube.com/vi/hYdO9CscNes/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】自注意力機制 (Self-attention) (上)


</details>

<details>
<summary>304. 【機器學習2021】卷積神經網路 (Convolutional Neural Networks, CNN)</summary><br>

<a href="https://www.youtube.com/watch?v=OP5HcXJg2Aw" target="_blank">
    <img src="https://img.youtube.com/vi/OP5HcXJg2Aw/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】卷積神經網路 (Convolutional Neural Networks, CNN)


</details>

<details>
<summary>305. [ML 2021 (English version)] Lecture 3: Roadmap of Improving Model</summary><br>

<a href="https://www.youtube.com/watch?v=3qgKpBptyFY" target="_blank">
    <img src="https://img.youtube.com/vi/3qgKpBptyFY/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 3: Roadmap of Improving Model


</details>

<details>
<summary>306. [ML 2021 (English version)] Lecture 4: What to do when optimization fails? (1/4)</summary><br>

<a href="https://www.youtube.com/watch?v=yz7QS1I6omw" target="_blank">
    <img src="https://img.youtube.com/vi/yz7QS1I6omw/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 4: What to do when optimization fails? (1/4)


</details>

<details>
<summary>307. [ML 2021 (English version)] Lecture 5: What to do when optimization fails? (2/4)</summary><br>

<a href="https://www.youtube.com/watch?v=MNoEQ9w-AbE" target="_blank">
    <img src="https://img.youtube.com/vi/MNoEQ9w-AbE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 5: What to do when optimization fails? (2/4)


</details>

<details>
<summary>308. [ML 2021 (English version)] Lecture 8: Classification (Short Version)</summary><br>

<a href="https://www.youtube.com/watch?v=jqVONJ-Wn8w" target="_blank">
    <img src="https://img.youtube.com/vi/jqVONJ-Wn8w/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 8: Classification (Short Version)


</details>

<details>
<summary>309. [ML 2021 (English version)] Lecture 9: Convolutional Neural Networks</summary><br>

<a href="https://www.youtube.com/watch?v=I4eLIsPM9Yc" target="_blank">
    <img src="https://img.youtube.com/vi/I4eLIsPM9Yc/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 9: Convolutional Neural Networks


</details>

<details>
<summary>310. [ML 2021 (English version)] Lecture 10: Self-attention (1/2)</summary><br>

<a href="https://www.youtube.com/watch?v=0djMUi2-uV4" target="_blank">
    <img src="https://img.youtube.com/vi/0djMUi2-uV4/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 10: Self-attention (1/2)


</details>

<details>
<summary>311. 【機器學習2021】類神經網路訓練不起來怎麼辦 (五)： 批次標準化 (Batch Normalization) 簡介</summary><br>

<a href="https://www.youtube.com/watch?v=BABPWOkSbLE" target="_blank">
    <img src="https://img.youtube.com/vi/BABPWOkSbLE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】類神經網路訓練不起來怎麼辦 (五)： 批次標準化 (Batch Normalization) 簡介


</details>

<details>
<summary>312. 【機器學習2021】Transformer (上)</summary><br>

<a href="https://www.youtube.com/watch?v=n9TlOhRjYoc" target="_blank">
    <img src="https://img.youtube.com/vi/n9TlOhRjYoc/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】Transformer (上)


</details>

<details>
<summary>313. 【機器學習2021】自注意力機制 (Self-attention) (下)</summary><br>

<a href="https://www.youtube.com/watch?v=gmsMY5kc-zw" target="_blank">
    <img src="https://img.youtube.com/vi/gmsMY5kc-zw/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】自注意力機制 (Self-attention) (下)


</details>

<details>
<summary>314. 【機器學習2021】Transformer (下)</summary><br>

<a href="https://www.youtube.com/watch?v=N6aRv06iv2g" target="_blank">
    <img src="https://img.youtube.com/vi/N6aRv06iv2g/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】Transformer (下)


</details>

<details>
<summary>315. 【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (一) – 基本概念介紹</summary><br>

<a href="https://www.youtube.com/watch?v=4OWp0wDu6Xw" target="_blank">
    <img src="https://img.youtube.com/vi/4OWp0wDu6Xw/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (一) – 基本概念介紹


</details>

<details>
<summary>316. 【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (二) – 理論介紹與WGAN</summary><br>

<a href="https://www.youtube.com/watch?v=jNY1WBb8l4U" target="_blank">
    <img src="https://img.youtube.com/vi/jNY1WBb8l4U/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (二) – 理論介紹與WGAN


</details>

<details>
<summary>317. [ML 2021 (English version)] Lecture 6: What to do when optimization fails? (3/4)</summary><br>

<a href="https://www.youtube.com/watch?v=8yf-tU7zm7w" target="_blank">
    <img src="https://img.youtube.com/vi/8yf-tU7zm7w/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 6: What to do when optimization fails? (3/4)


</details>

<details>
<summary>318. [ML 2021 (English version)] Lecture 11: Self-attention (2/2)</summary><br>

<a href="https://www.youtube.com/watch?v=zeCDPYZli0k" target="_blank">
    <img src="https://img.youtube.com/vi/zeCDPYZli0k/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 11: Self-attention (2/2)


</details>

<details>
<summary>319. [ML 2021 (English version)] Lecture 7: What to do when optimization fails? (4/4)</summary><br>

<a href="https://www.youtube.com/watch?v=t3u3WshJQV8" target="_blank">
    <img src="https://img.youtube.com/vi/t3u3WshJQV8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 7: What to do when optimization fails? (4/4)


</details>

<details>
<summary>320. [ML 2021 (English version)] Lecture 12: Transformer (1/2)</summary><br>

<a href="https://www.youtube.com/watch?v=zmOuJkH9l9M" target="_blank">
    <img src="https://img.youtube.com/vi/zmOuJkH9l9M/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 12: Transformer (1/2)


</details>

<details>
<summary>321. [ML 2021 (English version)] Lecture 13: Transformer (2/2)</summary><br>

<a href="https://www.youtube.com/watch?v=fPTj5Zh1ACo" target="_blank">
    <img src="https://img.youtube.com/vi/fPTj5Zh1ACo/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 13: Transformer (2/2)


</details>

<details>
<summary>322. [ML 2021 (English version)] Lecture 14:  Generative Adversarial Network (GAN) (1/4)</summary><br>

<a href="https://www.youtube.com/watch?v=Mb9kddLfLRI" target="_blank">
    <img src="https://img.youtube.com/vi/Mb9kddLfLRI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 14:  Generative Adversarial Network (GAN) (1/4)


</details>

<details>
<summary>323. [ML 2021 (English version)] Lecture 15:  Generative Adversarial Network (GAN) (2/4)</summary><br>

<a href="https://www.youtube.com/watch?v=kFhv1I_fbZI" target="_blank">
    <img src="https://img.youtube.com/vi/kFhv1I_fbZI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 15:  Generative Adversarial Network (GAN) (2/4)


</details>

<details>
<summary>324. 【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (三) – 生成器效能評估與條件式生成</summary><br>

<a href="https://www.youtube.com/watch?v=MP0BnVH2yOo" target="_blank">
    <img src="https://img.youtube.com/vi/MP0BnVH2yOo/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (三) – 生成器效能評估與條件式生成


</details>

<details>
<summary>325. 【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (四) – Cycle GAN</summary><br>

<a href="https://www.youtube.com/watch?v=wulqhgnDr7E" target="_blank">
    <img src="https://img.youtube.com/vi/wulqhgnDr7E/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (四) – Cycle GAN


</details>

<details>
<summary>326. 【機器學習2021】自督導式學習 (Self-supervised Learning) (一) – 芝麻街與進擊的巨人</summary><br>

<a href="https://www.youtube.com/watch?v=e422eloJ0W4" target="_blank">
    <img src="https://img.youtube.com/vi/e422eloJ0W4/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】自督導式學習 (Self-supervised Learning) (一) – 芝麻街與進擊的巨人


</details>

<details>
<summary>327. 【機器學習2021】自督導式學習 (Self-supervised Learning) (二) – BERT簡介</summary><br>

<a href="https://www.youtube.com/watch?v=gh0hewYkjgo" target="_blank">
    <img src="https://img.youtube.com/vi/gh0hewYkjgo/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】自督導式學習 (Self-supervised Learning) (二) – BERT簡介


</details>

<details>
<summary>328. 【機器學習2021】自督導式學習 (Self-supervised Learning) (三) – BERT的奇聞軼事</summary><br>

<a href="https://www.youtube.com/watch?v=ExXA05i8DEQ" target="_blank">
    <img src="https://img.youtube.com/vi/ExXA05i8DEQ/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】自督導式學習 (Self-supervised Learning) (三) – BERT的奇聞軼事


</details>

<details>
<summary>329. [ML 2021 (English version)] Lecture 19:  Self-supervised Learning (aka Foundation Model) (2/3)</summary><br>

<a href="https://www.youtube.com/watch?v=L-ZQ-6vKOxU" target="_blank">
    <img src="https://img.youtube.com/vi/L-ZQ-6vKOxU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 19:  Self-supervised Learning (aka Foundation Model) (2/3)


</details>

<details>
<summary>330. [ML 2021 (English version)] Lecture 18:  Self-supervised Learning (aka Foundation Model) (1/3)</summary><br>

<a href="https://www.youtube.com/watch?v=mEcVirwmrkA" target="_blank">
    <img src="https://img.youtube.com/vi/mEcVirwmrkA/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 18:  Self-supervised Learning (aka Foundation Model) (1/3)


</details>

<details>
<summary>331. [ML 2021 (English version)] Lecture 17:  Generative Adversarial Network (GAN) (4/4)</summary><br>

<a href="https://www.youtube.com/watch?v=6xRAiKAYPxU" target="_blank">
    <img src="https://img.youtube.com/vi/6xRAiKAYPxU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 17:  Generative Adversarial Network (GAN) (4/4)


</details>

<details>
<summary>332. [ML 2021 (English version)] Lecture 16:  Generative Adversarial Network (GAN) (3/4)</summary><br>

<a href="https://www.youtube.com/watch?v=XcAmPtMQqS8" target="_blank">
    <img src="https://img.youtube.com/vi/XcAmPtMQqS8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 16:  Generative Adversarial Network (GAN) (3/4)


</details>

<details>
<summary>333. 【機器學習2021】自督導式學習 (Self-supervised Learning) (四) – GPT的野望</summary><br>

<a href="https://www.youtube.com/watch?v=WY_E0Sd4K80" target="_blank">
    <img src="https://img.youtube.com/vi/WY_E0Sd4K80/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】自督導式學習 (Self-supervised Learning) (四) – GPT的野望


</details>

<details>
<summary>334. 【機器學習2021】自編碼器 (Auto-encoder) (上) – 基本概念</summary><br>

<a href="https://www.youtube.com/watch?v=3oHlf8-J3Nc" target="_blank">
    <img src="https://img.youtube.com/vi/3oHlf8-J3Nc/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】自編碼器 (Auto-encoder) (上) – 基本概念


</details>

<details>
<summary>335. 【機器學習2021】來自人類的惡意攻擊 (Adversarial Attack) (上) – 基本概念</summary><br>

<a href="https://www.youtube.com/watch?v=xGQKhbjrFRk" target="_blank">
    <img src="https://img.youtube.com/vi/xGQKhbjrFRk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】來自人類的惡意攻擊 (Adversarial Attack) (上) – 基本概念


</details>

<details>
<summary>336. 【機器學習2021】自編碼器 (Auto-encoder) (下) – 領結變聲器與更多應用</summary><br>

<a href="https://www.youtube.com/watch?v=JZvEzb5PV3U" target="_blank">
    <img src="https://img.youtube.com/vi/JZvEzb5PV3U/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】自編碼器 (Auto-encoder) (下) – 領結變聲器與更多應用


</details>

<details>
<summary>337. [ML 2021 (English version)] Lecture 22:  Auto-encoder (2/2)</summary><br>

<a href="https://www.youtube.com/watch?v=PsBHWq9KKqk" target="_blank">
    <img src="https://img.youtube.com/vi/PsBHWq9KKqk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 22:  Auto-encoder (2/2)


</details>

<details>
<summary>338. [ML 2021 (English version)] Lecture 23:  Adversarial Attack (1/2)</summary><br>

<a href="https://www.youtube.com/watch?v=xw6K4naFWFg" target="_blank">
    <img src="https://img.youtube.com/vi/xw6K4naFWFg/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 23:  Adversarial Attack (1/2)


</details>

<details>
<summary>339. [ML 2021 (English version)] Lecture 21:  Auto-encoder (1/2)</summary><br>

<a href="https://www.youtube.com/watch?v=E7wlA85RxcI" target="_blank">
    <img src="https://img.youtube.com/vi/E7wlA85RxcI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 21:  Auto-encoder (1/2)


</details>

<details>
<summary>340. [ML 2021 (English version)] Lecture 20:  Self-supervised Learning (aka Foundation Model) (3/3)</summary><br>

<a href="https://www.youtube.com/watch?v=6sAf24QvJEY" target="_blank">
    <img src="https://img.youtube.com/vi/6sAf24QvJEY/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 20:  Self-supervised Learning (aka Foundation Model) (3/3)


</details>

<details>
<summary>341. 【機器學習2021】來自人類的惡意攻擊 (Adversarial Attack) (下) – 類神經網路能否躲過人類深不見底的惡意？</summary><br>

<a href="https://www.youtube.com/watch?v=z-Q9ia5H2Ig" target="_blank">
    <img src="https://img.youtube.com/vi/z-Q9ia5H2Ig/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】來自人類的惡意攻擊 (Adversarial Attack) (下) – 類神經網路能否躲過人類深不見底的惡意？


</details>

<details>
<summary>342. 【機器學習2021】機器學習模型的可解釋性 (Explainable ML) (上) – 為什麼類神經網路可以正確分辨寶可夢和數碼寶貝呢？</summary><br>

<a href="https://www.youtube.com/watch?v=WQY85vaQfTI" target="_blank">
    <img src="https://img.youtube.com/vi/WQY85vaQfTI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】機器學習模型的可解釋性 (Explainable ML) (上) – 為什麼類神經網路可以正確分辨寶可夢和數碼寶貝呢？


</details>

<details>
<summary>343. [ML 2021 (English version)] Lecture 24:  Adversarial Attack (2/2)</summary><br>

<a href="https://www.youtube.com/watch?v=kRmBiV2X810" target="_blank">
    <img src="https://img.youtube.com/vi/kRmBiV2X810/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 24:  Adversarial Attack (2/2)


</details>

<details>
<summary>344. [2021-05-13] [ML 2021 (English version)] Lecture 25: Explainable ML (1/2)</summary><br>

<a href="https://www.youtube.com/watch?v=4rVD1EOaAX4" target="_blank">
    <img src="https://img.youtube.com/vi/4rVD1EOaAX4/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 25: Explainable ML (1/2)

### 文章重點整理

#### 核心主題
本文圍繞深度學習模型在聲音處理和信息提取方面的應用展開討論，探討了模型如何進行語音識別、聲紋消除以及噪音過濾等操作。文章強調通過後 처리模型（如TTS）來分析前處理模型的特性，從而揭示模型在特徵提取過程中的行為。

#### 主要觀念
1. **語音特徵提取**：深度學習模型（如Bi-Directional LSTM和CNN）能夠有效地提取語音內容的特徵，同時可能消除或改變某些非-content特徵（如說話人的聲音特徵）。
2. **聲紋消除**：文中通過示例展示了模型在多層處理後，能夠將不同說話人的語音轉化為相似的聲音，這表明模型可能在訓練過程中有意無意地去除了聲紋信息。
3. **噪音過濾**：研究表明，LSTM層在某些情況下可以有效去除信號中的噪音（如鋼琴噪音），而CNN層在此案例中未顯著發揮噪音過濾作用。

#### 問題原因
1. **信息損失**：深度學習模型在提取特徵時，可能不可避免地丟失了一些重要的非-content信息，如說話人的聲紋特徵。
2. **模型特性揭示不足**： traditional visualization和分析方法（如Hinton的 visulization）雖然有用，但缺乏交互性和深入的行為揭示。

#### 解決方法
1. **後處理模型分析**：通過使用TTS模型對前處理模型的特徵進行重建，可以有效分析前處理模型的信息保留情況，進而理解其特性。
2. **多層網絡分析**：通過分層分析（如CNN和LSTM的不同深度），能夠更好地理解每個層在信號處理中的作用。

#### 優化方式
1. **模型結構改進**：根據分析結果，可以對模型結構進行優化，例如強化某些層的噪音過濾能力或聲紋保留功能。
2. **特徵保留策略**：在模型設計階段，可以有意識地設置機制來保留或去除特定類型的信息（如選擇性地保留聲紋特徵）。

#### 結論
1. 深度學習模型在聲音信息處理方面展現出強大的能力，但其特性揭示和行為分析仍具挑戰性。
2. 使用後處理模型進行反向分析是一種有效的研究方法，能夠幫助理解前處理模型的特徵提取機制。
3. 分層網絡分析對於揭示各級層的信息處理特徵具有重要意義，未來的研究可以進一步探索如何通過這種方式來優化模型性能。
</details>

<details>
<summary>345. [2021-05-21] 【機器學習2021】概述領域自適應 (Domain Adaptation)</summary><br>

<a href="https://www.youtube.com/watch?v=Mnk_oUrgppM" target="_blank">
    <img src="https://img.youtube.com/vi/Mnk_oUrgppM/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】概述領域自適應 (Domain Adaptation)

### 文章重點整理

#### 1. 核心主題
- **Domain Adaptation**：研究如何在數據分布不同的情況下，遷移學習模型以適應目標域（Target Domain）的任務。
- **Domain Generalization**：探討模型如何泛化到未知的多個領域。

#### 2. 主要觀念
- **數據分布差異性**：源域和目標域之間可能存在顯著的數據分布差異，導致模型在目標域上的性能下降。
- **無標籤數據處理**：在目標域缺乏標註的情況下，需要設計有效的遷移學習策略。
- **小樣本挑戰**：當目標域數據量極小時，傳統的遷移方法可能失效。

#### 3. 問題原因
- **領域差異**：源域和目標域之間的特徵分布、統計特性或標籤空間存在差異，導致模型無法直接適應。
- **標註稀缺性**：在目標域中獲取標註數據困難，限制了監督學習的效果。
- **小樣本問題**：極少量的目標域數據使得難以訓練有效的遷移模型。

#### 4. 解決方法
##### (1) 基於對抗訓練的Domain Adaptation
- **對抗網絡**：通過設計對抗網絡（GANs）來最小化源域和目標域之間的分布差異。
- **領域對齊**：利用判別器迫使生成器學習跨領域的共享特徵，實現特徵空間的對齊。

##### (2) 測試時間訓練（Testing Time Training, TTT）
- **小樣本處理**：針對目標域數據極少的情況，採用測試時微調的方法，通過少量樣本快速調整模型參數。
- **增量學習**：利用目標域的小樣本數據，在測試階段進行在線學習，提升模型適應性。

##### (3) 域泛化（Domain Generalization）
- **多領域訓練集**：構建包含多個領域的訓練數據集，使模型在訓練時自然學習到跨領域的特徵。
- **數據增強**：通過模擬不同域的數據分布，生成虛擬的多領域樣本，豐富訓練數據。

##### (4) 參數調節與模型優化
- **權重調整**：在遷移過程中，平衡領域對齊和任務損失的權重，避免過度適應判別器。
- **正則化技術**：引入適當的正則化方法，保持模型在不同域之間的泛化能力。

#### 5. 確保模型性能的關鍵因素
- **特徵空間設計**：確保學習到的特徵能夠有效區分類別，同時保持跨域一致性。
- **領域對齊策略**：採用多種對齊方法（如對抗訓練、統計距離最小化）優化特徵分布。
- **泛化能力驗證**：通過在多個未知領域的測試驗證模型的泛化性能。

#### 6. 研究展望
- **複雜領域適應**：探索更高效的算法，應對更加複雜的領域差異和數據稀疏性問題。
- **多任務學習結合**：研究如何將域適應與多任務學習相結合，提升模型的整體表現。
- **實時應用優化**：針對實時應用場景，設計輕量級的遷移學習解決方案。

#### 7. 結論
- **技術可行性**：通過對抗訓練和測試時間微調等方法，能夠在一定程度上解決領域適配問題。
- **研究必要性**：域適應和泛化技術對於實際應用中的模型部署至關重要，特別是在標註數據稀缺或領域分布多變的情況下。
</details>

<details>
<summary>346. [2021-05-21] 【機器學習2021】機器學習模型的可解釋性 (Explainable ML) (下) –機器心中的貓長什麼樣子？</summary><br>

<a href="https://www.youtube.com/watch?v=0ayIPqbdHYQ" target="_blank">
    <img src="https://img.youtube.com/vi/0ayIPqbdHYQ/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】機器學習模型的可解釋性 (Explainable ML) (下) –機器心中的貓長什麼樣子？

### 小節一：核心主題  
- 本文圍繞**可解釋機器學習（Explainable Machine Learning）**展開討論，探討如何提高模型決策過程的透明性和可解釋性。  

### 小節二：主要觀念  
1. **機器學習模型的黑箱問題**：  
   - 深度學習模型（如神經網絡）因其複雜性而被視為「黑箱」，其決策過程不易被人理解。  
2. **可解釋性的重要性**：  
   - 可解釋性在模型的信任、驗證和改進中具有關鍵作用，尤其是在醫學、金融等高風險領域。  
3. **提升可解釋性的方法**：  
   - 使用簡單的模型（如線性模型）或對黑箱模型進行後處理解讀。  

### 小節三：問題原因  
- 神經網絡等複雜模型的決策機制缺乏透明度，導致用戶難以信任和驗證模型的行為。  
- 模型易受敵對攻擊（Adversarial Attacks）影響，進一步暴露其脆弱性。  

### 小節四：解決方法  
1. **後處理解讀技術**：  
   - 使用簡單的模型（如LASSO、 Ridge Regression）對黑箱模型的決策進行局部解讀。  
2. **局部可解釋性模型**：  
   - 引入**LIME（Local Interpretable Model-agnostic Explanations）**等方法，通過在小數據範圍內訓練線性模型來模擬黑箱模型的行為，從而提供局部解讀。  
3. **增設額外類別**：  
   - 在分類器中增加「都不是」的選項，並最小化其機率，以降低模型錯誤分類的風險。  

### 小節五：優化方式  
1. **整合多種解釋方法**：  
   - 結合全局和局部解讀技術，提供更全面的模型行為分析。  
2. **提升模型 robustness**：  
   - 遷移學習、數據增強等技術可提高模型對敵對攻擊的抵抗能力。  

### 小節六：結論  
- 可解釋性是機器學習模型可信度的重要指標，需通過多種技術手段（如LIME）來提升模型的透明性與用戶信任。  
- 增設額外類別並最小化其機率可在一定程度上降低錯誤分類風險，但仍需進一步實驗驗證其效果。
</details>

<details>
<summary>347. [2021-05-22] 【機器學習2021】概述增強式學習 (Reinforcement Learning, RL) (一) – 增強式學習跟機器學習一樣都是三個步驟</summary><br>

<a href="https://www.youtube.com/watch?v=XWukX-ayIrs" target="_blank">
    <img src="https://img.youtube.com/vi/XWukX-ayIrs/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】概述增強式學習 (Reinforcement Learning, RL) (一) – 增強式學習跟機器學習一樣都是三個步驟

### 文章重點整理

#### 核心主題
本文主要探討如何訓練一個.Actor*（代理）來執行符合特定期望的行為，通過定義適當的損失函數和訓練數據來實現此目標。

---

#### 主要觀念

1. **Actor 的訓練模式**  
   - Actor 的訓練類似於監督學習（Supervised Learning），其中訓練數據包含狀態（State, s）和對應動作（Action, a）的配對。
   - 每個動作可以被分為兩種類型：
     - **期望執行的動作**：用 +1 表示。
     - **不期望執行的動作**：用 -1 表示。

2. **訓練數據的定義**  
   - 訓練數據由一系列（s, a）配對組成，其中 s 是環境中的狀態，a 是在該狀態下期望Actor採取的行動。
   - 每個動作可以具有不同的權重（如 +1.5 或 +0.5），表示不同程度的執行期望。

3. **損失函數的定義**  
   - 使用交叉熵損失（Cross-Entropy Loss）來衡量.Actor*的預測行動與實際期望行動之間的差異。
   - 損失函數的形式為：
     \[
     L = -\sum_{i=1}^{N} w_i \cdot y_i \cdot \log(\hat{y}_i) + (1 - y_i) \cdot \log(1 - \hat{y}_i)
     \]
     其中，\(w_i\) 表示第 i 筆數據的權重，\(y_i\) 是期望行動（+1 或 -1），\(\hat{y}_i\) 是.Actor*預測的概率。

---

#### 問題原因

1. **訓練數據的來源**  
   - 如何有效收集和定義訓練數據，以確保Actor在各種狀態下能夠執行正確的行動。
   - 經驗表明，直接定義訓練數據可能具有挑戰性，特別是對於複雜環境。

2. **損失函數的優化**  
   - 損失函數需要能夠反映不同程度的行動期望，以便Actor在訓練過程中學會區分重要和次要的行動。

---

#### 解決方法

1. **訓練數據的生成**  
   - 通過專家示範或模擬環境來收集training data。
   - 確定每個狀態下Actor應該執行的行動，並為其分配適當的權重。

2. **損失函數的設計**  
   - 在交叉熵損失中引入權重因子 \(w_i\)，以反映不同行動的重要程度。
   - 通過反向傳播（Backpropagation）來優化.Actor*的參數θ，最小化損失函數。

---

#### 優化方式

1. **動機設計**  
   - 給予Actor執行某些行動更高的權重，以強調這些行動的重要性。
   - 例如，在特定狀態下，Actor應該避免某個有害行動，可以給予該行動的權重為-2，以增強訓練效果。

2. **分級期望**  
   - 根據不同行動的影響程度，將其分級（如高、中、低），並在損失函數中體現這些差異。
   - 這可以幫助.Actor*在訓練過程中更精準地掌握各類行動的執行策略。

---

#### 結論

本文提出了一種基於監督學習的.Actor*訓練方法，通過定義具體的訓練數據和損失函數，使得Actor能夠學習到符合期望的行為。核心思想是將行動期望轉化為交叉熵損失中的權重因子，並通過反向傳播優化.Actor*的參數。此方法為Actor在複雜環境下的行為控制提供了一種可行的途徑，但仍需進一步研究如何高效收集和定義訓練數據，以提高訓練效果。
</details>

<details>
<summary>348. [2021-05-22] 【機器學習2021】概述增強式學習 (Reinforcement Learning, RL) (二) – Policy Gradient 與修課心情</summary><br>

<a href="https://www.youtube.com/watch?v=US8DFaAZcp4" target="_blank">
    <img src="https://img.youtube.com/vi/US8DFaAZcp4/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】概述增強式學習 (Reinforcement Learning, RL) (二) – Policy Gradient 與修課心情

### 一、核心主題：強化學習（Reinforcement Learning）中的策略優化方法

1. **主要焦點**：
   - 探討在強化學習中如何通過策略優化方法提升智能體的行爲表現。
   - 強調策略優化的核心思想及其在實際應用中的重要性。

2. **核心概念**：
   - 策略（Policy）：定義了智能體在給定狀態下的行爲選擇規則。
   - 獎勵信號（Reward Signal）：用於評估智能體行爲的優劣，指導學習過程。
   - 探索與利用（Exploration vs. Exploitation）：平衡新舊行爲選擇以確保有效學習。

### 二、主要觀念：策略優化的基本原理

1. **基本原理**：
   - 策略優化的目標是通過迭代更新策略，使得智能體在環境中獲得的累積獎勵最大化。
   - 每次迭代中，智能體會與環境交互，收集經驗並根據反饋調整行爲。

2. **關鍵步驟**：
   - 行爲選擇：基於當前策略生成動作。
   - 反饋接收：從環境中獲取獎勵信號。
   - 策略更新：利用反饋優化策略參數。

### 三、問題原因：傳統策略優化方法的局限性

1. **主要挑戰**：
   - **探索不足**：初始策略可能過於固定，缺乏多樣性，導致無法充分探索狀態空間。
   - **梯度估算偏差**：直接優化策略可能導致梯度估計不穩定或偏離最優解。
   - **樣本效率低下**：傳統方法需要大量交互才能收斂，限制了在複雜環境中的應用。

2. **具體問題**：
   - 策略更新過程中可能陷入局部最優。
   - 高維狀態空間和動作空間增加了學習難度。
   - 動作選擇的隨機性不足可能導致智能體無法發現最佳行爲路徑。

### 四、解決方法：PPO（Proximal Policy Optimization）算法

1. **基本思想**：
   - 通過限制策略更新的幅度，確保每次迭代中策略的變化量在合理範圍內。
   - 分離舊策略和新策略的概率分布，利用比值估計法優化目標函數。

2. **具體實現**：
   - 引入兩個概率比值（old policy 和 new policy）來計算損失函數。
   - 通過限制策略更新的幅度（clip parameter）防止參數更新過大導致性能下降。
   - 利用信任區域方法確保每次更新穩定可靠。

3. **優點**：
   - 算法穩定性高，收斂速度快。
   - 具有良好的樣本效率，在複雜環境中表現優異。
   - 易於實現且具有較強的理論基礎支持。

### 五、優化方式：提升策略探索的有效性

1. **增加動作隨機性**：
   - 在策略輸出中引入熵正則化（Entropy Regularization），鼓勵智能體嘗試更多動作，避免過早收斂。
   - 在策略參數更新過程中添加噪聲，增強探索能力。

2. **經驗重放（Experience Replay）**：
   - 通過回放歷史經驗，增加訓練數據的多樣性，提升學習效率。

3. **多-threaded exploration**：
   - 利用多線程或多進程同時進行環境交互和策略優化，加速學習過程。

### 六、結論與未來方向

1. **研究結論**：
   - PPO算法在強化學習領域展現了顯著優勢，特別是在複雜任務中表現優異。
   - 策略優化的關鍵在於平衡探索與利用，有效提升智能體的學習效率和性能。

2. **未來方向**：
   - 探索更高效的梯度估算方法，進一步提高樣本利用率。
   - 研究更加魯棒的策略更新機制，增強算法在不同環境中的適應性。
   - 結合其他機器學習技術（如深度神經網絡、圖結構學習等），拓展強化學習的應用範圍。

3. **實際應用**：
   - 在機器人控制、遊戲AI、自動駕駛等領域具有廣泛前景。
   - 通過不斷優化策略優化方法，推動人工智能技術的進一步發展。
</details>

<details>
<summary>349. [2021-05-26] [ML 2021 (English version)] Lecture 26: Explainable ML (2/2)</summary><br>

<a href="https://www.youtube.com/watch?v=5VPy7OGlGMQ" target="_blank">
    <img src="https://img.youtube.com/vi/5VPy7OGlGMQ/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 26: Explainable ML (2/2)

### 核心主題
- **解釋性機器學習**：探討如何使機器學習模型更加透明和易於理解。

---

### 主要觀念
1. **可解釋性的重要性**：
   - 機器學習模型的決策過程需要被人類理解和信任。
   - 特別是在醫療、金融等高風險領域，可解釋性至關重要。

2. **黑箱問題**：
   - 現代深度學習模型（如神經網路）通常被視為「黑箱」，其內部運作機制不易理解。

3. **對抗攻擊與可靠性**：
   - 機器學習模型可能受到對抗攻擊，導致錯誤的決策。
   - 提高可解釋性有助於增強模型的可靠性和抗幹預能力。

---

### 問題原因
1. **模型複雜性**：
   - 神經網路等深度 learning模型結構複雜，導致其運作機制不易解讀。

2. **缺乏透明度**：
   - 黑箱模型的決策缺乏明確的解釋，影響用戶的信任和應用。

3. **對抗攻擊的挑戰**：
   - 模型可能被設計外的輸入操縱，導致不可預測的行為。

---

### 解決方法
1. **局部可解釋性方法**：
   - 使用如 LIME（Local Interpretable Model-Agnostic Explanations）等技術，僅解析模型在特定區域的行爲。
   - 通過簡單的線性模型模擬小範圍的黑箱行為來提供解釋。

2. ** adversarial attacks 的防禦**：
   - 增加額外的分類選項（如「非上述任何一類」），以降低模型錯誤分類的可能性。
   - 通過數據增強和正則化技術提高模型魯棒性。

3. **模型簡化與可視化**：
   - 使用線性模型或其他簡單模型來近似黑箱模型的行為。
   - 通過可視化工具展示模型決策的關鍵特徵。

---

### 優化方式
1. **多分類策略**：
   - 添加額外的分類選項（如「非上述任何一類」），以提高模型的可靠性和解釋性。

2. **混合模型設計**：
   - 結合深度學習模型和簡單模型，利用深度模型的強大能力和簡單模型的可解讀性。

3. **持續研究與實驗**：
   - 從事更多的實驗來驗證不同方法的效果，進一步優化解釋性機器學習技術。

---

### 總結
- 可解釋性是機器學習模型應用的重要條件。
- 地區性的可解釋性方法（如 LIME）和防禦對抗攻擊的策略（如添加額外分類選項）提供了有效的解決方案。
- 未來的研究需進一步探索如何平衡模型的複雜性和可解釋性，以實現更可靠的機器學習系統。
</details>

<details>
<summary>350. [2021-05-27] [ML 2021 (English version)] Lecture 28: Introduction of Reinforcement Learning (RL) (1/5)</summary><br>

<a href="https://www.youtube.com/watch?v=0oucZNfBXlI" target="_blank">
    <img src="https://img.youtube.com/vi/0oucZNfBXlI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 28: Introduction of Reinforcement Learning (RL) (1/5)

### 文章整理：基於監督學習的Actor訓練框架

#### 核心主題
- **Actor訓練的本質**：通過定義明確的行爲期望，利用監督學習方法訓練一個能夠執行預期動作的智能體（Actor）。

#### 主要觀念
1. **Actor的訓練目標**：
   - 使Actor在特定狀態下執行期望的動作。
   - 區分「應該執行」的動作和「不應該執行」的動作。

2. **行爲數據的表示**：
   - 每對狀態-動作（s, a）配對有一個評分（An），表示對該動作的期望程度。
   - 評分可以是正數（期望執行）、負數（不期望執行）或零（中立態度）。

3. **監督學習框架的應用**：
   - 將Actor訓練類比爲分類任務，狀態s被視爲輸入，動作a被視爲輸出標籤。
   - 使用交叉熵損失函數衡量預測與真實動作的差異。

4. **行爲評分的作用**：
   - 通過An調節不同狀態-動作對的權重，實現差異化的行爲管理。
   - 高正數評分表示強烈期望執行，低或負評分表示較低或完全不期望執行。

#### 問題原因
1. **數據來源的挑戰**：
   - 如何生成大量高質量的狀態-動作配對（s, a）作爲訓練數據？
   - 數據的質量直接影響Actor的學習效果。

2. **行爲評分的主觀性**：
   - 評分An需要明確定義，不同場景下的人爲主觀判斷可能影響模型性能。

3. **動態環境適應性**：
   - 在實際應用中，環境可能是動態變化的，固定訓練數據可能導致Actor在新環境中表現不佳。

#### 解決方法
1. **數據收集策略**：
   - 通過專家示範、模擬器生成或人機交互等方式收集狀態-動作配對。
   - 確保數據覆蓋不同狀態和動作的多樣性。

2. **評分機制的設計**：
   - 根據任務需求設計評分規則，明確每個狀態-動作對的期望程度。
   - 使用領域知識或專家意見來制定評分標準。

3. **模型優化與調整**：
   - 採用合適的監督學習算法（如隨機森林、神經網絡等）訓練Actor。
   - 調整模型參數以優化損失函數，提升預測準確性。

4. **動態適應機制**：
   - 引入強化學習或其他自適應方法，使Actor能夠根據環境反饋調整行爲。
   - 定期更新訓練數據和模型，以應對環境變化。

#### 結論
- 通過明確的行爲期望定義和監督學習框架，可以有效地訓練一個Actor來執行預期動作。
- 行爲評分機制提供了靈活的控制手段，可以根據具體需求調節不同狀態-動作對的重要性。
- 數據收集和評分機制的設計是Actor訓練的關鍵挑戰，需要結合領域知識和實際應用需求進行優化。
</details>

