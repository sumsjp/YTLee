<details>
<summary>301. 【機器學習2021】類神經網路訓練不起來怎麼辦 (四)：損失函數 (Loss) 也可能有影響</summary><br>

<a href="https://www.youtube.com/watch?v=O2VkP8dJ5FE" target="_blank">
    <img src="https://img.youtube.com/vi/O2VkP8dJ5FE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】類神經網路訓練不起來怎麼辦 (四)：損失函數 (Loss) 也可能有影響


</details>

<details>
<summary>302. 【機器學習2021】類神經網路訓練不起來怎麼辦 (三)：自動調整學習速率 (Learning Rate)</summary><br>

<a href="https://www.youtube.com/watch?v=HYUXEeh3kwY" target="_blank">
    <img src="https://img.youtube.com/vi/HYUXEeh3kwY/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】類神經網路訓練不起來怎麼辦 (三)：自動調整學習速率 (Learning Rate)


</details>

<details>
<summary>303. 【機器學習2021】自注意力機制 (Self-attention) (上)</summary><br>

<a href="https://www.youtube.com/watch?v=hYdO9CscNes" target="_blank">
    <img src="https://img.youtube.com/vi/hYdO9CscNes/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】自注意力機制 (Self-attention) (上)


</details>

<details>
<summary>304. 【機器學習2021】卷積神經網路 (Convolutional Neural Networks, CNN)</summary><br>

<a href="https://www.youtube.com/watch?v=OP5HcXJg2Aw" target="_blank">
    <img src="https://img.youtube.com/vi/OP5HcXJg2Aw/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】卷積神經網路 (Convolutional Neural Networks, CNN)


</details>

<details>
<summary>305. [ML 2021 (English version)] Lecture 3: Roadmap of Improving Model</summary><br>

<a href="https://www.youtube.com/watch?v=3qgKpBptyFY" target="_blank">
    <img src="https://img.youtube.com/vi/3qgKpBptyFY/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 3: Roadmap of Improving Model


</details>

<details>
<summary>306. [ML 2021 (English version)] Lecture 4: What to do when optimization fails? (1/4)</summary><br>

<a href="https://www.youtube.com/watch?v=yz7QS1I6omw" target="_blank">
    <img src="https://img.youtube.com/vi/yz7QS1I6omw/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 4: What to do when optimization fails? (1/4)


</details>

<details>
<summary>307. [ML 2021 (English version)] Lecture 5: What to do when optimization fails? (2/4)</summary><br>

<a href="https://www.youtube.com/watch?v=MNoEQ9w-AbE" target="_blank">
    <img src="https://img.youtube.com/vi/MNoEQ9w-AbE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 5: What to do when optimization fails? (2/4)


</details>

<details>
<summary>308. [ML 2021 (English version)] Lecture 8: Classification (Short Version)</summary><br>

<a href="https://www.youtube.com/watch?v=jqVONJ-Wn8w" target="_blank">
    <img src="https://img.youtube.com/vi/jqVONJ-Wn8w/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 8: Classification (Short Version)


</details>

<details>
<summary>309. [ML 2021 (English version)] Lecture 9: Convolutional Neural Networks</summary><br>

<a href="https://www.youtube.com/watch?v=I4eLIsPM9Yc" target="_blank">
    <img src="https://img.youtube.com/vi/I4eLIsPM9Yc/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 9: Convolutional Neural Networks


</details>

<details>
<summary>310. [ML 2021 (English version)] Lecture 10: Self-attention (1/2)</summary><br>

<a href="https://www.youtube.com/watch?v=0djMUi2-uV4" target="_blank">
    <img src="https://img.youtube.com/vi/0djMUi2-uV4/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 10: Self-attention (1/2)


</details>

<details>
<summary>311. 【機器學習2021】類神經網路訓練不起來怎麼辦 (五)： 批次標準化 (Batch Normalization) 簡介</summary><br>

<a href="https://www.youtube.com/watch?v=BABPWOkSbLE" target="_blank">
    <img src="https://img.youtube.com/vi/BABPWOkSbLE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】類神經網路訓練不起來怎麼辦 (五)： 批次標準化 (Batch Normalization) 簡介


</details>

<details>
<summary>312. 【機器學習2021】Transformer (上)</summary><br>

<a href="https://www.youtube.com/watch?v=n9TlOhRjYoc" target="_blank">
    <img src="https://img.youtube.com/vi/n9TlOhRjYoc/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】Transformer (上)


</details>

<details>
<summary>313. 【機器學習2021】自注意力機制 (Self-attention) (下)</summary><br>

<a href="https://www.youtube.com/watch?v=gmsMY5kc-zw" target="_blank">
    <img src="https://img.youtube.com/vi/gmsMY5kc-zw/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】自注意力機制 (Self-attention) (下)


</details>

<details>
<summary>314. 【機器學習2021】Transformer (下)</summary><br>

<a href="https://www.youtube.com/watch?v=N6aRv06iv2g" target="_blank">
    <img src="https://img.youtube.com/vi/N6aRv06iv2g/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】Transformer (下)


</details>

<details>
<summary>315. 【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (一) – 基本概念介紹</summary><br>

<a href="https://www.youtube.com/watch?v=4OWp0wDu6Xw" target="_blank">
    <img src="https://img.youtube.com/vi/4OWp0wDu6Xw/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (一) – 基本概念介紹


</details>

<details>
<summary>316. 【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (二) – 理論介紹與WGAN</summary><br>

<a href="https://www.youtube.com/watch?v=jNY1WBb8l4U" target="_blank">
    <img src="https://img.youtube.com/vi/jNY1WBb8l4U/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (二) – 理論介紹與WGAN


</details>

<details>
<summary>317. [ML 2021 (English version)] Lecture 6: What to do when optimization fails? (3/4)</summary><br>

<a href="https://www.youtube.com/watch?v=8yf-tU7zm7w" target="_blank">
    <img src="https://img.youtube.com/vi/8yf-tU7zm7w/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 6: What to do when optimization fails? (3/4)


</details>

<details>
<summary>318. [ML 2021 (English version)] Lecture 11: Self-attention (2/2)</summary><br>

<a href="https://www.youtube.com/watch?v=zeCDPYZli0k" target="_blank">
    <img src="https://img.youtube.com/vi/zeCDPYZli0k/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 11: Self-attention (2/2)


</details>

<details>
<summary>319. [ML 2021 (English version)] Lecture 7: What to do when optimization fails? (4/4)</summary><br>

<a href="https://www.youtube.com/watch?v=t3u3WshJQV8" target="_blank">
    <img src="https://img.youtube.com/vi/t3u3WshJQV8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 7: What to do when optimization fails? (4/4)


</details>

<details>
<summary>320. [ML 2021 (English version)] Lecture 12: Transformer (1/2)</summary><br>

<a href="https://www.youtube.com/watch?v=zmOuJkH9l9M" target="_blank">
    <img src="https://img.youtube.com/vi/zmOuJkH9l9M/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 12: Transformer (1/2)


</details>

<details>
<summary>321. [ML 2021 (English version)] Lecture 13: Transformer (2/2)</summary><br>

<a href="https://www.youtube.com/watch?v=fPTj5Zh1ACo" target="_blank">
    <img src="https://img.youtube.com/vi/fPTj5Zh1ACo/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 13: Transformer (2/2)


</details>

<details>
<summary>322. [ML 2021 (English version)] Lecture 14:  Generative Adversarial Network (GAN) (1/4)</summary><br>

<a href="https://www.youtube.com/watch?v=Mb9kddLfLRI" target="_blank">
    <img src="https://img.youtube.com/vi/Mb9kddLfLRI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 14:  Generative Adversarial Network (GAN) (1/4)


</details>

<details>
<summary>323. [ML 2021 (English version)] Lecture 15:  Generative Adversarial Network (GAN) (2/4)</summary><br>

<a href="https://www.youtube.com/watch?v=kFhv1I_fbZI" target="_blank">
    <img src="https://img.youtube.com/vi/kFhv1I_fbZI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 15:  Generative Adversarial Network (GAN) (2/4)


</details>

<details>
<summary>324. 【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (三) – 生成器效能評估與條件式生成</summary><br>

<a href="https://www.youtube.com/watch?v=MP0BnVH2yOo" target="_blank">
    <img src="https://img.youtube.com/vi/MP0BnVH2yOo/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (三) – 生成器效能評估與條件式生成


</details>

<details>
<summary>325. 【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (四) – Cycle GAN</summary><br>

<a href="https://www.youtube.com/watch?v=wulqhgnDr7E" target="_blank">
    <img src="https://img.youtube.com/vi/wulqhgnDr7E/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (四) – Cycle GAN


</details>

<details>
<summary>326. 【機器學習2021】自督導式學習 (Self-supervised Learning) (一) – 芝麻街與進擊的巨人</summary><br>

<a href="https://www.youtube.com/watch?v=e422eloJ0W4" target="_blank">
    <img src="https://img.youtube.com/vi/e422eloJ0W4/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】自督導式學習 (Self-supervised Learning) (一) – 芝麻街與進擊的巨人


</details>

<details>
<summary>327. 【機器學習2021】自督導式學習 (Self-supervised Learning) (二) – BERT簡介</summary><br>

<a href="https://www.youtube.com/watch?v=gh0hewYkjgo" target="_blank">
    <img src="https://img.youtube.com/vi/gh0hewYkjgo/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】自督導式學習 (Self-supervised Learning) (二) – BERT簡介


</details>

<details>
<summary>328. 【機器學習2021】自督導式學習 (Self-supervised Learning) (三) – BERT的奇聞軼事</summary><br>

<a href="https://www.youtube.com/watch?v=ExXA05i8DEQ" target="_blank">
    <img src="https://img.youtube.com/vi/ExXA05i8DEQ/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】自督導式學習 (Self-supervised Learning) (三) – BERT的奇聞軼事


</details>

<details>
<summary>329. [ML 2021 (English version)] Lecture 19:  Self-supervised Learning (aka Foundation Model) (2/3)</summary><br>

<a href="https://www.youtube.com/watch?v=L-ZQ-6vKOxU" target="_blank">
    <img src="https://img.youtube.com/vi/L-ZQ-6vKOxU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 19:  Self-supervised Learning (aka Foundation Model) (2/3)


</details>

<details>
<summary>330. [ML 2021 (English version)] Lecture 18:  Self-supervised Learning (aka Foundation Model) (1/3)</summary><br>

<a href="https://www.youtube.com/watch?v=mEcVirwmrkA" target="_blank">
    <img src="https://img.youtube.com/vi/mEcVirwmrkA/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 18:  Self-supervised Learning (aka Foundation Model) (1/3)


</details>

<details>
<summary>331. [ML 2021 (English version)] Lecture 17:  Generative Adversarial Network (GAN) (4/4)</summary><br>

<a href="https://www.youtube.com/watch?v=6xRAiKAYPxU" target="_blank">
    <img src="https://img.youtube.com/vi/6xRAiKAYPxU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 17:  Generative Adversarial Network (GAN) (4/4)


</details>

<details>
<summary>332. [ML 2021 (English version)] Lecture 16:  Generative Adversarial Network (GAN) (3/4)</summary><br>

<a href="https://www.youtube.com/watch?v=XcAmPtMQqS8" target="_blank">
    <img src="https://img.youtube.com/vi/XcAmPtMQqS8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 16:  Generative Adversarial Network (GAN) (3/4)


</details>

<details>
<summary>333. 【機器學習2021】自督導式學習 (Self-supervised Learning) (四) – GPT的野望</summary><br>

<a href="https://www.youtube.com/watch?v=WY_E0Sd4K80" target="_blank">
    <img src="https://img.youtube.com/vi/WY_E0Sd4K80/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】自督導式學習 (Self-supervised Learning) (四) – GPT的野望


</details>

<details>
<summary>334. 【機器學習2021】自編碼器 (Auto-encoder) (上) – 基本概念</summary><br>

<a href="https://www.youtube.com/watch?v=3oHlf8-J3Nc" target="_blank">
    <img src="https://img.youtube.com/vi/3oHlf8-J3Nc/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】自編碼器 (Auto-encoder) (上) – 基本概念


</details>

<details>
<summary>335. 【機器學習2021】來自人類的惡意攻擊 (Adversarial Attack) (上) – 基本概念</summary><br>

<a href="https://www.youtube.com/watch?v=xGQKhbjrFRk" target="_blank">
    <img src="https://img.youtube.com/vi/xGQKhbjrFRk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】來自人類的惡意攻擊 (Adversarial Attack) (上) – 基本概念


</details>

<details>
<summary>336. 【機器學習2021】自編碼器 (Auto-encoder) (下) – 領結變聲器與更多應用</summary><br>

<a href="https://www.youtube.com/watch?v=JZvEzb5PV3U" target="_blank">
    <img src="https://img.youtube.com/vi/JZvEzb5PV3U/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】自編碼器 (Auto-encoder) (下) – 領結變聲器與更多應用


</details>

<details>
<summary>337. [ML 2021 (English version)] Lecture 22:  Auto-encoder (2/2)</summary><br>

<a href="https://www.youtube.com/watch?v=PsBHWq9KKqk" target="_blank">
    <img src="https://img.youtube.com/vi/PsBHWq9KKqk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 22:  Auto-encoder (2/2)


</details>

<details>
<summary>338. [2021-05-07] [ML 2021 (English version)] Lecture 23:  Adversarial Attack (1/2)</summary><br>

<a href="https://www.youtube.com/watch?v=xw6K4naFWFg" target="_blank">
    <img src="https://img.youtube.com/vi/xw6K4naFWFg/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 23:  Adversarial Attack (1/2)

### 英文標題  
A Comprehensive Overview of Adversarial Attack Methods in Deep Learning  

### 中文摘要  
本文系統性地介紹了深度學習中對抗攻擊的核心主題、主要觀念、問題原因、解決方法及優化方式，並總結了相關結論。文章重點圍繞.gradient descent method和.fast gradient sign method (FGSM) 展開，探討了單次攻擊與多次迭代攻擊的差異及其在不同基線模型中的表現。

---

### 重點整理  

#### 核心主題  
1. **對抗攻擊在深度學習中的應用**：探討通過擾動資料來 fool 模型的可能性與方法。  
2. **梯度下降法（Gradient Descent Method）**：用於迭代更新參數以實現對抗攻擊的技術。  
3. **Fast Gradient Sign Method (FGSM)**：一種基於.gradient descent 的快速對抗攻擊方法，只需一次迭代即可完成攻擊。  

#### 主要觀念  
1. **梯度下降法的基本原理**：通過反覆更新參數來接近目標，是一種常見的最優化技術。  
2. **FGSM的核心思想**：基於梯度的方向簽號進行一次性更新，將擾動限制在特定範圍內（如 ε）。  
3. **對抗攻擊的目的**：使模型在受到微小擾動後產生錯誤分類，測試模型的魯棒性。  

#### 問題原因  
1. **簡單基線模型易受對抗攻擊**：如未經過充分訓練或缺乏防禦機制的模型，容易被 FGSM 等方法攻破。  
2. **多次迭代可能超出擾動範圍**：在多輪更新中，參數可能超出行為邊界（如 L∞ 球），影響攻擊的有效性。  

#### 解決方法  
1. **一擊必殺法（FGSM）**：一次性完成對抗攻擊，確保擾動範圍在可控之內。  
2. **迭代式 FGSM**：允許多輪更新，但需定期將參數拉回行為邊界以避免越界。  
3. **防禦機制**：如加入正則化、訓練時增強對抗樣本等方法，提升模型的 robustness。  

#### 優化方式  
1. **調整學習率（ε）**：根據具體場景調節擾動幅度，平衡攻擊效果與可感知性。  
2. **多輪迭代**：通過增加迭代次數來增強攻擊能力，但需注意邊界控制。  
3. **智能擾動設計**：基於模型特性設計更有針對性的擾動，提升攻擊成功率。  

#### 結論  
1. **FGSM的優勢**：快速、簡潔，適合用於簡單基線模型的攻擊測試。  
2. **迭代式 FGSM 的提升**：通過多次更新可進一步提升攻擊效果，但需注意邊界的控制與恢復。  
3. **未來研究方向**：探索更高效的對抗攻擊方法，並結合防禦技術提升模型的 robustness。  

---

### 總結  
本文系統性地介紹了深度學習中對抗攻擊的核心技術與方法，特別是.gradient descent method 和.FGSM 的原理、應用及優化方式。文章強調了在不同基線模型下，一擊必殺法與多次迭代法的差異，並提出了未來研究的方向。
</details>

<details>
<summary>339. [2021-05-07] [ML 2021 (English version)] Lecture 21:  Auto-encoder (1/2)</summary><br>

<a href="https://www.youtube.com/watch?v=E7wlA85RxcI" target="_blank">
    <img src="https://img.youtube.com/vi/E7wlA85RxcI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 21:  Auto-encoder (1/2)

### 核心主題
- 自動編碼器（Auto-Encoder）及其變體的研究與發展。

### 主要觀念
1. **自動編碼器的基本原理**：用於學習數據的表徵，通過編碼器將數據壓縮為潛在空間表示，再通過解碼器重建原始數據。
2. **去噪自動編碼器（Denoising Auto-Encoder）**：在編碼器輸入端加入噪聲，並要求解碼器恢復原始乾淨數據，從而學習去除噪聲的能力。

### 問題原因
- 早期研究中，受限玻耳茲曼機（Restricted Boltzmann Machine, RBM）被廣泛使用，但其複雜性限制了其實用性。
- RBM 的對稱編解碼器結構限制了模型的靈活性。

### 解決方法
1. **去噪自動編碼器**：通過添加噪聲來提高模型的魯棒性，並強迫模型學習更健壯的表徵。
2. **大型語言模型如BERT**：可以視作一種去噪自動編碼器，通過掩蔽詞令牌加入噪聲，並重建原始句子。

### 優化方式
- 棄對稱結構限制，改用更靈活的深度神經網絡架構。
- BERT等現代模型將自動編碼器與解碼器結合，提升表徵學習能力。

### 總結
- 自動編碼器是一種歷史悠久但重要性持續的研究方向。
- 去噪自動編碼器及其衍生物BERT展示了其在現代深度學習中的廣泛應用價值。
</details>

<details>
<summary>340. [2021-05-07] [ML 2021 (English version)] Lecture 20:  Self-supervised Learning (aka Foundation Model) (3/3)</summary><br>

<a href="https://www.youtube.com/watch?v=6sAf24QvJEY" target="_blank">
    <img src="https://img.youtube.com/vi/6sAf24QvJEY/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 20:  Self-supervised Learning (aka Foundation Model) (3/3)

### 小節歸納

#### 核心主題  
- 自我監督學習（Self-Supervised Learning, SSL）在多模態數據（如文本、語音和圖像）中的應用。  

#### 主要觀念  
1. **自我監督學習的定義與核心思想**  
   - 自我監督學習是一種無監督學習方法，通過設計偽任務從未標注數據中提取有用的特徵。  
   - 常見於自然語言處理（NLP）領域，如BERT和GPT模型的訓練。  

2. **文本領域的應用**  
   - BERT模型：通過填空任務（Masked Language Model, MLM）學習上下文語義。  
   - GPT模型：預測下一步詞元，實現生成式語言模型。  

3. **語音領域的挑戰與進展**  
   - 語音BERT和GPT的實現：將文本模型的概念遷移至語音處理。  
   - 常用策略包括遮蔽語音信號片段並讓模型重複預測缺失部分，或預測下一音節。  

4. **圖像領域的探索**  
   - 對圖像進行旋轉分類、區域掩蔽等任務，提取多模態特徵。  

5. **SUPERBbenchmark的提出**  
   - 為語音處理領域提供一個綜合性的benchmark，涵蓋內容識別、說話人識別、語調分析和語義理解等功能。  

#### 問題原因  
1. **語音領域的限制**  
   - 相對於文本領域（如GLUE benchmark），語音處理缺乏統一的benchmark來評估模型性能。  

2. **多模態數據的複雜性**  
   - 語音數據包含內容、語調、語義等多方面信息，難以用單一任務全面評估模型能力。  

#### 解決方法  
1. **SUPERBbenchmark的設計與實現**  
   - 提供十個不同類型的任務，涵蓋語音處理的多個層面（內容識別、說話人特徵提取、語義分析等）。  
   - 統一評估標準，便於研究者比較不同模型性能。  

2. **工具包的開發**  
   - 提供一套包含多種自我監督學習模型和下遊任務的工具包，簡化研究流程。  

#### 優化方式  
1. **benchmark的持續更新與完善**  
   - 根據研究進展不斷增加新任務和數據集，提升benchmark的代表性和實用性。  

2. **跨模態對比研究**  
   - 探索文本、語音和圖像等不同模態之間的相互作用，提升多模態自我監督學習的效果。  

#### 結論  
- 自我監督學習在文本、語音和圖像等數據處理中顯示出巨大的潛力。  
- 通過設計綜合性benchmark（如SUPERB）和工具包，可以推動語音處理領域的研究進展。  
- 未來的研究方向包括多模態對比學習、benchmark的拓展以及實用化場景的探索。
</details>

<details>
<summary>341. [2021-05-07] 【機器學習2021】來自人類的惡意攻擊 (Adversarial Attack) (下) – 類神經網路能否躲過人類深不見底的惡意？</summary><br>

<a href="https://www.youtube.com/watch?v=z-Q9ia5H2Ig" target="_blank">
    <img src="https://img.youtube.com/vi/z-Q9ia5H2Ig/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】來自人類的惡意攻擊 (Adversarial Attack) (下) – 類神經網路能否躲過人類深不見底的惡意？

### 文章整理：深度學習模型的安全性與防護措施

#### 1. 核心主題
- **深度學習模型的脆弱性**：深度學習模型易受 adversarial examples（敵意樣本）攻擊。
- **攻擊與防禦的研究進展**：介紹了攻擊方法、防禦策略及其局限性。

#### 2. 主要觀念
- **Adversarial Examples 的定義**： специально crafted 的輸入數據，會導致深度學習模型產生錯誤的預測。
- **黑箱攻擊的可能性**：即使缺乏模型內部結構知識，也能成功發動攻擊。
- **防禦策略的重要性**：需研究有效的防禦方法以提升模型安全性。

#### 3. 問題原因
- **模型脆弱性**：
  - 深度學習模型在高維空間中對小擾動敏感。
  - 對 adversarial perturbations（敵意幹擾）缺乏魯棒性。
- **防禦挑戰**：
  - 已存在的防禦方法可能被新型攻擊繞過。
  - Adversarial Training 資源消耗大，限制其廣泛應用。

#### 4. 解決方法
- **防禦技術**：
  - **幹擾キャンセル（Interference Cancellation）**：移除輸入數據中的敵意幹擾。
  - **模型修復（Model Repairing）**：修正模型預測以抵禦攻擊。
  - **模型魯棒化（Robustification）**：改進模型結構或訓練方法以增強抗攻擊能力。
- **Adversarial Training**：
  - 從現有數據生成 adversarial examples，加入訓練集以增強模型 robustness。
  - 可視為一種資料增強技術，提升模型泛化能力。

#### 5. 優化方式
- **提高計算效率**：
  - 研究輕量級的防禦方法，降低資源消耗。
  - 探索「免費」的 Adversarial Training 技術，減輕計算負擔。
- **持續改進防禦策略**：
  - 不斷更新防禦算法以應對新興攻擊方式。
  - 加強模型結構改進，提升先天抗攻擊能力。

#### 6. 結論
- **研究現況**：攻擊與防禦方法仍在不斷演化，尚無最終勝負者。
- **未來展望**：
  - 需持續研發更高效的防禦技術。
  - 提升模型魯棒性為當前研究關鍵方向。

#### 7. 參考文獻
- **Adversarial Training For Free**：介紹了一種低計算成本的 Adversarial Training 方法，值得進一步探究。
</details>

<details>
<summary>342. [2021-05-07] 【機器學習2021】機器學習模型的可解釋性 (Explainable ML) (上) – 為什麼類神經網路可以正確分辨寶可夢和數碼寶貝呢？</summary><br>

<a href="https://www.youtube.com/watch?v=WQY85vaQfTI" target="_blank">
    <img src="https://img.youtube.com/vi/WQY85vaQfTI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】機器學習模型的可解釋性 (Explainable ML) (上) – 為什麼類神經網路可以正確分辨寶可夢和數碼寶貝呢？

### 文章整理：探索深度學習模型中的語音訊號處理特性

#### 核心主題
- **深度學習在語音訊號處理中的應用**  
  探索深度學習模型（如LSTM、CNN）在語音訊號處理中的特性，包括聲音特徵提取、雜訊濾除及語者辨識等方面。

#### 主要觀念
1. **語音訊號的深度學習模型分析**  
   - 模型能夠學習並提取語音內容的高層特徵。
   - 舉例：5層Bi-LSTM模型在語音辨識中的應用。

2. **語者特徵的去除與保留**  
   - 通過模型分析，發現某些層次可能抹去語者的聲音特徵，僅保留語音內容。

3. **雜訊處理特性**  
   - 模型在不同層次對雜訊（如鋼琴聲）的濾除能力不同。
   - 舉例：CNN未能有效濾除雜訊，而LSTM在某層開始濔除雜訊。

#### 問題原因
- **語者特徵的丟失**  
  在某些深度學習模型中，後續層次可能失去語者的聲音特性，導致無法分辨不同語者。

- **雜訊濾除不一致**  
  不同網路層次對雜訊的處理效果存在差異，需進一步研究其原因。

#### 解決方法
1. **.Layer-wise 分析法**  
   - 過模型各層輸出進行逐層分析，以了解特徵提取和變化的具體過程。

2. **多模態模型整合**  
   - 結合語音和其它模態數據（如文本），以提升模型對語者特徵的保留能力。

3. **優化網路架構**  
   - 根據分析結果調整網路結構，強化雜訊濾除能力。

#### 優化方式
1. **網路結構設計**  
   - 在關鍵層次引入特定模塊以保留語者特徵。

2. **數據增強技術**  
   - 使用多樣化的訓練數據，提升模型的魯棒性和泛化能力。

3. **反向分析法**  
   - 通過逆向工程了解模型如何處理和改變輸入信號。

#### 結論
- **深度學習模型的有效性**  
  深度學習模型在語音內容提取方面表現出色，但在語者特徵保留上仍有改進空間。

- **層次分析的重要性**  
  過逐層輸出分析可揭示模型特性，為網路優化提供方向。

- **未來研究方向**  
  探索更多方法以平衡語音內容提取和語者特徵保留，提升模型在複雜環境下的性能。
</details>

<details>
<summary>343. [2021-05-13] [ML 2021 (English version)] Lecture 24:  Adversarial Attack (2/2)</summary><br>

<a href="https://www.youtube.com/watch?v=kRmBiV2X810" target="_blank">
    <img src="https://img.youtube.com/vi/kRmBiV2X810/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 24:  Adversarial Attack (2/2)

# 文章重點整理

## 核心主題
- **深度學習模型的安全性**：探討深度學習模型在面對攻擊時的脆弱性以及防禦方法。

## 主要觀念
1. **模型的脆弱性**：
   - 深度學習模型易受 adversarial examples（敵對樣本）影響，導致分類錯誤。
   - 攻擊者可利用.gradient information（梯度資訊）生成不可見的幹擾，降低模型性能。

2. **攻擊方法**：
   - **白盒攻擊**：攻擊者擁有模型的所有詳細信息，可用於精確設計敵對樣本。
   - **黑盒攻擊**：攻擊者僅知道模型的輸出，仍能通過遷移學習等技術影響目標模型。

3. **防禦策略**：
   - **事前防禦（Preventative Defence）**：
     - **數據增強（Data Augmentation）**：增加訓練數據的多樣性，提升模型 robustness。
     - **敵對訓練（Adversarial Training）**：在訓練階段引入敵對樣本，增強模型的抗幹擾能力。
   - **事後防禦（Post-Training Defence）**：
     - **.Gradient Masking**：隱藏梯度資訊，防止攻擊者利用。
     - **模型壓縮與量化**：降低模型複雜性，增加攻擊難度。

## 問題原因
1. 深度學習模型的高維特徵表示使其易受敵對樣本影響。
2. 攻擊方法的多樣化和技術門檻低，導致防禦挑戰增大。

## 解決方法
1. **敵對訓練**：
   - 在訓練過程中生成並加入敵對樣本，增強模型的魯棒性。
   - 可視為一種高效的數據增強方法，同時降低過擬合風險。

2. **數據增強**：
   - 通過增加多樣化的訓練數據，提升模型的 generalization 能力。

3. **防禦技術**：
   - 隱藏梯度資訊（Gradient Masking）：防止攻擊者利用梯度信息生成敵對樣本。
   - 模型壓縮與量化：降低模型複雜性，增加攻擊成本和難度。

## 優化方式
1. **計算效率**：
   - 確保防禦方法在大型數據集上具備可擴展性。
   - 採用「免費的敵對訓練」等技術，降低計算資源消耗。

2. **攻擊與防禦的平衡**：
   - 持續研究新型防禦策略，應對不斷進化的攻擊技術。

## 結論
- 目前深度學習模型的攻擊與防禦方法仍在快速發展中，尚無明確勝負者。
- 防禦側需結合多種策略，提升模型的安全性。

---

### 總結與建議
1. **研究建議**：
   - 進一步探索敵對訓練和其他防禦技術的 combination，提升防禦效果。
   - 加強對黑盒攻擊的研究，提高模型在未知環境下的安全性。

2. **實踐建議**：
   - 在實際應用中結合數據增強和敵對訓練等方法，增強模型 robustness。
   - 定期更新模型和防禦策略，跟蹤最新研究成果，保持技術先進性。
</details>

<details>
<summary>344. [2021-05-13] [ML 2021 (English version)] Lecture 25: Explainable ML (1/2)</summary><br>

<a href="https://www.youtube.com/watch?v=4rVD1EOaAX4" target="_blank">
    <img src="https://img.youtube.com/vi/4rVD1EOaAX4/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 25: Explainable ML (1/2)

### 文章重點整理

#### 核心主題
本文圍繞深度學習模型在聲音處理和信息提取方面的應用展開討論，探討了模型如何進行語音識別、聲紋消除以及噪音過濾等操作。文章強調通過後 처리模型（如TTS）來分析前處理模型的特性，從而揭示模型在特徵提取過程中的行為。

#### 主要觀念
1. **語音特徵提取**：深度學習模型（如Bi-Directional LSTM和CNN）能夠有效地提取語音內容的特徵，同時可能消除或改變某些非-content特徵（如說話人的聲音特徵）。
2. **聲紋消除**：文中通過示例展示了模型在多層處理後，能夠將不同說話人的語音轉化為相似的聲音，這表明模型可能在訓練過程中有意無意地去除了聲紋信息。
3. **噪音過濾**：研究表明，LSTM層在某些情況下可以有效去除信號中的噪音（如鋼琴噪音），而CNN層在此案例中未顯著發揮噪音過濾作用。

#### 問題原因
1. **信息損失**：深度學習模型在提取特徵時，可能不可避免地丟失了一些重要的非-content信息，如說話人的聲紋特徵。
2. **模型特性揭示不足**： traditional visualization和分析方法（如Hinton的 visulization）雖然有用，但缺乏交互性和深入的行為揭示。

#### 解決方法
1. **後處理模型分析**：通過使用TTS模型對前處理模型的特徵進行重建，可以有效分析前處理模型的信息保留情況，進而理解其特性。
2. **多層網絡分析**：通過分層分析（如CNN和LSTM的不同深度），能夠更好地理解每個層在信號處理中的作用。

#### 優化方式
1. **模型結構改進**：根據分析結果，可以對模型結構進行優化，例如強化某些層的噪音過濾能力或聲紋保留功能。
2. **特徵保留策略**：在模型設計階段，可以有意識地設置機制來保留或去除特定類型的信息（如選擇性地保留聲紋特徵）。

#### 結論
1. 深度學習模型在聲音信息處理方面展現出強大的能力，但其特性揭示和行為分析仍具挑戰性。
2. 使用後處理模型進行反向分析是一種有效的研究方法，能夠幫助理解前處理模型的特徵提取機制。
3. 分層網絡分析對於揭示各級層的信息處理特徵具有重要意義，未來的研究可以進一步探索如何通過這種方式來優化模型性能。
</details>

<details>
<summary>345. [2021-05-21] 【機器學習2021】概述領域自適應 (Domain Adaptation)</summary><br>

<a href="https://www.youtube.com/watch?v=Mnk_oUrgppM" target="_blank">
    <img src="https://img.youtube.com/vi/Mnk_oUrgppM/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】概述領域自適應 (Domain Adaptation)

### 文章重點整理

#### 1. 核心主題
- **Domain Adaptation**：研究如何在數據分布不同的情況下，遷移學習模型以適應目標域（Target Domain）的任務。
- **Domain Generalization**：探討模型如何泛化到未知的多個領域。

#### 2. 主要觀念
- **數據分布差異性**：源域和目標域之間可能存在顯著的數據分布差異，導致模型在目標域上的性能下降。
- **無標籤數據處理**：在目標域缺乏標註的情況下，需要設計有效的遷移學習策略。
- **小樣本挑戰**：當目標域數據量極小時，傳統的遷移方法可能失效。

#### 3. 問題原因
- **領域差異**：源域和目標域之間的特徵分布、統計特性或標籤空間存在差異，導致模型無法直接適應。
- **標註稀缺性**：在目標域中獲取標註數據困難，限制了監督學習的效果。
- **小樣本問題**：極少量的目標域數據使得難以訓練有效的遷移模型。

#### 4. 解決方法
##### (1) 基於對抗訓練的Domain Adaptation
- **對抗網絡**：通過設計對抗網絡（GANs）來最小化源域和目標域之間的分布差異。
- **領域對齊**：利用判別器迫使生成器學習跨領域的共享特徵，實現特徵空間的對齊。

##### (2) 測試時間訓練（Testing Time Training, TTT）
- **小樣本處理**：針對目標域數據極少的情況，採用測試時微調的方法，通過少量樣本快速調整模型參數。
- **增量學習**：利用目標域的小樣本數據，在測試階段進行在線學習，提升模型適應性。

##### (3) 域泛化（Domain Generalization）
- **多領域訓練集**：構建包含多個領域的訓練數據集，使模型在訓練時自然學習到跨領域的特徵。
- **數據增強**：通過模擬不同域的數據分布，生成虛擬的多領域樣本，豐富訓練數據。

##### (4) 參數調節與模型優化
- **權重調整**：在遷移過程中，平衡領域對齊和任務損失的權重，避免過度適應判別器。
- **正則化技術**：引入適當的正則化方法，保持模型在不同域之間的泛化能力。

#### 5. 確保模型性能的關鍵因素
- **特徵空間設計**：確保學習到的特徵能夠有效區分類別，同時保持跨域一致性。
- **領域對齊策略**：採用多種對齊方法（如對抗訓練、統計距離最小化）優化特徵分布。
- **泛化能力驗證**：通過在多個未知領域的測試驗證模型的泛化性能。

#### 6. 研究展望
- **複雜領域適應**：探索更高效的算法，應對更加複雜的領域差異和數據稀疏性問題。
- **多任務學習結合**：研究如何將域適應與多任務學習相結合，提升模型的整體表現。
- **實時應用優化**：針對實時應用場景，設計輕量級的遷移學習解決方案。

#### 7. 結論
- **技術可行性**：通過對抗訓練和測試時間微調等方法，能夠在一定程度上解決領域適配問題。
- **研究必要性**：域適應和泛化技術對於實際應用中的模型部署至關重要，特別是在標註數據稀缺或領域分布多變的情況下。
</details>

<details>
<summary>346. [2021-05-21] 【機器學習2021】機器學習模型的可解釋性 (Explainable ML) (下) –機器心中的貓長什麼樣子？</summary><br>

<a href="https://www.youtube.com/watch?v=0ayIPqbdHYQ" target="_blank">
    <img src="https://img.youtube.com/vi/0ayIPqbdHYQ/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】機器學習模型的可解釋性 (Explainable ML) (下) –機器心中的貓長什麼樣子？

### 小節一：核心主題  
- 本文圍繞**可解釋機器學習（Explainable Machine Learning）**展開討論，探討如何提高模型決策過程的透明性和可解釋性。  

### 小節二：主要觀念  
1. **機器學習模型的黑箱問題**：  
   - 深度學習模型（如神經網絡）因其複雜性而被視為「黑箱」，其決策過程不易被人理解。  
2. **可解釋性的重要性**：  
   - 可解釋性在模型的信任、驗證和改進中具有關鍵作用，尤其是在醫學、金融等高風險領域。  
3. **提升可解釋性的方法**：  
   - 使用簡單的模型（如線性模型）或對黑箱模型進行後處理解讀。  

### 小節三：問題原因  
- 神經網絡等複雜模型的決策機制缺乏透明度，導致用戶難以信任和驗證模型的行為。  
- 模型易受敵對攻擊（Adversarial Attacks）影響，進一步暴露其脆弱性。  

### 小節四：解決方法  
1. **後處理解讀技術**：  
   - 使用簡單的模型（如LASSO、 Ridge Regression）對黑箱模型的決策進行局部解讀。  
2. **局部可解釋性模型**：  
   - 引入**LIME（Local Interpretable Model-agnostic Explanations）**等方法，通過在小數據範圍內訓練線性模型來模擬黑箱模型的行為，從而提供局部解讀。  
3. **增設額外類別**：  
   - 在分類器中增加「都不是」的選項，並最小化其機率，以降低模型錯誤分類的風險。  

### 小節五：優化方式  
1. **整合多種解釋方法**：  
   - 結合全局和局部解讀技術，提供更全面的模型行為分析。  
2. **提升模型 robustness**：  
   - 遷移學習、數據增強等技術可提高模型對敵對攻擊的抵抗能力。  

### 小節六：結論  
- 可解釋性是機器學習模型可信度的重要指標，需通過多種技術手段（如LIME）來提升模型的透明性與用戶信任。  
- 增設額外類別並最小化其機率可在一定程度上降低錯誤分類風險，但仍需進一步實驗驗證其效果。
</details>

<details>
<summary>347. [2021-05-22] 【機器學習2021】概述增強式學習 (Reinforcement Learning, RL) (一) – 增強式學習跟機器學習一樣都是三個步驟</summary><br>

<a href="https://www.youtube.com/watch?v=XWukX-ayIrs" target="_blank">
    <img src="https://img.youtube.com/vi/XWukX-ayIrs/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】概述增強式學習 (Reinforcement Learning, RL) (一) – 增強式學習跟機器學習一樣都是三個步驟

### 文章重點整理

#### 核心主題
本文主要探討如何訓練一個.Actor*（代理）來執行符合特定期望的行為，通過定義適當的損失函數和訓練數據來實現此目標。

---

#### 主要觀念

1. **Actor 的訓練模式**  
   - Actor 的訓練類似於監督學習（Supervised Learning），其中訓練數據包含狀態（State, s）和對應動作（Action, a）的配對。
   - 每個動作可以被分為兩種類型：
     - **期望執行的動作**：用 +1 表示。
     - **不期望執行的動作**：用 -1 表示。

2. **訓練數據的定義**  
   - 訓練數據由一系列（s, a）配對組成，其中 s 是環境中的狀態，a 是在該狀態下期望Actor採取的行動。
   - 每個動作可以具有不同的權重（如 +1.5 或 +0.5），表示不同程度的執行期望。

3. **損失函數的定義**  
   - 使用交叉熵損失（Cross-Entropy Loss）來衡量.Actor*的預測行動與實際期望行動之間的差異。
   - 損失函數的形式為：
     \[
     L = -\sum_{i=1}^{N} w_i \cdot y_i \cdot \log(\hat{y}_i) + (1 - y_i) \cdot \log(1 - \hat{y}_i)
     \]
     其中，\(w_i\) 表示第 i 筆數據的權重，\(y_i\) 是期望行動（+1 或 -1），\(\hat{y}_i\) 是.Actor*預測的概率。

---

#### 問題原因

1. **訓練數據的來源**  
   - 如何有效收集和定義訓練數據，以確保Actor在各種狀態下能夠執行正確的行動。
   - 經驗表明，直接定義訓練數據可能具有挑戰性，特別是對於複雜環境。

2. **損失函數的優化**  
   - 損失函數需要能夠反映不同程度的行動期望，以便Actor在訓練過程中學會區分重要和次要的行動。

---

#### 解決方法

1. **訓練數據的生成**  
   - 通過專家示範或模擬環境來收集training data。
   - 確定每個狀態下Actor應該執行的行動，並為其分配適當的權重。

2. **損失函數的設計**  
   - 在交叉熵損失中引入權重因子 \(w_i\)，以反映不同行動的重要程度。
   - 通過反向傳播（Backpropagation）來優化.Actor*的參數θ，最小化損失函數。

---

#### 優化方式

1. **動機設計**  
   - 給予Actor執行某些行動更高的權重，以強調這些行動的重要性。
   - 例如，在特定狀態下，Actor應該避免某個有害行動，可以給予該行動的權重為-2，以增強訓練效果。

2. **分級期望**  
   - 根據不同行動的影響程度，將其分級（如高、中、低），並在損失函數中體現這些差異。
   - 這可以幫助.Actor*在訓練過程中更精準地掌握各類行動的執行策略。

---

#### 結論

本文提出了一種基於監督學習的.Actor*訓練方法，通過定義具體的訓練數據和損失函數，使得Actor能夠學習到符合期望的行為。核心思想是將行動期望轉化為交叉熵損失中的權重因子，並通過反向傳播優化.Actor*的參數。此方法為Actor在複雜環境下的行為控制提供了一種可行的途徑，但仍需進一步研究如何高效收集和定義訓練數據，以提高訓練效果。
</details>

<details>
<summary>348. [2021-05-22] 【機器學習2021】概述增強式學習 (Reinforcement Learning, RL) (二) – Policy Gradient 與修課心情</summary><br>

<a href="https://www.youtube.com/watch?v=US8DFaAZcp4" target="_blank">
    <img src="https://img.youtube.com/vi/US8DFaAZcp4/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【機器學習2021】概述增強式學習 (Reinforcement Learning, RL) (二) – Policy Gradient 與修課心情

### 一、核心主題：強化學習（Reinforcement Learning）中的策略優化方法

1. **主要焦點**：
   - 探討在強化學習中如何通過策略優化方法提升智能體的行爲表現。
   - 強調策略優化的核心思想及其在實際應用中的重要性。

2. **核心概念**：
   - 策略（Policy）：定義了智能體在給定狀態下的行爲選擇規則。
   - 獎勵信號（Reward Signal）：用於評估智能體行爲的優劣，指導學習過程。
   - 探索與利用（Exploration vs. Exploitation）：平衡新舊行爲選擇以確保有效學習。

### 二、主要觀念：策略優化的基本原理

1. **基本原理**：
   - 策略優化的目標是通過迭代更新策略，使得智能體在環境中獲得的累積獎勵最大化。
   - 每次迭代中，智能體會與環境交互，收集經驗並根據反饋調整行爲。

2. **關鍵步驟**：
   - 行爲選擇：基於當前策略生成動作。
   - 反饋接收：從環境中獲取獎勵信號。
   - 策略更新：利用反饋優化策略參數。

### 三、問題原因：傳統策略優化方法的局限性

1. **主要挑戰**：
   - **探索不足**：初始策略可能過於固定，缺乏多樣性，導致無法充分探索狀態空間。
   - **梯度估算偏差**：直接優化策略可能導致梯度估計不穩定或偏離最優解。
   - **樣本效率低下**：傳統方法需要大量交互才能收斂，限制了在複雜環境中的應用。

2. **具體問題**：
   - 策略更新過程中可能陷入局部最優。
   - 高維狀態空間和動作空間增加了學習難度。
   - 動作選擇的隨機性不足可能導致智能體無法發現最佳行爲路徑。

### 四、解決方法：PPO（Proximal Policy Optimization）算法

1. **基本思想**：
   - 通過限制策略更新的幅度，確保每次迭代中策略的變化量在合理範圍內。
   - 分離舊策略和新策略的概率分布，利用比值估計法優化目標函數。

2. **具體實現**：
   - 引入兩個概率比值（old policy 和 new policy）來計算損失函數。
   - 通過限制策略更新的幅度（clip parameter）防止參數更新過大導致性能下降。
   - 利用信任區域方法確保每次更新穩定可靠。

3. **優點**：
   - 算法穩定性高，收斂速度快。
   - 具有良好的樣本效率，在複雜環境中表現優異。
   - 易於實現且具有較強的理論基礎支持。

### 五、優化方式：提升策略探索的有效性

1. **增加動作隨機性**：
   - 在策略輸出中引入熵正則化（Entropy Regularization），鼓勵智能體嘗試更多動作，避免過早收斂。
   - 在策略參數更新過程中添加噪聲，增強探索能力。

2. **經驗重放（Experience Replay）**：
   - 通過回放歷史經驗，增加訓練數據的多樣性，提升學習效率。

3. **多-threaded exploration**：
   - 利用多線程或多進程同時進行環境交互和策略優化，加速學習過程。

### 六、結論與未來方向

1. **研究結論**：
   - PPO算法在強化學習領域展現了顯著優勢，特別是在複雜任務中表現優異。
   - 策略優化的關鍵在於平衡探索與利用，有效提升智能體的學習效率和性能。

2. **未來方向**：
   - 探索更高效的梯度估算方法，進一步提高樣本利用率。
   - 研究更加魯棒的策略更新機制，增強算法在不同環境中的適應性。
   - 結合其他機器學習技術（如深度神經網絡、圖結構學習等），拓展強化學習的應用範圍。

3. **實際應用**：
   - 在機器人控制、遊戲AI、自動駕駛等領域具有廣泛前景。
   - 通過不斷優化策略優化方法，推動人工智能技術的進一步發展。
</details>

<details>
<summary>349. [2021-05-26] [ML 2021 (English version)] Lecture 26: Explainable ML (2/2)</summary><br>

<a href="https://www.youtube.com/watch?v=5VPy7OGlGMQ" target="_blank">
    <img src="https://img.youtube.com/vi/5VPy7OGlGMQ/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 26: Explainable ML (2/2)

### 核心主題
- **解釋性機器學習**：探討如何使機器學習模型更加透明和易於理解。

---

### 主要觀念
1. **可解釋性的重要性**：
   - 機器學習模型的決策過程需要被人類理解和信任。
   - 特別是在醫療、金融等高風險領域，可解釋性至關重要。

2. **黑箱問題**：
   - 現代深度學習模型（如神經網路）通常被視為「黑箱」，其內部運作機制不易理解。

3. **對抗攻擊與可靠性**：
   - 機器學習模型可能受到對抗攻擊，導致錯誤的決策。
   - 提高可解釋性有助於增強模型的可靠性和抗幹預能力。

---

### 問題原因
1. **模型複雜性**：
   - 神經網路等深度 learning模型結構複雜，導致其運作機制不易解讀。

2. **缺乏透明度**：
   - 黑箱模型的決策缺乏明確的解釋，影響用戶的信任和應用。

3. **對抗攻擊的挑戰**：
   - 模型可能被設計外的輸入操縱，導致不可預測的行為。

---

### 解決方法
1. **局部可解釋性方法**：
   - 使用如 LIME（Local Interpretable Model-Agnostic Explanations）等技術，僅解析模型在特定區域的行爲。
   - 通過簡單的線性模型模擬小範圍的黑箱行為來提供解釋。

2. ** adversarial attacks 的防禦**：
   - 增加額外的分類選項（如「非上述任何一類」），以降低模型錯誤分類的可能性。
   - 通過數據增強和正則化技術提高模型魯棒性。

3. **模型簡化與可視化**：
   - 使用線性模型或其他簡單模型來近似黑箱模型的行為。
   - 通過可視化工具展示模型決策的關鍵特徵。

---

### 優化方式
1. **多分類策略**：
   - 添加額外的分類選項（如「非上述任何一類」），以提高模型的可靠性和解釋性。

2. **混合模型設計**：
   - 結合深度學習模型和簡單模型，利用深度模型的強大能力和簡單模型的可解讀性。

3. **持續研究與實驗**：
   - 從事更多的實驗來驗證不同方法的效果，進一步優化解釋性機器學習技術。

---

### 總結
- 可解釋性是機器學習模型應用的重要條件。
- 地區性的可解釋性方法（如 LIME）和防禦對抗攻擊的策略（如添加額外分類選項）提供了有效的解決方案。
- 未來的研究需進一步探索如何平衡模型的複雜性和可解釋性，以實現更可靠的機器學習系統。
</details>

<details>
<summary>350. [2021-05-27] [ML 2021 (English version)] Lecture 28: Introduction of Reinforcement Learning (RL) (1/5)</summary><br>

<a href="https://www.youtube.com/watch?v=0oucZNfBXlI" target="_blank">
    <img src="https://img.youtube.com/vi/0oucZNfBXlI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# [ML 2021 (English version)] Lecture 28: Introduction of Reinforcement Learning (RL) (1/5)

### 文章整理：基於監督學習的Actor訓練框架

#### 核心主題
- **Actor訓練的本質**：通過定義明確的行爲期望，利用監督學習方法訓練一個能夠執行預期動作的智能體（Actor）。

#### 主要觀念
1. **Actor的訓練目標**：
   - 使Actor在特定狀態下執行期望的動作。
   - 區分「應該執行」的動作和「不應該執行」的動作。

2. **行爲數據的表示**：
   - 每對狀態-動作（s, a）配對有一個評分（An），表示對該動作的期望程度。
   - 評分可以是正數（期望執行）、負數（不期望執行）或零（中立態度）。

3. **監督學習框架的應用**：
   - 將Actor訓練類比爲分類任務，狀態s被視爲輸入，動作a被視爲輸出標籤。
   - 使用交叉熵損失函數衡量預測與真實動作的差異。

4. **行爲評分的作用**：
   - 通過An調節不同狀態-動作對的權重，實現差異化的行爲管理。
   - 高正數評分表示強烈期望執行，低或負評分表示較低或完全不期望執行。

#### 問題原因
1. **數據來源的挑戰**：
   - 如何生成大量高質量的狀態-動作配對（s, a）作爲訓練數據？
   - 數據的質量直接影響Actor的學習效果。

2. **行爲評分的主觀性**：
   - 評分An需要明確定義，不同場景下的人爲主觀判斷可能影響模型性能。

3. **動態環境適應性**：
   - 在實際應用中，環境可能是動態變化的，固定訓練數據可能導致Actor在新環境中表現不佳。

#### 解決方法
1. **數據收集策略**：
   - 通過專家示範、模擬器生成或人機交互等方式收集狀態-動作配對。
   - 確保數據覆蓋不同狀態和動作的多樣性。

2. **評分機制的設計**：
   - 根據任務需求設計評分規則，明確每個狀態-動作對的期望程度。
   - 使用領域知識或專家意見來制定評分標準。

3. **模型優化與調整**：
   - 採用合適的監督學習算法（如隨機森林、神經網絡等）訓練Actor。
   - 調整模型參數以優化損失函數，提升預測準確性。

4. **動態適應機制**：
   - 引入強化學習或其他自適應方法，使Actor能夠根據環境反饋調整行爲。
   - 定期更新訓練數據和模型，以應對環境變化。

#### 結論
- 通過明確的行爲期望定義和監督學習框架，可以有效地訓練一個Actor來執行預期動作。
- 行爲評分機制提供了靈活的控制手段，可以根據具體需求調節不同狀態-動作對的重要性。
- 數據收集和評分機制的設計是Actor訓練的關鍵挑戰，需要結合領域知識和實際應用需求進行優化。
</details>

