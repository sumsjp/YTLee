<details>
<summary>351. [ML 2021 (English version)] Lecture 27: Domain Adaptation</summary><br>

<a href="https://www.youtube.com/watch?v=8AKqH6V9kjE" target="_blank">
    <img src="https://img.youtube.com/vi/8AKqH6V9kjE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>352. [ML 2021 (English version)] Lecture 29: Introduction of Reinforcement Learning (RL) (2/5)</summary><br>

<a href="https://www.youtube.com/watch?v=jbN0oYLtXps" target="_blank">
    <img src="https://img.youtube.com/vi/jbN0oYLtXps/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>353. 【機器學習2021】概述增強式學習 (Reinforcement Learning, RL) (三) - Actor-Critic</summary><br>

<a href="https://www.youtube.com/watch?v=kk6DqWreLeU" target="_blank">
    <img src="https://img.youtube.com/vi/kk6DqWreLeU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>354. 【機器學習2021】概述增強式學習 (Reinforcement Learning, RL) (四) - 回饋非常罕見的時候怎麼辦？機器的望梅止渴</summary><br>

<a href="https://www.youtube.com/watch?v=73YyF1gmIus" target="_blank">
    <img src="https://img.youtube.com/vi/73YyF1gmIus/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>355. 【機器學習2021】概述增強式學習 (Reinforcement Learning, RL) (五) - 如何從示範中學習？逆向增強式學習 (Inverse RL)</summary><br>

<a href="https://www.youtube.com/watch?v=75rZwxKBAf0" target="_blank">
    <img src="https://img.youtube.com/vi/75rZwxKBAf0/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>356. 【機器學習2021】機器終身學習 (Life Long Learning, LL) (一) - 為什麼今日的人工智慧無法成為天網？災難性遺忘(Catastrophic Forgetting)</summary><br>

<a href="https://www.youtube.com/watch?v=rWF9sg5w6Zk" target="_blank">
    <img src="https://img.youtube.com/vi/rWF9sg5w6Zk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>357. 【機器學習2021】機器終身學習 (Life Long Learning, LL) (二) - 災難性遺忘(Catastrophic Forgetting)的克服之道</summary><br>

<a href="https://www.youtube.com/watch?v=Y9Jay_vxOsM" target="_blank">
    <img src="https://img.youtube.com/vi/Y9Jay_vxOsM/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>358. 【機器學習2021】神經網路壓縮 (Network Compression) (一) - 類神經網路剪枝 (Pruning) 與大樂透假說 (Lottery Ticket Hypothesis)</summary><br>

<a href="https://www.youtube.com/watch?v=utk3EnAUh-g" target="_blank">
    <img src="https://img.youtube.com/vi/utk3EnAUh-g/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>359. [ML 2021 (English version)] Lecture 34: Life-long Learning (2/2)</summary><br>

<a href="https://www.youtube.com/watch?v=-2r4cqDP4BY" target="_blank">
    <img src="https://img.youtube.com/vi/-2r4cqDP4BY/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>360. [ML 2021 (English version)] Lecture 33: Life-long Learning (1/2)</summary><br>

<a href="https://www.youtube.com/watch?v=yAX8Ydfek_I" target="_blank">
    <img src="https://img.youtube.com/vi/yAX8Ydfek_I/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>361. [ML 2021 (English version)] Lecture 31: Introduction of Reinforcement Learning (RL) (4/5)</summary><br>

<a href="https://www.youtube.com/watch?v=pibO_5JhQ4U" target="_blank">
    <img src="https://img.youtube.com/vi/pibO_5JhQ4U/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>362. [ML 2021 (English version)] Lecture 30: Introduction of Reinforcement Learning (RL) (3/5)</summary><br>

<a href="https://www.youtube.com/watch?v=Cf-WkM-Xef0" target="_blank">
    <img src="https://img.youtube.com/vi/Cf-WkM-Xef0/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>363. [ML 2021 (English version)] Lecture 32: Introduction of Reinforcement Learning (RL) (5/5)</summary><br>

<a href="https://www.youtube.com/watch?v=9H3ShV57lHs" target="_blank">
    <img src="https://img.youtube.com/vi/9H3ShV57lHs/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>364. 【機器學習2021】神經網路壓縮 (Network Compression) (二) - 從各種不同的面向來壓縮神經網路</summary><br>

<a href="https://www.youtube.com/watch?v=xrlbLPaq_Og" target="_blank">
    <img src="https://img.youtube.com/vi/xrlbLPaq_Og/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>365. 【機器學習2021】元學習 Meta Learning (一) - 元學習跟機器學習一樣也是三個步驟</summary><br>

<a href="https://www.youtube.com/watch?v=xoastiYx9JU" target="_blank">
    <img src="https://img.youtube.com/vi/xoastiYx9JU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>366. 【機器學習2021】元學習 Meta Learning (二) - 萬物皆可 Meta</summary><br>

<a href="https://www.youtube.com/watch?v=Q68Eh-wm1Ts" target="_blank">
    <img src="https://img.youtube.com/vi/Q68Eh-wm1Ts/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>367. 【機器學習2021】課程結語 - 最後的業配並改編《為學一首示子姪》作結</summary><br>

<a href="https://www.youtube.com/watch?v=JXDjNh2qlfc" target="_blank">
    <img src="https://img.youtube.com/vi/JXDjNh2qlfc/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>368. [ML 2021 (English version)] Lecture 35: Network Compression (1/2)</summary><br>

<a href="https://www.youtube.com/watch?v=CB0a3aBwND8" target="_blank">
    <img src="https://img.youtube.com/vi/CB0a3aBwND8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>369. [ML 2021 (English version)] Lecture 36: Network Compression (2/2)</summary><br>

<a href="https://www.youtube.com/watch?v=mGRdOGdOZ-4" target="_blank">
    <img src="https://img.youtube.com/vi/mGRdOGdOZ-4/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>370. SUPERB: 語音上的自督導式學習模型居然十項全能？</summary><br>

<a href="https://www.youtube.com/watch?v=MpsVE60iRLM" target="_blank">
    <img src="https://img.youtube.com/vi/MpsVE60iRLM/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>371. SUPERB: Is self-supervised learning universal in speech processing tasks? (English version)</summary><br>

<a href="https://www.youtube.com/watch?v=GTjwYzFG54E" target="_blank">
    <img src="https://img.youtube.com/vi/GTjwYzFG54E/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>372. 【機器學習2022】開學囉~ 又要週更了~</summary><br>

<a href="https://www.youtube.com/watch?v=7XZR0-4uS5s" target="_blank">
    <img src="https://img.youtube.com/vi/7XZR0-4uS5s/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>373. 【機器學習 2022】再探寶可夢、數碼寶貝分類器 — 淺談機器學習原理</summary><br>

<a href="https://www.youtube.com/watch?v=_j9MVVcvyZI" target="_blank">
    <img src="https://img.youtube.com/vi/_j9MVVcvyZI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>374. 【機器學習 2022】為什麼用了驗證集 (validation set) 結果卻還是過擬合(overfitting)了呢？</summary><br>

<a href="https://www.youtube.com/watch?v=xQXh3fSvD1A" target="_blank">
    <img src="https://img.youtube.com/vi/xQXh3fSvD1A/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>375. 【機器學習 2022】魚與熊掌可以兼得的深度學習</summary><br>

<a href="https://www.youtube.com/watch?v=yXd2D5J0QDU" target="_blank">
    <img src="https://img.youtube.com/vi/yXd2D5J0QDU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>376. 【機器學習 2022】各式各樣神奇的自注意力機制 (Self-attention) 變型</summary><br>

<a href="https://www.youtube.com/watch?v=yHoAq1IT_og" target="_blank">
    <img src="https://img.youtube.com/vi/yHoAq1IT_og/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>377. 【機器學習 2022】如何有效的使用自督導式模型 - Data-Efficient & Parameter-Efficient Tuning (由姜成翰助教講授)</summary><br>

<a href="https://www.youtube.com/watch?v=NzElV8jTNmw" target="_blank">
    <img src="https://img.youtube.com/vi/NzElV8jTNmw/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>378. 【機器學習 2022】語音與影像上的神奇自督導式學習 (Self-supervised Learning) 模型</summary><br>

<a href="https://www.youtube.com/watch?v=lMIN1iKYNmA" target="_blank">
    <img src="https://img.youtube.com/vi/lMIN1iKYNmA/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>379. 【機器學習2022】自然語言處理上的對抗式攻擊 (由姜成翰助教講授) - Part 1</summary><br>

<a href="https://www.youtube.com/watch?v=z-lRPFFYVJc" target="_blank">
    <img src="https://img.youtube.com/vi/z-lRPFFYVJc/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>380. 【機器學習2022】自然語言處理上的對抗式攻擊 (由姜成翰助教講授) - Part 2</summary><br>

<a href="https://www.youtube.com/watch?v=68lwXWFzCmg" target="_blank">
    <img src="https://img.youtube.com/vi/68lwXWFzCmg/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>381. 【機器學習2022】自然語言處理上的對抗式攻擊 (由姜成翰助教講授) - Part 3</summary><br>

<a href="https://www.youtube.com/watch?v=LP3q72MwE7A" target="_blank">
    <img src="https://img.youtube.com/vi/LP3q72MwE7A/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>382. 【機器學習2022】自然語言處理上的模仿攻擊 (Imitation Attack) 以及後門攻擊 (Backdoor Attack) (由姜成翰助教講授)</summary><br>

<a href="https://www.youtube.com/watch?v=uHKXwwQ7A_s" target="_blank">
    <img src="https://img.youtube.com/vi/uHKXwwQ7A_s/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>383. 【機器學習 2022】惡搞自督導式學習模型 BERT 的三個故事</summary><br>

<a href="https://www.youtube.com/watch?v=Pal2DbmiYpk" target="_blank">
    <img src="https://img.youtube.com/vi/Pal2DbmiYpk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>384. 【機器學習 2022】各種奇葩的元學習 (Meta Learning) 用法</summary><br>

<a href="https://www.youtube.com/watch?v=QNfymMRUg3M" target="_blank">
    <img src="https://img.youtube.com/vi/QNfymMRUg3M/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>385. AlphaTensor: 用增強式學習 (Reinforcement Learning) 找出更有效率的矩陣相乘演算法 (線性代數 2022 課程補充)</summary><br>

<a href="https://www.youtube.com/watch?v=KPcA8QCTm5U" target="_blank">
    <img src="https://img.youtube.com/vi/KPcA8QCTm5U/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>386. Meta 語音對語音翻譯技術背後的黑科技 (在繪圖 AI 中也有用上喔!)</summary><br>

<a href="https://www.youtube.com/watch?v=sWz4e-DM4JU" target="_blank">
    <img src="https://img.youtube.com/vi/sWz4e-DM4JU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>387. ChatGPT (可能)是怎麼煉成的 - GPT 社會化的過程</summary><br>

<a href="https://www.youtube.com/watch?v=e0aKI2GGZNg" target="_blank">
    <img src="https://img.youtube.com/vi/e0aKI2GGZNg/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>388. 【生成式AI】ChatGPT 原理剖析 (1/3) — 對 ChatGPT 的常見誤解</summary><br>

<a href="https://www.youtube.com/watch?v=yiY4nPOzJEg" target="_blank">
    <img src="https://img.youtube.com/vi/yiY4nPOzJEg/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>389. 【生成式AI】ChatGPT 原理剖析 (2/3) — 預訓練 (Pre-train)</summary><br>

<a href="https://www.youtube.com/watch?v=1ah7Qsri_c8" target="_blank">
    <img src="https://img.youtube.com/vi/1ah7Qsri_c8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>390. 【生成式AI】ChatGPT 原理剖析 (3/3) — ChatGPT 所帶來的研究問題</summary><br>

<a href="https://www.youtube.com/watch?v=UsaZhQ9bY2k" target="_blank">
    <img src="https://img.youtube.com/vi/UsaZhQ9bY2k/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>391. 【生成式AI】用 ChatGPT 和 Midjourney 來玩文字冒險遊戲</summary><br>

<a href="https://www.youtube.com/watch?v=A-6c584jxX8" target="_blank">
    <img src="https://img.youtube.com/vi/A-6c584jxX8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>392. 【生成式AI】快速了解機器學習基本原理 (1/2) (已經略懂機器學習的同學可以跳過這段)</summary><br>

<a href="https://www.youtube.com/watch?v=phQK8xZpgoU" target="_blank">
    <img src="https://img.youtube.com/vi/phQK8xZpgoU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>393. 【生成式AI】快速了解機器學習基本原理 (2/2) (已經略懂機器學習的同學可以跳過這段)</summary><br>

<a href="https://www.youtube.com/watch?v=XLyPFnephpY" target="_blank">
    <img src="https://img.youtube.com/vi/XLyPFnephpY/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>394. 【生成式AI】生成式學習的兩種策略：要各個擊破，還是要一次到位</summary><br>

<a href="https://www.youtube.com/watch?v=AihBniegMKg" target="_blank">
    <img src="https://img.youtube.com/vi/AihBniegMKg/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>395. 【生成式AI】能夠使用工具的AI：New Bing, WebGPT, Toolformer</summary><br>

<a href="https://www.youtube.com/watch?v=ZID220t_MpI" target="_blank">
    <img src="https://img.youtube.com/vi/ZID220t_MpI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>396. [2023-03-10] 【生成式AI】Finetuning vs. Prompting：對於大型語言模型的不同期待所衍生的兩類使用方式 (1/3)</summary><br>

<a href="https://www.youtube.com/watch?v=F58vJcGgjt0" target="_blank">
    <img src="https://img.youtube.com/vi/F58vJcGgjt0/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 文章重點整理

## 核心主題
本文圍繞如何通過插入適配器（Adapter）技術來提升大型語言模型（如GPT-3）的多任務處理能力展開探討，強調了在不顯著增加計算和存儲負擔的情況下，實現高效微調的重要性。

## 主要觀念
1. **大型語言模型的局限性**：當前主流的大語言模型（如GPT-3）參數量巨大，直接微調整個模型來完成多個任務在計算資源和存儲空間上存在顯著限制。
2. **適配器技術的作用**：通過插入輕量級的適配器模塊，可以在不修改原始模型參數的情況下，實現針對不同任務的高效微調。

## 問題原因
1. **多任務處理的計算成本高**：直接微調大型模型以支持多個任務需要存儲和運行多個版本的大模型，這在計算資源上是不可持續的。
2. **模型更新的影響範圍大**：傳統微調方法會修改整個模型的所有參數，導致模型性能受到全局性影響。

## 解決方法
1. **適配器插入技術**：
   - 在原始模型的不同位置（如注意力層或前饋網絡）插入輕量級的適配器模塊。
   - 這些適配器通常只包含少量新增參數，可以在保持原始模型結構完整性的前提下，實現針對特定任務的微調。

2. **高效微調策略**：
   - 只對適配器參數進行微調，而保留原始大模型的參數不變。
   - 這種方法顯著降低了計算和存儲成本，使得多任務處理變得更加可行。

## 優化方式
1. **適配器設計的靈活性**：有多種適配器插入方式可供選擇，如：
   - **BitFit**：只微調神經元偏置項。
   - **Houlsby Adapter**：在前饋網絡後增加一層新的前(feed-forward)網絡。
   - **Adapter Bias**：對前饋輸出進行平移操作。
   - **Prefix Tuning**：修改注意力機制。
   - **LoRA（Low-Rank Adaptation）**：針對注意力層的低秩適配器。

2. **根據任務特性選擇適配器位置**：不同的任務可能需要將適配器插入到模型的不同位置，以獲得最佳性能。例如：
   - LoRA在自然語言處理任務中表現優異。
   - 但LoRA在語音相關任務中的效果較差，需根據具體應用場景進行選擇。

## 結論
適配器技術為大規模多任務學習提供了一種高效且可持續的解決方案。通過插入輕量級的適配器模塊，可以在不顯著增加計算和存儲負擔的前提下，實現針對不同任務的有效微調，極大地提升了模型的實用價值。

---

# 問題清單
1. **適配器技術在現實應用中是否存在性能瓶頸？**  
   - 需要進一步研究不同類型的適配器在實際場景中的性能表現及其影響因素。

2. **如何系統化地選擇適合特定任務的適配器位置和結構？**  
   - 探索基於任務特性自動選擇最佳適配器插入位置的方法。

3. **適配器技術能否進一步降低計算資源消耗？**  
   - 研究更高效的適配器設計，以進一步降低模型微調的計算成本。

4. **多適配器並行處理是否會影響模型性能？**  
   - 探索在大型語言模型中插入多個適配器的可能性及其對模型性能的影響。

5. **適配器技術能否拓展到其他模態（如圖像或語音）？**  
   - 研究適配器技術在跨模態應用中的可行性與效果。
</details>

<details>
<summary>397. [2023-03-10] 【生成式AI】Finetuning vs. Prompting：對於大型語言模型的不同期待所衍生的兩類使用方式 (2/3)</summary><br>

<a href="https://www.youtube.com/watch?v=aZ_jXZvxyVg" target="_blank">
    <img src="https://img.youtube.com/vi/aZ_jXZvxyVg/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

### 小節分類整理

#### 1. 核心主題
- 探討大型語言模型（LLM）通過指令調適（instruction tuning）實現理解和執行複雜自然語言處理任務的能力。
- 重點分析現有方法如In-context Learning和Few-shot Learning的局限性，以及Instruction Tuning在提升模型泛化能力方面的優勢。

#### 2. 主要觀念
1. **In-context Learning**：通過提供少量示例或上下文讓模型直接理解和執行指令。然而，這種方法嚴重依賴於高質量的示例，並且在面對未見過的新指令時效果有限。
2. **Instruction Tuning**：預先訓練模型理解並響應各種人類指令，使其能夠在沒有額外示例的情況下處理新任務。

#### 3. 問題原因
- 原有的In-context Learning方法依賴於高質量的示例數據，獲取這些數據往往需要大量的人力和時間。
- 模型在面對未見過的新指令時，缺乏足夠的泛化能力，導致性能下降。

#### 4. 解決方法
1. **Instruction Tuning**：
   - 收集並整理多種自然語言處理任務的數據集。
   - 將這些任務轉化爲人類可理解的指令形式，並進行模型訓練。
   - 通過大量多樣化的人類指令訓練模型，使其能夠理解並執行各種任務。

#### 5. 優化方式
1. **數據收集與整理**：
   - 收集廣泛的自然語言處理任務數據集，涵蓋翻譯、摘要、問答等多種類型。
2. **指令轉換**：
   - 將每個NLP任務轉化爲多個不同的指令描述方式，提升模型的適應性。
3. **模型訓練策略**：
   - 使用多樣化的人類指令進行監督訓練，確保模型理解不同表達方式下的同一任務。

#### 6. 結論
- Instruction Tuning是一種有效的提升LLM理解和執行複雜自然語言處理任務能力的方法。
- 通過這種方式，模型能夠在未見過的新指令下表現出色，展現出良好的泛化能力。
</details>

<details>
<summary>398. 【生成式AI】Finetuning vs. Prompting：對於大型語言模型的不同期待所衍生的兩類使用方式 (3/3)</summary><br>

<a href="https://www.youtube.com/watch?v=HnzDaEiN_eg" target="_blank">
    <img src="https://img.youtube.com/vi/HnzDaEiN_eg/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

<details>
<summary>399. [2023-03-17] 【生成式AI】大模型 + 大資料 = 神奇結果？(1/3)：大模型的頓悟時刻</summary><br>

<a href="https://www.youtube.com/watch?v=SaZTJJNOCOY" target="_blank">
    <img src="https://img.youtube.com/vi/SaZTJJNOCOY/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

### 文章重點整理

#### 核心主題
1. 大模型在特定任務中的表現優於中小模型。
2. 中小型模型在某些任務中可能表現不佳，甚至劣於隨機猜測。
3. 模型規模與性能之間的複雜關係。

#### 主要觀念
1. **規模效應**：大型模型在計算期望值等複雜任務中表現更優。
2. **陷阱任務（Distractor Task）**：某些任務設計包含誤導因素，導致部分模型失敗。
3. **一知半解的風險**：中小型模型可能因有限的理解能力而做出錯誤判斷。

#### 問題原因
1. 中小型模型缺乏足夠的參數容量，無法處理複雜的上下文信息。
2. 一些任務需要重新定義基本概念（如π=10），中小型模型難以適應。
3. 模型在推理過程中未能正確計算期望值或忽視了潛在的陷阱。

#### 解決方法
1. **增加模型規模**：使用更大的參數量以提升模型的理解和處理能力。
2. **任務設計優化**：明確任務要求，減少誤導因素。
3. **混合專家模型（Mixture-of-Expert）**：通過並行計算提高效率，同時降低資源消耗。

#### 優化方式
1. 使用混合專家模型結構，在推理時僅調用部分模組以節省資源。
2. 重新定義模型架構，如Switch Transformer，以適應超大規模參數。
3. 在訓練過程中逐步增加任務的複雜度，提升模型的適應能力。

#### 結論
1. 模型規模與性能呈非線性關係，存在最佳規模點。
2. 超大規模模型在特定任務中表現更優，但需考慮計算資源限制。
3. 未來研究應關注如何平衡模型規模與效率，優化模型設計。
</details>

<details>
<summary>400. 【生成式AI】大模型 + 大資料 = 神奇結果？(2/3)：到底要多少資料才夠</summary><br>

<a href="https://www.youtube.com/watch?v=qycxA-xX_OY" target="_blank">
    <img src="https://img.youtube.com/vi/qycxA-xX_OY/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>


</details>

