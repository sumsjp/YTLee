臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw/
稍微講一下，sparse reward 的 problem，然後再下課
我們稍微講一下 sparse reward problem，然後再下課
那個 sparse reward 是什麼意思呢？
就是實際上當我們在 learn agent
用 reinforcement learning learn agent 的時候
多數的時候 agent 都是沒有辦法得到 reward 的
那在沒有辦法得到 reward 的情況下
對 agent 來說它的訓練是非常困難的
假設説舉例來說，假設你今天要訓練一個機器手臂
然後桌上有一個螺絲釘跟螺絲起子
那你要訓練他用螺絲起子把螺絲釘栓進去
那這個很難，為什麼？因為你知道一開始你的 agent
它是什麼都不知道的
它唯一能夠做不同的 action 的原因
是因為 exploration
舉例來說你在做 Q learning 的時候
你會有一些隨機性
讓它去採取一些過去沒有採取過的 action
那你要隨機到說它把螺絲起子撿起來
再把螺絲栓進去，然後就會得到 reward 1
這件事情是永遠不可能發生的
所以，你會發現說
不管今天你的 actor 它做了什麼事情
它得到 reward 永遠都是 0
對它來說不管採取什麼樣的 action
都是一樣糟或者是一樣的好
所以，它最後什麼都不會學到
所以，今天如果你環境中的 reward 非常的 sparse
那這個 reinforcement learning 的問題
就會變得非常的困難
但是對人類來說，人類很厲害
人類可以在非常 sparse 的 reward 上面去學習
就我們的人生通常多數的時候我們就只是活在那裡
都沒有得到什麼 reward 或者是 penalty
但是，人還是可以採取各種各式各樣的行為
所以，一個真正厲害的人工智慧
它應該能夠在 sparse reward 的情況下
也學到要怎麼跟這個環境互動
所以，接下來我想要跟大家很快的
非常簡單的介紹
就是一些handle sparse reward 的方法
在這個作業 4-3 裡面
除了要做 Actor-Critic 以外
那會要求大家做 Actor-Critic
然後比較一下說
你現在的 Actor-Critic 如果做在 4-2 上，有沒有比較好
如果做在 4-1 上，有沒有比較好
那當然是希望應該是要有比較好
那接下來會希望你 implement 某一個方法
來 improve 你的 model，
那這個方法是什麼呢？你就自己決定
你不一定要 implement 很難的方法
你就看你自己想要做什麼
舉例來說，我們等一下會講一些
handle sparse reward 的方法
你就看看說裡面有沒有是你用得上的
我們會講 imitation 類的方法
看你要不要做一下 imitation learning
你可以給你的 agent 一些示範
看看它能不能夠學得比較好
等等就想一個你喜歡的方法
來 improve 你的 model
那怎麼解決 sparse reward 的這件事情呢？
我們等一下會講三個方向
我們先講第一個方向
第一個方向是常容易理解的
第一個方向叫做 reward shaping
reward shaping 是什麼意思呢？
reward shaping 的意思是說
環境有一個固定的 reward
它是真正的 reward
但是我們為了引導 machine
為了引導 agent
讓他學出來的結果是我們要的樣子
這個 developer，就是我們人類
刻意的去設計了一些 reward
來引導我們的 agent
舉例來說，如果是把人類當作 agent
小孩當作一個 agent 的話
那一個小孩，他可以 take 兩個 actions
一個 action 是他可以出去玩
那他出去玩的話
在下一秒鐘它會得到 reward 1
但是他可能在月考的時候，成績會很差
所以，在 100 個小時之後呢
他會得到 reward -100
然後，他也可以決定他要唸書
然後在下一個時間，因為他沒有出去玩
所以他覺得很不爽，所以他得到 reward -1
但是在 100 個小時後，他可以得到 reward 100
但是，對一個小孩來說
他可能就會想要 take play
而不是 take study
因為，今天我們雖然說
我們計算的是 accumulated reward
但是，也許對小孩來說
他的 discount factor 很大這樣
所以，未來的 reward
他就不太在意未來的 reward
而且也許因為他是一個小孩
他還沒有很多 experience
所以，他的 Q function estimate 是非常不精準的
所以要他去 estimate 很遙遠以後
會得到的 accumulated reward
他其實是預測不出來的
所以怎麼辦呢？這時候大人就要引導他
怎麼引導呢？就騙他說
如果你坐下來唸書我就給你吃一個棒棒糖
所以，對他來說
下一個時間點會得到的 reward 就變成是 positive 的
所以他就覺得說，也許 take 這個 study 是比 play 好的
雖然實際上這並不是真正的 reward
而是其他人去騙他的 reward
告訴他說你採取這個 action是好的
所以，我給你一個 reward
雖然這個不是環境真正的 reward
所以 reward shaping 的概念是一樣的
簡單來說，就是你自己想辦法 design 一些 reward
他不是環境真正的 reward
有在玩 ATARI 遊戲裡面
真的 reward 是那個遊戲的主機給你的 reward
但是你自己去設一些 reward
好引導你的 machine
做你想要它做的事情，舉例來說這個例子是
Example demo
所以，接下來就是介紹各種你可以自己加進去
然後，In general 看起來是有用的 reward
舉例來說，一個技術是
給 machine 加上 curiosity
給它加上好奇心
所以叫 curiosity driven 的 reward
那這個是我們之前講 Actor-Critic 的時候看過的圖
所以，我們有一個 reward function
它給你某一個 state
給你某一個 action
它就會評斷說
在這個 state 採取這個 action 得到多少的 reward
那我們當然是希望 total reward 越大越好
那在 curiosity driven 的這種技術裡面
你會加上一個新的 reward function
這個新的 reward function 叫做 ICM
Intrinsic curiosity module
它就是要給機器加上好奇心
這個 ICM，它會吃 3 個東西
它會吃 state, s1，它會吃 action, a1 跟 state, s2
根據 s1, a1, a2，它會 output 另外一個 reward
我們這邊叫做 r1(i)
那你最後你的 total reward，對 machine 來說
total reward 並不是只有 r 而已，還有 r(i)
它不是只有把所有的 r 都加起來
他把所有 r(i) 加起來當作 total reward
所以，它在跟環境互動的時候
它不是只希望 r 越大越好
它還同時希望 r(i) 越大越好
它希望從 ICM 的 module 裡面
得到的 reward 越大越好
那這個 ICM 呢
它就代表了一種 curiosity
那怎麼設計這個 ICM？讓它變成一種
讓他有類似這種好奇心的功能呢？
這個是最原始的設計
這個設計是這樣
我們說 curiosity module 就是 input 3 個東西
input 現在的 state
input 在這個 state 採取的 action
然後接下來 input 下一個 state, s(t+1)
然後接下來會 output 一個 reward, r(i)
那這個 r(i) 怎麼算出來的呢？
在 ICM 裡面，你有一個 network
這個 network 會 take a(t) 跟 s(t)
然後呢，去 output s(t+1) hat
也就是這個 network 做的事情
是根據 a(t) 跟 s(t)
去 predict 接下來我們會看到的 s(t+1) hat
你會根據現在的 state
跟在現在這個 state 採取的 action
我們有另外一個 network 去預測
接下來會發生什麼事
接下來再看說，machine 自己的預測
這個 network 自己的預測
跟真實的情況像不像
越不像，那越不像那得到的 reward 就越大
所以今天這個 reward 呢
它的意思是說，如果今天未來的 state
越難被預測的話，那得到的 reward 就越大
這就是鼓勵 machine 去冒險
現在採取這個 action，未來會發生什麼事
越沒有辦法預測的話
那這個 action 的 reward 就大
所以，machine 如果有這樣子的 ICM
它就會傾向於採取一些風險比較大的 action
它想要去探索未知的世界
它想要去看看說，假設某一個 state
是它沒有辦法預測
假設它沒有辦法預測未來會發生什麼事
它會特別去想要採取那種 state
可以增加 machine exploration 的能力
那這邊這個 network 1
其實是另外 train 出來的
大家瞭解我的意思嗎？
這個 training 的時候
你這個 network 1，它就是
在 training 的時候
你會給它 at, st, s(t+1)
然後讓這個 network 1 去學說 given at, st
怎麼 predict s(t+1) hat
apply 到 agent 互動的時候
這個 ICM module，其實要把它 fix 住
那大家知道我的意思的話
其實，這一整個想法裡面
是有一個問題的，這個問題是什麼呢？
這個問題是，某一些 state
它很難被預測，並不代表它就是好的
它就應該要去被嘗試的
舉例來說，俄羅斯輪盤的結果也是沒有辦法預測的
並不代表說，人應該每天去玩俄羅斯輪盤這樣子
所以，今天光是告訴 machine，鼓勵 machine 去冒險
是不夠的，因為如果光是只有這個 network 的架構
machine 只知道說什麼東西它無法預測
如果在某一個 state 採取某一個 action
它無法預測接下來結果
它就會採取那個 action
但並不代表這樣的結果一定是好的
舉例來說，可能在某個遊戲裡面
背景會有風吹草動，會有樹葉飄動
那也許樹葉飄動這件事情，是很難被預測的
對 machine 來說它在某一個 state 什麼都不做
看著樹葉飄動
然後，發現這個樹葉飄動是沒有辦法預測的
接下來它就會一直站在那邊，看樹葉飄動
所以說，光是有好奇心是不夠的
還要讓它知道說，什麼事情是真正重要的
那怎麼讓 machine 真的知道說什麼事情是真正重要的
而不是讓他只是一直看樹葉飄動呢？
你要加上另外一個 module
我們要 learn 一個 feature 的 extractor
這個黃色的格子代表 feature extractor
它是 input 一個 state
然後 output 一個 feature vector
代表這個 state，那我們現在期待的是
這個 feature extractor 可以做的事情是把那種
沒有意義的畫面
state 裡面沒有意義的東西把它濾掉
比如說風吹草動，白雲的飄動，樹葉的飄動這種
沒有意義的東西直接把它濾掉
我們希望這個 feature extractor 可以做到這個功能
那我們等一下再講說，這件事情是怎麼做到的
假設這個 feature extractor
真的可以把無關緊要的東西，濾掉以後
那我們的 network 1 實際上做的事情是
給它一個 actor
給他一個 state s1 的 feature representation
讓它預測，state, s(t+1) 的 feature representation
然接下來我們再看說，這個預測的結果
跟真正的 state, s(t+1) 的 feature representation 像不像
越不像，reward 就越大
接下來的問題就是
怎麼 learn 這個 feature extractor 呢？
讓這個 feature extractor 它可以把無關緊要的事情濾掉呢？
這邊的 learn 法就是
learn 另外一個 network 2
這個 network 2 啊
它是吃這兩個 vector 當做 input
然後接下來它要 predict action a 是什麼
然後它希望呢這個 action a
跟真正的 action a，越接近越好
我突然發現我這個地方其實寫得沒有很對，你有發現嗎？
這裡這個 a 跟 a hat，我應該要反過來吧
預測出來的東西我們用 hat 來表示
真正的東西沒有 hat，應該是這樣感覺比較對
所以 at 跟 at hat 應該是反過來的
所以這個 network 2，它會 output 一個 action
就根據 state, st 的 feature 跟 state, s(t+1) 的 feature
它 output 説，從 state, st，跳到 state, s(t+1)
要採取哪一個 action，才能夠做到
那希望這個action 跟真正的 action，越接近越好
那加上這個 network 的好處就是
因為這兩個東西要拿去預測 action
所以，今天我們抽出來的 feature
就會變成是跟 action
跟預測 action 這件事情是有關的
所以，假設是一些無聊的東西
是跟 machine 本身採取的 action 無關的東西
風吹草動或是白雲飄過去
是 machine 自己要採取的 action 無關的東西
那就會被濾掉
就不會被放在抽出來的 vector representation 裡面
那剛才講的是 reward shaping 的方法
那接下來我們要講 Curriculum learning 這件事情
那 Curriculum learning 不是
reinforcement learning 所獨有的概念
那其實在很多 machine learning
尤其是 deep learning 裡面
你都會用到 Curriculum learning 的概念，舉例來說
所謂 Curriculum learning 的意思是説
你為機器的學習啊，做規劃
你給他餵 training data 的時候，是有順序的
那通常都是由簡單到難，就好比說，
假設你今天要交一個小朋友作微積分，
他做錯就打他一巴掌
可是他永遠都不會做對，太難了
你要先教他 99 乘法，然後才教他微積分
打死他，他都學不起來這樣
所以很難，所以 Curriculum learning 的意思就是在教機器的時候
從簡單的題目，教到難的題目
那如果不是 reinforcement learning
一般在 train deep network 的時候
你有時候也會這麼做，舉例來說，在 train RNN 的時候
已經有很多的文獻，都 report 説
你給機器先看短的 sequence
再慢慢給它長的 sequence，通常可以學得比較好
那用在 reinforcement learning 裡面
你就是要幫機器規劃一下它的課程
從最簡單的到最難的， 舉例來說
在 Facebook 玩 VizDoom 的 agent 裡面
Facebook 那個 VizDoom 的 agent 據說蠻強的
他們在參加這個 VizDoom 的比賽
機器的 VizDoom 比賽是得第一名的
他們是有為機器規劃課程的
先從課程 0 一直上到課程 7
在這個課程裡面
那些怪就是有不同的 speed 跟 health
怪物的速度跟血量是不一樣的
所以，在越進階的課程裡面
怪物的速度越快，然後他的血量越多
在 paper 裡面也有講說
如果直接上課程 7，machine 是學不起來的
你就是要從課程 0 一路玩上去
這樣 machine 才學得起來
所以，再拿剛才的這個
把藍色的板子放到柱子上的時間
怎麼讓機器一直從簡單學到難呢？
也許一開始你讓機器初始的時候
它的板子就已經不在柱子上了
這個時候，你要做的事情只有
這個時候，機器要做的事情只有把藍色的板子壓下去
就結束了，這比較簡單，它應該很快就學的會
它只有往上跟往下這兩個選擇嘛
往下，就得到 reward 就結束了，他也不知道學的是甚麼
這邊是把板子挪高一點
挪高一點，所以它有時候會很笨的往上拉，就拿出來了
如果它這個學得會的話，這個也比較有機會學得會
假設它現在學的到說，只要板子接近柱子
它就可以把這個板子壓下去的話
接下來，你再讓它學更 general 的 case
先讓一開始，板子離柱子遠一點
然後，板子放到柱子上面的時候
它就會知道把板子壓下去
這個就是 Curriculum Learning 的概念
當然 Curriculum learning 這邊有點 ad hoc
就是你需要人，當作老師去為機器設計它的課程
那有一個比較 general 的方法叫做
Reverse Curriculum Generation
你可以用一個比較通用的方法
來幫機器設計課程
這個比較通用的方法是怎麼樣呢？
假設你現在一開始有一個 state, sg
這是你的 gold state，也就是最後最理想的結果
如果是拿剛才那個板子和柱子的例子的話
就把板子放到柱子裡面
這樣子叫做 gold state
你就已經完成了，或者你讓機器去抓東西
你訓練一個機器手臂抓東西
抓到東西以後叫做 gold state
那接下來你根據你的 gold state
去找其他的 state，這些其他的 state
跟 gold state 是比較接近的
舉例來說，如果是讓機器抓東西的例子裡面
你的機器手臂可能還沒有抓到東西
假裝這些跟 gold state 很近的 state 我們叫做 s1
你的機械手臂還沒有抓到東西
但是，它離 gold state 很近，那這個叫做 s1
至於什麼叫做近，這個就麻煩
就是 case dependent，你要根據你的 task
來 design 説怎麼從 sg sample 出 s1
如果是機械手臂的例子，可能就比較好想
其他例子可能就比較難想
接下來呢，你再從這些 state 1 開始做互動
看它能不能夠達到 gold state, sg
那每一個 state，你跟環境做互動的時候
你都會得到一個 reward, R
接下來，我們把 reward 特別極端的 case 去掉
reward 特別極端的 case 的意思就是說
那些 case 它太簡單，或者是太難
就 reward 如果很大，代表說這個 case 太簡單了
就不用學了，因為機器已經會了
它可以得到很大的 reward
那 reward 如果太小代表這個 case 太難了
依照機器現在的能力這個課程太難了
它學不會，所以就不要學這個
所以只找一些 reward 適中的 case
那當然什麼叫做適中，這個就是你要調的參數嘛
找一些 reward 適中的 case
接下來，再根據這些 reward 適中的 case
再去 sample 出更多的 state，更多的 state
就假設你一開始，你的東西在這裡
你機械手臂在這邊，可以抓的到以後
接下來，就再離遠一點，看看能不能夠抓得到
又抓的到以後，再離遠一點
看看能不能抓得到，這個方法很直覺
但是，它是一個有用的方法就是了
特別叫做 Reverse Curriculum learning
那剛才講的是 Curriculum learning
就是你要為機器規劃它學習的順序
那最後一個要跟大家講的 tip
叫做 Hierarchical Reinforcement learning
有階層式的 reinforcement learning
因為它說從 gold state 去反推
就是說你原來的目標是長這個樣子
我們從我們的目標去反推，所以這個叫做 reverse
接下來要講階層式的 Reinforcement learning
所謂階層式的 Reinforcement learning 是說
我們有好幾個 agent
然後，有一些 agent 負責比較 high level 的東西
它負責訂目標，然後它訂完目標以後
再分配給其他的 agent，去把它執行完成
那這樣的想法其實也是很合理的
因為我們知道說，我們人啊
在一生之中，我們並不是時時刻刻都在做決定
舉例來說，假設你想要寫一篇 paper
那你會先想說我要寫一篇 paper 的時候
我要做那些 process，就是說我先想個梗這樣子
然後想完梗以後，你還要跑個實驗
跑完實驗以後，你還要寫
寫完以後呢，你還要這個去發表這樣子
那每一個動作下面又還會再細分
比如說，怎麼跑實驗呢？
你要先 collect data，collect 完 data 以後
你要再 label，你要弄一個 network
然後又 train 不起來，要 train 很多次
然後，重新 design network 架構好幾次
最後才把 network train 起來
所以，我們要完成一個很大的 task 的時候
我們並不是從非常底層的那些 action，開始想起
我們其實是有個 plan
我們先想說，如果要完成這個最大的任務
那接下來要拆解成哪些小任務
每一個小任務要再怎麼拆解成，小小的任務
這個是我們人類做事情的方法
舉例來說，叫你直接寫一本書可能很困難
但叫你先把一本書拆成好幾個章節
每個章節拆成好幾段，每一段又拆成好幾個句子，
每一個句子又拆成好幾個詞彙
這樣你可能就比較寫得出來
這個就是階層式的 Reinforcement learning 的概念
這邊是隨便舉一個好像可能不恰當的例子
就是假設校長跟教授跟研究生通通都是 agent
那今天假設我們的 reward 就是
只要進入百大就可以得到 reward 這樣
假設進入百大的話，校長就要提出願景
告訴其他的 agent 説
現在你要達到什麼樣的目標
那校長的願景可能就是說
教授每年都要發三篇期刊，然後接下來
這些 agent 都是有階層式的，所以上面的 agent
他的 action 他所提出的動作
他不真的做事，他的動作就是提出願景這樣
那他把他的願景傳給下一層的 agent
下一層的 agent 就把這個願景吃下去
如果他下面還有其他人的話，它就會提出新的願景
比如說，校長要教授發期刊
但是其實教授自己也是不做實驗的
所以，教授也只能夠叫下面的苦命研究生做實驗
所以教授就提出願景，就做出實驗的規劃
然後研究生才是真的去執行這個實驗的人
然後，真的把實驗做出來
最後大家就可以得到 reward
這個例子其實有點差啦
為什麼說這個例子其實有點差呢？
因為真實的情況是，校長其實是不會管這些事情的
校長並不會管教授有沒有發期刊
而且發期刊跟進入百大其實關係也不大
而且更退一步說好了，我們現在是沒有校長的
所以，現在顯然這個就不是指台大
所以，這是一個虛構的故事
我隨便亂編的，沒有很恰當
那現在是這樣子的，在 learn 的時候
其實每一個 agent 都會 learn
那他們的整體的目標，就是要達成
就是要達到最後的 reward
那前面的這些 agent，他提出來的 actions
就是願景這樣
你如果是玩遊戲的話
他提出來的就是，我現在想要產生這樣的遊戲畫面
然後，下面的能不能夠做到這件事情
上面的人就是提出願景
但是，假設他提出來的願景
是下面的 agent 達不到的，那就會被討厭
舉例來說，教授對研究生
都一直逼迫研究生做一些很困難的實驗
研究生都做不出來的話
研究生就會跑掉，所以他就會得到一個 penalty
所以t，如果今天下層的 agent
他沒有辦法達到上層 agent 所提出來的 goal 的話
上層的 agent 就會被討厭
它就會得到一個 negative reward
所以，他要避免提出那些願景是 底下的 agent 所做不到的
那每一個 agent 他都是吃
上層的 agent 所提出來的願景
當作輸入，然後決定他自己要產生甚麼輸出
決定他自己要產生什麼輸出
但是你知道說，就算你看到
上面的的願景說，叫你做這一件事情
你最後也不見得，做得到這一件事情，對不對？
假設，本來教授目標是要寫期刊
但是不知道怎麼回事，他就要變成一個 YouTuber
這個 paper 裡面的 solution，我覺得非常有趣
給大家做一個參考
這其實本來的目標是要寫期刊，但卻變成 YouTuber
那怎麼辦呢 ?
把原來的願景改成變成 YouTuber
就結束了，這樣子
在 paper 裡面就是這麼做的，為甚麼這麼做呢 ?
因為雖然本來的願景是要寫期刊
但是，後來變成 YouTube
難道這些動作都浪費了嗎 ?
不是，這些動作是沒有被浪費的
我們就假設說，本來的願景
其實就是要成為 YouTuber
那你就知道說，成為 YouTuber 要怎做了
這個細節我們就不講了，你自己去研究一下 paper
這個是階層式 RL，可以做得起來的 tip
那這個是真實的例子
真實的例子，就是給大家參考一下
實際上呢，這裡面就做了一些比較簡單的遊戲
這個是走迷宮，藍色是 agent
藍色的 agent 要走走走，走到黃色的目標
這邊也是
這個單擺要碰到黃色的球
那願景是甚麼呢？在這個 task 裡面
它只有兩個 agent 啦，只有下面的一個
最底層的 agent 負責執行，決定說要怎麼走
還有一個上層的 agent，負責提出願景
雖然，實際上你 general 而言可以用很多層
但是，paper 我看那個實驗，主要是這樣子
那今天這個例子是說
粉紅色的這個點，代表的就是願景
上面這個 agent，它告訴藍色的這個 agent 說
你現在的第一個目標是先走到這個地方
藍色的 agent 走到以後，再說你的新的目標是走到這裡
藍色的 agent 再走到以後，新的目標在這裡
接下來又跑到這邊
然後，最後希望藍色的 agent 就可以走到黃色的
這個位置，這邊也是一樣，就是
粉紅色的這個點，代表的是目標
代表的是上層的 agent 所提出來的願景，所以
這個 agent 先擺到這邊
接下來，新的願景又跑到這邊，所以它又擺到這裡
然後，新的願景又跑到上面
然後又擺到上面，最後就走到黃色的位置了
這個就是 hierarchical 的 Reinforcement Learning
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
