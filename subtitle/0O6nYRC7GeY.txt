臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心http://ai.ntu.edu.tw
有講到 deep learning 這樣子 剛才講的是一個 general 的 case
現在我們來講 deep learning
講真正的 deep learning 之前
我們先來分析 deep 的 linear 的 network
他長什麼樣子，他的 loss function 長什麼樣子
我們先來假設一個非常非常簡單的
deep 的 linear network
這個 deep linear network，他只有兩個 neuron
他只有兩個 neuron，他的 input 就只有一維，就是 x
他的每一個 neuron
都沒有 nonlinear 的 activation function
每一個 neuron 的 activation function 都是 linear
每一個 neuron 就只有一個 weight，w1 跟 w2
今天把一個 x 代進去
他只會 output 一個 value y
y 顯然就是 x * w1 * w2，就得到 y，就結束了
我們今天在做 deep learning 的時候
你會給這個 network training data
我們假設現在 training data 只有一筆
也就是說，input 是 1，x = 1 的時候
我們希望 network 的 output，跟 1 愈接近愈好
我們現在有這樣子的，一個
有現在這樣一個 linear 的 network
他只有兩個 weight，因為他只有兩個 weight
我們甚至可以把他在一個二維平面上畫出來
其中一維是
w1 的值的變化
另外一維是
w2 的值的變化
然後
這個圖上可能你就畫一些等高線
畫一些等高線，代表他的 loss
代表他的 loss，我們寫作 L
你要不要猜猜看
這個
現在這一個 loss function
他是不是 convex 的
我們想想看
假設我們有一個
先問一下大家，給你想個三秒鐘
這一個問題
這個問題
你覺得他的 loss function 是 convex 的同學，舉手一下
有一些同學覺得是
你覺得他不是 convex 的同學舉手一下
有一些同學
有人覺得是 convex，有人覺得不是 convex
那我們再給你一個提示吧
我們再考慮一個更簡單的 network
這個 network 只有一個參數
他只有一個 neuron，只有一個參數叫 w
input x，output y = wx
這是一個，這一個 network
他是一個 linear 的 function
這一個 network
他就跟 linear regression 是一模一樣的
如果我們今天考我們機器學習那門課
第一堂課就講 linear regression
他跟 linear regression 是一樣的
input 是 1，那希望他的 output
跟 1 愈接近愈好
這一個 network
如果我們橫軸是 w
縱軸是他的 loss
他是 convex 的
如果今天 w = 1
這個時候他的 loss 會是 0
當 w 偏離 1愈來愈多
loss 就會逐漸上升
這是一個二次曲線
他長這個樣子
他是 convex 的
那你會說
這一個 function，w1 跟 w2 的 function
跟這個只有 w 的 function
他們有什麼不同嗎？其實沒什麼不同
你只是把這個 w 拆成 w1 乘上 w2 而已
這樣講完以後，你覺得
他是一個 convex 的 function 嗎？
你覺得他是一個 convex 的 function 的同學舉手一下
沒有人舉手，你覺得他 不是一個 convex function 的同學舉手一下
還是有一些同學覺得他不是 convex 的 function
我們實際上把它畫出來看看是什麼樣子
實際上畫出來，這個圖不是我自己畫的
我是截了那個 Ian J. Goodfellow 的 paper
是長這個樣子的
所以他不是 convex 的
今天這個顏色<，代表 loss 的值
越偏藍，代表 loss 的值越小
他不是 convex 的，他很神奇就是
同樣是 linear 的 function
只有一個 weight 的時候
他是 convex 的
今天有兩個 weight 的時候
你的 loss 對 weight 來說，就不是 convex 了
他是一個 linear 的 function，但他的 loss 不是 convex
很多人聽到 linear function， 就覺得說
他是 convex，沒有，這只有在這個 case 才是 convex 的
這是一個 linear 的 function
但他的 loss 就已經不是 convex 的了
他的 loss 長什麼樣，他的 loss 長這個樣子
在這個地方
跟這個地方
他的 loss 是最低的
loss 最低的值，可以是
可以是 0 對不對，因為你看
w1 代 1，w2 代 1，loss 就是 0 了
所以，w1 代 1，w2 代 1
loss 就是 0 了，當然還有其他的組合就是了
連成一條曲線
這個圖上，黑色的箭頭代表 gradient 的方向
所以，在這個地方你就順著 gradient
方向走走走走走，走到這個
local minima 的地方
不過這個 local minima 也是 global minima
因為他整個 function 裡面是最小的地方
然後，這個綠色的線代表是 global minima 的 manifold
他只是把 global minima 連起來而已
紅色的線是一個 saddle point，其實在 (0,0) 這個地方
你就自己等一下算一下
他有一個 saddle point 在 (0,0) 這個地方
他 gradient 是 0，但他不是 local minima
也不是 local maxima
因為從這個方向走會增加
從這個方向走也是會變小
然後，藍色的這一條線是說
假設你這邊是起始點
gradient descent 會這樣子走
走走走走走，走進去
然後，今天這條粉紅色的線是要告訴我們說
他不是一個 convex 的 function
對不對，他不是一個
他不是一個 convex 的 function
因為把這兩邊連起來的時候
我們會發現說這個曲面上的這個值
是比紅色這一條線連起來的值還要大的
剛才是真的因為你只有兩個參數
你真的把他硬畫出來以後
看起來是這個樣子
如果今天沒有辦法硬畫呢？
我們就來分析一下
我們知道說
現在這個 loss = (y/hat - w1 * w2 * x)^2
然後今天 y/hat 跟 x 都代 1
都代 1，所以變 (1 - w1 * w2)^2
然後，我們計算一下他的 gradient
你把 L 對 w1 算 gradient，對 w2 算 gradient
你得到這樣的式子
你得到這樣子的式子
然後接下來你就可以分析看看說
什麼情況下
你會得到 critical point
什麼情況下 gradient 是 0
有兩個可能，一個可能是說
我把圖都畫出來好了
我右上角本來有一個小圖啦
但不知道為什麼，他在這個螢幕上
沒有辦法顯示出來
什麼情況下，他會有
這個 critical point 呢，一個可能是 w1
跟 w2 相乘等於 1 的時候，對不對？
所以假設這一項是 0，這一項是 0
他是一個 critical point
這個 critical point，他是 local minima
我們不需要想這件事情
因為 w1 * w2 = 1
這就是一個 global minima
今天有一些 critical point 他可以讓 w1 * w2 = 1
他是global minima
另外一個可能是 w1 = 0，w2 = 0
有在原點的地方，他是另外一個 critical point
這個 critical point 長什麼樣子呢？
如果我們今天算一下他的 hashing 的話
我們現在把 hashing 的 matrix 算出來
把 hashing matrix 算出來長這樣
那我們把
w1 跟 w2 都代 0，也就是考慮
原點那個地方的 hashing
我們會發現那個 hashing 算出來呢
那個 hashing 算出來呢
是 0、-2 、-2
那接下來我們要考慮一下
這一個 hashing 他是
saddle point 還是 local minima
還是 local maximum 呢
那你就要找一下他的
我這邊有沒有算錯啊
應該
應該沒有
今天這個 h，我們今天要找他的
我們要算一下他是 local minima 還是 saddle point
那我們就要看一下他的 eigen vector
那他有什麼樣的 eigen vector 呢
你可以自己回去翻一下線代課本看這個怎麼算
我心算一下以後覺得應該是 (1,1) 跟 (1,-1)
直覺就是這個樣子
那你再算一下他的 eigen value，λ1
是什麼？你把 (1,1) 乘進去，所以是 -2
λ2 是 2
所以現在有正的 eigen value，有負的 eigen value
我們剛才講說有正的 eigen value，有負的 eigen value
他是一個 saddle point
當我們今天從原點的地方往 1 的方向走
會減少 -2，往 (1,-1) 的方向走<，會增加 2
不知道為什麼那個圖沒有顯示出來，我們把他
如果有錄音的話，他就會彈出一個要不要按確認
所以，如果沒有彈出來就代表說
沒有錄到音
不過沒有關係剛才講的那個也還蠻 trivial 的
剛才講的還蠻簡單的，所以
我剛才想要表的事情是說
我們在原點這個地方
他是一個 saddle point，往 (1,1) 走會變小
原點這個地方是往 (1,1) 走會變小呢？
是，往 (1, -1) 走會變大
x 是 1
y 是 -1，往 (1,-1) 走這個方向會變大，沒錯
跟這個圖上看起來的是一模一樣
我們今天知道說在做 gradient descent 的時候
你最後可能會停在一個 saddle point
也可能會停在一個 local minima
停在 local minima 沒有問題
因為今天這個地方 local minima 就是 global minima
停在 saddle point 就有問題了
因為 saddle point 不是我們要的
他不是一個 global minima
他的 loss 是很大的，他的 loss 不是最小的
但是，今天在這個問題裡面
你不太需要害怕 saddle point，為什麼？
因為這個 saddle point，他非常容易被逃出去
首先，假設
你從這個圖上的任何一個點開始
你要最後走到 saddle point 是還蠻困難的
要怎麼樣才能走到 saddle point
你要正好初始在這個對角線的地方
順著 gradient 走，你才能夠走到 saddle point
如果你稍微偏一點，像這個 case 一樣
你就錯過 saddle point，你就走到 global minima 去了
所以，現在這個 case 裡面
其實不太容易卡在 saddle point
就算你不幸卡在 saddle point
也沒有關係，因為你可以算 hashing
你算了 hashing 以後
hashing 可以告訴你說
往哪個地方走，可以讓 loss 下降
你就可以逃出那個 saddle point
往 loss 比較小的地方走
這個是只有兩個 neuron 的狀況
我們現在來考慮一個更複雜的狀況是有三個 neuron
也就是有兩個 hidden layer 的狀況
當我們今天有三個 neuron 的時候
我們畫出來的 loss
這個 loss， L = (1 - w1 w2*w3)^2
接下來，你知道有三個參數所以就不太好畫圖
所以，這個時候你就用 gradient 跟 hashing 來分析一下
今天這個 lossＬ應該長什麼樣子
我們現在把這個 lossＬ
分別對 w1 w2 和 w3，做偏微分
做完偏微分以後，我們就可以知道說
哪些地方是 critical point
光知道 critical point 還不夠
我們要至少算出 hashing
才能夠知道說這個 critical point，他的性質是什麼
那這個 hashing
如果今天這個 hashing 算出來啊
理論上應該要有 3*3，9 個值啦
有點麻煩，我這邊就隨便舉一個
隨便舉一個例子，就 w1 偏微分兩次
跟先對 w1 做偏微分，再對 w2 做偏微分的結果
那你可以輕易的想像說，剩下的值長什麼樣子
我們先看一下 critical point 吧
有哪些地方可能是 critical point 呢
一個可能是 w1w2w3 = 1
如果 w1w2w3 都 = 1 會發生什麼事呢？
這三個偏微分的值
也就是 gradient 的第一項都是 0
所以 gradient = 0
那還有另外兩個地方，兩個可能性
可能會製造出 critical point
一個可能是 w1w2w3 都是 0，也就是在原點的地方
如果 w1w2w3 都是 0 的話
後面這一項會是 0，所以這邊也是一個 critical point
那其實不需要三項都是 0
才能夠製造 critical point
其實只要三項裡面的兩項是 0，不一定要 w1 w2 是 0
只要三項
我這個想要表達意思是說，只要三項裡面的兩項是 0
舉例來說，w1 w2 是 0 就可以
讓他變成一個 critical point
接下來，我們來分析說這三個 critical point
他分別是什麼樣的狀況
第一個 case，w1 w2 w3 = 0
他是一個 global minima
你說沒分析還沒算 hashing
你怎麼知道他是 global minima
你要至少先算個 hashing
看看他的那個 eigen value 是不是都是正的才知道
他是不是一個 local minima
你又不知道他是不是 global minima
但是今天這個 case 是比較特殊的
我知道說 w1 w2 w3
這個 loss 的最小值就是 0
w1 w2 w3 = 1 的時候可以讓 loss 是 0
最小值是 0，這邊正好讓 loss 是 0
所以，他就已經是 global minima，沒有別的可能了
所以，這個問題、這個 critical point 就不用分析了
那分析 w1 = w2 = w3 = 0
如果 w1 w2 w3 都是 0 的話
你就算一下他的 hashing，發現他的 hashing
是一個 zero matrix
hashing 是 zero matrix 是一件很可怕的事情，因為他的
eigen value 都是 0，意味著說
如果我們今天用 hashing 來考慮這一個
原點的地方的話
hashing 告訴你說，不管往什麼方向走
都是 0，就如果我們不考慮更高的次數
更高的項，只考慮 hashing 那一項的話
這個 hashing 告訴我們說，在原點的地方
不管往哪個方向走，都是 0
所以你今天如果走走走，走到
hashing 那個地方，你就逃不出去了，因為
你就算是算了 hashing
你仍然不知道哪個地方可以讓你的 loss
變小，除非你真的
就是挪動一下你的數值去試試看，不然你
光是算 gradient，光是算 hashing
你就逃不出這個地方了，因為這個 hashing 告訴我們說
不管往哪個方向走，值都不會增加
也不會減少，他是不增不減
你永遠都逃不出去了，所以這是一個可怕的
可怕的 critical point
我們其實，你如果憑著直覺想你知道說
他是一個 saddle point，就有些
在原點的地方往旁邊走，有些時候
可以讓 loss 增加，有些時候可以讓 loss 減小，他是一個 saddle point
但是從 hashing 我們看不出來，從 hashing 這邊我們只知道說
這邊是一個黑洞，陷進去
以後就出不來了
那如果是 w1 w2 = 0
w3 = k 呢
這個時候我們得到了
這樣子的
一個 hashing matrix
這個 hashing 的 matrix，你實際上去算一下他的
eigen value 以後，你會發現說
他有一個
positive eigen value，一個 negetive eigen value，一個
zero eigen value
雖然他有 zero eigen value，但是他的 eigen value 又
正好一個 positive 一個 negetive，所以他往
某個方向走會增加，往某個方向走會減少
那往 zero 的那個方向，zero eigen value 那個方向走
不知道增加還是減少不過沒關係反正有增有減
所以他一定就是
saddle point
從這個例子裡面，我們發現什麼
從這個例子裡面我想要告訴大家的事情是，我們最後發現說
所有的 minima
至少在這個 case 裡面
所有的 minima
他都是 global minima
其他的 critical point，他都不是
local minima，只要你是一個 local minima
那你就是一個 global minima，另外一件事情
是有一些 critical point，他是很差的 critical point
你陷進去以後，他就會跑不出來了
那什麼叫差的 critical point 就是
你從那個 critical point 算 eigen value
沒有任何 eigen value 是負的
這樣你就不知道說哪個方向可以讓你的 loss 的值減少
你就走不出去了
這個是兩個 hidden layer 的情況
如果是十個 hidden layer 的話
你其實也可以自己分析一下，也不會太難
那我就懶得再分析了
我這邊做的事情就是，在那個
十個 hidden layer 就有十個參數
兩個 hidden layer 有三個參數
所以十個 hidden layer 有十一個參數
那十一個參數的 space 上面，我取兩個點
那兩個點他們正好在原點的兩邊
然後把他們拉起來
再算說在這個區間之內的 loss 的變化
如果取某兩個點
他們的 loss 的變化可能是這樣
然後再取某兩個點
他們 loss 的變化可能是這個樣子
你會發現說在原點的附近，這邊兩邊高是一樣
他們都是 1
那在原點附近，有一個 saddle point
他是 saddle point
在原點附近呢
但是如果你光看 hashing 的話
你不知道他是不是 saddle point
在原點附近有一個 critical point
這個 critical point 他算出來的 hashing 都是 0
然後在原點附近非常的平
非常的平，如果你走到原點附近
你就再也逃不出來，就像被吸入黑洞裡面
這個就是我們在開學學期初的時候
demo 的那一個狀況
我們現在真的來看一個非常簡單的
只有兩個 neuron 的 linear network
現在我們的 training data 就像我們剛才講的
他的 input 是 1，然後 output 的 target 也是 1
這個 network 只有兩個 neuron
第一個 neuron input 是只有一維
output 只有一維，然後沒有 bias
所以這個 neuron 他只有一個參數
這個參數是 weight，他沒有 bias
那他的 activation function 是 linear 的
接下來用 for 迴圈再加第二個 neuron
那這第二個 neuron
他的 input dimension 是 1， output dimension 也是 1
一樣沒有加 bias
一樣他的 activation function，是 linear 的
這種 network 我們可以輕易的把他 train 起來
但是我現在故意在做參數的 initialization 的時候
把這個 neuron 的所有的 weight
通通 initialize 在這個 weight = 0 的狀況
那要怎麼做到這件事情呢
其實在 Keras 裡面非常的容易
因為我這邊是用 randon normal
來做參數的 initialization
我為什麼要把 standard deviation 設成 0
在做參數的 initialization 的時候
initialize 的參數就會自動的被設為 0 了
我們剛才有講說
今天一個只有兩個 neuron 的 linear network
他的這個
在原點的地方有一個 saddle point
所以，今天如果 initialize 是在原點那個地方
你會發現說你 train 的 loss 就完全降不下去
完全降不下去了
就算用 Adam 加 momentum 也沒有用
因為一開始，初始的地方
gradient 就是 0，那你一開始初始在一個
saddle point 的地方
你就沒有辦法再 train 下去了
但是初始在 saddle point 的地方這件事情
其實是並沒有那麼容易發生
你現在只要在做 initialization 的時候
給這個 standard deviation 非常非常小的值
就算他的值非常非常小，也就是 initialize 的時候
你 initialize 的點在原點的附近
在兩個參數都是 0
在所有參數都是 0 在這個原點的附近，你會發現說其實
network 只要他初始化
離原點有一點點的距離，就不會被卡在
那個 saddle point，所以我們現在給這個
standard deviation 一個非常小，非常小的值
這樣夠小了吧
我們實際來 train 一下
你會發現說，現在 loss 就可以輕易的降為 0
如果你的參數初始化的時候，exactly 在原點的地方
沒有辦法 train，但只要離原點稍微偏離一點
就可以 train 了
接下來，我們稍微再加深一下這個 network
現在變成有三個 neuron
同樣的 initialization
你發現說，三個 neuron，同樣的 initialization
結果就 train 不起來了
因為我們剛才有分析說
如果今天有三個 neuron
那在原點那個地方的 saddle point
他會變成一個高高的 saddle point
這個 saddle point 他的 hashing
是一個 zero matrix
也就是說在原點的地方
這個 error 的 surface 是非常平坦的
他就是一個一望無際的大地一樣
平坦的就跟一個鏡面一樣
那邊完全沒有任何的 curvature
所以，如果你今天 initialize 的時候
initialize 在原點的附近
initialize 在非常平的那個 saddle point 附近
你就很難逃出那一個區域
接下來我們試一下
當把 network 加了非常深的時候
我們現在總共有十個 neuron
假設你現在有十個 neuron
就算是你用正常的 initialization
你其實也逃不出去了
因為我剛才有講過說
如果你今天有
你今天的 network 是非常深的
這個 linear network 是非常深的，在原點的地方
會產生一個這個
非常非常平的區域
如果你今天 initialize 的時候，在這個
非常非常平的區域的附近
你就會沒有辦法逃脫了
我們現在給參數做 initialization
用一個比較正常的 standard deviation
然後 train train 看，現在 network 非常深
用一個正常的 standard deviation 看看
能不能跑出去，發現說
現在 loss 降不下去了，這個
一萬個 epoch，但是這個 loss 完全
都沒有辦法降下去
怎麼辦呢？
這個時候，因為我們知道在原點附近
有一個非常平的區域
所以，你就要避免在你做 initialization 的時候
initialize 在原點附近
盡量離原點足夠的遠
不要 initialize 在那個非常平的地方
initialize 離那個非常平的地方遠一點
不要在那個很平的區域
其實就可以順利的把他 train 起來了
我們試試看把 standard deviation 故意設的大一點
你突然發現
這個時候雖然 network 非常深
但是 loss 也輕易的就下降到 0 了
非常的遠，那我就告訴你說
general 現在可以證明出來的東西是什麼樣子
那這裡的細節
我就把 reference 放在後面，你再自己
你再自己看看
現在我們知道的事情是
假設有一個 network 他是 linear 的，什麼意思？
剛才我們舉的是一個 linear 的 network 的
一個非常 special 的 case
special 的 case
那現在是一個 general 的 case
input 是一個 vector, x
x 乘以 matrix w1 以後
再乘以 matrix w2 再乘以 matrix wk
得到 y 希望 y 跟 y/hat 他們的 tunel
越接近越好
接下來，可以證明說
只要滿足一些非常寬鬆的條件
這一個 linear 的 network
不管他疊幾層
他的所有 local minima
都是 global minima
就跟我剛才在
兩個 hidden layer 証的 case 是一樣的
所有的
你只要算一算他的 critical point
反正你找到一個 critical point，就發現說他是
local minima，那他就一定是 global minima
為什麼我們的 paper
他們證明的時候需要的條件不太一樣
我現在找到的一個最寬鬆的條件是
要求所有 hidden layer 的 size
要大於等於 input dimension 跟 output dimension
舉例來說
你 input dimension 是五維，output dimension 是五維
中間每一個 hidden layer 的 output 都要至少五維
那為什麼需要這個限制呢
其實是在證明裡面會用到說
因為在證明裡面，實際上會用到的概念是說
現在 loss 最小是 0
所以你需要讓你的這個 linear 的 network
他的 global minima 的 loss 是 0
也就是實際上證明的時候會用到說
你找到一個 critical point
發現他是 local minima
接下來你又說，如果他是 local minima
他的 loss 一定是 0
因為 local minima 的 loss 是 0， 所以他一定是 local minima
所以他是這樣子證的
所以是這樣子證出來的所以需要加上這個條件
確保 global minima 的 loss 是 0
其實還有另外一個發現，這個發現是說
當今天 network 的深度
大過兩個 hidden layer
那你就會產生我們剛才講的
不好的 saddle point
就會產生不好的 saddle point
所謂不好的 saddle point 是說
這個 saddle point
他沒有任何 negative 的 eigen value
他是個 saddle point
但是他沒有 negative 的 eigen value
所以，不知道往哪個方向走可以讓你的 loss 下降
這件事情在 network 的 hidden layer 一層的時候
不會發生，兩層以上就會發生
我今天在講課的時候
我並沒有直接證，我就只是舉個例子
我們就剛才舉的例子是說，只有兩個 neuron
也就是一個 hidden layer 的時候
沒有差的 saddle point
但多加一個 neuron，有兩個 hidden layer， 三個 neuron 的時候
他有差的 saddle point，但是 in general
不是只有好幾個 neuron，而是一個
而是更 general 的 case，其實也是這樣
那詳情，大家自己再去看一下 paper
最早的完整的證明
應該是來自 NIPS , 2016 這一篇
Deep Learning without Poor Local Minima
那其實，這個 2016 離現在其實也沒有太久
但是在 deep learning 這個地方的時空的
這個時間的流動是比較快的
所以 2016 就覺得說非常久以前
那個時候
第一次有這樣證明的時候大家其實都還蠻驚訝
想說，哇！
原來 deep linear network 他雖然是 Non-convex 的
但是他是沒有 local minima 的，好神奇
而且這個就是
deep linear network 沒有 local minima 這件事
好像在不知道是 80 年還是 90 年的時候，就有人提出
這樣子的假說，但是沒有證
然後這篇 paper 就說過去有人提這樣子的假說沒證
然後，幫他證了一下
但是因為時間過得非常快，那後面就有很多其他的 paper
比如說這個 2018 的 paper
他 submit 到 ICL 就被 reject
然後看到 reject 那個 cover 就說
linear 的 network 只證 linear network 實在是
太 linear了，其實沒有什麼
過去可以上 NIPS , 2016 的 oral
但同樣差不多的東西，現在已經會被 reject 了
大家已經覺得說 linear 的東西
分析的已經差不多了，所以接下來就是
更 general 的 case
因為 linear 的 network 簡單來說就沒有什麼用
我們真正處理問題的時候我們需要的是
nonlinear 的 network，但是
問題就是 nonlinear 的 network 到底長什麼樣子呢？
事實上在這一篇 paper 裡面
Deep Learning without Poor Local Minima 這篇 paper 裡面
他試著證明了
nonlinear 的 deep network
但他用的方式非常的怪異
如果你看他的 extra introduction，你會以為說
原來所有 deep 的 network，不管 linear nonlinear 通通
沒有 local minima，太強了
但是其實你之後仔細看一下，發現說
他真的證了 linear 的 network
如果是 nonlinear 的 network
他用一個非常奇怪的假設告訴你說
根據這兩個 assumption
linear network = nonlinear network
然後，那兩個 assumption 是完全不合理的
我就不打算講那兩個 assumption
因為如果講，等一下一定被大家問到頭炸裂
因為那個就是不合理的
所以，接下來要怎麼證呢？
就很多人會試著想要去往說
deep learninig
一般的 general 的 deep network
是沒有 poor local minima
就只有 global minima 這個方向去證明
但是發現非常困難，那你何不反過來
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
