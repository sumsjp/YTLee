臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw
那我們在前一堂課看了regression，
我們知道怎麼用gradient descent來找出regression model的參數
這個聽起來還滿容易的。
接下來，我就舉個例子，
讓大家知道實際在做regression的時候，
你會碰到甚麼樣的困難。
這邊我們假設x data有10筆，
y data也有10筆。
那x和y之間的關係，
是ydata= b+w*xdata
那b和w都是參數，
我們用gradient descent把b和w找出來。
我當然知道說今天這個問題有closed-form solution，
這個b和w有更好的方法可以找出來，
那我們假裝不知道這件事，
我們要練習用gradient descent把b和w找出來。
那怎麼做呢?
gradient descent其實非常的簡單，
所以這邊我們不需要說太多。
這個程式碼還沒有超過20行。
我們先給b一個初始值，
這邊b的初始值是 -120，
我們給w一個初始值，
這邊w的初始值是-4，
我們需要一個learning rate，那learning rate我們就給他一個很小的數字
iteration就設100000個
那在每一個iteration裡面，
我們要做的事是，計算出b和w對loss的偏微分，
計算的式子在之前的課程裡面已經講過了，
那你就把之前課程裡面導過的式子，把它寫出來就可以了
算出這個b和w對loss的偏微分以後，
你就把b的偏微分乘上learning rate去update b
把w的偏微分乘上learning rate去update w
反覆iteration多次，
最後就可以找出b和w了。
那我們就實際來執行一下程式，
看看我們會得到甚麼樣的結果。
這是我們得到的結果，
這個圖上面的顏色代表了不同的參數下我們會得到的loss。
縱軸代表w的變化，
橫軸代表b的變化，
不同的w和不同的b，我們得到不同的顏色，
也就是不同的loss。
那loss最低的點是在這個地方，
也就是b介於-180到-200之間，
w大概介於2到4之間的時候，
這個時候loss最低。
那我們初始的b和w，
在這個地方。
在做gradient descent的時候就從這個地方開始update參數，
所以參數就一路從這個地方開始一直變化，
走到這邊然後向左轉。
但是過了100000次參數update以後，
我們發現說我們現在的參數，
離最佳解仍然非常的遙遠。
怎麼辦?這顯然是learning rate不夠大，
把learning rate 調大一點。
這是learning rate 調大10倍後的結果，
你發現說最後，經過100000次參數的update以後
我們的參數在這個地方，
離最佳解稍微近了一點，
不過這邊有一個劇烈的震盪的現象發生。
那我們再把learning rate稍微設大一點，
我們設再大10倍，
你發現，啊糟糕了!
learning rate再大10倍以後就太大了
從這個地方開始update參數，
結果參數一update，
它就飛到這個圖外面去了。
就飛得很遠很遠了。
所以現在learning rate太大，
如果再把它變小，
又變成跟剛才一樣還是離最佳解很遠，
怎麼辦?
這問題明明就很簡單，
只有兩個參數，
結果gradient descent搞半天都搞不定。
連兩個參數都搞不定，
之後如果再做neural network有數百萬個參數的時候，
要怎麼辦呢?
這個就是一室之不治何以天下國家為的概念。
怎麼辦?
只好放個大絕來解決這個問題。
我本來不想要用這一招的，
但是只有兩個參數的問題，
我們不解決是不行的。
怎麼辦呢?
我們要給b和w客製化的learning rate。
它們兩個的learning rate要是不一樣的。
怎麼做呢?
實作起來是滿容易的，所以現場寫一下。
我們要給b和w客製化的learning rate，
原來b和w的偏微分都是直接乘上learning rate，
一個固定的learning rate，
那我們現在把它除掉不同的值，
所以它們會有不同的learning rate。
你可能看得一頭霧水，
不過沒有關係，這個就叫做AdaGrad
之後我們會再詳加解釋。
那這個learning rate就隨便設，設個1就好。
你會發現說有了新的learning rate以後，
從初始的值到終點，
我們就可以很順利的在十萬次update參數之內，走到我們的終點了
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw
