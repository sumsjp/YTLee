hi my name is Alexander due from
National Taiwan University I'm here to
present our paper sequence to sequence
automatic speech recognition with word
embedding regularization and field
decoding okay so that's begin our first
like to talk about our motivation our
goal is trying to utilize on pair tags
and attention based sequence to sequence
aids or training so I'm Pierre tax is
generally more easy to collect comparing
to the audio and transcription pair data
so we like to take the UM pair tags put
together was world embedding algorithms
and to obtain the word embeddings and
then we collect the pair data and for
embedding together as training data to
train our sequence to sequence a sore so
here's a paradigm of sequence to
sequence a so it's usually composed by
an encoder and attention module and also
a decoder and we'll be focusing more on
decoder and later in our methods so
before we go into our method
I'll first like to talk about why while
we select word embeddings as the target
so first of all what embeddings are
generally more easy to obtain and there
are already many of the shelf solutions
that is available and next word
embeddings are designed to be more
contextually meaningful and we would
like RSR to learn those contextual
information from the word embedding
vectors and finally we want our method
to have last additional computation cost
on SR training since the word embeddings
are usually per train so let's first
take a look at standard sequence to
sequence a sr training so for each time
step the ASR decoder will output a
distribution over the vocabulary and
let's say for this time set that target
world as how to train the area's are we
will need to minimize the cross entropy
between the decoder output this review
and the 100 citation of the word how and
this goes on for each time step to train
the ASR and the question now is can we
use word embedding as target instead of
one heart representation of words so
this is what it looks like if we set the
word embedding as the target of sequence
two sequences are so for each time step
is our decoder will output a vector and
how gonna would be maximizing the cosine
similarity which we know that she is
here between the decoder output vector
and the target word embedding vector so
this also goes on for each time step and
we hope that as art can be trained in
this way however this failed terribly in
practice so that now now the question is
is there a better way to use the word
embedding so here's our solution the
word embedding regularization so again
for each decoding time step the SRT
total will predict output vector and
just as we demonstrated in the standard
sequence to sequence ASR training the
output vector will be used to derive the
probability distribution over the
vocabulary set and this probability will
be used to minimize the cross entropy
just as we said before but at the same
time we would also like the architecture
to be used to predict the word embedding
and this predicted word embedding will
be used to maximize the cosine
similarity against the word vector from
the pre trending betting table so now we
are bringing back the cosine similarity
just as a regularization term so for
chaining sequence to sequence a service
word embedding especially as
regularization target we're now writing
or up a learning objective as minimizing
the cross entropy meanwhile maximizing
the curve
similarity was the regularization way to
London and since this is just a
regularization method during for
decoding we only need the probability
protection here and we do not need the
working writing part here so now again
we're trying to ask can we do more on
word embedding there even during the
decoding stage this is the second
proposed method called fuse encoding
with bird embedding so the go now is to
that the upper probability be
corresponding to cosine similarity to
each word embedding here we have the
figure from the previous slide with
predicted distribution predicted word
wedding together with the targets we
first define a new probability
distribution based on a softmax function
the softmax function takes because i
similarity between each entry of the
word embedding table and the predictive
bird embedding as input so this
distribution will be completely based on
cosine similarity and we have another
distribution that is the directly
predicted by the ASR decoder we will
take the weighted sum of the two
distribution with a fusion rate lambda
to obtain the fuse distribution and this
is the final output probability that is
used to maximize the cross entropy so as
you can see in this case the word
embedding what can be involved in the
decoding process and contributes to the
fuse there's a distribution and this is
pretty much all the proposed methods in
our paper but I'm skipping some
technical details due to the limited
time and please refer to our paper if
you are interested all right moving on
to our experiments I'll first go through
some settings of our experiment the data
were using as the clean speech from
liberal speech as the pair data and for
the unpaired tags we used the
transcriptions of the rest audio in the
in the
speech and here are some implementation
details of our model and I would like to
highlight that the regularization wait
we mentioned before it's at 2:10 and the
fusion wait is for similarity based
distribution is sent to 0.1 so the first
experiment is running on the high
resource setting in this setting we have
460 hours of pair data for the unpaired
text we use the transcription of the
other 500 hours of speech
these ampere tags are used to train the
word embedding as well as the language
model our basic model here is the sender
sequence to sequence ASR this model
performs around 14% of word error rate
on testing set and number can be reduced
by around two percent when a language
model is involved in the decoding
process so the first thing we want to
show is that where our proposed word
embedding regularization method provides
around 10 percent of relative
improvement in terms of word error rate
and notice that this is similar to the
improvement you gain from a language
model second the choice of the word
embedding algorithm does matter as you
can see when we are using bird as our
word embedding algorithm we have the
best performance and if you switch the
word embedding in to see pal you can see
the performance drop dropped
significantly finally there is a
consistent improvement when combining
our regularization method with joint
language model decoding so this means
that our method can be combined with
other decoding method and the
performance is gained by our message
stands for the second experiment we are
running on a low-resource setting in
this setting we only have 100 hours of
pair data notice that this is a quite
limited situation for sequence to
sequence ASR
and for the unpair tags we have the
transcription of the other 360 hours of
speech and again these transcriptions
are used to train the wording bedding as
well as the language model so the
baseline is still the standard sequence
to sequence ASR model and this time the
performance on testing set is around 22
percent of word error rate and when the
language model is involved that this
number can be reduced by around two
percent and so our first office
observation is that the word embedding
methods improved the word error rate by
one percent when paired era is really
limited and this this improvement is
made by the fuse decoding method and the
second observation is that there is a
still still a consistent improvement
when language model is involved so again
you can stack our method together with
other decoding method and the
performance gain is simply added so here
these are just some quick highlights of
our experiments and for more detailed
study or comparison again not against
other methods please refer to our paper
some takeaways in this paper we proposed
a new regularization method for sequence
to sequence ASR and our method allows
ASR to take benefit of additional text
source and powerful off-the-shelf word
embeddings our method is lightweight
this means that the improvement weekend
is just take using a small computational
price finally our method is compatible
with our existing decoding techniques
this means that you can just use them
all to get the best performance instead
of choosing one so this is the end of my
presentation and the source code of this
project will be available through the QR
code if you are interested thank you for
your attention
