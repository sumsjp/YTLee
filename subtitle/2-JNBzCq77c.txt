臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
Lecture 4，我們想要講的就是， 如何讓 machine 學會和環境作互動
那主要要講的就是 Reinforcement learning
那機器學習和環境作互動， 跟一般 machine learning 的問題，有什麼不樣呢
我覺得最大的差異就是，機器所採取的行為， 會影響到它未來發生的事情
比如我們假設以影像分類的問題為例
來對照讓機器學習和環境作互動
那如果你今天是做影像的分類
給機器看一張圖片， 讓它決定這張圖片是貓還是狗
那一個非常類似的 task 是要讓機器和環境互動
可能是給他一張圖片，假設那機器其實是自駕車好了
給它一張圖片，一個影像，它決定它要採取怎樣的行為
這跟一般的分類問題有什麼不同呢？
在一般的分類問題裡面，你得到輸出的結果以後
就沒事了，我給他一張圖片， 覺得是貓就是貓，覺得是狗就是狗
但是在互動的問題裡面
假設是一個自駕車，給他看一個圖片
要決定現在要往左轉與往右轉
它的決策，會影響接下來它看到的 data
也就是說它如果決定往左轉， 接下來它看到的就是往左轉街道的樣子
決定往右轉， 它看到的就是往右轉街道的樣子
所以今天讓機器學習和環境作互動
跟一般的 learning problem，是不太一樣的
所以會需要另外來討論它
它最不一樣的地方就是
今天機器和環境作互動的時候
它會影響環境，它會影響它接下來的 input
好，那有什麼樣的例子， 是機器要學習和環境作互動的呢？
大家都知道的就是 Alpha Go
在 Alpha Go 裡面，你有一個 machine 在下圍棋
Alpha Go 的輸入是什麼？
Alpha Go 的輸入，就是棋盤的盤勢
黑子和白子的位置
它的輸出就是接下來應該要落子的位置
那我們剛才講說， 今天在機器學習和環境作互動的過程中
最重要的特色就是，你所採取的行為， 會影響往後的發展
在圍棋裡面，顯然是這樣子
你落子的位置，當然會影響你對手落子的位置
你今天出手下在天元，對方一定守，跟你出手下在 5-5，對方一定守，顯然是會不一樣的
在做這種和環境互動的，你通常會訂一個目標
舉例來說在下圍棋裡面， 要達成的目的，當然就是要贏棋
舉例來說在下圍棋裡面， 要達成的目的，當然就是要贏棋
那在機器和環境作互動的文獻上， 你常常看到一個字眼叫做 state
這是什麼意思，在過去， 我們通常覺得說外界來的資訊太過複雜
我們會需要有另一個 model 幫我們把外接資訊做摘要
做完摘要以後，再丟給 machine
外界輸入資訊沒有處理的，就做 observation
但是今天隨著 deep learning 技術的發展， model 的能力已經越來越強
但是今天隨著 deep learning 技術的發展， model 的能力已經越來越強
我們不太再需要幫 model 另外再去做 資訊的 summarization
反正它自己有那麼多個 layer， 它自己會決定說 input 這個 raw 的 feature
observation 哪些資訊是可以用的，哪些是它不要的
所以 observation 跟 state 現在這兩個詞彙基本上是混著用的
所以當我講 state 跟 observation 其實我指的是一樣的東西
讓機器下圍棋，是一個機器和環境互動的例子
舉例來說，讓機器學會玩電玩
也是一個和環境作互動的例子
在機器學習玩電玩的時候
它有另外一個 AI，另外一個主機當作對手
機器採取的行為，當然會影響對方的回應
如果你想做一些 video game 的 playing 的話
你可以參考下面這兩個連結
Open AI 它們提供了一些平台， 讓你的機器可以練習去玩一些遊戲，比如說 GTA
有人可能會問說，讓機器學習玩遊戲， 好像沒有什麼特別的
有人可能會問說，讓機器學習玩遊戲， 好像沒有什麼特別的
遊戲主機裡面就有一個 AI，它也會玩遊戲嘛
當機器學習玩遊戲後，它看到的遊戲畫面， 跟人看到的遊戲畫面，是一模一樣的
再來呢，它並沒有 handcrafted 的 rule
告訴它什麼樣的行為是好的， 什麼樣的行為是不好的
告訴它什麼樣的行為是好的， 什麼樣的行為是不好的
它是人告訴他說看到這樣的畫面， 看到這樣的狀況，你就採取這樣的行為
它是人告訴他說看到這樣的畫面， 看到這樣的狀況，你就採取這樣的行為
但是當我們讓機器自己學習的時候， 沒有人告訴機器說，該採取什麼樣的行為
沒有人告訴機器說應該採取什麼樣的行為
它必須要自己 dig out 什麼樣的行為是好的， 什麼樣的行為是不好的
但和環境作互動，還有很多例子， 比如說自駕車也是一個例子
如果你今天要做一台自駕車的話
比如說看到一個紅燈， 然後它就決定說，現在要踩個煞車
那當然，我們剛才用自駕車的例子有講過說
比如說看到一個紅燈， 然後它就決定說，現在要踩個煞車
那當然，我們剛才用自駕車的例子有講過說
機器採取不同的行為， 就會影響它接下來看到的畫面
隨著採取的行為不同， 接下來它看到的 observation，就會不一樣
或者是說，假設你想要做一個 chat-bot， 也是一樣
然後 machine 回答說，你希望從哪裡出發
你說，你想要訂 11/5 到台北的機票
然後 machine 回答說，你希望從哪裡出發
這個輸入，和輸出
這個東西，你可以想成，你有一個巨大的 network
輸入一個 input，然後你就可以回一個輸出
但是今天你的 Dialog system 它的輸出， 會影響它接下來看到的 input
但是假如今天 dialog system 的輸出 是比如說你要幾點出發呢？
那它接下來看到的 input，可能就是， 我要從 Boston 出發
但是假如今天 dialog system 的輸出 是比如說你要幾點出發呢？
它看到的輸入，可能就是我要 6 點出發
所以跟其他互動的 task 一樣
今天你要訓練一個對話系統的時候
環境就是指你的客人
但你今天採取的行為，會影響你客人的回應
環境就是指你的客人
但你今天採取的行為，會影響你客人的回應
好，要怎麼解這種和環境互動的問題呢？
你當然可以說，和環境互動的問題
我們就直接把它想成是一個分類的問題
直接把它當成是一個 supervised learning 的問題
用 supervised learning 的方法，去學一個 network
怎麼說，假設是圍棋的話
我們就教 machine，看到這個盤勢，你就下 3 3
那你就跟人做的一樣， 那這不過是一個 supervised model
人看到這個盤勢，就會下 3 3
那你就跟人做的一樣， 那這不過是一個 supervised model
看到這個盤勢，輸出 3 3 的機率要越大越好
好，那如果是自駕車，你就教機器說
看到這個畫面，你就踩煞車
那你說要怎麼收集這種 data 呢？
你就 collect 人在開車的時候， 行車紀錄器的畫面，還有人的動作
你要調這個 network 的參數，讓它的 output 是踩煞車
機器要學到說，看這個畫面的時候
你要調這個 network 的參數，讓它的 output 是踩煞車
如果是對話系統的話， 你就搜集很多真人的客服，跟顧客的對話
那你知道真人的顧客說， 我想訂 11/5 到台北的機票
真人會問說，你要從哪裡出發？
那當我們用這種方式，讓機器做學習的時候
你要回答的跟真人客服，越像越好
你就這樣輸出
那當我們用這種方式，讓機器做學習的時候
這個叫做 behavior cloning， 就是複製行為，複製 expert 的行為
我們可以告訴機器說，真人駕駛， 看到這個畫面就採取怎麼樣的行為
真人的客服，聽到顧客說這句話，就這樣子回答
你就跟你的老師，你的 expert 做得一模一樣就好了
就希望你可以學得跟 expert 一樣厲害
這麼做，會有什麼樣的問題呢？
以下是一個 behavior cloning 的例子， 那其實跟 machine learning 沒什麼關係
以下部分省略
好，所以機器在學的時候， 它就跟 Shelton 一樣困惑了
當它有一個人類的老師的時候， 人類的老師，採取的一些行為
機器能夠做的事情，就是相信它的老師全然是對的， 然後完全去模仿它的老師
事實上，假設機器可以完全模仿它的老師
也許問題並沒有很大
機器只是學了一些不該學的東西而已
但真正害怕的 case 是什麼呢？
真正害怕的 case 是， 機器沒有辦法完全模仿它的老師
如果是只有 behavior cloning 的話，你沒有告訴 machine 說什麼樣的行為是重要的，什麼樣的行為是不重要的
到底哪些東西該學，那些東西不該學
就變成是一個問題了
這 behavior cloning 我覺得最大的問題就是
如果是只有 behavior cloning 的話，你沒有告訴 machine 說什麼樣的行為是重要的，什麼樣的行為是不重要的
舉例來說，剛剛 Shelton 在學中文的時候， 它不知道說語音是重要的，手勢是不重要的
對機器來說，你今天就給它一個示範， 它又不知道說到底是語音重要，還是手勢重要
就好像有一個人說，它想要成為成功的人物
那整個結果就會壞掉
所以 behavior cloning 最大的問題就是， 機器不知道說什麼是重要的，什麼是不重要的
它只能夠做照單全收這件事
就好像有一個人說，它想要成為成功的人物
比如說它想跟 Jobs 一樣， 他就列出了 Jobs 的 20 個人格特質
可能包括勤奮，創造力，還有壞脾氣
然後他就覺得說他能力很差， 三項裡面他學一樣就好了
他就決定他只學壞脾氣，然後就一無是處這樣子
所以 behavior cloning 的問題就是這樣
好，那在這種與環境互動的情況下， 有些行為是重要的，有些行為是不重要的
為什麼，有些行為是重要的，有些行為是不重要的呢？
因為你接下來 machine 採取的行為， 會影響接下來的發展
所以有些行為非常的關鍵， 有些行為也許沒那麼關鍵
如果說，behavior cloning，機器學不到這件事
你不能只把他當作一個簡單的， 一般的 supervised learning 來看待
你就不能單純的考慮每一個 step
不能說只是告訴機器說，在看到這個盤勢的時候
你就下在這個位置
你不能只把他當作一個簡單的， 一般的 supervised learning 來看待
機器是沒有辦法學好的
你要讓機器把所有 actions 都當作整體來看待
那怎麼做到這一件事呢？有兩個方向
一個方向，就是大家都知道的 reinforcement learning
那在 reinforcement learning 裡面，機器會去跟環境互動
他自己去跟環境互動
他在跟環境互動的過程中，他會得到一些 reward
另外一個也許大家可能比較沒有哪麼熟係的， 叫做 learning by demonstration
learning by demonstration 又叫做 imitation learning 或是 apprenticeship learning
怎麼樣多採取好的行為
避免採取會得到 negative reward 會得到差的評價的行為
另外一個也許大家可能比較沒有哪麼熟係的， 叫做 learning by demonstration
所以 learning by demonstration， 它並不是一般 supervised learning 的問題
那在這種 task 裡面， 機器它有一些 expert 的 demo
但是它今天在學習 expert 的行為的時候， 它必須要有特別的學習方式
而不是使用照單全收，behavior cloning 的方式
所以 learning by demonstration， 它並不是一般 supervised learning 的問題
機器不是複製 expert 的行為
而是用其他方法，來讓它可以學得跟 expert 一樣好
好，那我們就先來講 reinforcement learning
但其實我們除了講 reinforcement learning， 還會講 inverse reinforcement learning 的技術
其實 inverse reinforcement learning 的技術， 就是 learning by demonstration
所以，inverse reinforcement learning 的技術， 就是 learning by demonstration 的其中一種
好，那我們先來看一下 reinforcement learning
等一下如果時間夠的話
我們就會先介紹 Actor，再介紹 Critic， 然後再介紹 Actor + Critic 的方法
那我們分別以電玩，和下圍棋為例， 來說明這3 個東西分別是什麼
第一個 component 是一個 actor
第二個 component 是一個 environment
第三個 component 是一個 reward function
那我們分別以電玩，和下圍棋為例， 來說明這3 個東西分別是什麼
machine 現在要去玩遊戲， 至少要有主機跟遊戲玩，你的主機就是環境
決定要向左向右，還是開火
如果是在下圍棋的時候， 它就決定說現在要落子落在哪個位置
至於環境，在 Video game 裡面， 你的環境，就是主機
machine 現在要去玩遊戲， 至少要有主機跟遊戲玩，你的主機就是環境
那在下圍棋裡面，你的環境就是 machine 的對手
就是另外一個人類的對手
好，那 reward function 呢，在遊戲裡面， 你會先訂好比如說殺一隻怪獸得 20 分等等
這個就是 reward function
那在圍棋裡面， reward function 是很明確的，就是圍棋的規則
下到這個地方就贏了，就得一分
下到這個地方就輸了，就得負一分
那在 reinforcement learning 的 task
你要注意 environment 跟 reward function 是訂好的，是訂死的
你不能去動它， 你不能去動圍棋的規則，你不能去動你的對手
那都是你無法控制的
我們要做的事情是什麼，我們唯一能夠控制的， 就是 actor 它採取的行為
我們要做的事情是，調整 actor 採取的行為
使得它可以得到，最大的 reward
好，那我們以電玩為例，來說明一下
這個環境，還有 actor， 還有 reward 它們互動的情形
假設我們現在要讓機器去玩電玩，但是什麼樣的狀況呢？
首先機器會先看到一個遊戲畫面， 這個遊戲畫面，我們就叫做 s1
這個遊戲畫面，就輸給 machine，就輸入給 actor
actor 就要做一個決定，決定現在要做什麼
舉例來說，它決定說，看到這個畫面 s1， 我要採取的行為，叫做 a1
那 a1 就是向右移動
好，那它向右移動以後，它會得到一個 reward
在每一個 time set，在每一個互動的過程中
reward function 都會給 machine 一個 reward
那在這一步，只是採取向右， 所以得到的 reward 是 0
也就是採取向右，不會得到任何的分數，所以 r1 是 0
那機器採取這個行為之後，它就會看到新的遊戲畫面
至少它看到自己往右移了
這畫面是我從真實的遊戲截下來的
它本來在這個地方，然後它就往右移了
然後往右移以後，就會看到新的遊戲畫面
機器就會決定要採取新的行為
舉例來說，它可能看到新的畫面
這次他決定說，看到畫面 s2，它要採取 a2 這個行為
a2 這個行為，就是開火 它決定要開火
好，哪假設它開火以後，它殺了一個怪
根據 reward function 的定義， 就會告訴他說，你殺了一隻怪
那假設，殺那一隻怪，值 5 分 那你就得到 5 分，那 r2 就是 5
那你又看到新的遊戲畫面，就是 s3
那這互動的過程，就反覆的繼續下去
直到說，現在呢，在某一個遊戲畫面， 機器決定採取 action at，得到 reward rt
然後進入 terminal state，然後進入按照這個主機的設定
走到這邊，遊戲就結束，那互動的過程就結束了
那每一場遊戲，叫做一個 Episode，每一個互動， 從頭到尾，在文獻上，我們叫它一個 Episode
在電玩裡面，每一場遊戲叫做一個 Episode
在圍棋裡面，每一局棋叫做一個 Episode
那在一個 Episode 中，我們把每一個 step 得到的 reward
r1 r2 r3 rT 都加起來，就叫做 total reward
那 total reward 我們就寫成大 R
那現在機器要做的事情是， 我們希望大 R 就是 total reward 的值
在互動的過程中，越大越好
那我們再用另外一個簡化的圖，來說明一下 environment，actor，還有 reward function 之間的關係
Environment 輸出一個 state，其實就是遊戲的畫面
它也可以說是輸出一個 observation，我剛才講過 state 跟 observation 在我心裡面是一樣的東西
輸出一個遊戲畫面 s1，s1 輸入給 actor， actor 就輸出說，我要執行 a1
那再把 a1 輸給 environment ， environment 就說，我要執行 s2
s2 輸給 actor，actor 就說我要執行 a2
然後 environment 看你執行 a2 以後， 它再說現在有新的畫面 s3
就這樣反覆執行下去
那 reward 怎麼計算呢？你有一個 reward function
就是這個遊戲的規則
結果 reward function 告訴我們說，在 state s1，採取 a1，你把 state s1 a1，丟到 reward function
你得到一個東西，數值是 r1，得到分數是 r1
在 s2 a2，在 s2 這個 state， 採取 a2 這個行為，得到 reward 是 r2
以此類推
那我們如果把 state 跟 action 這個序列啊
就是在 state s1 採取 action a1， state 2 s2 採取 action a2 的這個序列
通通記錄起來，叫做一個 Trajectory
那在這個 Trajectory 裡面呢
這個 s，a 的序列叫做 Trajectory
那你把這些 reward 通通都加起來， 就得到 total reward 大 R
或者你可以說， 你得到某一個 trajectory tau 的 reward，R of tau
那我現在說我們目標，希望可以調整這個 actor， 使得最後可以得到的 reward 越大越好
好，那接下來就是要說怎麼調整這個 actor
在講調整 actor 之前， 我們當然要先來看一下 actor 長什麼樣子
好，那 actor 長什麼樣子了， actor 也是一個 neural network
所以其實這個 reinforcement learning 從來都不是一個特別新的題目
現在我們講的這些技術， 其實在 80 年代就已經有相當完整的版本
近年來，deep reinforcement learning 突然又變得很紅， 到底有什麼樣不一樣的地方
其實他最不一樣的地方就是， 我們現在使用了 neural network
過去的 actor 通常都是查表 而不是 neural network
長久以來，多數人都相信說
當我們把這個 actor 換成一個 non-linear 的 network 的時候，是無法 trained
它沒有辦法收斂，沒有辦法證明它會收斂
那怎麼辦，就不要用它？
可是後來就是 google 他們的貢獻就是， 他們想了一大堆的 tip
讓這個 training 能夠真的 work 起來
所以現在，當我們講 actor 的時候， 它其實都是一個 neural network
好，那這個 neural network 它的輸入輸出分別是什麼呢？
actor，這個 neural network， 它的輸入就是一個遊戲的畫面
那這個遊戲的畫面，就是由 pixel 組成的嘛
那你要處理影像，通常就是需要用到 CNN
所以這個 actor 它的前幾個 layer 可能都是 CNN
為了要處理影像遊戲的畫面
好，那這個輸出呢，就是看你有幾個 actions
你輸出的每一個 neuron，就對應到一個 action
就假設說你現在可以採取的 actions， 就是向左，向右跟開火
那你的這個 network， 它的 output layer，就有 3 個 neurons
分別對應到向左，向右跟開火
好，哪假設現在輸入這個遊戲畫面
向左的分數是 0.7，向右的分數是 0.2， 開火的分數是 0.1
那最後 actor 會決定採取哪一個 action 呢？
通常你會做一個 sampling，你就根據這個數值， 產生一個 probability distribution
有 70% 機率向左，20% 機率向右，10% 機率開火
當然你也可以說，我不想要 stochastic 的 actor
我們剛剛有說 70% 機率向左，20% 機率向右，10% 機率開火
意味著說，你的 actor 在同一個畫面下，會採取不同行為
然後它是 stochastic，它每次採取的行為都不一樣
這樣當然有好處， 這樣的好處就是你的對手比較不容易識破你要做的事情
當然如果你不喜歡這樣的話，你也可以說，看哪一個 action 得到的分數最高，我們就採取那個 action
這樣也可以，反正就 depend on 你要怎麼設計你的 actor
都是可以的
那過去其實會用一個 lookup table， 來當作你的 actor
那用 lookup table 有什麼壞處呢？有人會說，如果用 network 參數會比較多
用 lookup table 參數會比較少
其實不是，用 lookup table 參數太多
用 lookup table 如果你 input 是遊戲畫面， 根本無法處理
因為遊戲畫面是無窮無盡的， 你根本無法窮舉所有可能發生的遊戲畫面
你用 lookup table 就不 work
但是如果你是用 neural network， 就算是從來沒有看過的遊戲畫面
你也可以把遊戲畫面丟進去看它會得到什麼樣的結果
假如你這個 network train 的夠好， 它有 generalization 的能力
那你也可能可以得到好的回應
那我們回到我們剛才要做的事情
我們說 actor/environment/reward， 他們中間的互動就是這個樣子
那我們最想要做的事情是什麼，我們想要做的是事情是
希望調整 actor 的參數， 注意一下我們都講說 actor 都是一個 neural network 嗎
neural network 就是輸入一個遊戲畫面，就是 state
output 就是現在要採取的一個 action a
所有遊戲畫面，採取一個 action a
actor 是一個 neural network
那我們希望說，現在能夠達成的目標
是希望整個 episode 所有 reward 合起來， 它得到 total reward 大 R 的值
越大越好
你，如果看這個圖，你知道怎麼解這個問題嗎？
仔細想想，這個問題應該沒有那麼難 對不對
你想想看，假設
reward function 也是一個 neural network
假設 reward function 也是一個 neural network
這個 neural network 就是輸入 state and action 給你一個分數
假設 environment 也是一個 neural network
給它一個 action 它就輸出一個 state
這一整個畫面上，這三個neural network 串起來
不就只是一個巨大的 neural network 而已嗎？
它可能在這邊有一個類似隨機的 random seed
當作 input 去決定初始的畫面是什麼
然後最後 output 就是一個數值
然後我們現在這個目標， 就是希望這個數值，越大越好
大家可以想像啊
這是一個巨大的，假設這些東西都是 network
把它們通通串起來，你就只是有一個巨大的 network 然後你希望它的輸出，越大越好
那我們剛才有講說 reward and environment 你是動不了的
那是別人的東西，你動不了， 你唯一能調的只有 actor 參數
在這個巨大的 network 裡面， 只有藍色的部分的參數是你是可以調的
你要調藍色的這部分的參數， 使得最終輸出的值，越大越好
這個你會做嗎？ 這不就只是 Gradient ascent 而已嗎？
你想想看，你想讓這個 R 越大越好
然後你從這個地方，一路 back propagate 回來
然後你就可以調這個 actor 的參數， 希望最終讓的 output 越大越好
這樣大家可以接受這個想法嗎？
你自己想想看， 這東西聽起來跟 GAN 也蠻像的，那講 GAN 的時候
我們說有一個 generator， 然後 generator 會把它的輸出接給 discriminator
然後 discriminator 要調整它的參數， 讓輸出的值越大越好
那現在我們有 environment，有 reward，有 actor
這個 environment 跟 reward， 你可以想成是 discriminator
我們要調 actor 的參數，actor 你可以想成是 generator
我們要調 actor 參數，讓它輸出的東西
通過 reward function 以後，output 的值，越大越好
這個不是跟前面 GAN 做的事情，非常地像嗎？
那這個東西你會不會做呢？
假設 reward 是一個 network，environment 是一個 network
用 back propagation 一路 back propagate 回去， 就可以調 actor 參數，去 maximize 最終的 output
這個我假設你是會做的
那現在的難點是什麼，現在的難點是
這個 environment 跟 reward 就不是 neural network 啊
它是個黑盒子
你根本不知道它是什麼東西
如果下圍棋的話，你的 environment 是個對手
它其實也是個 neural network 啦， 不過你沒有辦法把它剖開來看就是了
如果下圍棋的話， 你的 reward 就是圍棋的規則，那它很複雜
你可能也不知道怎麼 把它用一個 neural network 來表示它
現在真正遇到的問題是，我們想要 maximize 這個 R
但是 reward function 它的參數我們不知道 environment function 的參數，我們也不知道
怎麼辦？
本來如果這兩個東西就是 neural network， 它是我們知道怎麼調整參數去 maximize 它
但實際上不知道
那怎麼辦呢，這邊遇到記得一個口訣
那個口訣就是， 如果你今天發現你要 optimize 的 function 不能微分的話
就用 policy gradient 這個技術硬 train 一發， 就結束了
如果你想要知道 policy gradient 是什麼的話， 請參看下面這個連結
今天只要記得說，如果有叫你 maximize 一個東西
比如說我要 maximize 這個 R
但問題就是這個東西， 有這個 environment 跟 reward，導致我們無法微分
那怎麼辦，反正就是有一招，叫做 policy gradient
它可以去調這個 actor 的參數 讓我們最終可以 output 這個 R，就結束了
這個就是 reinforcement learning 做的事情
我知道這跟你一般平常聽到的 reinforcement learning 講法有點不太一樣
通常如果你看 David Silver 的 video， 通常先從 Markov decision process 開始講
然後等你聽完 Markov decision process，以為自己聽懂了，然後就開始想睡了，然後剩下的東西，你都聽不懂了
然後再講一次，其實你也是聽不懂， 所以這邊是採取一個不太一樣的講法
告訴你，其實 reinforcement learning 做的事情， 就是這樣
Q&A 時間
好那我們就不要講 policy gradient 的部分， 你只要記得這個口訣
就是發現只要是不能微分的東西， policy gradient 就是可以幫你 optimize 就是了
那 policy gradient，這邊其實也不是推導啦
這邊只是要講實作是怎麼做的， 推導的部分，還有實做具體怎麼做
永遠可以看底下我線上課程的錄影
這個部分，就把它跳過
好，那我們剛才講了 actor 怎麼 train
接下來我們要講另外一個東西，這個東西叫做 critic
Critic 做的事情是什麼， Critic 本身並沒有辦法決定要採取哪一個 action
那 Critic 可以做的事情是什麼呢？Critic 可以做的事情是
給它一個 actor pi， 它可以告訴你說這個 actor pi 有多好
什麼叫做這個 actor pi 有多好呢？ 這個 Critic 其實有很多種
我們今天介紹一個 state value function，寫成 V(pi) of x
它做的事情是，給它一個 actor pi， 它告訴你說，現在在給一個 actor pi 的前提下
假設我們看到一個 observation or state s
接下來，一直到遊戲結束的時候， 會得到 reward 的總和期望值有多大
注意一下，今天我們算的， 並不是看到這個 state 之後，下一秒會得到的 reward
而是看到這個 state 以後， 所有 accumulated 的 reward 的期望值
或者舉例來說，以下圍棋為例的話
這個 V(pi) of x 的意思就是說， 假設你已經有一個下圍棋的 agent
叫做 pi
那你現在給它一個 observation，就是棋盤的盤勢
比如說，出手天元
接下來 V(pi) of x 的意思就是說， 從出手下到天元，一直到遊戲結束為止
假設今天在圍棋裡面，遊戲結束， 贏了就得到分數 1，輸了就得到分數 -1
在其他的狀況下，得到的分數都是 0
那 V(pi) of x，x 是出手天元，假設 x 出手天元的話
V(pi) of x 就是 假設出手下在天元， 接下來獲勝的機率有多大，就是 V(pi) of x
或出手下在天元，你的 actor 是 pi， 那你獲勝的機率有多大
這個就是 V(pi) of x
而透過圖像化的方式來畫它的話， 就是有一個 function 叫做 V(pi)
給它一個 state，然後它就會 output 一個數值
叫做 V(pi) of x
那這個數值代表什麼？這個數值就代表了說，這個 actor
假設我們用 pi 這個 actor， 在 state s 的時候，接下來，看到 state s
接下來它會得到的 reward 期望值有多大
舉例來說，假設你有一個很強的 actor
然後它看到這個遊戲的畫面，接下來還有很多怪
因為它有很多怪可以殺， 所以接下來它可以得到很高的分數
所以 V(pi) of s 就很大
這邊舉另外一個例子，遊戲畫面剩下怪已經很少了
V(pi) of s 就會比較小，因為它到遊戲結束的時候
可以得到的分數就比較少了， 因為其他怪都已經被殺完了
剩下的怪，已經很少了
這邊有一件事你要特別注意， 當你看到這種 Critic 的時候
Critic 都是 depend on actor 的， 給不同的 actor 就算是同樣的 state
Critic 的 output 也是不一樣的
所以我們說 Critic 的工作，就是衡量一個 actor 好不好
所以給它不同的 actor，就算是同一個 state
它得到的分數也是不一樣
我們舉一個棋靈王的例子， 這例子是這樣子（只註記提到 Critic 部分）
所以如果把它對應到 Critic 的話
如果 actor 是以前的阿光， 那你不應該下，大馬步飛
以前的阿光， 如果 state 是下大馬步飛，是壞的
因為以前的阿光比較弱，下大馬步飛的話， 這個比較複雜，所以會下的不好
但是因為阿光它後來變強了，所以如果是要 evaluate 變強的阿光這個 actor 的話
下大馬步飛，就會變得好
這個所要強調的意思是說， 今天你的 Critic 其實會隨著 actor 的不同
而得到不同的分數
怎麼算這個 Critic，怎麼評估這個 Critic
有兩個方法，一個是 Monte-Carlo 的方法， 另外一個是 temporal-difference 的方法
那 Monte-Carlo 的方法其實非常的直觀，它就是說
今天 Critic 怎麼衡量一個 actor 好不好， 它就去看那個 actor 玩遊戲
假設我們以玩遊戲為例子，打電玩為例子
Critic 就去看那個 actor pi 玩遊戲， 看 actor pi 玩得怎麼樣
那假設現在 Critic 觀察到說， actor pi 在經過這個 state Sa 以後
它會得到的 accumulated 的 reward，這從 state Sa 之後
這個 actor pi 它會得到的 reward 是 Ga， 那這個 Critic 就要學說
如果 input state Sa，那我的 output 要跟 Ga 越接近越好
這不過是一個 regression 的問題嘛
這個 actor 要調它的參數， 那它的 output 跟 Ga 越接近越好
那假設又觀察到說，現在 actor 跑到 state b
玩到遊戲結束的時候，會得到 accumulated reward Gb
那現在輸入 state b
那它的 output 就要跟 Gb 越接近越好
這個很直觀，這個就是 Monte-Carlo 的方法
另外一種叫做不直觀的方法， 叫做 Temporal-Difference 的方法
Temporal-Difference 方法是說， 現在一樣讓 Critic 去看 actor 玩遊戲
那 Critic 看到 actor 在做什麼呢？
它看到 actor 在 state st 採取 action at， 接下來得到 reward rt，然後跳到 state s(t+1)
光看到這樣一個 data，一筆 data， 之前我們在前一頁做 Monte-Carlo 的時候
看到某一個 state，我們必須要一直玩到遊戲結束， 才知道 accumulated reward 是多少
但是在 temporal-difference 的時候， 只要看這樣一筆 data
那個，Critic 就可以學了， actor 只要在某一個 state 採取某一個行為
Critic 就可以學了
為什麼Critic 就可以學呢，它就是 based on 這個式子
因為我們現在 V(pi) of s(t)
是要衡量在 s(t) 這個 state 會得到的 accumulated 的 reward
V(pi) of s(t+1)
是要衡量在 s(t+1) 這個 state 會得到的 accumulated 的 reward
今天如果我們觀察到，在 s(t) 這個 state
會得到 reward r(t)，跳到 s(t+1)
意味著說， 在 s(t+1) 和 s(t) 中間它們差了 reward 就是 r(t)
這一項，你會得到 accumulated reward 是這一項
然後這一項，你得到 accumulated reward 是這一項
他們中間經過了得到 reward r(t) 這一件事， 所以他們中間的差異，就是 r(t)
那你在訓練 network 的時候怎麼辦呢？
訓練 network 你就說，現在把 s(t) 丟進去，你就會得到一個分數 ，把 s(t+1) 丟進去，你會得到另外一個分數
我們希望這兩個分數的差，跟 r(t) 越接近越好
所以現在的訓練目標，你不知道這個實際上的是值多少，你不知道這個實際上的是值多少，不知道
因為還沒有玩到遊戲結束嘛， 你不知道 accumulated reward 是多少
你學到一件事，你學到的事情是
雖然我不知道他們值是多少，但我知道它們差了 r(t)
所有就告訴 machine 說，看到 s(t) 你輸出的值
跟你看到 s(t+1) 你輸出的值， 中間要差了 r(t)，然後 learn 下去，就結束了
那用 temporal-difference， 有一個非常明確的好處，就是
今天當遊戲還沒有結束，玩到一半的時候
你就可以開始 update 你的 network
那有時候有些遊戲非常的長，如果你沒有辦法一邊玩遊戲
一邊 update 你的 network 的話，那你會搞太久， 所以在 temporal-difference 是有它的好處的
略過
接下來，我們剛才講了一個 critic， 這個 critic 是給它一個 state
它會衡量說，這個 state 到遊戲結束的時候 會得到多少的 reward
但那一種 critic 沒有辦法拿來決定 action
但是有另外一種 critic，它可以拿來決定 action， 這種 critic 我們叫做 Q function
這種 Q function 它的 input 就是一個 state，一個 action
那它到底在量什麼，它量的就是，給我一個 actor pi
然後量說，在給我一個 actor pi 的前提之下，
在 observation s，採取了 action a
在這個 state 採取了 action a 的話，到遊戲結束的時候，會得到多少 accumulated reward
之前第一它只量在 s 的時候，會得到多少的 reward
現在是量測，在 s 採取了 a 會得到多少 的 reward
當你採取的 action 不同，非常直觀， 你採取 action 不同，你得到的 reward 就不一樣嘛
之前 V 它沒量這件事，它沒量說你會採取哪一個 action
今天 Q 會量說，在 state s 採取 action a 的時候， 會得到多少的 reward
那所以 Q 呢，理論上它會有兩個 input，s 跟 a
它吃 s 跟 a，決定說它要得到多少的分數
那有時候我們會改寫這個 Q function， 假設你的 a 是可以窮舉的
舉例來說，在玩遊戲的時候， a 只有向左／向右，跟開火三個選擇
那你就可以說，我們現在呢，任一個 Q function
我們的 Q function 是 input 一個 state s
它的 output 分別就是 Q(pi) of (s, a=left)， Q(pi) of (s, a=right)，Q(pi) of (s, a=fire)
這樣的好處就是，你只要輸入一個 state s ， 你就可以知道說
s 配上，向左的時候，分數是多少，s 配上向右的時候，分數是多少，s 配上開火的時候，分數是多少
那在這個 case 你必須要把不同的 a 一個一個帶進去， 你才可以算出它的 Q function
但在這個 case，你只要 s 帶進去， 就可以一次把不同的 action，它的分數都算出來
那有了這個 Q 有什麼用呢？
它的妙用是這個樣子， 你可以用 Q function 找出一個比較好的 actor
這一招就叫做 Q learning
所以這個 Q learning 的整個 process 是這樣， 你有一個已經初始的 actor pi
然後這個 actor pi，去玩，去跟這個環境互動
然後我們說，critic 的工作就是去觀察， 這個 actor pi 它跟環境的互動
那它可以透過 TD or MC 的方法，去學出這個 Q function
它可以去估測說，根據這個 pi 跟環境互動的資料， 用 TD or MC 的方法
你可以估測說，現在給定這個 actor 的前提之下
在某一個 state 採取某一個 action， 得到的 Q value 是多少
假設估出這種 Q function 以後，估測出這種 critic 以後，可以保證一件事，細節我們下一頁投影片講
可以保證什麼事？可以保證我們說， 我們一定能夠找到一個新的 actor pi，
它比原來的 pi 更好，我們本來有一個 actor pi
我們就觀察它，去跟環境互動的狀況，然後我們估測出
我們認出一個 critic，它估測 actor pi， 在某一個 state 採取某一個 action 的時候
會得到的分數
然後接下來，有了這個 Q 以後
保證我們可以找到一個，另外一個新的 actor pi prime
它比原來的 pi 還要好， 這樣我們是不是就找到一個比較好的 actor 了
所以你有這個比較好的 actor pi prime 以後， 你就把這個 pi 用 pi prime 取代掉
你有新的 actor，觀察一下， 量出新的 actor 的 Q function，再找到一個更好的 actor
本來是 pi prime，那現在就變成 pi double prime，
然後再重新來一次，那你的 actor 是不是越找越好？
那這就是我們要的，你就可以找到越來越好的
所以 Q function 的精神就是這樣， 重點的地方就是這一步
就這個打問號這一步，只要量得出 Q function
接下來就一定可以找到一個更好的 actor pi prime
好，那 **** 到底怎麼說的呢？
它的理論就是這個樣子，它的理論是說， 證明在下一頁，證明我就不要講這樣子
理論是這樣，理論上是說， 什麼叫做 pi prime 一定比 pi 好
pi prime 比 pi 好的定義是說，給所有可能的 state s
如果你用 pi 去玩這個遊戲，得到的 reward
用 pi 去玩遊戲得到的 accumulated reward， 一定會小於
我們已經定義過 V 了嘛，這就是為什麼前面要講 V， 就是為了要講這一個啦，V 的定義大家已經知道了
給所有可能的 state s，如果你採取 pi 這個 actor
跟你採取 pi prime 這個 actor， pi prime 這個 actor 得到的 accumulated reward
一定會大過 pi 這個 actor， 所以 pi prime 會得到 reward 一定會比 pi 大
不管是哪一個 state， 那就代表 pi prime 是一個比較好的 actor
那怎麼根據這個 Q 找到一個比較好的 actor pi prime 呢？
它的原理就是只有下面這條式子
這個比較好的 actor pi prime 怎麼來啊？
這個 pi prime 就是，給你一個 Q function
這個 Q function 是拿來衡量 pi 這個 actor 的 Q function
然後我們說，給某一個 state 的時候
窮舉所有可能的 action， 看哪一個 action 的 Q value 最大
把那個 action 當作新的 actor，pi prime 的輸出
就我們一個新的 actor pi prime，但它其實是空的，它其實不太會做決定，它怎麼做決定
它都聽 Q 的，它自己其實沒有參數，它都聽 Q 的
所以你說給你一個 state s， pi prime 你想要做什麼樣的行為呢？
它就說，那我們把 Q 找出來， Q 其實只看過 pi ，它是看 pi 的
Q 是說，它之前看過 pi 做過的事情，它知道說
pi 這個 actor 在 s 採取 a 的時候，會得到多少的 reward
然後它說窮舉所有可能的 a， 看看哪一個 a 可以讓這個 function 最大
然後這個 a，pi prime 就說這就是它的輸出了， 然後就結束了
然後這個 pi prime 呢，就一定會比 pi 還要好
這邊有一個顯然的問題是什麼問題？ 就是你怎麼解這個 arg max 的 problem
如果 a 是 discrete 的，只有向左，向右，開火， 你就只需要把向左，向右，開火
分別帶到 Q function 裡面，看你會得到什麼樣的結果
但是比較慘的地方是，Q learning 好像聽起來很厲害
但是如果今天你的 action 無法窮舉
它是 continuous 的，你就爆炸了， 你就不能解這個 arg max 的 problem 了
那至於這個理論的證明，其實還蠻簡單， 一頁就可以講完，但我們就不要講這個了
那 Q 怎了量，你就可以用 TD 來量
那其實 Q learning 有非常非常多的 trick 啦
你要怎麼找那些 trick 呢？ 你就去找一篇 google paper 叫做 rainbow
然後裡面就講了，7 種不同的 DQN 的 tip
因為正好 7 種，就對應到彩虹的 7 個顏色
他在做圖的時候，每一個方法就對應到彩虹的一個顏色
所以他把它的 paper，就取做 rainbow
那我們細節，也許就不需要講，那裏面有很多的技術啦
我覺得比較好實作的是那個 double DQN 跟 Dueling DQN
那細節如果你自己要 implement DQN 的時候， 你再去看看那些 paper
總之，DQN 有很多的 tip 可以讓他做得比較好， 那這些都整理在 rainbow 那篇 paper 裡面了
那最後，我們要講 actor+critic
同時使用的技術， 就我們剛才有講說，怎麼 learn 一個 actor
我們也講說怎麼 learn 一個 critic，我們也講說其實 critic 也有辦法告訴我們說要採取什麼樣的 action 才是對的
那接下來我們要講的是 actor+critic 的技術
那什麼是 actor+critic 的技術呢？
有一個非常知名的方法，叫做 A3C 也許大家都有聽過
那 A3C 就是，的 3 個 A 分別是什麼呢？ 他的前兩個 A 就是 Advantage Actor-Critic
所以這是 A2C，然後等一下再講第三個 A 是什麼
那這個 Advantage Actor-Critic 它是什麼意思呢？
他是說，這邊我們就沒有把細節說出來， 那他的概念其實很簡單
我們之前在 learn 這個 actor 的時候， 我們是看 reward function 的 output
來決定 actor 要怎麼樣 update， 才可以得到最好的 reward
但是今天實際上在這個互動的過程中
有非常大的隨機性，所以直接根據互動的過程學， 可能沒有辦法學得很好
所以 actor-critic 這種方法， 他的精神，細節我們就不要講了
他的精神是什麼？他的精神就是， 今天 actor 不要真的去看環境的 reward
因為環境的 reward 變化太大了， 因為中間有隨機性，變化太大
但是我不要跟環境的 reward 學， 我們只跟 critic 學，這個方法就叫 actor-critic
那怎麼跟 critic 學呢？其實有非常多不同的方法
Advantage Actor-Critic 只是眾多的方法的其中一種而已
那他之所以變得比較有名， 是因為他的 performance，是比較好的
那當然還有很多其他的方法， 可以讓 actor 跟著 critic 學這樣
那總之，只要是 actor 不是真的看環境的 reward， 而是看 critic 的比較來學習的
就叫做 actor-critic，那其中的某有一種方法， 叫做 Advantage Actor-Critic
好，那我們要講 A3C， 我們剛才只講了 Advantage Actor-Critic，只有兩個 A
第三個 A 是什麼呢？第三個 A 是這個 Asynchronous
所以 A3C 完整的名字， 叫做 Asynchronous Advantage Actor-Critic
Asynchronous 的意思是什麼呢？
Asynchronous 的意思是說，你有一個 global 的 network
你本來有一個 global 的 actor 跟 global 的 critic
那現在要去學習的時候，每到學習的時候呢
就去跟 global 的 actor 和 critic copy 一組參數過來
你可以開分身， 假設你要開 N 個分身的話，就是 copy N 組參數，
好，那把參數 copy 完以後呢？ 就讓這個 actor 實際去跟環境互動
那有 N 個 actor，它們就 N 個 actor 各自去跟環境做互動
那互動完以後，就會知道說要怎麼樣，critic 就會告訴 actor 說要怎麼樣 update 參數
那把這個 updated 參數，傳回去 global 的 network
所以每一個分身，都會傳一個 update 的方向， 那把所有 update 的方向合起來
可以一起做 update，那你等於就是做平行的運算
你等於就是平行的開 N個分身學習， 所以可以學得比較快
以下部分略過
這跟 Asynchronous actor critic 的方法是一模一樣的， 你開越多的分身，學習的速度就越快
不過當然在實作上，你要做 asynchronous 這一招， 前提就是，要有很多很多的 machine 這樣子
你想要開 8 個分身，就要 8 個 machine， 開一千個分身，就要一千個 machine
如果你只有一台 machine，你就只能降到 A2C， 你其實也沒有辦法做 A3C 就是了
這邊有一些同學實作做的 actor-critic 在一些遊戲上的結果
以下部分略過
那這邊有一個在網路上找到的 Doom 的比賽
這個其實也蠻知名的， 就有一個 machine 去玩 Doom 的比賽
那還有另外一個技術，是 actor-critic 的一個變形
那我之所以要把他提出來是因為，他非常地像是 GAN
我們剛才有講說， 在做 Q-learning 的時候，我們遇到的一個問題就是
我們要解一個 arg max 的 problem
我們要找一個 action，他要讓 Q function 最大
但是你今天常常會遇到的一個問題就是， 你沒有辦法窮舉所有的 a
今天如果尤其是 a 是一個 continuous vector
舉例來說， 什麼時候你的 action output 會是 continuous？
舉例來說，你想要控制機器手臂， 那你 output 的是關節的角度，那他就是 continuous，
一般的 Q-learning 只能處理 discrete 的 case， 那如果要處理 continuous 要怎麼辦呢？
你就 learn 一個 actor
那這個 actor 做的事情就是， 給他一個 state，那它 output 的那個 a
會是讓 Q function 的值最大的那個 a
這樣大家懂嗎？ 我們有一個 Q function，然後 actor 它 output 的 a
actor 它要去學習，它學習的目標就是希望它 output 的 a
會讓 Q function 的值呢，越大越好
那你如果仔細想想，這不就跟 GAN 是一樣的嗎？
如果這個東西把它當作是 Discriminator， 這個東西當作是 Generator
Generator 要做的事情，就是產生一個 image 是 Discriminator 覺得分數好的
那這邊是 actor 要產生一個 action， 這個 action 是 Q function 覺得分數好的
那這個叫做 Pathwise Derivative Policy Gradient， 那其中比較著名的方法，就是 DDPG
那這個我們就跳過，只是要告訴大家有這個技術而已
那剩下的時間，我想要講一下， Inverse reinforcement learning ，它是什麼呢？
它是 Imitation learning 的一種， 在 inverse reinforcement learning 裡面
你只有 environment 跟 actor
我們剛才講過 environment 跟 actor 它們互動關係是長這個樣子
但是在 Inverse reinforcement learning 裡面， 我們沒有 reward function
我們有的東西是什麼， 我們有的東西就只有這個 expert demo trajectory
如果是遊戲的話，有專家，有高手， 去把這個遊戲，玩了 N 遍
給 machine 看， 告訴 machine 說玩這個遊戲看起來是什麼樣子
但是沒有 reward function
那你可能會說，什麼樣的狀況， 會沒有 reward function 呢？
事實上多數生活中的 case， 我們都是沒有 reward function 的
今天下圍棋是用 reinforcement learning， 是因為圍棋的規則是明確的
輸就是輸，贏就是贏
今天玩電玩可以用 reinforcement learning 就玩電玩的規則是明確的，殺一隻怪得到幾分是訂好的
但在多數的 case，我們根本就不知道 reward function 是什麼，比如說，自駕車
撞到一個人，要扣 10000 分嗎？ 撞到一個狗要扣多少分呢？
或者是說，今天如果你拿 reinforcement learning 的技術去學一個 chat bot
chat bot 要做到什麼樣的事情，才能得到分數呢？
舉例來說，它把人激怒，會扣 100 分嗎？那人掛掉電話，扣 50 分嗎？這個東西你怎麼訂，都是訂不清楚的
而且有時候你用一些自己訂出來的 reward
那如果它跟現實的狀況差很多， machine 會學出很奇怪的東西
舉例來說，其中一個例子就是機械公敵
機械公敵那部影片說，創造機器的人，它訂了 3 大法則
這 3 大法則你就可以想成是，只要違反這 3 大法則
就會得到非常 negative 的 reward， 遵守這 3 大法則，就會得到 positive 的 reward
那機器自己想辦法根據這個規則， 根據這個 reward function
去找出最好的 action
然後它就有一個神邏輯，它決定說最好的 action 就是，為了保護人類，應該把人類監禁起來
那這可能是一個比較極端的例子， 但在真實的研究上，確實有，這樣的例子
舉例來說，你想讓機器學習收盤子， 然後它就把盤子放到櫃子裡面，告訴他說
盤子放到櫃子裏面，你就可以得到一分， 機器確實可以學到，為了要得到分數
它會把盤子放到放到櫃子裏面，但是它可能都用摔的
然後盤子通通都被打破了， 因為你沒有告訴他說，盤子打破要扣分啊
那以後變成說可能盤子都打破以後，才發現
這樣不行，只好再加上新的 reward， 就是打破盤子要扣分，但盤子都已經被打破了
今天我們丟很多現實的任務， 是我們根本就不知道 reward function 長怎麼樣子
所以我們需要 inverse reinforcement learning 這個技術
inverse reinforcement learning 這個技術， 它做的事情就是
在原來的 reinforcement learning 裡面， 我們有 reward function，有 environment
根據 reward function 還有 environment， 用 reinforcement learning 技術找出最好的 actor，
inverse reinforcement learning 技術， 剛好是反過來的，我們有 actor
我們雖然不知道最好的 actor 是什麼，但是我們有專家，專家去玩了 N 場遊戲，告訴我們說
厲害的人玩這個遊戲，看起來就是怎麼樣的
根據專家的 demo，還有 environment
透過一個叫做 inverse reinforcement learning 的技術， 我們可以推出 reward function 應該長什麼樣子
把 reward function 推出來以後， 你就可以根據你推導出的 reward function
再去 apply reinforcement learning 的方法， 去找出最好的 actor
所以你是用 inverse reinforcement learning 的方法去推出 reward function
再用 reinforcement learning 的方法去找出最好的 actor
那 inverse reinforcement learning 是怎麼做的呢？ 它的原則就是
你的老師，就是那些 experts 他永遠是對的
什麼意思，就是說，現在你一開始你有一個 actor， 先隨機的找出初始化一個 actor 出來
然後去用這個 actor 去跟環境做互動， 那這個 actor 會得到很多的 trajectory
會得到很多的遊戲紀錄，然後接下來啊， 你比較 actor 的遊戲紀錄，跟老師的遊戲紀錄
然後你訂一個 reward function，一定要讓， 老師得到的分數，比 actor 得到的分數高
就是先射箭，再畫靶的概念， 就是 expert 去玩一堆遊戲，他有一堆遊戲的紀錄
然後 actor 也去玩遊戲，也有遊戲的紀錄，那我們不知道 reward function 是什麼
等他們都玩完以後，再訂一個 reward function
訂的標準就是，老師，就是這個 expert 得到的分數
一定要比學生還要高這樣子，先射箭，再畫靶的概念
好那把靶畫完以後，學生說，好吧， 雖然因為根據這個新的 reward，我是比較弱的
沒關係，那我就再去學習， 我想要 maximize 新的 reward function
actor 學到 maximize 新的 reward function 以後， 他就再去跟環境互動，他就會得到新的 trajectory
他得到新的 trajectory 以後，他本來以為，他跟老師一樣厲害了
但是不幸的就是，那個規則是會一直改的
當他變得跟老師一樣厲害以後， 我們再改一下規格，讓老師算出來的分數，還是比較高
然後 actor 就只好很生氣的，想辦法學， 想要跟老師做的一樣好
就反覆反覆進行這個 process，最後， 就可以找到一個 reward function
那這整個 process， 我們用圖示畫來表示一下，有一個 expert
他有 N 筆遊戲紀錄，然後你有一個 actor， 它也有 N 筆遊戲紀錄
然後你要訂一個 reward function， 讓 expert 得到的分數，永遠贏過 actor
然後你接下來，反正你去找一個 reward function， 老師一定是好的，它一定是不好的
接下來根據這個 reward function
你可以去學這個 actor，根據這個 reward function， 你可以去學這個 actor
讓這個 actor 根據這個 reward function， 可以得到最好的分數
但等這個 actor 做得比較好之後， 這個規則又變了，然後這個 process
又反覆的循環，這個 process，你有沒有覺得很熟悉呢？
它跟 GAN 的 process，其實是一模一樣的，怎麼說？
在 GAN 裡面，有一堆 expert 畫的圖
generator 會產生一堆圖，discriminator 說， 只要 expert 畫的就是好的，這些就是高分
這些就是低分，你 learn 出一個 discriminator
generator 要做的事情是，調整它畫出來的圖， 使得 discriminator，覺得它是高分
但是 generator 以為它畫的圖
discriminator 會給它高分， 但是 discriminator 會 update 參數
再使得 generator 畫的圖，得到低分， 然後就反覆的，不斷去畫
事實上，在 inverse reinforcement learning 裡面， 我們只是把 generator 換個名字叫做 actor
把 discriminator 換個名字，叫做 reward function，
我們說 actor 會產生一大堆遊戲的紀錄， 但是我們要訂一個 reward function
反正就是要讓 actor 輸，讓老師贏， 然後 actor 就會修改它做的事情
希望可以得到比較好的分數
在 actor 修改以後，reward function 也會跟著修改
然後就這樣反覆地進行這個 process
在這個結束之前呢，我就給大家看一個， 這個是 Berkeley 做的
用 inverse reinforcement learning 的技術來教機器人做一些行為
以下部分省略
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
