臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
接下來我們要講的是 train Q learning 的一些 tip
在作業裡面當然是會要求你 implement 某一個 tip
那你等一下就看看說，哪一個 tip 是你覺得最簡單的
第一個要介紹的 tip，叫做 double DQN
那為什麼要有 double DQN 呢？
因為在實作上，你會發現說
Q value 往往是被高估的
那下面這幾張圖是來自於 double DQN 的原始 paper
它想要顯示的結果就是
Q value 往往是被高估的
這邊就是有 4 個不同的小遊戲
那橫軸是 training 的時間
然後紅色這個鋸齒狀一直在變的線就是 Q function 所 estimate 出來的
對不同的 state estimate 出來的平均 Q value
就有很多不同的 state，每個 state 你都 sample 一下
然後算它們的 Q value，把它們平均起來
這是紅色這一條線
它在 training 的過程中會改變
但它是不斷上升的，為什麼它不斷上升，因為很直覺
不要忘了 Q function 是 depend on 你的 policy 的
你今天在 learn 的過程中你的 policy 越來越強
所以你得到 Q 的 value 會越來越大
在某一個 state，同一個 state， 你得到 expected reward 會越來越大
所以 general 而言，這個值都是上升的
但是它說，這是 Q network 估測出來的值
接下來你真的去算它，那怎麼真的去算， 很容易啊，你有那個 policy
然後真的去玩那個遊戲
你就可以估說，你就可以真的去算說， 就玩很多次，玩個 1 百萬次
然後就去真的估說，在某一個 state， 你會得到的 Q value，到底有多少
你會得到說在某一個 state，採取某一個 action
你接下來會得到 accumulated reward 的總和是多少
那你會發先說，實際的值，跟估測出來的值
估測出來的值是遠比實際的值大
在每一個遊戲都是這樣，都大很多
所以他今天要 propose double DQN 的方法就是
它可以讓估測的值跟實際的值是比較接近的
那我們還沒有講 double DQN 的方法， 但我們先看它的結果
藍色的會鋸齒狀的線是 double DQN 的 Q network 所估測出來的 Q value
藍色的是真正的 Q value，你會發現他們是比較接近的
但還有另外一個有趣可以觀察的點就是說
你會發現用 double DQN 所估出來的 Q value
就是說估測出來的都不用管它， 用 network 估測出來的就不用管它
那比較沒有參考價值
但是如果是真正的 accumulated reward
用 double DQN 得出來真正的 accumulated reward
在這 3 個 case，都是比原來的 DQN 高的
代表 double DQN learn 出來那個 policy 比較強
所以它實際上得到的 reward 是比較大的
雖然說看那個 Q network 的話
一般的 DQN 的 Q network 虛張聲勢
高估了自己會得到的 reward
但實際上它得到的 reward 是比較低的
那接下來要講的第一個問題就是
為什麼 Q value 總是被高估了呢？
這個是有道理的
因為想想看，我們實際上在做的時候
我們是要讓左邊這個式子
跟右邊我們這個 target，越接近越好
那你會發現說，target 的值
很容易一不小心就被設得太高
為什麼 target 的值很容易一不小心就被設得太高呢？
因為你想想看，在算這個 target 的時候
我們實際上在做的事情是說
看哪一個 a 它可以得到最大的 Q value
就把它加上去，就變成我們的 target
所以今天假設有某一個 action
它得到的值是被高估的，舉例來說， 我們現在有 4 個 actions
那本來其實它們得到的值都是差不多的
他們得到的 reward 都是差不多的
但是在 estimate 的時候，那畢竟是個 network
所以 estimate 的時候是有誤差的
所以假設今天是第一個 action
它被高估了，假設綠色的東西代表是被高估的量
它被高估了，那這個 target 就會選這個 action
然後就會選這個高估的 Q value
來加上 rt，來當作你的 target
如果第 4 個 action 被高估了
那就會選第 4 個 action 來加上 rt
來當作你的 target value
所以你總是會選那個 Q value 被高估的
你總是會選那個 reward 被高估的 action 當作這個 max 的結果
去加上 rt 當作你的 target
所以你的 target 總是太大，對不對
那怎麼解決這 target 總是太大的問題呢？
那 double DQN 它的設計是這個樣子的
在 double DQN 裡面
選 action 的 Q function
跟算 value 的 Q function，不是同一個
今天在原來的 DQN 裡面
你窮舉所有的 a
把每一個 a 都帶進去， 看哪一個 a 可以給你的 Q value 最高
那你就把那個 Q value 加上 rt
但是在 double DQN 裡面
你有兩個 Q network
第一個 Q network，決定那一個 action 的 Q value 最大
你用第一個 Q network 去帶入所有的 a
去看看哪一個 Q value 最大
然後你決定你的 action 以後
實際上你的 Q value 是用 Q prime 所算出來的
這樣子有什麼好處呢？ 為什麼這樣就可以避免 over estimate 的問題呢？
因為今天假設我們有兩個 Q function
假設第一個 Q function 它高估了它現在選出來的 action a
那沒關係，只要第二個 Q function Q prime
它沒有高估這個 action a 的值
那你算出來的，就還是正常的值
那今天假設反過來是 Q prime 高估了某一個 action 的值
那也沒差， 因為反正只要前面這個 Q 不要選那個 action 出來
就沒事了
這個就跟行政跟立法是分立的概念
是一樣的，這樣大家了解嗎？
這一個 Q，它只能夠提案
它不能夠執行，Q 負責提案，它負責選 a
Q prime 負責執行，它負責算出 Q value 的值
所以今天就算是前面這個 Q，做了不好的提案
它選的 a 是被高估的
只要後面 Q prime 不要高估這個值就好了
那就算 Q prime 會高估某個 a 的值
只要前面這個 Q 不提案那個 a
算出來的值就不會被高估了
所以這個就是 double DQN 神奇的地方
然後你可能會說，哪來兩個 Q 跟 Q prime 呢？
哪來兩個 network 呢？其實在實作上
你確實是有兩個 Q value 的
因為一個就是你真正在 update 的 Q
另外一個就是 target 的 Q network
就是你其實有兩個 Q network， 一個是 target 的 Q network
一個是真正你會 update 的 Q network
所以在 double DQN 裡面，你的實作方法會是
你拿真正的 Q network
你會 update 參數的那個 Q network
去選 action
然後你拿 target 的 network，那個固定住不動的 network
去算 value
而那 double DQN 相較於原來的 DQN 的更動是最少的
它幾乎沒有增加任何的運算量，你看連新的 network 都不用
因為你原來就有兩個 network 了
你唯一要做的事情只有，本來你在找最大的 a 的時候
你在決定這個 a 要放哪一個的時候
你是用 Q prime 來算
你是用 freeze 的 那個 network 來算
你是用 target network 來算
現在改成用另外一個會 update 的 Q network 來算
這個應該是改一行 code 就可以解決了，就這樣
所以假設今天，等一下會講
好幾個 tip，假如你今天只選一個 tip 的話
正常人都是 implement double DQN
所以這個就是輕易的就可以 implement
這是第一個你可以嘗試的 tip
那第二個 tip，叫做 dueling 的 DQN
dueling DQN 是什麼呢？
其實 dueling DQN 也蠻好做的，相較於原來的 DQN
它唯一的差別是改了 network 的架構
等一下你聽了如果覺得
聽了有點沒有辦法跟上的話，你就要記住一件事
dueling DQN 它唯一做的事情
是改 network 的架構
我們說 Q network 就是 input state
output 就是每一個 action 的 Q value
dueling DQN 唯一做的事情，是改了 network 的架構
其它的演算法，你都不要去動它
那 dueling DQN 它是怎麼改了 network 的架構呢
它是這樣說的
本來的 DQN 就是直接output Q value 的值
現在這個 dueling 的 DQN 就是下面這個 network 的架構
它不直接 output Q value 的值
它是怎麼做的？
它在做的時候，它分成兩條 path 去運算
第一個 path，它算出一個 scalar
那這個 scalar 我們叫做 V of s
因為它跟 input s 是有關係，所以叫做 V of s，
V of s 是一個 scalar
那下面這個呢，它會 output 另外一個 vector
這個 vector 叫做 A of (s, a)
那下面這個 vector，它是每一個 action 都有一個 value
然後你再把這兩個東西加起來，就得到你的 Q value
如果你覺得這樣不夠具體的話
實際上做的事情就是這樣
以下為白板解說
但是接下來你要問的問題就是，這麼改有什麼好？
所以接下來就是想要講一下，說這麼改有什麼好呢？
那我們假設說，原來的 Q of (s, a)
它其實就是一個 table
對不對？我們假設 state 是 discrete 的
那實際上 state 不是 discrete 的
那為了說明方便，我們假設就是只有 4 個不同的 state
只有3 個不同的 action
所以 Q of (s, a) 你可以看作是一個 table
那我們說 Q of (s, a) 等於 V of s 加上 A of (s, a)
那 V of s 是對不同的 state 它都有一個值
A of (s, a) 它是對不同的 state，不同的 action
都有一個值，那你把這個 V 的值加到 A 的每一個 column
V 的值加到 A 的每一個 column
就會得到 Q 的值
把 2+1，2+(-1)，2+0，就得到 3，1，2，以此類推
所以你就把這個東西，給它加上去
加上去，加上去，就得到上面這個
你把 V 加上 A，就得到 Q
那今天假設說，你在 train network 的時候
你現在的 target 是希望，這一個值變成 4
這一個值變成 0
但是你實際上能更動的，並不是 Q 的值
你的 network 更動的是 V 跟 A 的值
根據 network 的參數，V 跟 A 的值 output 以後
就直接把它們加起來，所以其實不是更動 Q 的值
然後在 learn network 的時候
假設你希望這邊的值，這個 3 增加 1 變成 4
這個 -1 增加 1 變成 0
最後你在 train network 的時候，network 可能會選擇說
我們就不要動這個 A 的值
就動 V 的值，把 V 的值，從 0 變成 1
那你把 0 變成 1 有什麼好處呢？
這個時候你會發現說，本來你只想動這兩個東西的值
那你會發現說，這個第三個值也動了
所以有可能說你在某一個 state
你明明只 sample 到這 2 個 action
你沒 sample 到第三個 action
但是你其實也可以更動到第三個 action 的 Q value
那這樣的好處就是
你就變成你不需要把所有的 state action pair 都 sample 過
你可以用比較 efficient 的方式
去 estimate Q value 出來
因為有時候你 update 的時候
不一定是 update 下面這個 table
而是只 update 了 V of s
但 update V of s 的時候，只要一改
所有的值就會跟著改
這是一個比較有效率的方法，去使用你的 data
這個是 Dueling DQN 可以帶給我們的好處
那可是接下來有人就會問說
真的會這麼容易嗎？
會不會最後 learn 出來的結果是說，反正 machine 就學到說我們也不要管什麼 V 了，V 就永遠都是 0
然後反正 A 就等於 Q
那你就沒有得到任何 Dueling DQN 可以帶給你的好處， 就變成跟原來的 DQN 一模一樣
所以為了避免這個問題，實際上你會對下面這個 A
下一些 constrain，你要給 A 一些 constrain
讓 update A 其實比較麻煩
讓 network 傾向於 會想要去用 V 來解問題
舉例來說
你可以看原始的文件，它有不同的 constrain 啦
那一個最直覺的 constrain 是
你必須要讓這個 A 的每一個 column 的和都是 0
每一個 column 的值的和都是 0
所以看我這邊舉的的例子，我的 column 的和都是 0
那如果這邊 column 的和都是 0
這邊這個 V 的值
你就可以想成是上面 Q 的每一個 column 的平均值
這個平均值，加上這些值
才會變成是 Q 的 value
所以今天假設你發現說你在 update 參數的時候
你是要讓整個 row 一起被 update
你就不會想要 update 這邊
因為你不會想要 update Ａ這個 matrix
因為 A 這個 matrix 的每一個 column 的和都要是 0
所以你沒有辦法說，讓這邊的值，通通都 +1
這件事是做不到的
因為它的 constrain 就是你不可以你的和永遠都是要 0
所以不可以都 +1
這時候就會強迫 network 去 update V 的值
然後讓你可以用比較有效率的方法，去使用你的 data
那實作上怎麼做呢？
所以實作上我們剛才說
你要給這個 A 一個 constrain
那所以在實際 implement 的時候，你會這樣 implement
假設現在你的 network 的 output 是，7 3 2 好了
就舉個例子，假設你有 3 個 actions
然後在這邊 output 的 vector 是 7 3 2
你在把這個 A 跟這個 B 加起來之前
先加一個 normalization
就好像做那個 layer normalization 一樣
加一個 normalization，這個 normalization 做的事情
就是把 7+3+2 加起來等於 12，12/3 = 4
然後把這邊通通減掉 4，變成 3, -1, 2
再把 3, -1, 2 加上 1.0，得到最後的 Q value
然後這個東西啊，就是 network 的一部分
這樣你聽得懂嗎？這個 normalization 的這個 step
就是 network 的其中一部分，在 train 的時候
你從這邊也是一路 back propagate 回來的
只是 normalization 這一個地方，是沒有參數的
它是沒有參數，它就是一個 normalization 的 operation
那它可以放到 network 裡面
跟 network 的其他部分 jointly trained
這樣 A 就會有比較大的 constrain
這樣 network 就會給它一些 benefit， 傾向於去 update V 的值
這個是 Dueling DQN
那其實還有很多技巧可以用， 這邊我們就比較快的帶過去
有一個技巧叫做 Prioritized Replay
Prioritized Replay 是什麼意思呢？
我們原來在 sample data 去 train 你的 Q-network 的時候
你是 uniformly 地從 experience buffer
從 buffer 裡面去 sample data
你就是 uniform 地去 sample 每一筆 data
那這樣不見得是最好的， 因為也許有一些 data 比較重要呢
你做不好的那些 data
就假設有一些 data，你之前有 sample 過
你發現說那一筆 data 的 TD error，所謂 TD error 就是
你的 network 的 output 跟 target 之間的差距
你的 TD error 特別大
那這些 data 代表說你在 train network 的時候， 你是比較 train 不好的
那既然比較 train 不好， 那你就應該給它比較大的機率被 sample 到
所以這樣在 training 的時候
才會考慮那些 train 不好的 training data 多次一點
這個非常的直覺
那詳細的式子呢
你再去看一下 paper， 那其實等一下助教在講 prioritized replay 的時候
會講更多東西，因為實際上在做 prioritized replay 的時候
你還不只會更改 sampling 的 process
你還會因為更改了 sampling 的 process
你會更改 update 參數的方法
那這個我們就留給助教講就好
所以 prioritized replay 其實並不只是改變了 sample data 的 distribution 這麼簡單
你也會改 training process，這個我們就不細講
那另外一個可以做的方法是，你可以 balance MC 跟 TD
我們剛才講說 MC 跟 TD 的方法
他們各自有各自的優劣
我們怎麼在 MC 跟 TD 裡面取得一個平衡呢？
那我們的做法是這樣
在 TD 裡面，你只需要存
在某一個 state st
採取某一個 action at
得到 reward rt，還有接下來跳到那一個 state s(t+1)
但是我們現在可以不要只存一個 step 的 data
我們存大 N 個 step 的 data
我們記錄在 st 採取 at，得到 rt，會跳到什麼樣 st
一直紀錄到在第 N 個 step 以後
在 s(t+N) 採取 a(t+N) 得到 reward r(t+N)
跳到 s(t+N+1) 的這個經驗，通通把它存下來
實際上你今天在做 update 的時候， 在做你 Q network learning 的時候
你的 learning 的方法會是這樣，你 learning 的時候
你是要讓 Q of (st, at)
跟你的 target value 越接近越好
而你的 target value 是什麼呢？
你的 target value 是會把從時間 t
一直到 t+N 的 N 個 reward 通通都加起來
然後你現在 Q hat 所計算的，不是 s(t+1)
而是 s(t+N+1)
你會把大 N 個 step 以後的 state 丟進來
去計算大 N 個 step 以後，你會得到的 reward
再加上 multi-step 的 reward
然後希望你的 target value
跟這個 multi-step reward 越接近越好
那你會發現說這個方法，它就是 MC 跟 TD 的結合
因為它就有 MC 的好處跟壞處
也有 TD 的好處跟壞處
那如果看它的這個好處的話
因為我們現在 sample 了比較多的 step
之前是只 sample 了一個 step， 所以某一個 step 得到的 data 是 real 的
接下來都是 Q value 估測出來的
現在 sample 比較多 step，sample 大 N 個 step
才估測 value
所以估測的部分所造成的影響
就會比較輕微，當然它的壞處就跟 MC 的壞處一樣
因為你的 r 比較多項
你把大 N 項的 r 加起來
你的 variance 就會比較大，但是你可以去調這個 N 的值
去在 variance 跟不精確的 Q 之間呢
取得一個平衡
那這個就是一個 hyper parameter，你要調這個大 N 到底是多少
你是要多 sample 三步
還是多 sample 五步
這個就跟 network structure 是一樣
是一個你需要自己調一下的值
那還有其他的技術
有一個技術是要 improve 這個 exploration 這件事
我們之前講的 Epsilon Greedy 這樣的 exploration
它是在 action 的 space 上面加 noise
但是有另外一個更好的方法叫做 Noisy Net
它是在參數的 space 上面加 noise
什麼意思，Noisy Net 的意思是說
每一次在一個 episode 開始的時候
在你要跟環境互動的時候
你就把你的 Q function 拿出來
那 Q function 裡面其實就是一個 network 嘛， 就變成你把那個 network 拿出來
在 network 的每一個參數上面
加上一個 Gaussian noise
那你就把原來的 Q function，變成 Q tilde
因為 Q hat 已經用過，Q hat 是那個 target network
我們用 Q tilde 來代表一個 Noisy Q function
那我們把每一個參數都可能都加上一個 Gaussian noise
你就得到一個新的 network 叫做 Q tilde
那這邊要注意的事情是， 我們每次在 sample 在 sample noise 的時候
要注意在每一個 episode 開始的時候
我們才 sample network
這樣大家了解我意思嗎？每個 episode 開始的時候
開始跟環境互動之前，我們就 sample network
接下來你就會用這個固定住的 noisy network
去玩這個遊戲直到遊戲結束
你才重新再去 sample 新的 noise
那這個方法神奇的地方就是
OpenAI 跟 Deep mind 又在同時間 propose 一模一樣的方法
通通都 publish 在 ICLR 2018，兩篇 paper 的方法就是一樣的
不一樣的地方是，他們用不同的方法，去加 noise
那我記得那個 OpenAI 加的方法好像比較簡單
他就直接加一個 Gaussian noise，就結束了
就你把每一個參數，每一個 weight
都加一個 Gaussian noise 就結束了
然後 Deep mind 他們做比較複雜
他們的 noise 是由一組參數控制的
也就是說 network 可以自己決定說
它那個 noise 要加多大
但是概念就是一樣的，總之你就是把你的 Q function
的裡面的那個 network 加上一些 noise
把它變得有點不一樣，跟原來的 Q function 不一樣
然後拿去跟環境做互動
那兩篇 paper 裡面都有強調說，你這個參數啊
雖然會加 noise，但在同一個 episode 裡面
你的參數就是固定的，你是在換 episode， 玩第二場新的遊戲的時候
你才會重新 sample noise，在同一場遊戲裡面
就是同一個 noisy Q network，在玩那一場遊戲
這件事非常重要，為什麼這件事非常重要呢？
因為這是導致了為什麼在 network 這個方法 Noisy Net
跟原來的 Epsilon Greedy 或是其他
在 action 做 sample 方法本質上的差異
有什麼樣本質上的差異呢？
在原來 sample 的方法，比如說 Epsilon Greedy 裡面
就算是給同樣的 state
你的 agent 採取的 action，也不一定是一樣的，對不對
因為你是用 sample 決定的，given 同一個 state
你如果 sample 到說喔，要根據 Q function 的 network
你會得到一個 action，你 sample 到 random
你會採取另外一個 action
所以 given 同樣的 state，如果你今天是用 Epsilon Greedy 的方法
它得到的 action，是不一樣的
但是你想想看，實際上你的 policy
並不是這樣運作的啊
在一個真實世界的 policy，給同樣的 state
他應該會有同樣的回應
而不是給同樣的 state，它其實有時候吃 Q function
然後有時候又是隨機的
所以這是一個比較奇怪的，不正常的 action
是在真實的情況下不會出現的 action
但是如果你是在 Q function 上面去加 noise 的話， 就不會有這個情形
因為如果你今天在 Q function 上加 noise
在 Q function 的 network 的參數上加 noise
那在整個互動的過程中，在同一個 episode 裡面
它的 network 的參數總是固定的
所以看到同樣的 state，或是相似的 state
就會採取同樣的 action
那這個是比較正常的，那在 paper 裡面有說
這個叫做 state dependent exploration
也就是說你雖然會做 explore 這件事， 但是你的 explore 是跟 state 有關係的
看到同樣的 state， 你就會採取同樣的 exploration 的方式
也就是說你在 explore 你的環境的時候
你是用一個比較 consistent 一致的方式
去測試這個環境
也就是上面你是 noisy 的 action，你只是隨機亂試
但是如果你是在參數下加 noise
那在同一個 episode 裡面
裡面你的參數是固定的
那你就是有系統地在嘗試
每次會試說，在某一個 state，我都向左試試看
然後再下一次在玩這個同樣遊戲的時候
看到同樣的 state，你就說我再向右試試看
你是有系統地在 explore 這個環境
Demo
還有另外一個東西， 這個東西叫做 Distributional Q-function
我們就不講它的細節，只告訴你他的大概念
那 Distributional Q-function 我覺得還蠻有道理的， 但是它沒有紅起來
你就發現說沒有太多人真的在實作的時候用這個技術
可能一個原因就是，是因為他不好實作
它怎麼實作，它的意思是什麼？它說
我們說 Q function 到底是什麼意思啊
我們說 Q function 是
accumulated reward 的期望值
所以我們算出來的這個 Q value
它其實是一個期望值
也就是說實際上我在某一個 state 採取某一個 action 的時候，因為環境是有隨機性
在某一個 state 採取某一個 action 的時候
實際上我們把所有的 reward 玩到遊戲結束 的時候所有的 reward，進行一個統計
你其實得到的是一個 distribution
也許在 reward 得到 0 的機率很高
在 -10 的機率比較低，在 +10 的機率比較低
但是它是一個 distribution
那這個 Q value 代表的值是說 我們對這一個 distribution 算它的 mean
才是這個 Q value
我們算出來是 expected accumulated reward
所以這 accumulated reward 是一個 distribution
對它取 expectation，對它取 mean
你得到了 Q value，但是有趣的地方是
不同的 distribution，他們其實可以有同樣的 mean
也許真正的 distribution 是這個樣子
它算出來的 mean 跟這個 distribution 算出來的 mean， 其實是一樣的
但他們背後所代表的 distribution，其實是不一樣的
所以今天假設我們只用一個 expected 的 Q value
來代表整個 reward 的話
其實可能是有一些 information 是 loss 的
你沒有辦法 model reward 的 distribution
所以今天 Distributional Q function 它想要做的事情是
model distribution
所以怎麼做？在原來的 Q function 裡面
假設你只能夠採取 a1, a2, a3, 3 個 actions
那你就是 input 一個 state，output 3 個 values
3 個 values 分別代表 3 個actions 的 Q value
但是這個 Q value，是一個 distribution 的期望值
所以今天 Distributional Q function，它的 ideas 就是
何不直接 output 那個 distribution
但是要直接 output 一個 distribution 也不知道怎麼做嘛， 對不對
但實際上的做法是說， 假設 distribution 的值就分佈在某一個 range 裡面
比如說 -10 到 10，那把 -10 到 10 中間
拆成一個一個的 bin，拆成一個一個的長條圖
拆成一個一個的 bin，舉例來說，在這個例子裡面
每一個 action， 對我們把 reward 的 space 就拆成 5 個 bin
拆成 5 個 bin，拆成 5 個 bin
詳細一點的作法就是， 假設 reward 可以拆成 5 個 bin 的話
今天你的 Q function 的 output， 是要預測說在某一個 bin 裡面
你在某一個 state，採取某一個 action
你得到的 reward，落在某一個 bin 裡面的機率
所以其實這邊的機率的和，這些綠色的 bar 的和應該是 1
它的和應該是 1，它的和應該是 1，它的高度代表說
在某一個 state，採取某一個 action 的時候
它落在某一個 bin 的機率，這邊綠色的代表 action 1
紅色的代表 action 2，藍色的代表 action 3
所以今天你就可以真的用 Q function 去 estimate a1 的 distribution
a2 的 distribution，a3 的 distribution
那實際上在做 testing 的時候， 我們還是要選某一個 action
去執行嘛，那選哪一個 action 呢？
實際上在做的時候
他還是選這個 mean 最大的那個 action 去執行
但是假設我們今天可以 model distribution 的話
除了選 mean 最大的以外
也許在未來你可以有更多其他的運用
舉例來說，你可以考慮它的 distribution 長什麼樣子
若 distribution variance 很大
代表說採取這個 action
雖然 mean 可能平均而言很不錯
但也許風險很高
你可以 train一個 network 它是可以規避風險的
就在 2 個 action mean 都差不多的情況下
也許他可以選一個風險比較小的 action 來執行
這是 Distributional Q function 的好處
那細節怎麼 train 這樣的 Q network，我們就不講
你只要記得說反正 Q network 有辦法 output 一個 distribution 就對了
我們可以不只是估測 mean 的值
我們不只是估測得到的期望 reward mean 的值
我們其實是可以估測一個 distribution 的
Demo
那最後跟大家講的是一個叫做 rainbow 的技術
這個 rainbow 它的技術是什麼呢？
rainbow 這個技術就是
把剛才所有的方法都綜合起來就變成 rainbow 啊
因為剛才每一個方法，就是有一種自己的顏色
把所有的顏色通通都合起來，就變成 rainbow
我們算算看是不是真的有所有的方法
結果我仔細算一下，不是才 6 種方法而已嗎？
為什麼你會變成是 7 色的
也許它把原來的 DQN 也算是一種方法
那我們來看看這些不同的方法
這個灰色這一條，它這個縱軸就是
這個橫軸是你 training process
縱軸是玩了 10 幾個 ATARI 小遊戲的平均的分數的和
但它取的是 median 的分數
為什麼是取 median 不是直接取平均呢？
因為它說每一個小遊戲的分數，其實差很多
如果你取平均的話，到時候某幾個遊戲就 dominate 你的結果
所以它取 median 的值
那這個如果你是一般的 DQN
就灰色這一條線，就沒有很強
那如果是你換 noisy DQN，就強很多
然後如果這邊每一個單一顏色的線是代表說只用某一個方法
那紫色這一條線是 DDQN double DQN，DDQN 還蠻有效的
你換 DDQN 就從灰色這條線跳成紫色這一條線
然後 Prioritized DQN， Dueling DQN
還有 Distributional DQN 都蠻強的，它們都差不多很強的
那這邊有個 A3C
A3C 其實是 Actor-Critic 的方法，這我們下週會講
那單純的 A3C 看起來是比 DQN 強的
那發現這邊怎麼沒有 Multi step 的方法
他們講的 Multi step 的方法就 balance TD 跟 MC
我猜是因為 A3C 本身內部就有做 Multi step 的方法
所以他可能覺得說有 implement A3C 就算是有 implement
Multi step 的方法
所以可以把這個 A3C 的結果想成是 Multi step 的方法
最後其實這些方法他們本身之間是沒有衝突的
所以全部都用上去
就變成七彩的一個方法
就叫做 rainbow，然後它很高這樣
這是 rainbow 的第一張圖，這是下一張圖
這張圖要說的是什麼呢？這張圖要說的事情是說
在 rainbow 這個方法裡面， 如果我們每次拿掉其中一個技術
到底差多少，因為現在是把所有的方法通通倒在一起
發現說進步很多，但會不會有些方法其實是沒用的
所以看看說， 每一個方法哪些方法特別有用，哪些方法特別沒用
所以這邊的虛線就是，拿掉某一種方法以後的結果
那你發現說，黃色，拿掉 Multi time step 掉很多
rainbow 是彩色這一條，拿掉 Multi time step， 馬上就掉到這裡
掉到這裡，然後拿掉 Prioritized replay，也馬上就掉下來
拿掉這個 distribution，它也就掉下來
那這邊有一個有趣的地方是說，在開始的時候
distribution 訓練的方法跟其他方法速度差不多
但是如果你拿掉 distribution 的時候，你的訓練不會變慢
但是你最後 performance，最後會收斂在比較差的地方
然後拿掉 Noisy Net，performance 也是差一點
拿掉 Dueling 也是差一點，那發現拿掉 Double
沒什麼用這樣子，你就拿掉 Double 沒什麼差
所以看來全部到再一起的時候，Double 是比較沒有影響的
那其實在 paper 裡面有給一個 make sense 的解釋，他說
其實當你有用 Distributional DQN的時候
本質上就不會 over estimate 你的 reward
因為我們之所以用 Double 的理由是因為， 害怕會 over estimate reward 嘛
over estimate 因為，避免 over estimate reward 才加了 double DQN
那在 paper 裡面有講說，如果有做 Distributional DQN
就比較不會有 over estimate 的結果， 事實上他有真的算了一下發現説
它其實多數的狀況
是 under estimate reward 的
所以會變成 Double DQN 沒有用
那為什麼做Distributional DQN，不會 over estimate reward
反而會 under estimate reward 呢？
因為可能是說，現在這個 distributional DQN
我們不是說它 output 的是一個 distribution 的 range 嗎？
所以你 output 的那個 range 啊，不可能是無限寬的
你一定是設一個 range， 比如說我最大 output range 就是從 -10 到 10
那假設今天得到的 reward 超過 10 怎麼辦？ 是 100 怎麼辦，就當作沒看到這件事
所以會變成說，reward 很極端的值，很大的值
其實是會被丟掉的， 所以變成說你今天用 Distributional DQN 的時候
你不會有 over estimate 的現象
反而有 under estimate 的傾向就是了
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
