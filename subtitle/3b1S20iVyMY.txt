hi this is
tao from national taiwan university i'm
here to present our work
semi-supervised learning for modern
speaker text
speech synthesis using discrete speech
representation
with yunji chain alexander lee's
collaboration
and professor hong release supervision
we propose a semester device learning
framework
for modern speaker tts
recently pds systems achieve promising
results in a situation
where a lot of high quality speech plus
their corresponding transcription
are available typically to obtain a dds
model of good quality
it needs more than 20 hours per data
however
the borious payer data collection
process is time-consuming
and expensive hence it prevents many
institutes
from building multi-speaker tds systems
of great
performance so can we just use
one hour pair data to train the dds
model of good quality
the answer is yes by utilizing unlabeled
audio data
in the proposed same specs learning way
we can use only one hour pair data
to build a tts model whose output
quality is comparable with the fully
supplied baseline
which is trend with 25 hours paired data
so how to use unlabeled audio data to
train a tds model
in supervised learning we input phoneme
sequence and speaker id to the dds model
and we encourage the model output as
close to ground truth audio
as possible in the view of deep learning
we represent phonemes and speakers by
vectors
for those represent team phonemes we
call them phone number stations
and for those representing speakers we
call them speaker
stations now that we have a bunch of
unlabeled audio data
if we can somehow turn audio into phone
number stations
then audio can be used to train a dds
model
using reconstruction those in an
unsupervised learning way
in this paper we propose a
semi-supervised learning approach
which uses both supervised and
unsupervised doses
the proposed framework multi-speakers
representation quantization auto encoder
is designed according to
dimension id in this framework an audio
can be reconstructed
in two steps in step 1
the audio is turned into phone number
stations
and then in step 2 the model
reconstructs the audio
based on the phone number station and
the speaker representation
now let's show how to turn audio into
furnace stations
in step 1 we first perform phonetic
clustering
where each audio frame is assigned to a
cluster
according to each sound in our framework
there's a learnable code book where each
code corresponds to a phoneme
we use an encoder to encode input and
for each frame
we replace it with the closest one in
the code book
after this operation the output vector
will have only finite outcomes
and each outcome corresponds to a
phoneme
to turn friend level sequence into
funding level sequence
we merge consecutive same vectors into
one
and we ensure the mapping between chords
and phonemes by recognition loss
which maximizing the probability of
outputting correct phoneme sequence
in our experiment only one hour paired
data is
used for this lows now by step one
we can turn an audio into a funding web
stations
to perform audio reconstruction we need
two information
one is phonetic information and the
other
is speaker information here we simply
use a sequential sequence decoder to
transform phonetic and speaker
information into corresponding audio
specifically the decoder first
transforms the phone in reputation
sequence
into front level hidden representation
sequence
then a speaker f5 transformation is
performed
where the parameters are generated from
the speaker rib station
and then the decoder output will be the
reconstructed audio
hence having both state 1 and step 2
we can utilize our label audio data to
try the model
by reconstruction loss
to perform text-to-speech synthesis we
first look up the code book and retrieve
the corresponding phone representations
afterwards we pass the phone number
stations and speaker
station to the decoder and we expect
that
the decoder outputs audio containing
such phonetic
and speaker information to summarize
there are three doses in the training
the first one
is the recognition loss for ensuring the
maping
between codeword and funding the second
one
is the text-to-speech thus for improving
the quality of the decoder output
in our experiment we use one hour pair
data
for this two doses the third one
is the reconstruction loss which enables
our model
learn from our label audio data
in this slide we show the related works
the first one is speeching which we code
asr plus tts here
it uses separate sr and dts models
and it can be viewed as our model
without the learnable code book and
strategy gradient estimator
for on labor audio data the asi is like
our encoder
and the ts is like our decoder in this
model
the gradients of reconstruction cannot
flow back to the encoder
thus the reconstruction loss cannot
improve the encoder model
the second one is proposed by john and
others they use tacotron as bait bond
model which is an encoder decoder model
with the tension mechanism they protrude
the encoder in decoder with unlabeled
text
and audio then defining the model for
pure data
this one is proposed by the ren and
others
they propose to jointly trend a phone
name recognition model and a speech
synthesis model with unlabeled audio
data
however they do not explicitly deal with
the input mismatch problem of speech and
natural language
know that ellis works focus on single
speaker setting
in the experiment we prepare one hour
pay data for both the single speaker
and multi-speaker for our labor audio
data
we prepare 25 hours data for model
speaker
and 23 hours data for single speaker
the multi-speaker part comes from the
vcdk corpus
the single speaker part comes from the
lg speech
know that for semi-supervised methods
the speakers
in tests say are not in the paid patron
data set
to make our setting clear we put the
partition table on the upper right
with one hour model speaker pay data
supervised learning
obtains a model that cannot generate
intelligible speech
however after utilizing and labor audio
data
the performance of our method improves a
lot
from table we can see that the proposed
method outperforms asl plus tds baseline
with conjecture this is because the
proposed method allows the gradients to
flow from the decoder
through encoded using the straight
through gradient estimator
while as a plus tts model does not
this makes the encoder and the code book
also been updated to obtain
superior representations for better
audio reconstruction
to see if the model is robust for noises
we train a model
with noisy on labor audio data we can
see that
even if the unlabeled audio data are
noisy the performance is still
better than the supervised one which
means it still helps the model training
and it only slightly hurts the
performance when compared to the coin
setting
also we found that model trend with
single speaker pair data
can perform multi-speaker synthesis
however
the model trend with modern speaker peer
data are still better
than the one trend with single speaker
pay data
we highlight that using only one hour
pair data
we can obtain a tds model using our
proposed semi-supervised
learning whose output quality is
comparable with supervised learning
method
which uses 25 hours data
to see if the model can generate audio
output of correct speaker
characteristics we let the subjects
listen to two audio files
one is ground truth audio and the other
is generated audio of the same speaker
we then ask the subjects do you think
the same
speaker has produced these two samples
if the model generates the audio
incorrect speaker
style subjects will answer yes they are
the same
otherwise subjects will answer no they
are different
we can see that the speaker's similarity
results
align with speech quality results that
is
the higher the mls the more number of
answers
are yes they are the same this results
verify our model can perform
multi-specialty as well
because we do not need it because we do
not need it
we do not need it
how do you get it back
how did you get it back
how do you get it back
friday night was a pretty good night
you could feel the heat you could feel
the heat
you could feel the heat
you
