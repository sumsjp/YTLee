臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw/
開始上課
今天我們要講的主題是 New Optimization
先講一下，剛剛忘記講
等一下大家如果上課中途有問題的話
因為現在人沒有很多
所以可以直接插話進來應該沒有關係
應該不會拖到大家進度，反正人不多
如果不好意思的話也可以打字在聊天室裡面
那我們今天要來講一下 New Optimization 這個主題
也就是大家平常訓練 neural network 的時候常用到的
Adam 或是 SGD 這種 optimization 的方法
那在我們開始之前
我們會先需要
一些 background knowledge
這些 background knowledge 可能包含
這幾個名詞
但是大家會覺得很奇怪，這裡的名詞怎麼完全都沒有聽過
沒錯，我在大家這麼大的時候也沒有聽過
這些都是一些很數學的方法
任何一個東西其實都不是一堂課或兩堂課講得完的
所以其實我們今天不會 go through 這些很詳細的數學證明
我們今天不會去詳細證明
任何一個 optimizer 它會在某一些情況下可以達到 convergent 的 guarantee
所以我們今天其實會比較概念上的來講解 Optimization 這個主題
所以我覺得這堂課與其叫 New Optimization
我們不如把它叫做
New Optimizers for Deep Learning
也就是我們今天會讓大家
帶走一些像 Adam 像 SGD
這種你在 keras 在 pytorch 裡面你可以隨 call 隨用的東西
然後還有他們一些概念上的解釋
但我們不會很詳細去證明說
他們在什麼樣的情況下可以 converge
畢竟這些 guarantee 都是在很強的情況下才能保證他可以 converge
所以在 Deep Learning 這種這麼複雜的應用上其實
不管你有再強的 guarantee
其實對 Deep Learning 來說都是不夠用的
好那我們先來複習一下
我們之前學過什麼東西
這個應該是李宏毅老師上課有講過的
在以前有一份投影片是
Some tips for deep learning 裡面
應該會有講過這些主題
首先我們有 SGD
就是 stochastic gradient descent
SGD with momentum
之後會用 SGDM 來代表這個算法
然後 Adagrad
RMSProp
還有最後的 Adam 這五個算法
好那我們先講一下這堂課整堂會用到的 notation
好首先我們會用 θt 代表整個神經網路的參數
也就是我們這個訓練中我們想要去 optimize 的問題
接下來我們會用到
L 這個東西來估計某一組 θ 在特定時候他的 Loss
並且會對這個 Loss
去算 gradient
所以我們對 Loss 算 gradient 以後
我們就可以根據目前的 θt
來 optimize 得到下一個 constant 的 θt+1
好接下來我們還有 momentum，就是 m 這個東西
m 這個東西李宏毅老師以前上課有講過
如果之前沒看過影片的你可以先把他想成
它是紀錄了一些關於 gradient 的 history
他會記錄 mt+1 這個東西他其實包含
所有第零個 time step 到第 t+1 個 time step
他的 θ 的 gradient
所以其實他就可以含括了前面所有資訊
但是他會有一些壓縮
並不是完全
全部都概括
看最下面這個圖
好我開一下我的螢光筆
最下面這個圖就是一個 general 神經網路的架構
首先會有一個 input 就是 xt
接著送進 θt，就是在
第 t 個 time step 上面神經網路的參數
我們會得到一個 prediction
就是 y(t)，這個 prediction 有可能如果你們在做 classification 的話他就是一個
一個過 softmax 後的機率向量
或者是如果你們做的是 generative model
比如生成圖片的模型的話
y(t) 就會是一張生成的圖片
這張 y(t) 會跟 y\hat(t)
去算一個 Loss
y\hat (t) 就是訓練的時候的 ground truth
就是訓練的時候那些 one cut 的 label 或者是
在你做 generative task 的時候可能是
你訓練的圖片
我們先來考慮一下
我們做 Optimization 是要做什麼事情
為什麼要對神經網路做 Optimization
我們其實就是要找出一個 θ
它可以讓 training data 裡面所有 x
算出來的 Loss 可以最小
也就是要找到一個神經網路的參數可以
越貼近訓練資料越好
讓任何一個 xt 丟進
你的神經網路算出來的 y(t) 都要很貼近 y\hat(t)
所以他讓你 summation over 所有 x 的 Loss 越小越好
換句話說，如果現在
就是用特定 x 的話，其實就是要再找一組
θ 可以在一個像是這樣的平面中達到最小值
可能是這個點或是這個點
我們要找到一組 θ
他可以等於，這是一個 Loss surface 上面有最小的值
好那我們要怎麼找這個東西
首先，有同學會問說
每個 time step 只會看到一個 xt
要怎麼 sum over 所有的 time step
所有的 dataset 裡面的 x
因為我們隨時其實都只知道一部分的資訊
那其實這個背後有一些理論
但是我們今天的處理就是
我們不要理他
我們在剛剛說的這種方法其實就是在每個 time step 我們只會看到一個 xt
在正式上來說我們會把它叫做 On-line learning
相對來說另一種就是 Off-line learning
也就是我們在每一個 time step
可以看到所有的 x
還有所有的 y 還有所有的 ground truth
相對來說我們做 off-line learning
應該會比較容易一些，因為我們一次就可以看到所有 training data
但其實會有一個問題
因為我們在訓練神經網路的時候
我們並沒有這麼大的記憶體空間
或者是我們沒有這麼多資源
所以我們沒辦法把所有 data 都塞進去
於是我們一次只會得到一個 x
但是我們在這堂課我們會先忽略掉這部分
我們會假裝我們任何一個時刻我們都可以得到所有的 x 跟所有的 y
然後算出一個完整的 L(θ)
這個東西是 depend on θ 不 depend on x
這樣我們接下來的討論會比較容易一些
那我們剛剛說的這種 setting 就是，假設我們已經知道所有 x 跟所有 y 的話
其實這個東西我們就會叫他是 Off-line Optimization
也就是他一次可以拿到所有訓練資料
這堂課主要會 focus on 這樣的 setting
那我們現在就要開始複習之前學過的一些 Optimization 的方法
首先是 SGD，SGD 是
stochastic gradient descent
是所有方法裡面最基本的一種，首先我們會
在 θ0，這個 initialization 的位置開始
接著算他的 gradient
接著算完 gradient 以後，我們就根據 gradient 方向
我們往反方向走一格
因為 gradient 的方向就是 L 增加的方向
所以我們要往反方向走
也就是往 L 減少的方向走
因為目標是要找到一個 θ 可以有最小的 L
所以我們就直接往這邊走一步
也就是在這個圖上
我們的 gradient 往這邊所以往反方向走
接著在下一個 time step 我們再算 θ1 這一點的 gradient
接著再往反方向走
每一個 time step 都是往反方向走一步，就是這樣子
這張是李宏毅老師上課的投影片
借用一下
第二個方法是 SGD with Momentum
這個東西會用到剛剛說的 momentum
一開始一樣先從 θ0 開始
這個時候我們 定義一個參數叫做 momentum
應該不要說參數，定義一個向量叫 momentum
一開始 initialize 的值也是被設成 0
接著在 θ0 這點算他的 gradient
也就是這個紅色的箭頭
接著我們要去更新剛剛說的 v0 momentum 的值
我們要怎麼更新他，我們就把原本舊的 v0
因為 v0 原本是 0 所以這一項其實是 0 沒錯
把他再剪掉 η 是 learning rate
η 乘上 gradient L
這是什麼東西，這個東西其實就是 0
剛剛 SGD 算出來要 update 的方向
所以你在這裡得到這個 gradient
然後把 update 的方向累計到 momentum 裡面
然後你就根據 momentum 的大小我們移動一點點就是這個東西
那這個 momentum 為什麼要乘上一個過去的 momentum
最後再加上你算出來的 gradient
再減掉你算出來的 gradient
其實他是要做一個在時間的 weighted sum
也就是他要把過去的 time step 的 history 累加起來
所以其實每個 time step 的 gradient
會影響到不只這個 time step 走的方向
其實會影響到下個 time step 走的方向
因為在這邊會有一個過去的 momentum
那為什麼要做這個東西
原因是這樣，因為你在做這個東西的時候，其實
有可能你在某一個 time step 他會算到一個接近是 0 的 gradient
假設這個 time step 他 gradient 接近 0 的話
這時候就會出問題，為什麼，如果你用 SGD
這樣他就會卡在這裡不動了
但是在做 deep learning 的時候有可能他只是
並不是一個最低點，有可能他只是剛好算出一個 gradient 是 0
有點像是 local minimum 的概念
用 momentum 的算法以後
因為這邊還有過去累加的項
所以他就可以確保他一直在移動
不會停在這個點，因為不管怎麼來說
你多看一些東西總是會比較好一點
如果用畫圖來說就是這樣
一開始在最左邊這個點
然後我們開始做 optimization，一開始我們算了一個 gradient
所以移動過來
接著在這個點上
我們又算了一個 gradient
但這個點很平，所以這個 gradient 比較小
但是因為我們前面有了這個東西，所以我們
我們前面的移動會累加到下一步，所以我們這邊的移動會受到前面 gradient 的影響
所以我們會繼續移動
接著到了這個點
到這個點就出事了
因為你的 gradient 算出來是 0
那如果用 SGD 的話就會卡在這裡不會動
但是因為我們剛剛有前兩個 time step
累加的移動
所以我們這邊 momentum 還是會有一個固定的大小
所以我們會繼續往下移動
到了這個點
現在他的 gradient 其實往反方向了
但是因為你看到所有的圖，所以你知道
這個坡在這邊會有更低的值
如果你用 SGD 的話他會直接往回走，會卡在這個點
但是你現在用 momentum 所以你因為前面有累加的 momentum
所以他這個 gradient 雖然叫他往反方向走，但因為他有一些過去的 history
所以他會繼續往前走
然後可以達到更低的點
接下來忘記講這個東西
接下來我們介紹 Adagrad，Adagrad 概念是說
你把 SGD η learning rate 加上一個分母，為什麼要加上這個分母
其實他的概念就是說
如果你的 gradient
在前幾個 time step
他的值很大的時候會發生什麼事
前幾個 gradient time step 值很大
他有可能一開始就會爆走
就會走太多步，那你可能走到反而更差的位置
那他加上下面這個分母，他是過去所有 time step gradient 的和
也就是說如果過去 gradient 很大的話
這個東西就會很大，所以他就走小步一點
因為過去的 gradient 很大就表示他是在一個比較崎嶇的一個方向
可能表示你在
這個方向他是比較陡的
所以在這個方向的 gradient
每個 gradient 都是一個向量
在這個方向的 gradient
他的過去從第 0 到第 t-1 個 time step
他過去的 gradient 會比較大
所以這樣你就用小一點的 learning rate
避免你一下走太大步就跳過去了
那在你的 gradient 比較小的方向
你就用一個比較大的 learning rate 因為這裡比較平緩
所以你可以放心地往那個方向走下去
好那接下來
下一個算法是 RMSProp
這個東西跟 Adagrad 的差別
唯一的差別只在一件事
他這個分母的算法不大一樣
原本 Adagrad 你是直接把所有的過去的 gradient 加起來
但是他現在有點借用類似 admin 的算法
類似 momentum 的算法
他現在會把一個過去 time step
累積的 gradient sum，這個 v 其實就是 sum of gradient
他把它乘上一個 α
然後再把目前這個 time step 的 gradient 乘上 1-α
把這個東西加起來，為什麼要這樣加呢
因為回來在 Adagrad 裡面
它這個東西是 monotone 的
也就是說他會一直無止盡的累加
所以如果你一開始的 gradient 太大的話，那會發生一件事情
就是你的 learning rate 一下子會變得很小
一下子會就變得很小
那如果這樣的話，他可能走沒幾步就卡住了
所以這邊用這個算法，確保這個 vt 它會不斷的變化，就是他不會
永無止盡的變大
而是他會在時間上有一個 weighted sum
所以這樣我們可以確保
這個 optimizer 不會在走沒幾步以後就因為前幾步的 gradient 太大
所以停下來
但是
這樣的話確實是可以解決它下面說的 EMA
讓他不會永無止盡的增加
但是這樣還是沒辦法處理到一開始說
用 SGD 的話他可能會卡在一個
gradient 為 0 的位置的這個問題
所以接下來 Adam 算法就是把 SGD
加上 RMSProp，這兩個東西結合在一起
他怎麼合在一起
這邊對於 momentum 的東西我們記號有稍微改一下
我有配合一下 Adam 的論文所以記號稍微改一下
但大家可以回去 check 一下
跟前面的東西其實是差不多的
在 Adam 裡面
我們的公式就變成這樣 原本後面是一個 gradient term，現在這個東西
在 RMSProp 裡面，這個 gradient term
就是改成 SGDM 的這個 momentum
但是大家注意一下這裡有個 hat，為什麼會有這個 hat
因為這個 hat 其實是 mt 這個 momentum 除上一個東西
為什麼要除上這個東西
原因是因為在一開始的時候
你的 m 一定是會隨著時間累積，因為這個 β 是一個小於 1 的值
所以他一開始的時候會比較小
會隨著時間累積才慢慢穩定下來
所以在 t 小的時候
幫他除上一個小於 1 的數，所以讓你
t 小的時候的這個 estimation
可以是準的
而不會在 t 小的時候就因為 β 小於 1 所以這個 m 其實是一個不夠大的數字
除上這個東西以後確保
weighted sum 在從 t 到 0 到 t 到
最後一個 time step 的時候，這個 mt
他的大小變化不會隨著時間增加而越來越大
而是會，因為在 t 小的時候，分母會比較小
所以他讓 m 隨著時間的變化不會這麼大
那對 vt 也是做同樣一件事情
然後 β1 β2 其實作者有提出一個值
也就是這邊這個 weighted sum 的值
作者提出來 0.9 跟 0.999
然後還有最下面這個 ε 就是怕一開始 t = 0 的時候
的這個 vt 會是 0
所以加上 ε 避免一開始的時候會爆掉
造成一個無限大 learning rate 的情況
這就是 Adam
講到這邊其實都在複習之前學過的東西就是
這五個算法
這五個算法可以看一下它是什麼時候提出來的
第一個 SGD
這個東西其實 1847 年就提出來了
SGD with momentum 是 1986 年提出來的
接下來下面這三個 Adaptive learning rate 的方法
Adaptive learning rate 就是 learning rate 下面除上剛剛那個分母
那三個用了不同的分母算法但是
就統稱他們是叫做 Adaptive learning rate 的方法
Adagrad 是 2011 年提出來的
RMSProp 是 2013 年提出來的
這比較有趣一點他是 Hinton 在他一個 lecture slide 裡面提出來的
其他都算是一些正是論文或者是著作裡面
但這個東西第一次被提出來是在 coursera 上面
Adam 是 2015 年被提出來的
現在除了這幾個以外，大家還有聽過什麼樣的 optimizers
其實就沒有了，我也差不多
我會用的可能也就是這幾個裡面隨便挑一挑
每一個東西都挑看看，試看看
為什麼大家的 optimizers 都是 2014 年提出來的
這中間到底是出了什麼問題
好等我喝一下水
大家前面的部分都沒有問題嗎
如果都沒有問題我就繼續講
剛剛講到為什麼大家用的 optimizers
剛剛講到為什麼這些 optimizers 都是在 2015 年甚至
雖然 Adam 發表在 2015 年的 iclear
但他其實是 2014 年就被提出來了
為什麼這些這麼有名的 optimizer 都是 2014 年被提出來
我們就來想一下
我們平常用的這些很知名的
大神 model，人家 pretrain 的 model 都是怎樣訓練出來的
第一個講 BERT
BERT 是做文意理解，或者是做 QA，甚至是做文章生成都超強的 model
這是 2018 年提出來的
這個東西是用 Adam 訓練出來的
Transformer 是 BERT 的 backbone
他是拿來做翻譯的，他也是用 Adam 訓練出來的
接下來是 Tacotron
Tacotron 是一個算是最早可以用 neural network 做到很逼真的語音生成的模型
那這個東西大概 2017 年被提出來，他也是用 Adam 訓練出來的
接下來來看 YOLO
YOLO 是 computer mission 裡面要做影像偵測裡面
一個算是早期最快的一個 model
那這個東西是用 SGDM 訓練出來的
這東西是 SGDM 訓練出來的，接下來看
Mask R-CNN
他也是一樣，他會在一個圖片框出它 detect 到的物體
跟 YOLO 做的事情是類似的，這個東西也是 SGDM 訓練出來的
接下來 ResNET 是比較早期做影像分類裡面最強大的 model
那這個東西也是 SGDM 訓練出來的
甚至在到近期一點一些新的東西
那首先 Big-GAN，這個是
GAN 是在做生成的，Big-GAN 是
應該算是一個很強的生成影像的 model
這個東西也是 Adam 訓練出來的
這一兩年比較紅的 MEMO 演算法
這個東西可以想像他要訓練一個 network
但是這個 network 可以很容易很快速地學會一些新的 task
比如說有點像是你在教小朋友的時候
你要教小朋友分類汽車，分類花，或者是分類家具
你教小朋友的時候其實他可以從這幾個 task 裡面去學到一些共通的資訊
MEMO 就是想要模擬這件事情
他想要讓他在不同的分類中學到共通資訊
所以你學會這幾個分類以後你要再想學新的分類
比如說要分類風景，他就可以很快地去學到新的分類方法
這也是 Adam 訓練出來的
再來問題就來了，為什麼連這些有名的 model
全部都不是用 SGDM
如果做 computer mission 可能會用 SGDM 來訓練
或者是用 Adam 來訓練出來的呢
其實這個問題我一開始也不知道
我一開始分到這個題目的時候
我想說要講這一兩年的新發展好了
我來想看看這一兩年我有看過什麼新的 optimizer
結果我想一下，還真的沒有什麼新的 optimizer
我感覺我比較會用到的
通常這種時候我們就會來想為什麼會發生這種事情
是因為 2014 年以後就沒有東西提出來了嗎，還是因為提出來的東西都有些缺陷
我們沒辦法用
但是這六年來說是很長一段時間
2014 年是什麼樣的時代
2014 年的時候宇宙勇士隊還沒有拿過冠軍
ALPHAGO 還沒有出現，所以人類還沒有被超越
所以那時候還沒有人討論人類什麼時候要被 AI 統治
川普也還沒有當選美國總統
2014 年的時候，其實我印象還滿深刻，Apple 第一次推出不是黑白的手機
第一次推出有顏色的手機
2014 年的時候
獵人還在休刊，不過現在也還在休刊
2014 年的時候李宏毅老師才剛從國外回來
所以他還沒有開過任何一門 machine learning 的課
但是現在老師的課程已經在網路上滿天飛了
這麼長一段時間為什麼都沒有東西
真的都沒有了嗎
還真的都沒有
因為在這一段時間被提出來的東西
都沒有能夠超過 Adam 或者沒有能夠
顯著的超過 SGDM
為什麼他們沒有辦法超過
原因是這樣，這個是我自己認為的原因
我認為的原因是因為 Adam 跟 SGDM 他們已經
先搶到了兩個最極端的位置
他們搶到兩個極端的位置以後
其他人都只是在他們之間來填一些空而已
但是因為你在使用的時候，你只需要第一名，你不需要第二名
所以那些第二名的 optimizer 就沒有人在用
那他們是怎麼搶到兩個極端的位置
我們可以來看一下實際的例子
這個例子是網路上一個訓練貓狗分類的實驗
原本的網路上的文章連結放在這邊，大家有興趣的話可以點進去看
這是在 training accuracy 上面的表現
可以看到 Adam 衝第一個，很快就到最高了，這條橘色的線
SGDM 是棕色這條線
他好像在中間看起來沒有特別強
這樣看起來 Adam 應該會稍微好一點
接下來再看 validation set 上面
然後就看到 SGDM
反而他剛剛在 training 上沒有特別強，但他在 validation 上面一下就衝到第一名
Adam 反而是在下面有些上下起伏
這個問題等一下再詳細討論一下為什麼有這樣的原因
而且看到 Adam 也稍微比較不穩定一點
接下來來看這是某一篇 paper 上面截下來的圖
這是他們在 train ResNET 34 的時候得到的結果，可以看到
Adam 在中間看起來沒有特別強
SGDM 就是藍色的這個
它比 Adam 上面
看起來也比較平穩一點
上面這兩條虛線先不要理他，這是等一下會講到的東西
最上面這兩條虛線，因為這篇論文是在做這兩個東西的
所以他當然會讓自己的方法做到第一名
但在這張圖不是我們要看的重點
這是訓練 LSTM 的結果
我們可以看到這個 matrix 叫 perplexity
他是一個在做機器翻譯或文意生成的時候
會用到的 matrix
這東西越低越好
Adam 一開始也是下降最快
但是到後來他竟然被 SGD 這些東西超越
他只是一開始下降比較快
但在後期好像沒有比較厲害
從剛剛四張圖我們可以得到一些結論
第一個就是 Adam 通常訓練的時候
會衝得很快
SGDM 訓練的時候比較慢
但是通常比較穩一些，比較不會有上下起伏的狀況
並且 Adam 我看到他 training 的時候
大部分來說都會比較好
但是他的 generalization 的能力
也就是在 validation set 上面他的表現會有比較大的落差 相對來說 SGDM 他的落差就會比較小
而且他好像可以 converge 比較好
那我們先解釋一下為什麼
兩個算法算出來的 minimum，也就是算出來的 θ
他的 generalization 的能力會不一樣
可以參考一下下面這張圖
這裡有兩個 minimum 他們值一樣
黑色線是 training 時候的 function
紅色線是 testing 時候的 function
這兩個東西其實就是前面講過的，你對所有的 dataset 裡面的 x
算出來的 l(θ)
還有 x sum 起來的東西
因為 training 跟 testing 他的 dataset 有點分布上的差異
他的 x 是不一樣的東西，所以他的
Loss function 可能會長得有點像但不會完全一樣
所以他們之間會有小小差距
如果你現在找到的是一個平坦的 minimum 的話
這兩個 Loss function 都會比較平坦
所以 generalization 的差距就不會太大
如果是一個比較 sharp 的 minimum 的話
這邊 generalization 的差距就會非常大
但這並不是說 Adam 一定是找到一個比較 sharp 的 minimum
而 SGDM 是找到一個比較平坦的 minimum
這個東西只是說為什麼可能會發生這個東西
其實還有別的原因，但我這邊只是
稍微解釋一下為什麼會發生這樣的事情而已
接下來就想說，既然 Adam 比較快
SGD 比較穩，而且他在最後
就是訓練最後階段他好像可以收斂到比較小的值
那有沒有辦法直接結合兩個東西
得到更好的效果
這東西早就有人想過了
這叫 SWATS
是 2017 年被提出來的
他其實就一句話，一開始用 Adam
因為 Adam 比較快，最後收斂的階段用 SGDM
他就是在做這件事情，但你在切換的時候
有些事情需要考慮到
第一個就是你在什麼時候要切換
第二個就是切換的時候，因為 Adam 的 learning rate 是一個 adaptive learning rate
這個 learning rate 你要怎麼樣去調整
因為 adaptive learning rate 需要過去的一些 momentum 這些東西的累積
所以如果你直接把 η 拿過來的話，Adam 會被分母影響
所以你不知道要怎麼 initialize SGDM 的值
SWATS 這個演算法提出了
一個 initialization 的方法
他切換的時候其實它用的方法沒有很科學
作者給出了一個切換的點
但其實他沒有很詳細去說明
也沒有去證明說為什麼要在這個地方切換
所以這基本上還是一個比較土炮的算法
就只是很單純的一開始用 Adam 最後用 SGDM 這個算法
Adam 的算法他的特性是他可以算得比較快
SGDM 的特性是會收斂得比較穩 ，收斂得比較好
我們就想說有沒有辦法去修正 Adam
讓 Adam 可以像 SGDM 一樣收斂的又穩又好
所以現在就先觀察一下 Adam 有什麼問題
因為 Adam 的 m 跟 SGDM 的 m 是一樣的東西
所以在這裡就乾脆假設 m
他的 β1 是 0，也就是他其實沒有用 momentum
所以我們 focus 在 adaptive learning rate
對 Adam 造成的影響
那我們可以看一下 adaptive learning rate 的公式
他每次都會乘上 0.999
所以其實這邊會變成一個等比級數的相加
也就是說在 vt 這邊他受到 gradient 的影響
大概會維持 1/(1-0.999)
大約是 1000 的 step 這麼久
那這樣會發生什麼事
這個作者在實驗中發現
如果你 train 到最後的時候，大部分的 gradient 都會很小
而且不會提供方向資訊
只有某幾個 mini-batch 的 gradient 會很大
然後很明確地告訴你應該要往哪個方向走
這樣會發生什麼事
我們就把值代進來算看看
首先我們假設已經 train 了 10 萬個 step
一開始就是很小的 gradient 假設是 1
過了 1000 的 step 以後，突然噴了一個 10 萬的 gradient
那會發生什麼事
首先我們來看 Adam 的公式
這個 m 因為我們假設 β1 是 0，所以 m 在這裡其實就是 gradient
就等於 gradient 的大小
那這個分母，假設前面很多個也都是 1 的話
那這個分母的 vt 大小就剛好是 1
那這個m 也是 1 所以他
這步的 movement 的大小就是 η
過來這幾步的大小也全部都是 η
到了這個點他噴了一個大小是 10 萬的 gradient 會發生什麼事
首先看這個 m
大小就會變成 10 萬，10 的 5 次方
下面這個 vt，因為 gradient 要平方
所以是 10 的 10 次方再乘上 0.001
根號裡面是 10 的 7 次方
開根號是 10 的 3.5 次方
所以分母是 10 的 3.5 次方
分子是 10 的 5 次方
最後算出來的結果就會是 10√10 η
再下一個 step，這個 m 是剩下的 1
分母還是 10 的 3.5 次方
所以他就是 10^(-3.5)η
這會發生什麼問題
可以看到前面的這一千步加起來移動的大小是 1000 η
最後這個真正提供一個有意義的方向
他只會移動 10√10 也就是 30 η 左右
所以這樣就表示你真的提供比較多 information 的 gradient
他在 update 的時候他反而只有辦法造成一個很小的影響
在前面 gradient 比較小的時候，因為他不知道後面會有一個這麼大的 gradient
所以他可能覺得這些 gradient 也不錯
所以前面這些 update 其實不知道該往哪邊走，他就亂走
所以反而會造成比較不好的影響
那會造成這個現象的原因其實就是因為他的 adaptive learning rate 的原因
所以他最多最多，不管你這個東西再大
他最多的移動大小
就是只能這麼大
所以只要你的 gradient
大部分的時候都很小
那它就會產生這樣的問題，他會被小的 gradient 牽著鼻子走
作者提出的解法叫做 AMSGrad
他的做法是這樣
他的目標是要說，他想要去移除這些比較小的 gradient 造成的影響
所以他在 v 下面加上一個 max
如果你下一個 time step
這個 v 比之前的 v 還要大的話
這個 vt 是前一個 time step 的 vt
加上這個 time step gradient 平方
兩個的 weighted sum
那如果這個比這個還大的話你就直接
把這個 vt 改成下面這個東西
如果他沒有比較大的話就用原本這個東西
為什麼要這樣做，他就是要避免
因為前面這些小的 gradient 會造成這麼大的移動的原因就是因為這樣
因為前面可能也有一個很大的 gradient
但是這幾個 time step 忘記前面有這麼大的東西
他就移動這麼多
但是等到後面很大的東西來的時候他反而移動的不夠多
所以他想的做法就是說前面這些小的 gradient
不能忘記前面可能有個很大的 gradient
所以你要移動少一點
也就是說這個 max 它去記住過去最大的時候的 vt
在大部分 gradient 很小的時候它就不會忘記以前看過很大的 gradient
所以他就會這樣移動，但是這樣可能會出一個問題
就是可能有人看到覺得不對
課程剛開始的時候
我們有比較為什麼要把 Adagrad 換成 RMSProp
因為 Adagrad 下面這個分母是
monotonic increasing
所以他最後會越走越小，越走越少
所以有可能他一開始 gradient 暴衝太快的話
那他就走沒幾步就不動了
因為下面東西太大讓這整個變得很小
所以 learning rate 一直變小
那這個東西不就是我們之前要解決的問題嗎，怎麼又走了回頭路
所以這個東西可能，這是我自己的 comment
不是一個非常好的解法
那再來要解決另一個類似的問題，又有人提出了另一個解法，等下剛剛忘記講了
這個東西就叫他是 AMSGrad
是 2018 年提出來的一個算法
接下來有另一個類似的方法想要解決類似的問題
前面 AMSGrad 的時候
它有提到這件事情
在最後的時候大部分 gradient 都很小
所以表示 learning rate 要嘛就很大要嘛就很小，因為你的
如果 gradient 很小的話
前面這個東西就會很小
所以他 learning rate 就會很大
如果 gradient 突然暴衝的話
這個 learning rate 就會很小
所以前面這篇 paper 只有想要解決 gradient
就是如果你的 gradient 小的時候
他的 learning rate 太大的狀況
它現在這篇，另一個作者他想要解決的問題是
他要解決 gradient 太大的時候，learning rate 也不能讓他變得太小，他要怎麼解決
下面這張圖就是他實際訓練一個 ResNET 的時候
裡面其中一個 convolution kernel
得到的 gradient 大小狀況
learning rate 大小狀況
所以可以看到大部分 gradient
大部分 learning rate 要嘛就超級小
10 的 -3 次方 -5 次方
要嘛就是超級大，10 的 4 次方 10 的 8 次方
所以 learning rate 其實是很極端的
要嘛很大要嘛很小，所以他解決的方法是這樣
因為前面那篇提到的 AMSGrad 他只去 handle learning rate 太大的時候的狀況
這個作者提出另外一個算法，這是2019 年提出來的，提出來叫 AdaBound
也就是我們前面剛剛有看到的
在某兩張圖上有看到虛線
他的方法是這樣，他就直接把這個 learning rate
去做一個 clip
這個 clip 怎麼算
作者直接給出這一條有點像經驗法則的算式
這個東西跟什麼有關係
β2 就是 Adam 下面當分母的 β2
還有 t 有關係
他把他 clip 一個下界還有一個上界
有人看到這裡可能想說
為什麼要做 adaptive learning rate
要做 adaptive learning rate 其實就是希望
learning rate 可以自己動態調整
但是根據不同的 test 不同的 Loss
可能今天做影像分類，今天做文意理解
他的 Loss surface 不一樣
但不管在怎樣的 Loss surface 他都可以他都可以自己調整出一個好的 learning rate
看到這邊
這個 lower bound 跟 upper bound 都是人工訂出來的
為什麼是這兩個數字
這是作者直接給出來的答案
也沒有說為什麼要是這兩個
這兩個東西其實跟 Loss 完全沒有關係
這樣其實有點失去我們在做 adaptive learning rate 的意義了
因為你希望他可以自己動態調整，
但你又強制不管在什麼 model 什麼 Loss 上都給他一個 lower bound 跟一個 upper bound
這完全就不是 adaptive 想要用的東西 太粗暴了
所以這東西我覺得也不是一個最漂亮的解答
因為我們在做數學的時候其實
除了這東西能不能用以外，我們其實有時候還蠻在意說
你的算式是不是有對稱
有一種優雅又不優雅那種感覺
這其實就是比較粗魯，有點像在工廠上的解決方法
所以可能就還不是一個最好的方法
現在從另一個方向來想
剛剛說希望把 Adam
讓 Adam 把它改得好一點
像 SGDM 一樣穩定一點
現在從另一個方向來想，SGDM 的優勢是他比較穩
收斂比較好，但是他就是慢
為什麼他會慢
因為 adaptive 的算法可以動態調整 learning rate
他在你需要大步的時候走大步，需要小步的時候走小步
在某些不重要的地方或比較平緩的地方
他就可以大步走過去
SGDM 沒有辦法這樣做
現在要怎麼讓 SGD 學會這件事情
其實沒有辦法，但是他就想說
既然這樣，那有沒有辦法幫 SGD 找到最佳的 learning rate
這 learning rate 不會隨著時間改變
但是他可能收斂比較快，所以可以讓 SGD 的 performance 比較接近 Adam，讓他快一點
其實在調 learning rate 的時候，大家如果做實驗常常會有類似這樣的經驗
那這個東西就是說你的 learning rate 很小的時候跟 learning rate 很大的時候
performance 都不會最好
一定是 learning rate 適中的時候 performance 比較好
這是一個蠻 trivial 的結果
這東西他們叫做 LR range test
所以我們可以用，在調整 learning rate 的時候你可以得到一個比較好的 learning rate
那接下來在 2017 這篇 paper 裡面
這個作者提出一個叫 Cyclical learning rate 的東西
他的結果就是這樣
你的 learning rate 是在大小大小這樣變化
一個週期性的變化，一下 learning rate 大，一下 learning rate 小
為什麼要這樣做
因為 learning rate 大的時候就相當於在做一個 explore 的動作
learning rate 小的時候就在做收斂的動作
通常在做 learning rate scheduling 的時候
learning rate 都是越來越小
這裡在變大的意思就是說
鼓勵這個 model 不要滿足於現狀
他要在適當的時候再去做更多 exploration
所以當你覺得收斂的不好的時候
等下先暫停，再看一下附近還有沒有更好的點
所以再把 learning rate 調大，再變小
但如果現在已經收斂到一個很好的點的話
那你 learning rate 變大應該也是不會很容易跑出這個山谷
這樣的一個週期性變化
它有一些參數要決定，第一個就是你的 stepsize
你要多長的週期
再來就是最大和最小的 learning rate
最大跟最小的 learning rate 可以用剛剛 LR range test 來決定
stepsize 的話通常是設成幾個 epochs
這個算法其實可以在你遇到 local minimum 的時候
再把 learning rate 調大
這是遇到平緩平地的地方
因為你藉由把 learning rate 調大就可以逃離這個地方
有點類似 Adam 在平緩的地方就會調大 learning rate 這個概念
再來這是另一個做法
很類似只是他調整方法不一樣
剛剛他是用一個三角波的函數來調整
這做法就是
你不用做 increase，慢慢 increase 這個動作
直接重來就好
所以原本剛剛這邊是慢慢增加
這邊他就直接變小以後，然後再直接調到最大再變小
再調到最大再變小，但是整體的概念其實是大同小異
最後一個也是 2017 年提出來的方法
這叫 One-cycle learning rate，他其實就是說你只做一個 cycle
這 cycle 把它分成三段
第一個是 warm-up，就是 learning rate 越來越大
所以他可以慢慢
從 initialize 慢慢增加 stepsize
再來等他找到一個還不錯的 minimum 附近以後
開始做 annealing，就是他 learning rate 在慢慢變小
在這附近他就是在做一個慢慢收斂的動作
最後這一小段，learning rate 越來越小就是讓他收斂到越來越好
畫出來的最後結果會有點像是這樣
他會到最後以後他會再往上一段
再往上這段就是 fine tune 的時候
它收斂的結果
但我覺得這東西有點做到太 ㄎ一ㄤ
前面你覺得做太多 cycle ，所以你現在只做一個 cycle 感覺還是不是一個很漂亮的解決方法
講到這邊是剛剛說對 SGD 要怎麼改進
讓他變得跟 Adam 一樣去加速他，所以提出了
我們看到的這幾個方法主要 focus 在你要怎麼調整 learning rate 上面
因為 adaptive learning rate 就是根據 Loss 來調整 learning rate
前面這幾個方法就是先給出 learning rate 調整的模式
等一下就根據這個模式來調整
現在 11:20
先休息 5 分鐘好了
上太久大家專注力不好，所以我們在這邊先休息一下
下一節課 11:25 開始
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw/
