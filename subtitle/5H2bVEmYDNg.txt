好,那各位同學大家好啊,我們來上課吧
那今天呢,我們要講跟影像有關的生成式AI
我們知道生成式AI呢,它本質上要做的事情就是它有一個輸入
它有一個condition,根據這個condition要生出一些東西
那跟影像有關的生成式AI可以從兩個方向來討論
這個是影像,它可以是生成式AI的condition
生成式AI可以根據它看到的一張圖片
它看到的一段影像來生成一些對應的東西
比如說生成對應的文字
這是一種類的跟影像有關的生成式AI
那另外一類的生成式AI是
這些生成式AI它的目標就是產生圖片或產生影片
你給它一個condition
根據你給他的Condition產生對應的圖片或影片
那在今天的課程裡面
我們會專注於討論第二塊部分
那我們先來很快的看一下第一類的生成式AI
那這一類可以看圖片、可以看影片的生成式AI
也許你已經不是很陌生了
因為現在GPT-4它是能夠看圖片的
所以你可以給GPT-4兩張人的照片
這兩個人呢,其實都叫李宏毅
那你就問他說,這個照片中的哪一個人比較帥
你也不用期待他會回答你
因為他現在都非常的精明
所以他不會直接回答哪一個人比較帥這種問題
但他會對兩人的衣著做一些評論
說左邊這一位穿著牛仔外套的年輕人
比較時尚而有型
右邊穿白襯衫的眼鏡男士
看起來比較專業而有親和力
但是我不知道誰比較帥,你覺得呢?
接下來我還想要問他說
你知不知道這兩個人從事什麼樣的職業
根據他們的衣著和打扮
猜一下他們是從事什麼樣的職業
那GPT4o猜的是蠻準的
他說牛仔外套的年輕人
可能從事時尚娛樂或創意領域的工作
這個沒錯
這個李鴻義是一個非常知名的演員
然後右邊這個呢
他是從事學術技術或商業領域的工作
例如教授工程師或者是經理
所以看起來這些照片拍的都是蠻不錯的
讓這個語言模型可以根據這個照片
大致可以猜出這個人是什麼樣的職業
看起來這些照片都非常符合他對應的那個人的形象
其實這個GPT-4o
他內心深處還是有一個審美標準的
所以如果你逼迫他說一定要決定誰比較帥的話
他還是會給你一個答案
不管我問他幾次
他都會說左邊那個人比較帥
但是我這邊還是特別把他的答案秀出來給你看
你可以仿照一下
你可以參學習一下GPT-4o的回答
雖然他覺得左邊這個人比較帥
他會強調說右邊這個人呢
看起來也很有魅力
他的魅力來自於專業性和親和力
所以GPT-4o呢
經過了大量的RLHF以後
他現在講話已經非常的精明
就是個人精
我們可以多學習他講話的風格
那這個是根據影片來產生
根據圖片來產生文字
那這門課我們會花比較多時間來探討
怎麼根據某一個condition來產生影片或者是產生圖片
那講到根據某一個condition來產生影片這件事情
就不能不提SORA這個模型
在我們學期初的時候
OpenAI 試出Sora這個模型的DEMO
在這個DEMO裡面
他們就展示說Sora有非常強的力量
可以讀一段文字
這段文字是對於一段影像的描述
Sora這個生成式AI
就可以生出非常清晰的對應的影像
那在學期初的時候
一時之間造成了非常大的轟動
不過Sora到現在都還沒有真的被開放使用
所以如果你看到網路上的一些DEMO
那個都是來自於OpenAI它本身的那個Blog的DEMO
就釋出SORA這個模型的DEMO的時候
寫了兩篇Blog
那多數人的影片其實都是從那兩個Blog來的
那我把那兩個Blog的連結呢
也放在投影片上給你參考
那根據OpenAI的DEMO
SORA產生出來的影片
長什麼樣子呢
非常的驚人
我這邊就擷取兩個例子給大家看
左邊下面這段文字
是輸入給Sora這個模型的文字
它是根據下面的文字
去產生出上面這段影片
左邊這個例子裡面
是要Sora畫一個小怪物
這個小怪物看著一個逐漸溶解的紅色蠟燭
Sora產生出來的影片是這個樣子的
右邊是要畫一個紐約的街景
但現在紐約沉在海底下
所以有很多的水生生物
Solar 畫起來是這個樣子
那這邊選的兩段影片
通通是特別選現實生活中不可能出現的場景
但Sora裡面有很多現實中出現的場景
它可以產生非常擬真的影片
但你就會搞不清楚說
到底是訓練資料裡面有一模一樣的影片
還是SORA真的創造出這樣的影片
所以這邊是特別選兩段
在現實生活中可能不會出現的場景
來告訴你說
SORA是有可能創造出
這些現實生活中不會出現的場景的
但你仔細看一下
你還是會發現說
這些影片還是有一點點瑕疵的
比如左邊這個小怪物
你會發現
雖然小怪物沒有動
但背後的牆呢
一直在左右移動
感覺蠻不自然的
那如果你有仔細觀察這個紐約街道上的水生生物的話
你會發現這些水生生物啊
還不是一般的水生生物
好多生物都在變形了
有的生物遠遠看明明是一隻海龜
近看他就突然縮成一隻魚了
這些生物是會變形的
那所以 Sora 在生影像的時候還是有一些瑕疵的
那這邊呢
是找了一些比較有趣的瑕疵
左邊這個圖呢
是要叫Sora畫五隻狼的幼崽
這個是Sora畫出來的狼的幼崽
是長這個樣子的
你看它分裂了
越來越多
所以這個有時候人工智慧還是會犯錯的
右邊呢
是考古學家挖出了一張塑膠椅
這個非常虛科幻的一個場景
那麼來看看發生了什麼事
挖了一個東西出來
變魔術
變出一張椅子
哎呀
椅子跑掉了
你看跑掉了
就算是他產生的影片有一些瑕疵
其實在今年年初的時候,也已經非常讓人震驚了
那剛才講的是用文字來生影片
那你其實也可以做影片來生影片
那用影片來生影片也有很多的應用,比如說你可以給你的AI一段未完成的影片
叫他把剩餘的部分續寫完產生出來
或者是你可以叫AI幫你做風格轉換
給他一段影片,他把他的風格轉換
比如說本來是黑白的影片
然後他把顏色塗上去
像這樣的技術可能可以被用在影像修復上面
比如說把一些老電影做影像修復
讓他看起來更有現代感
或者也可以做畫質的提升
本來你有一個影片的解析度是比較低的
那也許可以透過生成式AI直接幫他提升畫質
所以影像生影像也有很多的應用
那其實也並不侷限於透過影像來生影像
生成影像的生成式AI還有很多其他的應用
一個今天大家都能夠想得到的
而且已經有非常多人在使用的
就是所謂的Talking Head
所謂Talking Head的意思就是
你給這個生成式AI一段你的錄音
然後呢你給生成式AI一張你的照片
然後呢他就想像
你說這句話的時候
你的嘴型
你的臉
應該長什麼樣子
那我這邊用的是一篇今年三月的時候的論文裡面的模型這個模型
是有開源的
像這種talking head生存啊現在你其實可以找到很多的商用軟體
不過通常是要付錢的
那我這邊用的這個是一個免費開源的軟體
所以是每個人都可以試的
那長起來就給他一句話
給他隨便一張照片
他就把這句話合到這個照片的人的口中
聽起來看起來像是這個樣子
這門測試生成式AI導論
就是我就說這門測試生成式AI導論
那就看到說這個人就配合生成式AI導論
這句句話的嘴型在講話
這門課是生成式AI討論
而且我照片裡面是沒有牙齒的啦
他硬要幫我加個牙齒上去啦
加很白皙的牙齒上去
所以可以想像說這個有非常多的應用
這個我馬上可以想到的直接的應用
其實就是這個VTuber對不對
其實這個Talking Head的技術
只要背後直接接GPT-4O
馬上就可以直接做一個
沒有人的Youtuber了
所以我相信很不遠的未來,大概幾個月後
你就可能會聽到有人說
出現了一個新的VTuber
然後這個VTuber講話很有趣
然後再過幾週以後,他就突然承認說
其實這個VTuber是不存在的
他背後就是GPT4 Owner加一個Token Tag
我已經幫忙把這個劇本寫好了
大家要用就拿去用一下
我相信在幾個月,一定就會有這樣子的故事出現了
那還可以拿很多其他的東西
這些東西作為生成式AI輸入的condition
比如說你可以畫一個草圖
叫生成式AI幫你把精細的圖畫出來
或者是就打一個草稿
讓生成式AI根據草稿把圖畫出來
或者是給他一個人的站姿
再讓他根據這個站姿生出對應的人像
所以這篇paper裡面就講了
你可以拿各式各樣的condition來生成圖片
裡面有一個Network叫做ControlNet
就是告訴你說怎麼加入各式各樣的Condition
各種不同的Condition都可以拿來生成圖片
以上呢是講生圖片或影像的AI的種種的應用
那接下來我們就要來講
這一些生圖片或生影像的AI是怎麼被打造出來的
那在講圖片跟影像是怎麼被生成出來的之前
我們來複習一下圖片跟影像
他構成的基本單位是什麼
先來講圖片
那我們都知道說圖片是由像素所構成的
像素越多圖片看起來就越清晰
左邊這張是1024x1024解析度的圖片
他裡面有1024x1024的像素
所以非常的清晰
但是如果同一張圖
你只用16x16個像素
總共只有16x16
256個點去描述這張圖片
每個點都代表一個顏色
但只有256乘256個點
那左邊這張圖看起來就會非常的模糊
這個是圖片
那影片呢
影片其實就是由一張一張的圖片
所構成的
所以一段影片
他其實就是一串的圖片
在影片裡面的每一張圖片呢
叫做一個Frame
中文通常翻譯成 幀 叫做一幀
那有一個詞彙叫做frame per second
所寫是fps
代表的是每秒的幀數
也就是這個影片每秒有幾張圖片
只要圖片夠多
人看起來就會是非常流暢的影片
人看起來就不會覺得
一段影片是由一張一張的圖片所構成的
另外我們在YouTube上看的影片
通常是24fps
也就是一秒鐘呢是由24張圖片接起來所構成的
那看起來呢就是這個樣子啦
那就是你通常在YouTube上看到的影片呢
就是這個樣子
你完全感受不到它是一張一張圖片所構成的
那這個就是那個芙莉蓮呢
把一個絕對不可能打破的結界打破的一個段落
那右邊呢是同一個影片把它改成1FPS
也就是每秒只有一張圖
那看起來呢
就是非常的卡頓啦
看起來就非常的卡頓
好像你網路不好的時候
這個看起來的影片
人家也就會問說
要不要讓人好好看動畫啦
這看起來就會非常的痛苦
啊不過24fps呢
如果是玩遊戲的話
那這個解析度呢
還是太低了
這個每秒的幀數呢
還是太低了
如果玩遊戲的話
可能要60fps
你玩起來呢
才會覺得絲滑流暢
那剛才講的是圖片跟影片是由什麼所構成的
那接下來我們再來看一下
今天的人工智慧是如何看待一張圖片跟看待一段影像
其實今天多數人工智慧的模型都不是直接拿像素當作輸入
當然你要直接拿像素當作輸入也可以
在比較早的年代很多人會這麼做
後來發現像素實在是太複雜了
所以今天能夠產生非常大張非常清晰的圖
其實歸功於一些影像壓縮的技術
今天給一張64x64的圖片
你不會讓模型直接去產生一張解析度64x64的圖片
對人工智慧模型來說
通常會把圖片先通過一個編碼器
這個編碼器的英文是Encoder
那這個編碼器通常做的事情呢
就是先把圖片切成一塊一塊的
在今天這個例子裡面呢
64x64的圖片被切成4x4
總共16塊
那每一塊呢
叫做一個Patch
那接下來呢
每一個Patch呢
會被進行壓縮
那怎麼樣壓縮呢
壓縮的結果是什麼樣呢
取決於你的編碼器長什麼樣子
有的方法是會把每一個Patch
用一個獨特的符號來表示它
所以每一個Patch就好像是一種新的語言
每一個Patch可以看作是新的語言裡面的一個Token
這是一種表示的方式
也有一些編碼器
它是把每一個Patch變成一個低維度的向量
本來一個Patch裡面應該要有很多的像素
但是它把這些像素的資訊全部濃縮起來
用幾個數字,也就是一個低維度的向量來表示它
總之有很多種不同類型的編碼器
但是他們都是把影像變得更簡單
就是把影像切成一個一個Patch
每一個Patch用一個簡單的方法來表示它
然後接下來呢
你就會把圖片裡面的Patch拉直
本來一個圖片是2D的
但是給人工智慧模型
比如說Transformer處理之前
你就把它拉直,拉成一條線
然後呢 丟給Transformer
那人工智慧的模型
比如說Transformer在產生圖片的時候
其實他也是產生一排的Patch
然後這一排Patch呢
會通過一個解碼器
他的英文是Decoder
然後就會還原回原來的圖片
那講到這邊可能有同學會有問題說
那這個不就跟
只是調低影像的解析度一樣嗎
我們可不可以說這個編碼器做的事情
就是把原來64x64的圖片
把它解析度調低變成4x4呢
在概念上也許類似
但是實際上編碼器跟解碼器做的事情
遠比調低調高解析度更加的複雜
事實上這邊的編碼器跟解碼器呢
他們都是很複雜的類神經網路
他們裡面往往都是transformer
往往都有非常多層
所以今天你可能會覺得說
如果我把一張圖片變成4x4
總共16個patch
會不會通過解碼器還原回來以後
我這個狗呢
這個圖片呢
看起來就是4x4
總共只有16個格子呢
其實不會
這個解碼器在還原的時候
其實會做非常複雜的事情
他不是隻看一個patch來還原
他會考慮一個patch
跟他所有鄰近的patch
甚至是整張圖片裡面所有的patch
一起去做還原
那這邊這個壓縮跟還原的過程
其實都是透過類神經網路
來進行的
這個壓縮跟還原的過程
其實都非常的複雜
那我們可以把圖片
壓縮到非常簡單
然後還可以把它解回來
而解回來看起來仍然是
一樣的圖片
那至於實際上的細節是怎麼做的
我右邊就留了一篇論文
那其實類似的技術非常的多
比較有代表性的給大家參考
這邊講的是圖片的部分
那我們再來看影片
我們知道影片是由一堆圖片所構成的
那你當然可以用一個圖片的編碼器
把影片裡面的每一張圖片
都分別轉成一排的patch
但對於影像來說
你可以做更多的壓縮
本來圖片是2D的
只有高跟寬
所以你只能在高跟寬這兩個方向上去做壓縮
但影片是有時間這個維度的
所以你可以在時間的方向上也進行壓縮
比如說常見的做法
可能會把相鄰的幾個Frame合併在一起
一起做壓縮
比如說在這個例子裡面
這段影片總共有四個Frame
那可能第一個Frame跟第二個Frame會一起做壓縮
第三個Frame跟第四個Frame一起去做壓縮
那這邊實際上的做法可能是
第一個跟frame跟第二個frame
他們位置一樣的patch
會被再做進一步的壓縮
變成另外一個
video的patch
一樣位置的patch
進一步壓縮
變成video的patch
一樣位置的patch
做進一步壓縮
變成video的patch
圖片裡面
同樣位置的patch
做壓縮
變成新的
video的patch
變成影片的patch
然後再把
影片的patch呢
把他拉直排成一排
那這個就是今天這個生成式人工智慧
的這些模型的輸入跟輸出的影片的樣子
所以大家要記住等於在等一下的這個討論裡面
我們就假設你已經知道一段影片
就是描述成一排的patch
那我們就不會特別把影片轉patch這個部分再告訴你
總之你心裡要知道對於人工智慧來說
一段影片就是一排的Patch
那事實上SORA也用了類似的技術
但你知道今天OpenAI它都不會明著告訴你
它要做什麼樣的事情
不過從它畫的圖裡面
你也可以很明確的知道說
它使用了這個Patch這個技術
你看它的圖裡面
這是一段影片
而一段影片呢
就是說很多張圖片所構成的
通過一個Encoder
那這個Encoder呢
會把影片變成一堆Patch
那這個Patch壓縮的方向
可以是同一張圖裡面的長跟寬兩個方向
也可以在時間的方向上面做壓縮
所以這邊每一個小塊的Patch
可能都對應了影片裡面的
一段長、一段寬跟一段時間
都對應了影片的一小塊
最後把這些Patch通通拉直
再丟到你的人工智慧的模型
比如說Transformer裡面
那SORA也會使用Patch這樣子的技術
好,那我們接下來呢
就來進入模型的部分
我們來講文字
我們先用文字生圖為例
來跟你講說這些模型是怎麼被訓練出來的
那麼很快也會講到文字生影片的部分
好,那文字生圖
我們就是希望模型呢
可以讀一段文字
然後產生對應的圖片
那要怎麼訓練一個模型
可以讀一段文字
產生對應的圖片呢
你就需要大量的訓練資料
可是這邊需要什麼樣的訓練資料呢
這已經是我們這學期最後一堂實體的課了
不知道你能不能夠猜得出
如果要訓練一個模型
輸入是一段文字
輸出是一張圖片
需要什麼樣的訓練資料
我們知道如果今天要教遠模型做文字接龍
你就是要給他一句話
然後告訴他接下來要接哪一個Token
那對圖片生成來說道理也非常的類似
你要他看一段文字產生一張圖片
那你要教他的就是收集一大堆圖片
跟這些圖片對應的文字敘述
然後機器就可以學會說看到這段文字
應該要產生這樣的圖片
看到這段文字應該要產生這樣的圖片
以此類推
但你知道說現在這些生成圖片的模型都非常的厲害
你輸入什麼他往往都可以產生對應的輸出
甚至很多時候你直接打一個人名
如果那個人夠有名的話
生成是AI畫出來的人臉還會跟那個名人長得蠻像的
到底是用什麼樣的訓練資料才能夠達到這樣的地步呢
現在其實已經有非常大規模的開源的
可以拿來訓練文字生圖的資料了
有一個資料集是一個叫做LAION的公司所釋出的
裡面就是大量的圖片
每一張圖片都有它對應的文字
你就可以拿這樣子的
你就可以拿這樣子的大規模的資料來訓練一個模型
教他怎麼看一段文字產生對應的圖片
LAION最大的資料集有多少張圖呢?
有58億張圖啊
這個資料集大到說他們特別做了一個搜尋引擎
你在這個搜尋引擎上面打一段文字
他告訴你說這個資料集裡面有哪些圖片對應到類似的文字
LAION會問說這麼大規模的資料集是怎麼被收集來的呢?
你想也知道就是一定是從網路上爬下來的
他們做的事情很有可能是從網路上爬了一些圖片
看到很多網路的圖片
其實都有對應的文字敘述
他們就把圖片跟對應的文字敘述爬下來
當然他們也做了一些後處理
比如說把母湯的那些照片拿掉啊
你懂我的意思吧
18禁的照片拿掉啊
把覺得這段文字跟這個圖片
其實沒有對得很好的也拿掉啊等等
整理出這樣的資料集
但使用這樣的資料集
其實還是有一定程度的風險的
LAION這個公司
他其實對外宣稱說他只是幫忙收集了這些圖片
所以他並沒有這些圖片的所有權
那你拿去訓練那是你家的事情
你知道蠻多公司都是拿這些圖片去訓練的
比如說這個Stability AI就是做Stable Diffusion那家公司
他們都是拿這個資料集去訓練的
然後後來就被一些畫家告了
像OpenAI就死都不告訴你他們資料是哪裡來的
搞不好也是同一個資料集
但只要不講就不會被告這樣
總之就是這麼回事啦
總之這個LAION就只是把資料放在那邊
那至於能不能用
那是另外一件事情
好那有了這樣子的資料集以後
要怎麼訓練一個文字生圖的模型呢
所以你已經知道
有一段文字
它對應到某一張圖片
那我們現在圖片都用Patch來表示
所以你看到這些Patch
你知道這代表著某一張圖片
那你用這樣子的資料
你就可以去訓練一個Transformer
也許你可以訓練Transformer
做這個Patch接龍
我們都已經知道語言模型去做文字接龍
那我們可以訓練一個Transformer
去做Patch接龍
這個Transformer要學到的東西就是
看到這段文字
就先生出一個編號1的Patch
然後把編號1的patch貼到文字後面
那再看接下來要生哪一個patch
也許生編號7的patch
再把編號7的patch貼到後面
再產生下一個patch
生出編號99的patch
把一張圖片裡面的patch都生出來
通過decoder
你就可以還原出一張圖片
這是某一種用文字生圖的可能的方式
但其實在上上週講生成的策略的時候
也已經告訴你說
其實文字生圖很少用這種接龍的方式
那接龍的方式有個專有名詞叫做
Auto-regressive
其實文字生圖很少用Auto-regressive的方式來產生圖片
因為就算把圖片表示成Patch
一張圖裡面Patch的數量可能還是太多了
那我們在上上週的課程講生成的策略的時候
也有特別跟大家提醒說
其實在生圖的時候往往就是一步到位
用一個non-autoregressive的model
一張圖片裡面所有的patch是同時被生成出來的
那在上上週的課程裡面
我畫的圖呢
大概都是長這個樣子的
就是你有一個transformer
然後呢
你會告訴transformer說
我們產生位置1的patch
他就生出patch1
產生位置2的patch就出patch7
產生位置3的patch
就生成Patch99
那一張圖片裡面
所有的Patch都是透過同一個模型
同時生成的
所以你就不需要等這個Patch一個一個被生出來
一張圖片裡面
所有的Patch都是可以被同時生成的
這個是在概念上
所有的Patch是被同時生成的
但實際上的做法是這個樣子
我們其實會把所有要
就假設現在總共要生16個Patch
這張圖現在是由4x4
總共16個Patch所組成的
那你一開始其實就會告訴同一個Transformer說
我們要生Patch1到Patch16
然後Transformer呢
就會同時把16個Patch
通通都生出來
所以實際上呢
這個第一個位置到第16個位置
他們其實並不是完全分開生出來的
而是同時丟給一個Transformer
同時被生出來的
那這樣做有什麼好處呢
雖然這些Patch還是平行同時被生出來的
但是在生成的過程中
你知道Transformer裡面呢
就是會有Attention
Attention的意思就是
每一個位置在做處理的時候
都會參考其他位置的資訊
那因為Transformer裡面有Attention
所以我們在生成某一個Patch的時候
其實會去參考其他Patch生成的過程
再去生成某一個Patch
用這個方法你生出來的圖片
每一個Patch中間
他還是有一點關聯性的
透過這個Attention
他Patch跟Patch間還是會有一些關聯性
不會完全毫不相關
不過就算是用這樣子的方法
就算是你用Transformer裡面有Attention
來處理Patch跟Patch之間的關係
其實也不能保證能夠解決
我們上上週講生成策略的時候提到的
腦補的問題
就如果每一個位置
看到一隻在奔跑的狗
每一個位置
如果想要畫的狗長得是不一樣的話
那你可能就會畫出很多隻狗
疊在同一張圖片裡面
就算是用了Transformer
其實也不能夠完全解決這個問題
那至於這個問題實際上是怎麼被處理的
我們在等一下的課程中
還會再看到
但現在就假設說
Transformer可以處理這個問題
反正就是告訴他說生16個位置的Patch
他就把16個位置的Patch
一次都生成出來
Patch跟Patch之間的關係
我們透過Attention來進行處理
我們現在已經知道在概念上
怎麼產生一張圖片
那我們來談一下
怎麼衡量影像生成模型的好壞
如果你今天給你的影像生成模型輸出一個指令
它產生對應的圖片
那我們怎麼知道生成的結果好不好呢
那你記得我們在講怎麼評量文字模型的時候
我們最後有講到說
雖然有很多評量的方法
但是人類才是最好的Evaluator
人類才是最好的評量者
但是人類的時間又有限
現在人工智慧已經這麼強了
那我們在講這個語言模型的評量的時候說
也許我們可以讓GPT-4來代替人類
用比較強的語言模型來代替人類進行評量
其他模型的工作
事實上在影像裡面也有類似的趨勢
今天怎麼知道一個圖片生成的模型
生出來的圖對不對呢
當然你可以人去看
人去看說這個像不像一隻奔跑的狗
不過人可能沒有力氣看太多的圖片
所以今天一個常見的做法是引入一個叫做
clip CLIP 的模型
我們先來介紹一下 clip 這個模型
clip 這個模型他訓練的時候他學的事情就是
先從網路上有人收集了非常大量的圖片
跟這些圖片對應的文字
那你就跟 clip 說
如果今天給你一張圖片跟他對應的文字
給你一張圖片跟他對應的文字
你就要輸出高分
你就要輸出一個比較大的數值
你要輸出一個比較高的分數
反過來
如果我今天把我資料集裡面的圖片跟文字
任意配對
我把陽光下的貓去配這張狗的圖
我把沙灘上的狗去配這個貓的圖
再丟給clip的模型的時候
clip的模型要學到說這個配對
是一個不正確的配對
所以要給這樣不正確的配對低分
所以clip這個模型
可以從大量的資料裡面學到說
給正確的配對就是高分
給不正確的配對就是低分
那這種模型是已經被訓練好
已經開源的
所以今天你會發現很多的圖片生成模型
其實就是直接拿clip來衡量
他們把這個指標呢
叫做clip score
也就是說今天圖片生成模型
生成一張圖片以後
就把輸入的文字敘述
加上這張圖片
一起丟給Clip,看Clip給多少的分數
給的Clip的分數越高,Clip Score越高
就代表這個圖片生成的模型做得越好
雖然這個方法聽起來蠻簡單的
而且要依賴Clip這個模型的能力
但是這是今天蠻主流的衡量圖片生成模型好壞的一個做法
所以發現說今天我們的時代已經來到了
不只是人來評量這些生成式AI的好壞
今天通常是用另外一個模型
用另外一個人工智慧來評量這些生成式AI生成的好壞
但是生成圖片有一個特殊的難點
就是圖片是一張圖是勝過千言萬語的
很多東西你根本難以用文字來描述
它到底長什麼樣子
比如說這是我家客廳的一個鐘
我想要讓這個GPT-4
其實就是DALLI啦
我想讓DALLI呢
畫一個一模一樣的圖
可不可以畫出一個類似的鐘
我就試著跟他描述
這個鐘長什麼樣子
我跟他說這是一個有
現代簡約設計風格的鐘
鐘面是一個
大約圓形的框架
然後是黑色金屬材質
沒有刻度
鐘的中央是一個木板
上下摟空
patya的字樣沒有其他花紋
那指針呢是黑色的設計簡約
只有時針分針沒有秒針
讓他根據這些文字來想像
這個鐘應該長什麼樣子
好那我就試著碰了很久了
不斷改我的文字敘述
希望產生出來的鐘呢
跟我家客廳的鐘呢越像越好
在我把那天的額度都用完之前
我能夠畫出來最像的鐘呢
也只能夠長這個樣子而已
跟真正的我想要他畫的鐘
還是有一點點的差距
是已經很像了啦
但是不管我怎麼努力的描述
感覺都還是有一些差距
因為有很多圖像是文字難以描述的
所以怎麼辦呢
也許我們可以做個人化的圖像生成
那這個其實就是我們在作業時會做的事情
那怎麼做個人化的圖像生成呢
假設你的桌上有一個很特別的雕塑
你也很難描述它是什麼雕塑
那你就直接幫它取一個特別的名字
我們把它叫做S star
要把它叫什麼都可以
你就是要拿一個你平常沒有在用的符號
來稱呼這一個你想要客製化的特別的物件
那你就不要拿你平常會用的符號
比如說如果你把它叫做狗
那就不好了
以後你就再也畫不出狗來了
有時候我畫出來的狗都長這個樣子
而且他有可能會被那個詞彙的意思所汙染
因為你跟他說這是一個狗
只有他可能會把一個狗的頭呢
加到這個雕像上面
因為他覺得這其實應該要長得像一隻狗
總之你要選一個沒有什麼特殊的意思
然後你平常不會再用的符號
來代表這個你要客製化的對象
然後接下來呢
你就教你的文字生圖模型說
以後我跟你講
S-Star的照片
你就要畫這樣的圖出來哦
跟你講S-Star的照片
你要畫這樣的圖出來哦
然後微調你的文字生圖模型
他就可以認得這一個客製化的對象
那之後呢
你就可以把這個客製化的對象
放在各式各樣的場景中
比如說
這邊的例子是來自於右下角這邊論文
他們只用三到五張圖片而已
這種客製化的技術呢
不需要太多的圖片
他們只教這個文字生圖的模型說
這幾張圖片都是S-STAR這個東西
接下來他就可以下各式各樣
跟S-STAR有關的指令
比如說跟他說
An oil painting of X-STAR
這個X-STAR的油畫
就畫一個油畫版的X-STAR
或產生一些X-STAR的APP ICON
放一些X-STAR的APP ICON
跟他說產生一個ELMO
ELMO就是這個紅色怪物
這個 ELMO 要做得像一個 S-Dot
這個模型居然聽懂了
畫了一個 ELMO
就是用個盤做的方式
就跟 S-Dot 一樣
產生一個這個針織版的 S-Dot
他也知道什麼叫做針織版的 S-Dot
所以你可以透過少量的物件
來客製化你的圖片、生圖的模型
這個其實就是我們最後一個作業
會做的事情了
好那剛才講的是文字生圖
那接下來我們來擴展到文字生影片
那文字生圖跟文字生影片有什麼樣不同呢
其實沒有什麼樣本質上的不同
因為本來文字生圖就是產生一堆 patch
文字生影片只是產生了更多的 patch 而已
那如果是文字生圖產生出來的 patch
只對應到一張圖片
如果是文字生影片的話
你就要產生一大排的patch
然後這些patch集合起來
就對應到一段影片
就
結束了
就這樣其實他的原理就這樣
你可能想說文字生影片
感覺這個道理也還蠻簡單的
就是生出一大堆的patch
那到底為什麼大家覺得文字生影片
是一個很大的挑戰呢
那仔細想一想文字生影片的
複雜的程度
假設我們現在每秒鐘有24個Frame
你一般看影片的時候每秒鐘都是24個Frame
那我們現在假設每一個Frame用64x64個Patch來描述它
通常依照今天Encoder的能力
64x64個Patch通常還原回去
可能是1024x1024的圖
就是一個普通的解析度的
普通清晰程度的影像而已
假設每一個Frame有64x64個Patch
那一段一分鐘的影片應該要有多少的Patch呢
我算了一下
一分鐘的影片總共有1440個Frame
因為每秒鐘24個Frame
乘以60 1440個Frame
每個Frame有64x64個Patch
算出來是600萬個Patch
600萬個Patch聽起來數量還不是非常的驚人
因為今天你動不動就聽到一個類神經網路
它的參數量是上億等級的
所以我相信百萬對你來說已經不是一個非常大的數字了
但不要忘了Transformer裡面是要做Attention的
Attention是每一個位置跟每一個位置兩兩間都要做Attention
所以假設所有的Patch兩兩間都要做Attention
六百萬的Patch要做的Attention的次數是六百萬的平方
六百萬的平方是三十六兆啊
所以今天如果你要生一段一分鐘的影片
他不是一個特別高清的影片
但你每一次每一層在做Attention的時候
居然要做36兆次的Attention
這個運算量聽起來就非常的驚人
所以文字生影片最大的挑戰就是
這個運算量實在是太大了
所以怎麼辦呢
你會發現近年來一系列的研究
都是在想盡辦法減少需要的運算量
那怎麼減少需要的運算量呢?
首先第一個想法就是改造Attention
真的有必要所有的Patch間通通算Attention嗎?
算Attention的目的就是要讓不同的Patch間有關聯嗎?
這樣產生出來的影像每一個畫面才會是一致的
但是一個一分鐘的影像
最開頭的影片最開頭的Frame左上角的Patch
真的會跟最後一個Frame右下角的Patch有關聯嗎?
也許我們根本不需要計算那麼多的Attention
所以第一個改造的方向是減少Attention的運算
就這邊是三個Frame
每個Frame裡面有4x4個Patch
然後這一個Patch要計算Attention的話
他不只要跟同一個Frame裡面所有的Patch做Attention
他還要跟同一段影片裡面其他的Frame也做Attention
這種既考慮同一張圖片裡面的資訊
也考慮時間的資訊的Attention
叫做Spatial Temporal的Attention
那今天可能已經很少有人
硬做Spatial Temporal的Attention了
因為他運算量太大了
沒辦法幫你升高清的以下
所以怎麼辦呢
也許一個簡化的方向就是
我們不要管Frame跟Frame之間的關係
每一個Patch
只跟同一個Frame裡面的Patch
來考慮Attention
這樣至少可以確定
至少可以確定同一個Frame裡面
他們的畫面是不會太違和的
同一個Frame裡面他的畫面是一致的
那這種Attention呢
只考慮了一張圖片裡面的長跟寬
只考慮兩個方向
他是一個2D的Attention
叫做Spatial的Attention
那只是考慮Spatial的Attention顯然是不夠的
因為如果只有Spatial的Attention的話
你的畫面換畫面之間就不連續了
就不連貫了
那怎麼辦呢
所以也需要考慮時間的Attention
這種考慮時間的Attention叫做Temporal的Attention
它是1D的Attention
在考慮這個Patch的時候
考慮另外一個Friend同樣位置的Patch
但這種Attention顯然是有問題的
因為它只考慮時間這個維度的諮詢
它沒有考慮空間這個維度的諮詢
所以看起來2D的Attention跟1D的Attention
Special attention跟Temporal attention
都是有一些侷限的
那這兩個東西都有侷限
你要用哪一個呢
就是我全都要
不要想那麼多
就把這兩種Attention疊在一起
就可以創造一個偽3D的Attention
今天在做這種影片生成的時候
你常常看到這樣的架構
就是把Special attention
跟Temporal attention
2D的Attention跟1D的Attention
交替使用
把他們兩種Attention疊在一起
就可以製造出偽3D的Attention
那我們來計算一下這個偽3D的Attention裡面
需要計算Attention的次數
我們剛才已經知道
如果是完整的3D的Attention
Special Tempo的Attention的話
它算出來的Attention的次數
真的是一個天文數字
那我們來看一下Special的Attention
如果今天我們只考慮同一個Frame裡面的Patch
兩兩之間要做Attention的話
那要做多少次的Attention呢?
那一個Frame裡面有64x64個圖
兩兩要做Attention
64x64的平方
總共有1440個Frame
我們要生一分鐘的影片
總共有1440個Frame
所以把64x64的平方再乘以1440
是240億次的Attention
那如果是Temporal的Attention呢?
Temporal的Attention
是不同Frame裡面同樣位置的Patch
Patch才做Attention
那我們的時間的那個維度
總共有1440個位置
他們兩兩要做Attention
所以是1440的平方
那我們總共有
一個Frame裡面有64x64個Patch
所以假設是Temporal Attention的話
那要做Attention的次數
是1440的平方x64x64
是85億次
雖然這邊也是上億等級的次數
但是你會發現
240億次加85億次兩個合起來做一個偽3D的Attention
跟原來全3D的Attention
它的差別是有達到千倍那麼多
所以透過把temporal加Special的Attention兩種Attention疊起來
你可以製造偽3D的Attention
它需要的運算量是原來真正全3D的Attention的運算量的千分之一而已
這個是今天影片生成模型非常常用的一個技術
那還有什麼方法減少需要的運算量呢
那我們上週有講我們上上週講生成式策略的時候
有講過我們在生成的時候不一定要一步到位
可以把生成分成很多個步驟依序執行
那文字生影片今天也常常使用類似的技術
我們之前有講過說,你可以給你的生成式AI一個條件,先讓它生成第一版的物件,然後再有另外一個生成式AI的模型,把第一版變成第二版,然後最後有一個模型把第N-1版變成第N版,人們以為是輸入這個條件,直接就產生第N版,其實中間已經做過了非常多生成式AI的模型,但每個模型只要專注在某一個任務上面,製造一個流水線,每個流水線的任務都沒有很複雜,
但是全部接起來就可以做複雜的事情
影片生成今天也通常會做非常類似的設計
比如說第K-1版跟第K版的差別可以是
第K-1版裡面每一個frame的解析度是比較低的
第K版它的解析度是比較高的
這個時候生成式AI只需要專注於
把每一張圖片的解析度調高
把根據已經產生出來的低解析度的圖片
產生高解析度的圖片就好
那有時候第 K-1 版到第 K 版
可能是調高了幀數
調高了每秒裡面那個幀
每秒裡面的 frame 的速度
比如說 K-1 版可能每秒只有兩張圖片
第 K-1 版就把兩張圖片擴展到每秒有四張圖片
這個時候對 K-1 到第 K 版的這個深層式 AI 來說
他只需要專注於去預測兩張圖片中間
如果你要做Interleaving
你要做內插的話
應該生出什麼樣的內容
才會是看起來比較連貫的
雖然我們不知道Sora是怎麼做的
不過我們可以看另外一個Google的模型
叫做Imagen
Imagen裡面呢
就用了非常類似的技術
那上面這個圖呢
是來自於Imagen的論文
他就告訴你說
他最先產生出來的圖片
這裡面每一個框框代表一個模組
代表一個模型
他先用一個最簡單的模型
這個模型吃文字的輸入
他只產生一個這個
他的這個這個這邊有三個數字
第一個數字是有多少個frame
然後第二個跟第三個數字是生出來的每一個frame
他的長跟寬是多少
所以第一個模型只會生16個frame
長跟寬分別是40x24
然後他說他的這個 frame per second 每秒的幀數是3
那他生16張圖的話
那他生出來的影像長度大概就是5秒左右
而Base Model生出來這個影片呢
人呢一定是覺得根本不知道在做什麼的
但沒關係
有另外一個模組
有另外一個模型
他把三個fps 每秒三個frame
調高到每秒六個frame
再有下一個模組
把每一個frame的解析度變高
從原來40x24的圖變成80x48的圖
然後再有另外一個模型
把解析度再變更高一點
80x48變320x192
然後再有一個模型
提高一下每秒的幀數
把每秒6幀變成每秒12幀
再一個模型再把幀數再調高一點
每秒12幀調到每秒24幀
這個就是你一般在看動畫的時候的這個幀數了
然後再來呢
但是每個frame如果它的大小是320x192
這個解析度還是太低了
那把它擴展到1280x768
跟你的螢幕一樣大
那這樣就可以產生清晰的影片了
那這個也是今天文字生影片常用的技巧之一
如果你想要學更多有關影像生成的生成式AI的話
你可以看以下兩篇paper
這篇paper講的是語言模型怎麼把影像當作輸入
那這篇paper講的是Diffusion Model怎麼產生影片
