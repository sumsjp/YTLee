hi everyone I'm join me and his firm is
down gently with my colleague PJ yen and
our adviser Holly I'll be talking today
about new way to analyze hidden
representations of into an ASR there is
exercising speech for Network layers
this is today's outline we will first
give an introduction on analyzing DNS in
speech recognition a proposed approach
of using speech synthesis to analyze
speaker Keynesian models and then we
proceed to our experiments which
includes using our method to analyze the
speaker and North information contained
in two antennas our motto and finally
concludes our paper so let's start the
introduction there are already plenty of
work on analyzing neural networks in SSR
systems for example Muhammad and L
showed that DNA's are good at
normalizing feature vectors across
different speakers using visual TC
visualizations
well these analysis are already ready
cleared for researchers we feel like it
isn't still not intuitive enough for
public audience to understand so if we
are able to let them hear healthy NS
process which signals it would be an
interesting direction in model
explanation so the motivation is for
audio inputs could we directly hear how
the air cell model process audio inputs
for example you might expect as a model
to discard information irrelevant to the
quest encountered for example the
identity of the speaker or background
noise as the layer goes deeper here we
and intuitive method of auto firing sr
hidden states that is largely model
agnostic namely it doesn't require
special model architecture and is
therefore compatible with most existing
your sr models let's proceed to the next
session the proposed approach we now
introduce our workflow of auto PI s our
hidden States we per screen in n layer
SR model with CTC loss the input is a T
dimensional male spectrograms with
deltas and accelerations being stacked
along a channel dimension and we fixed
the SR model takes on layers hidden
States for example take their ones
hidden States as the input of the
probing model and then we train a pro-v
model to recovery input speech the
objective is to minimize the loss
between recovered spectrograms
and original spectrograms and so if the
original speech sounds like and the old
gentleman was so delighted with his
success that he had to burst out into a
series of short happy bit reconstructed
output from there one she'd sound like
and the old gentleman was so delighted
with his success that he had to burst
out into a series of short which doesn't
differ much since it's only one layer
deep and almost no information is lost
and we repeat this process for other
layers and train another probing model
to minimize loss for example here we
take the last layer of sr hidden states
and train another reporting model let's
listen to the output series of short so
we can't hurt them deeper layer actually
loses more timber and prosody of a
speaker which is aligned waste not
literature that again has effectively
normalizes representations of deepened
speaker so we found that our promo
actually can review what is left in a
hidden state theory will recover speech
now introduce the architecture of our
probing model which is shown in red
bounty box the length of the hidden
state is first up sample back to the
original length by a linear projection
if it has been down sent on previously
in the u.s. our network and we fed it
into a full layered highway network and
output the reconstructed spectrogram in
the next section I'll go through the
settings and results of our probing
model experiment so first we'd like to
compare areas our models under different
training conditions using our probing
model here we choose to do data
augmentation we use slivery speech train
clean 100 our subset as our speech
corpus and later augment and clean set
with museum corpus which has music
babbling and noise recordings we trade
two types of as our models in the
following experiment baseline model
which trained only on the Train clean
monitor a subset of livery speech and
noise robust model which trained on
Franklin 100 subsets and also augmented
with noises in music corpus in our
second
experimental setting we study the effect
of model architecture on processing
speaker and noise information there are
two n sr models being analyzed in
following experiments top one is a pure
ASTM model which has five bi-directional
Mayella steam layers which Townsend
playing performed after second third and
fourth layer the bottom one is an EEG
OST model which is comprised of four
convolutional layers and five
bi-directional STM layers each
convolutional layer is followed by a
relative activation and max pooling is
multiplied every two collusion or layers
both models are trained with CDC lost on
baseline and noise for bus cities and
here's the word error rate of our models
under all different experimental
settings rebounded PGG outperforms LST
on average and augmentation helps in all
of the following probing experiment
apart from clean speech we also pet
cleans noisy speech a different signal
to noise ratios and whatever input s are
received
we asked probing models to reconstruct
it so if a program model failed to
reconstruct noise in the input speech we
could infer Leddy as amazos is doing the
noisy we begin with experiments on
speaker information so first we'd like
to analyze the speaker information flow
in the air s our model here we take your
elders TN as example
this is the original input speech and
the old gentleman was so delighted with
his success that he had to burst out
into a series he gave way to the others
very readily and retreated unperceived
by the squire and Mistress fitzooth to
the room and after the birth volunteer
mayor female voice and the old gentleman
was so delighted with his success but he
had to burst out into a series of short
after the third bounced him their senior
boys until gentlemen was devising the
success that he had to burst out into a
series of shows so speaker information
is slightly lost in layer three and
would proceed to the last layer post
speaker female first and he had to burst
out into a series of short and male boys
last layer he gained weight to the
others very rarely ever to not proceed
by displaying his frustrations to the
rear we can directly here from Lisa's my
speech that timber and prosody of the
speaker are gradually removed out as
mayor goes deeper demonstrating led our
promoter provide a more intuitive way of
explaining known behavior as our models
we performed some speaker verification
experiments on the generated speech to
quantify loss of speaker information in
the area hidden states on the right of
the slides we have the speaker
verification pipeline so for speaker a
and B we fed it into the pipeline of ASR
model and probit model and generated the
recovery spectrogram and we fed lamb
into the speaker verification model and
asked our later sent speakers a higher
low eco error rate the less speaker
information is left in hidden states and
here's the results used to verification
model in ResNet and as TM D vector and
as can be seen from the bigger most
models observed and increased in ER as
the layer goes deeper which
demonstrating led
speaker characteristics are actually
gradually removed and which is in line
with our previous observations in this
generated speech
this demo we take a look at aluminum our
problem mother discovers in VG ji hyo's
TMS our models first we focus on CN part
Hertzian one gentleman was so delighted
with his success that he had to burst
out into a series of short benzene four
and the old gentleman was so delighted
with his success that he had to burst
out into a series of short his
interesting lensless en part of the
model does not affect the spectrum so
much which of course wisdom out
increased of a our values in speaker
verification experiment but belief led
it is because the CN part behaves like
filters and extract different patterns
in the spectra brands but does not
remove much information and then the
hosts impart first palestine one success
and he had to burst out into a series of
short in the masked layer bounced him
pipe and the old gentleman was so
delighted with his success that he had
to burst out into a series so same
behavior as pure OST n opt ember and
prosody of the speaker are seriously
eliminated in ellis team layer which can
also be observed from the drastically
increased ER values we proceed to the
noise information experiment so this
demo is about comparing the the noisy
cabin capability of the GTOs team based
on model and most robust model and
here's the noisy input speech and the
old gentleman was so delighted with his
success that he had to burst out into a
series of short and cn1 output of
baseline model and the old gentleman was
so delighted with his success that he
had to burst out into a series of short
scene one robust model and the old
gentleman was so delighted with his
success that he had to burst out into a
series of short CNN for of baseline
model and the old gentleman was so
delighted with his success but he had to
burst out into a
of short cn4 up north robust model and
the old gentleman was so delighted with
success and he had to burst out into a
series of short piano noise he's
severely suppressed in noise robots
model but not the baseline model and I
was team one or baseline model success
he had the first value to a series of
the old gentleman was so delighted like
a success that he had to burst out into
a series of short piano noise in noise
robots model almost disappears but
similarly not baseline model the last
layer of baseline model noise actually
greatly affects baseline model in the
last layer and last result in the higher
where there were rate of baseline model
the last layer of Northville boss model
analyzing success that he had a burst
out into a series as you can hear noise
for boss models successively removed
noise to further verify that our probing
model faithfully conveying the
difference of denoising capability
between baseline and noise for bust ASR
we adopt that the sto I measure which
stands for a short time objective
intelligibility the lower the sto I the
more the distortion so measured s TOI
between clean speech and the recovered
audio and compare the difference of s
TOI between baseline and noise robot
systems and here's the result red green
and blue correspond to upon SN R minus
20 DB minus 10 DB and 0 DB while we
observe an inevitable drop of Sto I mean
both baseline head robust SR which can
be attributed to the loss of speaker
information noise robust ears are a
solid loins actually suffers less from
the improve degradation and achieves
better it sto is compared to the
baseline which is the dotted lines
in his work we propose a new intuitive
way of analyzing Sr through or defying
hidden states it is largely model
agnostic there is no constraint on model
architecture and loves candy can be
widely applied to all kinds of new areas
bar and finally we conduct various
measures on generate his feet and his
show findings largely consistent with
the literature and if you'd like to
listen to more recovered speech here's
the QR code thank you very much
