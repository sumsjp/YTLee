好 這個接下來
想要跟大家分享的
是一個叫做 Reward Shaping 的概念
什麼是 Reward Shaping 呢
到目前為止我們學到的東西是
我們把我們的 Actor 拿去跟環境互動
得到一堆 Reward
那把這些 Reward 做某些的整理以後
得到這邊的分數 A
有了這邊的分數 A
你就可以去教你的 Actor
該做什麼 不做什麼
但是在這個 Reinforcement Learning 裡面
我們很怕遇到一種狀況是
假設 Reward 永遠都是 0 的時候怎麼辦呢
假設多數的時候 Reward 都是 0
只有非常低的機率
你會得到一個巨大的 Reward 的時候
那怎麼辦呢
假設 Reward 幾乎都是0
那意味著什麼
意味著說你今天這個 A 不管怎麼算
都是 0
對 每一個 Action 都是差不多的
反正不管執行什麼 Action
得到的 Reward 都是 0 嘛
所以執行這個 Action 執行那個 Action
根本沒差
那如果不管執行什麼 Action
Reward 幾乎都是 0 的話
那你根本沒有辦法去 Train 你的 Actor
那講到這種 Sparse Reward 的問題
也許有人馬上會想到的是
下圍棋也許是一個 Sparse Reward 的問題
因為在下圍棋的時候
你每落一子
你並沒有得到 Reward
你並沒有得到 Positive 或 Negative 的 Reward
只有在整場遊戲結束
落完最後一子的時候 在中盤
在落完最後一子遊戲結束的時候
你贏了才會得到 Positive 的 Reward
你輸了才會得到 Negative 的 Reward
但是我覺得相較於有一些其他任務
這個下圍棋
還算是比較有 Reward 的 RL 的問題
舉例來說假設今天的問題是
你要教機械手臂去拴螺絲
而教機械手臂拴螺絲這個問題
你合理的 Reward 的定義是
假設今天機械手臂成功把螺絲拴進去
它就得到 Positive Reward
沒有把螺絲拴進去
Reward 就是0
但是你想想看 一開始你的機械手臂
它裡面的 Actor 的參數是隨機的
所以它就在空中隨便揮舞
怎麼揮舞 Reward 都是 0
不像是下圍棋
你至少整場遊戲玩完
你還有正面的或負面的 Reward
而像機械手臂
你要叫機械手臂去拴螺絲
除非它正好非常巧合的拿起一個螺絲
再把它拴進去
它得到正向的 Reward
不管它做什麼事情都沒差
得到的 Reward 通通都是 0 分
好 那遇到這種狀況的時候怎麼辦呢
遇到這種狀況的時候有一個解法就是
我們想辦法去提供額外的 Reward
來引導我們的 Agent 學習
也就是說在原來的 Reward
也就是我們
真正要 Agent 去 Maximize 的 Reward 之外
我們再定義一些額外的 Reward
我們定義這些額外的 Reward
來幫助我們的 Agent 學習
那這種東
這種訂額外的 Reward
來幫助 Agent 學習這種事情呢
就叫做 Reward Shaping
那你知道我們人呢
其實也很擅長做 Reward Shaping
舉例來說
這讓我想到一個這個妙法蓮華金鐘的故事
這個故事呢
出自妙法蓮華金鐘的化城喻品
這個故事就是有一個領隊
然後帶一群人呢 去找寶藏
然後寶藏呢 在五百由旬之外
那由旬是什麼單位我也忘了
那你就當做很遠就對了
就當 500
500 萬公里之外
然後呢這群人呢 走到半路
已經過了半途了
覺得很累 不想再往前走了
那大家就坐在地上哀嚎 不想再往前走了
那領隊看大家不想再往前走 怎麼辦呢
他就跟大家說
再往前 10 公里有一個五星級的飯店
大家就可以去休息了
大家就看到五星級的飯店就很高興
就去休息了
然後隔天早上飯店就不見了
然後領隊說那個飯店呢
是我用法力變出來的
那領隊有法力
那飯店是領隊用法力變出來的
然後為了鼓勵大家再繼續往前走
免得大家半途而廢
這妙法蓮華經說這個故事是為了表示說
佛最後希望大家成就的是佛道
但佛道非常的長
所以中間呢
設了小聖 中聖 大聖等不同的位階
來引導大家前進
那如果對應到比較生活化的例子就是
比如說現在叫你念博士
你可能覺得你要博士畢業才能夠
拿到博士學位才能夠得到 Reward
那你就會覺得
哇 這個路呢 非常的長
然後就不想要念博班了
但是如果告訴你說
你先修個課就可以得到 Reward
然後做一點專題
雖然可能也沒有做出什麼厲害的成果
但老師就會說你好棒 你也得到 Reward
然後最後
先發一個 Second Tier 的 Conference
得到 Reward
再發表 Top-tier Conference
再就得到 Reward
然後最後你就可以博班畢業
這樣一步一步的往前走
最終你就可以達成最終的目標這樣
這種地方也要業配念博班
我實在是有點受不了了
不過沒關係
反正這一門課快結束了
要再業配也沒幾次了
好 那這個呢 就是 Reward Shaping 的概念
好 那我們接下來呢
就舉一個 Reward Shaping
真正實際使用在 RL 裡面的例子
給大家參考
那這邊舉的例子呢
是用 VizDoom 來跟大家舉例
那因為怕大家不知道 VizDoom 是什麼
所以這邊呢
還是放一個影片給大家看一下
這個 VizDoom
是一個第一人稱的射擊遊戲
那蠻常有人用 RL 來玩 VizDoom的
那以下呢 是一個 VizDoom
用 RL 來玩 VizDoom
用機器來玩 VizDoom 這個遊戲的
這個比賽的錄影
我們來看一下這個比賽的錄影
長起來像這個樣子
這邊的每一個選手 都不是人
每一個選手通通都是機器
所以你仔細看一下會發現說
有些人的行為很奇怪
比如說右上角這個人
它還蠻容易卡牆的
右上角這個人還蠻容易卡牆的
它在撞牆壁
為什麼
因為它不是個人 它是一個機器
好 這個是讓你知道一下 VizDoom
大概是什麼樣的遊戲
那這個影片非常長
它是個長達兩小時的史詩級的戰鬥
你有興趣再慢慢把它看完
在當年那個 VizDoom 的比賽裡面
第一名的隊伍
就有用到了 Reward Shaping 的概念
那在 VizDoom 這個遊戲裡面
你被敵人殺掉 你就扣分
你殺了敵人就加分
但如果你光憑著這個遊戲裡面真正的 Reward
來訓練 Agent
你是很難把它訓練起來的
所以呢 在這一篇文章裡面
就使用了一些 Reward Shaping 的概念
那我們就來看一下
它是怎麼定這些 Reward 的
舉例來說這個第一點呢 我們等一下再看
我們先看第二點
第二點 它說
如果今天有扣血
那你就得到負的 Reward
那其實在遊戲裡面扣血並沒有懲罰
扣血並不會扣分
你要死掉才扣分
但是如果機器要到死掉
才知道它得到負的 Reward
那它可能要很花很久的時間
才能學到扣血跟死掉之間的關聯性
所以這邊直接告訴機器不可以扣血
扣血是壞的事情
好 然後這邊是如果損失彈藥就扣一點分數
然後撿到這個醫藥包就加分
撿到彈藥的補給包就加分
那這些事情在遊戲裡面是不會影響分數的
但是我們人 我們 Developer
另外強加給機器來引導機器學習
好 接下來就是一些比較有趣的 Reward 了
舉例來說 它說呢
如果你的 Agent 總是待在原地
那就要扣分
為什麼需要這樣的 Reward 呢
因為如果你沒有這樣的 Reward
對 Agent 來說一開始它非常弱
它出去走呢 隨便走兩步就被敵人殺死了
所以對它來說 對一開始很弱的 Agent 來說
可以得到比較高 Reward 的做法
也許就是待在原地
待在原地至少 Reward 是 0
那出去碰到敵人還要被扣血
那多划不來
所以為了避免 Machine 最後什麼都沒有學到
就只會待在原地
所以強制告訴它說
你只要待在原地就直接扣分
然後還告訴它說如果你動
就給你一個很小的分數
這邊你只要每動一下
就給你 9 乘以 10 的 -5 次方
一個非常小的 但是是 Positive 的 Reward
好 但是光是要求機器動是不夠的
所以這邊又多加了一個非常有趣的 Reward
這個 Reward 是每次如果 Agent 活著
Agent 每活著 它就要被扣分
你可能覺得很奇怪 活著不是一件好事嗎
為什麼活著反而應該要被扣分呢
那是因為假設現在活著沒有扣分
或者是甚至是一件正面的事情
對 Agent 來說
它會學到的可能就是邊緣 OB
你雖然有要求它動
那你要求它動沒有關係
它就在邊緣一直自轉就好
它在邊緣一直轉圈圈就好
然後看到敵人就躲開
不要跟敵人做任何的正面交鋒
那對你的 Agent 來說
也許這樣是一個最安全的做法
但是為了強迫 Agent 學習去殺敵人
所以反而告訴它說你只要活著就是扣分
你要想辦法不可以活太久
你要想辦法去跟別人交手
所以這是一個非常有趣的
Reward Shaping 的方式
那看到這些 Reward Shaping 你就會發現說
Reward Shaping 這一件事情呢
其實是需要花 Domain Knowledge 的
它其實是需要憑藉著
人類對你現在環境的理解來強加上去
那今天再舉另外一個 Reward Shaping 的例子
假設你今天要訓練一個機器手臂
那這個機器手臂的工作呢
就是把這個藍色的板子
插到這個棍子上
好 那像這樣子的任務
你要憑著 RL 的方法讓機器憑空學會
把藍色的板子插到這個棍子上
沒有那麼容易
但是你可能會想到一個
很直覺的 Reward Shaping 的方法是
假設今天這個藍色的板子
離這個棍子越近
那我們得到的 Reward 就越大
但是如果你仔細想想會發現說
單純讓藍色的板子離這個棍子越近是不夠的
為什麼讓藍色的板子離棍子近是不夠的呢
你可以看看這個像右邊這兩個 Case
機械手臂也是想把藍色的板子挪進棍子
但它做的事情呢
其實就是去打那個棍子
從側面接近
從側面接近是沒有用的
把藍色的板子從側面接近棍子
並不能夠達到你最終的目標
所以如果我們單純只是說
現在你的藍色的板子離這個棍子越近
它的 Reward 就越大
你用 Reward Shaping 的方法
把藍色的板子跟棍子之間的距離
當做一個新的 Reward
但可能對你最終想要解決問題本身
是不一定有幫助的
所以 Reward Shaping 這個東西
你在用的時候必須要小心
它需要你對這個問題本身有足夠的理解
你才有辦法使用 Reward Shaping 這樣的招數
好 我們看一下有沒有同學有問問題的
好 好 目前沒有同學問問題
那我們就繼續啦
好 那 Reward Shaping 裡面
有一個特別有趣而知名的做法叫做
Curiosity Based 的 Reward Shaping
也就是呢 給機器加上好奇心
給機器加上好奇心
什麼叫做給機器加上好奇心呢
所謂好奇心的意思就是要去探索新的事物
所以在原來的 Reward 之外
我們加上一個 Reward
這個 Reward 是
如果機器它在活動的過程中看到新的東西
它就被加分
但這邊 又有一點要強調的是
新的東西必須是有意義的新
不是無謂的新
什麼叫做有意義的新 不是無謂的新呢
那這個我們等一下再解釋
好 那這個 Curiosity based 的
這個 Reinforcement Learning
它來自於 ICML 2017 的這一篇文章
那這篇文章裡面有一個非常驚人的 Demo
那我們來播一下這個 Demo
我想稍微解釋一下剛才那個影片的意思
剛才那個影片的意思是說
我把它停下來 等我一下
就是它讓機器玩瑪利歐
那他甚至在瑪利歐這個遊戲裡面
沒有任何的 Reward
他甚至沒有告訴機器說
破關就可以得到 Positive 的 Reward
那其實你告訴機器
破關可以得到 Positive Reward 也沒用啦
這種 Reward 太 Sparse 了
大概很難拿來訓練 Agent
它只告訴機器說
你要不斷地看到新東西
那光是這一件事情就可以讓機器學會
破瑪利歐的其中一些關卡
那當然這樣子的方法
也許對瑪利歐而言是最合適的
你知道瑪利歐是橫向捲軸遊戲嘛
那你要破關就是要不斷地往右走嘛
那機器它要看到新的東西
就不斷地需要往右走嘛
所以機器會光藉由 Curiosity 這件事
就可以學到破解瑪利歐的一些關卡
不過它有嘗試說訓練在前面幾關
然後直接測
直接把 Agent 放在那個地下的關卡
然後就發現說做不起來
在地下關卡還是要微調一些 Network
微調一下 Network 才做得起來
好 那這個影片還有後半段啦
我們
我們直接從某個地方開始看好了
好 它接下來是
他接下來是直接叫機器去玩那個 VizDoom
直接叫機器去玩 VizDoom
我們也來放一下 VizDoom 的部分好了
好 那最後那一個背景有一堆雜訊
有點像是那個電視機壞掉
那種雜訊的那個畫面
你可能有點不知道它這個影片想要表達什麼
那那個部分就是我想要
我剛才在最前面講到的有意義的新這件事情
等我一下
好 有意義的新這件事情
就是假設我們要求 Agent
要一直看到新的東西
那如果你的畫面背景有一個雜訊
那雜訊會不斷地變化
所以對 Agent 來說雜訊是新的東西
也許它就不會學到去探索新的環境了
反正光站在那邊看雜訊就夠了
它會覺得它不斷地看到新的東西
所以其實
在 Curiosity Based 的這個 RL 裡面
也有想辦法克服這種沒意義的新
這種看到雜訊的問題
至於實際上怎麼做
那你再參考他的論文
