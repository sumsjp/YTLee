好 那有關 RL 的最後一個部分
我想要跟大家分享的是
假設今天
如果我們連 Reward 都沒有
該怎麼辦呢
在剛才的課程裡面
我們是有 Reward 的
只是有時候 Reward 非常地 Sparse
所以你要做 Reward Shaping
但是假設今天
連 Reward 都沒有的話
到底該怎麼辦呢
那為什麼有時候會連 Reward 都沒有呢
其實像 Reward 這種東西
往往只在一些比較 artifisial 的環境
比如說遊戲裡面
特別容易被定義出來
在遊戲裡面
有一個記分板
所以你特別容易去定義說在遊戲裡面
怎麼樣的行為是好的
怎麼樣的行為是不好的
某一個行為有多好
或某一個行為有多不好
但是在真實的環境裡面
你要定義 Reward 這件事情
有可能是非常地困難的
假設你今天要用 RL 的方法來學
學叫自駕車學會在路上走
那到底在路上走
做什麼樣的事情
會得到什麼樣的 Reward 呢
它禮讓行人
就給它加 100 分嗎
還是應該加 1000 分
那闖紅燈
應該扣 50 分
還是扣 50000 分呢
那像這種東西
你根本不知道要怎麼訂
所以在更真實
在真實的環境中
有時候我們根本不知道要怎麼定義
Reward 這個東西
而且有時候
那你說
雖然我們不知道怎麼定 Reward
但我們可以憑著人類的智慧
去想一些 Reward 出來
來 Guide 一個 Machine
像我們剛才講的那種 Reward Shaping
不就是一個很好的例子嗎
我們自己想一些 Reward 出來
來叫 Machine 學
但有時候你在想 Reward 教 Machine 學的時候
如果你的 Reward 沒想好
Machine 可能會產生非常奇怪的行為
你無法預期的行為
那這邊舉一個比較極端的例子
有一個電影
叫做機械公敵
在機械公敵裡面呢
當然這就未來的世界啦
有一大堆機器人
那些機器人的行為
必須符合三條原則
這三條原則是
第一條 不能夠傷害人類
也不能夠作勢人類被傷害
第二條是
在不違反第一條的前提之下
機器必須要聽從人類的命令
那第三條是機器必須要保護自己
在不違反第一條跟第二條的前提之下
所以你可以想像
這三條 Law
這三條規則
就代表說
機器如果不違反這三條規則
就得到 Positive Rewards
違反這三條規則
就得到非常 Negative 的 Reward
然後有了這三條規則以後
機器呢
就自己去發展
自己去學習
但是最終呢
機器就得到神邏輯
在不違反這三條規則的前提之下
要得到最大 Reward 的方式
就是把人類監禁起來
因為人類會自我傷害
所以應該把人類監禁起來
避免他們自我傷害
這樣機器就可以得到最高的 Reward
所以這個例子告訴我們說
光訂 Reward 是不夠的
機器可能會有神邏輯
展現出你意想不到的行為
當然機械公敵裡面是一個比較極端的例子
那舉一個比較有可能發生的例子是
假設你訓練機器去
比如說收盤子
那這個是我在文獻上實際看過的例子啦
那你知道在文獻上有很多文獻是
要用 Reinforcement Learning 的方法
來訓練機械手臂嘛
假設你今天要訓練這個機器
把盤子擺到一個固定的位置
它把盤子放到指定的位置
就得到 Positive Rewards
這個是你定義的 Reward
然後你用 Reinforcement Learning 去學
發現機器會把盤子放到指定的位置
但它都非常大力 用摔的
然後結果盤子都被摔破了
那是因為你根本沒有告訴機器說
不能把盤子摔破
所以如果
你就會發現說
機器沒有告訴它不能把盤子摔破的前提之下
它為了達成目標
可能就把盤子摔破了
這個時候你只要趕快再訂新的 Reward 說
如果摔破盤子就要扣 100 分
但問題是
盤子已經被摔破了
已經來不及了
所以有時候
人訂的 Reward
不見得是最好的
所以怎麼辦呢
在沒有 Reward 的情況下
我們要怎麼訓練一個 Agent 去跟環境互動呢
那在沒有 Reward 的狀況下
讓機器跟環境互動
其中一個方法
叫做 Imtation 的 Learning
那在 Imtation Learning 裡面
我們假設 Actor
它仍然可以跟環境互動
但它不會從環境得到 Reward
Environment 仍然會送出 Observation 給 Actor
Actor 仍然會做出回應
Environment 仍然會隨著 Actor 的回應
給不同的 Observation
但是沒有 Reward 這個東西
沒有 Reward 這個東西要怎麼學呢
雖然沒有 Reward
但是我們有另外一個東西
這個東西
是 Expert 的 示範
我們找很多 Expert
通常是人類
我們找很多人類
也來跟這個環境互動
把人類跟環境的互動記錄下來
這些東西 就是
這些紀錄就是 Expert 的示範
就是 Expert 的示範
然後把這些 Expert 的示範呢
叫做 T^
我們用這個 ^ 這個上標
來代表人類的 Expert 的示範
那我們現在呢
就是要憑藉著這些示範
還有跟環境的互動
來進行學習
這樣講也許有點抽象
什麼叫做 Expert 的示範呢
假設你今天呢
要教機器開自駕車
那人類駕駛的行駛紀錄
那就是 Expert 的示範
人類駕駛的行駛紀錄可以告訴機器說
在這個路口
你應該打一下方向盤等等
那這些就是 Expert 的示範
或者是
你想要叫機器做一些指定的動作
比如說倒水 排碗盤
你可能會先拉著機械的手臂示範一次
那人去拉著機械手臂示範一次
這件事情
就是 Expert 的 Demonstration
就是這邊的 T^
那 Imtation Learning 要做的事情
就是從這些 T^
還有與環境的互動
進行學習
那講到這邊可能有同學就會想說
欸 這個問題聽起來好像挺簡單的
這個不就是 Supervised Learning 嗎
我們就把它當作 Supervised Learning 的問題
來看待就好啦
假設我們今天要訓練自駕車
你又有人類駕駛的紀錄
也就是你有記錄說
欸 人看到這樣子的路的畫面
那人就會採取某一種 Action
比如說他就會踩剎車
或者是踩油門等等
欸 你有這樣子的一堆的紀錄
我們不是直接就用 Supervised Learning
來 Learn 我們的 Agent 就好了嗎
你就說
你已經有人類給的資料說
看到 s1 這樣的畫面
最好的行為就是 a1^
s2 這樣的畫面
人類的行為就是 a2^
你已經有這樣子的訓練資料
那你就直接叫機器
去模仿人類的行為就好啦
今天機器給它看 si
然後它輸出 ai
你要讓它的 Action ai
跟人類的 Action ai^ 越接近越好
你就讓機器去模仿人類的行為
那沒錯
當你有這個 Expert 的示範的時候
這是一個做法
那這種做法呢
叫做 Behavior 的 Cloning
就是去複製人類的行為
但是光是讓機器去複製人類的行為
有可能有什麼樣的問題呢
一個可能的問題是
因為人類跟機器
他們可能可以觀察到的 s
會是不一樣的
什麼意思呢
舉例來說
假設我們一樣要叫機器學習開自駕車
那它是跟人類的 Expert 去學
人類的 Expert 在轉彎的時候
都可以輕鬆地轉過這個彎道
沒有人出車禍
沒有人任何馬路三寶
每個人都順利地
每一個 Expert 都可以順利地向右轉
所以對機器來說
它從來沒有看過失敗的狀態
它從來沒有看過一個車子快要撞牆的狀態
如果它從來沒有看過一個車子快要撞牆的狀態
它訓練資料裡面
就沒有這種東西
它就不知道
假設車子快要撞牆的時候
應該要怎麼處理
因為所有 Expert 都太厲害了
根本就不會讓車子靠近牆邊
根本就不會犯這種錯誤
那機器就不會學到說
在這一種人類平常不會經歷的狀態下
到底應該要怎麼處理
所以這是第一個可能遇到的問題
第二個 Behavior Cloning 可能遇到的問題是
雖然人類在開車的時候可能很厲害
但也許不是每一個行為
機器都需要亦步亦趨地去完全模仿
也許有一些行為需要模仿
有一些行為是人類個人的特質
根本不需要模仿
但是機器
它不知道什麼行為該模仿
什麼行為不該模仿
它只能完全複製人類的行為
那什麼叫做完全複製人類
完全複製老師的行為呢
以下就是一個完全複製老師行為的一個例子
那以下這個影片
出自 The Big Bang Theory
我就播給大家看一下
我的字是謝爾頓
No, it's 我的名字是謝爾頓
我的名字是謝爾頓
What's this?
That's what you did.
I assumed, as in a number of languages,
That the gesture was part of the phrase.
Well, it's not.
Why am i supposed to know that?
as the teacher, it your obligation.
To separate your personal idiosyncrasies
from the subject matter.
You know,
I'm really glad you decided to learn mandarin.
Why?
Once you're fluent.
You'll have a billion more people to annoy
instead of me.
好
所以剛才我們就是看到了一個
Behavior Cloning 的例子
那對機器來說
它就跟穿綠色衣服的 Sheldon 一樣
它的老師教什麼
它就會有一模一樣的示範
那它不知道什麼東西是需要學的
什麼東西是不需要學的
那如果機器
只是跟它老師採取一模一樣的行為
也許還沒有什麼問題
因為它跟老師採取一模一樣的行為
就跟老師做一模一樣的事情
雖然它可能會有一些多餘的行為
但也許是無傷大雅
但是我覺得更慘的另外一個狀況是
機器的能力
可能是有限的
今天也許機器沒有辦法完全模仿老師的行為
它只能選擇部分的行為
來進行模仿
就好像說有一個人呢
他想要變得跟賈伯斯一樣
然後他就去看了賈伯斯傳
然後把賈伯斯傳裡面
賈伯斯的所有的特質都列出來
比如說列了 20 個
包括有創意
然後脾氣暴躁
然後不修邊幅等等
但他覺得說
這些特質太多了
他只決定模仿一個而已
因為他能力有限
他只模仿一個而已
那如果他選擇有創意也許是好的
但也許他選到的是不修邊幅
那就沒有什麼用
所以假設機器能力是有限的情況下
Behavior Cloning
也許會造成更大的問題
所以怎麼辦呢
有另外一個技術
叫做 Inverse 的 Reinforcement Learning
好 我們在這邊看一下有沒有同學有問題
好 剛才有同學問一個問題說
要不要用機器去訂 Reward
沒錯
接下來 Inverse Reinforcement Learning
就是要讓機器自己來訂 Reward 啦
那怎麼做呢
我們先看原來的 Reinforcement Learning
是怎麼運作的
原來的 Reinforcement Learning 是
我們有 Reward
有環境
然後呢
用 RL 的 Algorithm
跟環境還有 Reward 互動
然後你就學出一個 Actor
但現在
我們沒有 Reward 了
我們有的只有 Expert 專家的示範
我們現在要做的事情
是一個叫
Inverse Reinforcement Learning 的 Algorithm
它是跟原來的 Reinforcement Learning 是相反的
它要做的事情
並不是根據 Reward 去學習
而是從 Expert 的 Demonstration
還有 Environment
去反推 Reward 應該長什麼樣子
也就是這邊的 Reward Funtion
是學出來的
那學出一個 Reward Funtion 以後
你就可以直接用一般的 Reinforcement Learning
來學你的 Actor
所以在 Inverse Reinforcement Learning 裡面
我們的概念就是
本來不知道 Reward Funtion
從 Expert 的示範
去反推 Reward Funtion 應該長什麼樣子
有了 Reward Funtion 以後
我們就可以再訓練一個 Optimal 的 Actor
去根據這些 Reward Funtion
來進行學習
那講到這邊
也許有人會有的疑惑是
欸 這個 Reward Funtion 是學出來的
它會不會太簡單了呢
我們會不會沒有辦法學出
非常複雜的 Reward Funtion 呢
因為畢竟 Reward Funtion 是學出來的
也許我們只能夠學出比較簡單的
Reward Funtion
但是簡單的 Reward Funtion
並不代表
根據這個 Reward Funtion 學出來的 Actor
一定也會是簡單的
舉例來說
對一個人類而言
也許人類的 Reward
是非常簡單的
也許人類的 Reward Funtion 就只有一條
就是活下來
但是是不是這樣子
就是一個見仁見智的問題啦
那也許人類的 Reward Funtion 只有
活下來這件事
但是光是人類想活下來這件事情
就可以讓人類的行為有千變萬化
所以簡單的 Reward Funtion
並不代表你一定會學出簡單的 Actor
有可能簡單的 Reward Funtion
但學出來的 Actor
仍然是複雜的
那這個是 Inverse Reinforcement Learning
Inverse Reinforcement Learning 的基本概念
是什麼呢
怎麼找出 Reward Funtion 呢
這邊最基本的概念就是
老師的行為
是最棒的
但是我這邊要強調一下所謂最棒
並不代表
你要完全去模仿老師的行為
而是你假設老師的行為
可以取得最高的 Reward
那老師的行為
可以取得最高的 Reward 這個假設
跟完全模仿老師的行為
這兩件事情並不是等價的
也許我們看完這個 Algorithm
你會更清楚我想表達的意思
好
現在呢
我們有一個 Actor
它一開始是什麼都不會
然後呢
在每一個 Iteration 裡面
這個 Actor 會去跟環境進行互動
學習蒐集一些 Actor 自己的 Trajectories
然後接下來呢
我們要定義一個 Reward Funtion
這個 Reward Funtion 怎麼定義呢
這個 Reward Funtion 定義的條件
這個 Reward
Learn 這個 Reward Funtion 的條件是
今天老師的行為
得到的 Reward
必須要高於學生的行為
就老師也有跟環境互動
我們得到一堆老師的 Demonstration
我們得到一堆老師的 Trajectory
當你用你 Learn 出來的 Reward Funtion
去計算老師的 Trajectory 的時候
我發現它有點要沒電了
所以我那個把這個電插一下
好 我把電插上去了
好 我們要訂一個 Reward Funtion
我們要訂一個 Reward Funtion
這個 Reward Funtion
去評估老師的 Trajectory 的時候
要給比較高的分數
去評估 Actor 的 Trajectory 的時候
要給它比較低的分數
然後接下來呢
你再去更新你的 Actor
你要去重新訓練你的 Actor
更新你 Update
更新你的 Actor 的參數
讓它去 Maximize 我們會得到的 Reward
然後接下來呢
就反覆執行這個步驟
你有新的 Actor
它會有新的 Trajectory
你再更新一次 Reward Funtion
讓這個 Reward Funtion 評估
這個老師的分數比較高
評估 Actor 的分數比較低
然後呢
Actor 呢
再想辦法去 Maximize Reward Funtion
然後就反覆這個循環
最終你就會得到一個 Reward Funtion
那這個
就是我們用 Inverse Reinforcement Learning
Learn 出來的 Reward Funtion
好 那如果剛才那個 Algorithm
你沒有看的很懂的話
那這邊是用圖像化的方法
來講一下 Inverse Reinforcement Learning
那它的縮寫呢
是 IRL
好 那現在呢
有 Expert 的 Demonstration
寫成 T^
有 Actor 跟環境的互動
寫成 T
那接下來你要定一個 Reward Funtion
這個 Reward Funtion 呢
會給 T^
也就是 Expert 的 Demonstration 比較高的分數
給 T
也就是你的 Actor 的 Trajectory 比較低的分數
那有了這一個 R 以後
你再去訓練你的 Actor
去 Maximize 這個 Reward Funtion
怎麼訓練 Actor 去 Maximize 這個
剛學出來的 Reward Funtion 呢
這邊你就要透過
Reinforcement Learning 的方法
接下來
你有了新的 Actor
新的 Actor 有新的行為
但這些新的行為
仍然要得到比老師低的分數
你會去更新你的 Reward Funtion
讓老師得到的分數
仍然高過於 Actor 得到的分數
然後就反覆反覆這一個迴圈
反覆反覆這個循環
最終
你就可以把一個 Reward Funtion Learn 出來
那這整個 Framework
你聽起來有沒有覺得有點熟悉呢
我們可以把 Actor
想像成是 GAN
Generative Adversarial Network
裡面的 Generator
把 Reward Funtion
想像成是 GAN 裡面的 Discriminator
我們來很快複習一下 GAN 的 Framework
在 GAN 的 Framework 裡面
你有一個 Generator
它會產生比較差的圖片
然後有一個 Discriminator
它要想辦法給真正的圖片高分
Generator 產生的圖片低分
然後呢
你的這個 Generator
會去想辦法騙過 Discriminator
產生新的圖片
Discriminator 又會 Update 它的參數
想辦法去
評價好的圖片
跟 Generator 產生出來圖片的差別
然後這個 Discriminator
跟這個 Generator
就會反覆地 Update
那這個是 GAN
Inverse Reinforcement Learning 跟 GAN
其實根本就是一樣的東西
只是把 Generator
跟 Discriminator 的名字換掉而已
Actor 產生一些行為
然後你要去訂一個 Reward Funtion
給 Expert 的 Trajectories 高分
給 Actor 的 Trajectories 低分
然後接下來
Actor 想辦法去
在這個 Reward Funtion 得到高分
那有了新的 Actor
有了新的行為
Reward Funtion 又會被 Update
想辦法給 Expert 高分
給 Actor 低分
所以 Reward Funtion
完全可以對應到 Discriminator
Actor 可以對應到 Generator
所以你會發現
GAN 跟 IRL
Inverse Reinforcement Learning
它們有異曲同工之妙
好像是同一個 Framework
用不同的方法
不同的角度來描述
好
那像 IRL 這種方法
常常被用來訓練機械手臂
那過去
在如果你不是用 Reinforcement Learning
來訓練機械手臂的話
可能看起來
是什麼樣子呢
以下又是從 The Big Bang Theory
裡面擷出來的一段
Hey, Is food here?
Woo, What's that?
That, dear lady,
Is the Wolowitz Programmable Hand,
designed for extravehicular repairs
on the International Space Station.
Oh, cool.
Ask me to pass the soy sauce.
Oh, does that come up much on the space station?
Mostly with Asian and Jewish astronauts.
All right, Pass the soy sauce.
Coming up.
So how's work?
Oh, it's not bad.
Kind of hungry.
Yeah, we all are.
Just wait.
You realize, Penny,
that the technology that went into this arm
will one day make unskilled food servers
such as yourself obsolete.
Really?
They're going to make a robot
that spits on your hamburger?
I thought you broke up with her.
Why is she here?
OK, here we go.
Passing the soy sauce.
Put out your hand.
Oh, That's amazing.
I wouldn't say amazing.
At best, it's a modest leap forward
from the basic technology
that gave us Country Bear Jamboree.
好 那這個影片想要告訴大家的事情是說
假設你今天想要用寫程式的方法
來操控一個機械手臂
雖然對人來說
我們要伸自己的手臂來做什麼事情
都是一件很簡單的事情
但是你要把這麼簡單的行為程式化
把它寫成程式
去操控機械手臂的每一個關節
做出某一些指定的動作
往往不是那麼容易
那這個時候
你就可以使用
Inverse Reinforcement Learning 的技術
你就示範給機器看
示範給它一次你要它做的行為
看看它能不能就藉此學會
我們要它做的行為
所以以下
就是用 Inverse Reinforcement Learning
其中的某一個技術達到的結果
我這邊是不是有點卡頓
我跳出來再跳進去好了
好 那我們繼續吧
好 那就播一下這個影片
在教機器擺盤子
先示範給它看
這邊會示範個 20 次
那這個是示範
這個是教機器那個倒東西
然後這個也是示範 20 次
好 那這個影片是想要告訴大家說
未來我們可能可以用 Demonstrate 的方法
來教機器事情
好 那事實上呢
如果你要教機械手臂一些行為
現在還有一個更潮的做法
那這個更潮的做法呢
是你直接給機器一個畫面
然後讓機器呢
做出這個畫面中的行為
那這個部分我們就不細講
我這邊就是列舉了一篇
NIPS 的 Paper
跟一篇 ICML 的 Paper 給大家參考
那它的基本概念是說
就給機器一個畫面
告訴它說
你就去達到這個目標
然後機器就會自己想辦法達到目標
那怎麼訓練機器有這個能力
看到一個畫面
就知道怎麼達到目標呢
這個訓練的過程也非常地有意思
這個訓練過程是
機器會自己創造目標
它自己在心裡呢
想像一些畫面
然後呢
再想辦法去達到這些畫面
這個就好像是說
有人告訴你說
欸 那你要念一個博班
你要拿一個博士學位
那你就會去想辦法呢
拿到這個博士學位
那中間的過程是怎麼樣
不知道
你要自己想辦法去 Figure Out
可是怎麼訓練自己
有拿到博士學位的能力呢
你就會自己給自己先設定一些目標
比如說你先設定說
我要成為一個 YouTuber
然後要做做做
做一些事情
然後成為一個 YouTuber
你就知道說
嗯 我有達成目標的能力
那再設定一些別的
別的各式各樣的目標
然後都想辦法去達成它
就可以培養自己達成目標的能力
那之後有人告訴你說
你現在的目標是拿一個博士學位
那你就會知道要怎麼拿一個博士學位這樣
哦 天
這個這種地方也要業配
我也是有點
我真的是受不了了
好那這個有關 RL 的部分呢
大概就
有關 RL 的部分呢
就講到這邊
我們看一下有沒有同學有問題要問的
好 有同學說那個聲音有點怪怪的
不知道
不知道是不是筆電的問題
我相信現在不是網路的問題
網路應該是順的
對
所以等一下
等一下我把電腦比如說重
就是讓它休息一下
也許就會好一點
好
有同學問說
使用 IRL 這類的方法的話
是不是就沒辦法找到比人類更好的方法
有沒有辦法讓機器青出於藍
欸 我覺得這是一個好問題
使用 IRL 這個做法
我們要注意一下
機器並不是去完全模仿人類的行為
所以機器的 Solution
跟人的 Solution
不一定會是一樣的
所以
所以今天
如果我們要讓機器
它得到的結果比人類更好的話
我覺得有一個可能的方法是
我們先用 IRL
先 Learn 出一個 Reward Funtion
然後在這個 Reward Funtion 上面
再加上額外的限制
舉例來說
我們剛才看到的例子都是
你今天有示範給機器看
然後機器就可以 Learn 出一個 Reward Funtion
知道說
你現在要叫它做的事情就是
擺盤子
那我們現在可以再加上新的 Reward 說
你擺盤子的速度要越快越好
如果你擺盤子的速度也很快的話
那你一樣
你會得到額外的 Reward
那這樣也許就有機會讓機器學一些
原來人類示範的時候
做不到的事情
所以我覺得
IRL 還是有機會做得比人類更好的
