hi I'm Yung from National Taiwan
University I'm going to present our work
speech word and audio and text jointly
learned language model for endtoend
spoken question answer this work is done
by
me and
L most of the question answering system
are based on plan text document we make
the machine read the document and then
input a question to the machine it will
output the
answer however in reality there is much
more information hiden in audio than in
text and it is much more difficult to
search in the audio
counter that is the reason why we need
the spoken question answering
system currently
there are two pipeline stages to solve
the spoken question answering
task the first step is to convert the
audio into text using a speech
recognition
system the second step is to put this
text into the text based question answer
model however in the study conducted in
inter speech
2018 it was found that the per
performance of the same question
answering data set with correct text and
speech recognition transcription are
very different the performance is
decreased by the about 20% if the model
take ASR transcription as input instead
of the correct
text the main reason is that speech
recognition errors greatly affect the QA
model and no matter how how powerful the
QA model is it is not possible to
recover these
errors to avoid this error bottle neck
we want to build an endtoend system that
can perform question answering directly
on audio
signals in the end to end spoken
question answering model we have the
audio document and a short question in
text our model will predict the time
span of the answer directly according to
the audio
signals before starting to design our
end to end question answer model let's
first understand how text based question
answering is currently done most of the
current set of the OD models are based
on bir proposed by Google in
2019 the core concept of bir is to
perform preening before starting to find
tun on QA data
set in preening we first make the model
learn to recover 15% of the words that
are masked out based on the contextual
information after preening the model
will have the basic ability to
understand language and then we find the
prent model on QA data set
so our idea is to follow the concept of
bird to create a speech version of
bird we Preen our model on audio data
and then finded on spoken question
answering data
set the difference is that the input of
our model is audio signal so it can be
trained in an end to endend manner
without the error button
let's talk about how do we fit the audio
signals into the
model because the audio signals are very
noisy it is hard to capture the semantic
information
inside thus here we segment the audio
into audio word segment according to the
force alignment results from an off the
shelf ASR
model after that we CH an auto encoder
to capture useful information in the
audio by minimizing the Reconstruction
loss and we align each audio words with
corresponding embedding from the input
layer of bir by minimizing the L1
distance between them finally we can
present each audio segment with a vector
that are consistent with the embedding
space of bir now now given the audio
signal input we can segment the audio
first and encode the segment with our
audio encoder to get the
representations of each audio
segment and the representations of each
audio segment can be treated as a or
embedding and fit into our speech bre
model for the text input we can just use
the word embedding layer to get the word
Vector of spech
word these two types of vectors are
concatenated and fit into our speech
word
model in ping stage we mixed out 15% of
the input vectors and make the model
learn to recover the corresponding text
of the masked
audio here we use the pent weight of bir
as initialization of our
model after pre training the model will
have a basic ability to understand the
audio data
set then we take the P model find your
dat on the spoken question answering
data set and make the model predict the
start and end position of the correct
H let's illustrate more on how we get
the world
boundaries in training time we have qu
Tru transcriptions of the audio so we
can just use the quru transcriptions to
wrong Force alignment and get the world
boundaries however in testing time we
cannot access to G transcriptions
according to our experimental
setup th We Run The ASR model first to
get the
hypothesis and then use the hypothesis
to run Force
alignment although there are recognition
errors in the hypothesis the world
boundaries founded by false alignment
are usually coding to some real
words because our model only take the
world boundaries information as input
instead of the ASR transcription it will
not be affected too
much we will prove this clan in our
experiment in the experiment we use the
squad data set in the previous work in
2018 spoken Squad is proposed as a
speech version of
squad however in spoken Squad all of the
questions with recognition errors in
this answer spense are dropped because
they are too difficult so spoken Squad
only contains a subset of original squad
questions the other questions here we
call them Square lost all the questions
with ASR errors in this answer
Spence we use both of the subset in our
experiment and these two subsets
contains comparable data
size for Simplicity we call spoken squat
as correct set and square lost as ever
say in the following video
in the main experimental result we
compute the tie level fan score and the
audio overlap score AOS to evaluate the
performance we found that on the easier
correct set the accuracy of the pipeline
Bird model is
higher but on the more difficult ER set
our speech bird can significantly
outperform the pipeline bird and the sum
of these two subset also shows the
advantage of switch
Ro then we show further experimental
results on the correct set which our
speech bir not outperform pip liate
model the first part of the results are
pipeline QA
models here we can either change this QA
models on the ground truth
transcriptions or the ASR
transcriptions then we can test the
model on the ground TR transcriptions or
the ASR transcriptions of the testing
set and we can see the performance Gap
in the different testing set the
performance of spoken data is much lower
than that of the text Data
and the best pipeline method is bir CH
on ASR
transcriptions the pipeline result in
the previous page also use this
method the second part of the results
are end to end
models we CH the models on audio with G
TR alignment and test the models on
audio with ASR alignment
the third part of the results are onmol
models we can either onmol bird with
speech bird or un symol two bird ched
with different
rendes first of all on the correct set
our speech bird can outperform the
previous method other than birth
although speech word has not yet
outperformed birth on the correct set on
model made a great Improvement when we
on Syle birth with speech
birth the Improvement is significantly
larger than that of onon two separately
transferred it means that our model and
text based bir have learned very
complimentary information thus the UN
simple result improved so much and this
result is current say of the r score
unspoken Squad data
set we have also done ablation study and
proved that if we skip the pre trending
stage the performance of speech bir will
drop a
lot in addition we want to know how much
will the better World boundaries affect
the model's
performance th we segment the audio by
force alignment using the ground Tru
transcription which is is not accessible
in our experimental
setup we show that it will bring
improvements about
2% but the improvements is not very huge
so we infer that the segmentation
quality is not the quick critical factor
in our
model finally we grouped the questions
according to the spe recognition will
error rate in the whole testing set we
found that when the error rate is less
than
40% the pipeline model performs
better however our model is able to take
the advantage of its endtoend
architecture when the recognition errors
was greater than
40% this is a brief introduction to our
work further detail can be found in the
paper thanks for listening
