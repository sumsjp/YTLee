臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
那我們上次講了 generation 這件事情
然後，可以用 PixelRNN 來做
這邊 create 了一個 generative 的 task
上次還講了 VAE
我們沒有講太多它的原理
我們來複習一下 VAE 做的事情
Auto-encoder 我想大家都很熟悉了
那 VAE 做的事情是甚麼呢？
VAE 做的事情是說
你一樣有一個 encode，有一個 decoder
你現在 decoder 會 output 兩組 vector
這邊一組是 m1 到 m3
另外一組是 σ1 到 σ3
接下來，你會 generate 一個 normal distribution
你會 generate 一個 vector
這個 vector 是從 normal distribution sample 出來的
接下來，你把 σ 這個 vector 取 exponential
然後，再乘上 random sample 出來的這個 vector
再加上原來 m 這個 vector
得到你的 code, c，然後把那個 code 丟進
decoder 裡面，產生 image，那你希望說
input 跟 output 越接近越好
另外 Auto-encoder 還有另外一項 constraint
那我回去的時候，我發現我上次的投影片呢
寫的是相反的，所以
差了一個負號
所以如果按照原來的寫法，這邊應該是 maximize
那如果是用 minimize 的話，下面就應該再加一個負號
再來的問題就是
為甚麼要用 VAE 這個方法
原來的 Auto-encoder 會有甚麼樣的問題呢？
如果你看文獻上的話
VAE，如果你看它原來的 paper 的話
它有很多很多的式子，你就會看的一頭霧水的
那在講那些式子之前呢
我們先來看 intuitive 的理由
為甚麼要用 VAE？
如果是原來的 Auto-encoder 的話
原來的 Auto-encoder 它做的事情是
我們把每一張 image
變成一個 code，假設我們現在的 code 是一維
就是圖上的這條紅色的線
那你把滿月的這個圖
變成 code 上的一個 value
然後，再從這個 value 做 decode
把它變成原來的圖，那如果是弦月的圖
也是一樣變成 code 上的一個 value
接下來，你再把它從 code 上的一個 value 變回原來的圖
假設我們今天是在滿月和弦月的 code 中間
sample 一個點，你覺得它原來
然後，再把這個點做 decode
變回一張 image，它會變成甚麼樣子呢？
你心裡或許期待著說
它會變成滿月和弦月中間的樣子
但是，這只是你的想像而已
其實，因為我們今天用的 encoder 和 decoder
我們今天用的 encoder 和 decoder，它都是 non-learner
它都是一個 neural network
所以，你其實很難預測說
在這個滿月和弦月中間到底會發生甚麼事情
你可能想像是，滿月和弦月中間的月象
但未必，它可能根本就是另外一個東西
那如果用 VAE 有甚麼好處呢？
如果用 VAE 的好處是
實際上，VAE 在做的事情
就等於我下面說的這件事情
當你把這個滿月的圖變成一個 code 的時候
它會在這 code 上面再加上 noise
它會希望，在加上 noise 以後
這個 code reconstruct 以後，還是一張滿月
也就是說，原來的 Auto encoder 只有這個點
需要被 reconstruct 回滿月的圖
但是對 VAE 來說，你會將上 noise
在這個範圍之內的圖 reconstruct 回來以後
都應該仍然要是滿月的圖
這個弦月的圖也是一樣
弦月的 code 再加一個 noise
reconstruct 回來以後
這個 range 的 code 都要變成弦月的圖
你會發現說，在這個位置
在這個地方，這個 code 的點
它同時希望被 reconstruct 回弦月的圖
同時也希望被 reconstruct 回滿月的圖
那可是你只能夠 reconstruct 回一張圖而已
怎麼辦？
那 VAE training 的時候
你要 minimize 這個 mean square error
所以，最後產生這個位置
所產生的圖
會是一張介於滿月和弦月中間的圖
同時讓它最像滿月，也最像弦月
那你產生的圖會是甚麼樣子呢？
或許就是，界於滿月和弦月之間的月象圖
所以，如果你用 VAE 的話
你從你 code 的 space上面
去 sample 一個 code
在產生 image 的時候
你可能會得到比較好的 image
如果是原來的 Auto-encoder 的話
你 random sample 一個 point
你得到的，可能看起來都不像是一個真實的 image
所以，VAE 就是這樣
這個 m，這個 encoder 的 output, m 代表是原來的 code
那這個 c，代表是加上 noise 以後的 code
decoder 要根據加上 noise 以後的 code
把它 reconstruct 回原來的 image
那這個 σ 跟 e 是甚麼意思呢？
這個 σ，它就代表了
現在這個 noise的 variance
它代表了你的 noise 應該要有多大
因為 variance 是正的
所以，這一邊會取一個 exponential
因為 neural network 的 output
假設你沒有用 activation function 去控制它的話
它的 output 可正可負
假設你這一邊是 linear的 output 的話
那 output 可正可負
所以取一個 exponential 確保它一定是正的
可以被當做是 variance 來看待
那現在當你把這個 σ
乘上這個 e
這個 e 是從一個 normal distribution sample 出來的值
當你把這個 σ 乘上 e 再加到 m 的時候
就等於是你把這個 m 加上了 noise
就等於是你把原來 code 加上 noise
那這個 e 呢
是從 normal distribution sample 出來的
所以，它的 variance 是固定的
但是，乘上不同 σ 以後呢
它的 variance 的大小就有所改變
所以這一個 variance 決定了 noise的大小
而這一個 variance大小
這個 variance 是從 encoder 產生的
也就是說 machine 在 training 的時候
它會自動去 learn 說，這個 variance 應該要有多大
但是，如果就只是這樣子是不夠的
假如你現在的 training 就只考慮說
我現在 input 一張 image
然後，我中間有這個加 noise 的機制
noise 的 variance 是自己 learn 的
然後，decoder 要 reconstruct 回原來的 image
那你要 minimize 這個 reconstruction error
如果你只有做這一件事情的話，是不夠的
你 train 出來的結果呢
是不會，並不會如同你預期的樣子
為甚麼呢？
因為這個 variance 現在自己學的
假設你讓 machine 自己決定說 variance 是多少
那它一定會決定說
variance 是 0 就好了 ，對不對
就讓大家自己決定分數是多少
那每一個人都會是 100 分
所以，這邊這個 variance
如果你只讓 machine 自己決定的話
它就會覺得說，variance 是 0 就好了
那你就等於是原來的 Auto-encoder
因為 variance 是 0 的話，就不會有這個
不同的 image overlap的情形
這樣你 reconstruction error 是最小的
所以，你要在這個 variance 上面呢
去做一些限制
你要強迫它的 variance 不可以太小
怎麼做呢？所以，我們另外再加了這一項
其實就是對 variance 做了一些限制
怎麼說呢，這一項是這樣子
你看它這一邊有 exp(σi) - (1 + σi)
那 exp(σi) 畫在圖上的話
它是藍色的這一條線
(1+σi) 畫在圖上，它是紅色的這一條線
當你把藍色這一條線減紅色這一條線的時候
你得到是綠色的這一條線
綠色這一條線的最低點呢
是落在 σ = 0 的地方
注意一下，σ 之後會再乘 exponential，所以σ = 0
意味這說，它的 variance 是 1
exp = 0，是 1
所以 σ = 0 的時候，loss 最低
意味著說， 你的 variance 等於 1 的時候
loss 最低
所以，machine 就不會說，讓 variance 等於 0，然後
minimizes reconstruction error，它還要考慮說
variance 是不能夠太小
那最後這一項 ( mi )^2
對這個 code 做這個
要 minimize code 的這個 L2-norm
怎麼解釋呢，其實很容易解釋
你就想成是我們現在加了 L2 的 regularization
我們本來常常在 train Auto-encoder 的時候
你就會在你的 code 上面呢
加一些 regularization
讓它結果比較 sparse
比較不會 overfitting
比較不會 learn 出太多 trivial 的 solution
那這個是直觀的理由
如果比較正式解釋的話
要怎麼解釋它呢
以下就是 paper 上，比較常見的說法
假設我們回歸到我們到底要做的事情是什麼
假設你現在要叫 machine 做的事情
是 generate 寶可夢的圖
每一張寶可夢的圖
你都可以想成是高維空間中的一個點
一張 image，假設它是 20*20 的 image
它在高維的空間中
就是一個 20*20，也就是一個 400 維的點
我們這邊寫做 x
雖然在圖上，我們只用一維來描述它
但它其實是一個高維的空間
那我們現在要做的事情
其實就是 estimate 高維空間上面的機率分佈，P(x)
我們要做的事情就是 estimate 這個 P(x)
只要我們能夠 estimate 出這個 P(x) 的樣子
注意，這個 x 其實是一個 vector
假設我們可以 estimate 出 P(x) 的樣子
我們就可以根據這個 P(x)
去 sample 出一張圖
那找出來的圖
就會像是寶可夢的樣子
因為你取 P(x) 的時候，你會從機率高的地方
比較容易被 sample 出來
所以，這個 P(x) 理論上應該是在
有寶可夢的圖的地方
這有寶可夢的圖，如果你今天
這個圖長得像一隻寶可夢的話，它的機率是大的
它的機率是大的
這個是噴火龍家族，他們的機率是大的
這個是水箭龜家族，他們機率是大的
如果是一張怪怪的圖的話
比如說，這一個看起來像是皮卡丘，又有一點不像
這個看起來像是一隻綿羊，又像是一個雲
這一邊呢，機率是低的
這一邊機率是低的
如果我們今天能夠 estimate 出
這一個 probability distribution
那就結束了
那怎麼 estimate 一個 probability 的 distribution 呢？
我們可以用 Gaussian mixture model
我不知道在座有多少人知道 Gaussian mixture model
我好奇問一下
知道 Gaussian mixture model 的同學舉手一下
好手放下，謝謝
很多人都知道 Gaussian mixture model
那太好了
有學過語音課的話，就聽過 Gaussian mixture model
Gaussian mixture model 在做甚麼呢？
我們現在有一個 distribution，它長這個樣子
黑色的、很複雜
我們說這個很複雜的黑色 distribution，它其實是
很多的 Gaussian
我這一邊藍色的代表 Gaussian
有很多的 Gaussian
用不同的 weight 疊合起來的結果
假設你今天 Gaussian 的數目夠多
你就可以產生很複雜的 distribution
所以，雖然黑色很複雜
但它背後其實是有很多 Gaussian 疊合起來的結果
那這個式子怎麼寫它呢？
你會把它寫成這樣
首先，如果你要從 P(x) sample 一個東西的時候
你怎麼做？
你先決定你要從哪一個 Gaussian sample 東西
假設現在有一把 Gaussian
有一把 Gaussian
每一個 Gaussian 背後都有一個 weight
每一個 Gaussian 都有自己的 weight
這一邊有一把 Gaussian，每一個都有它自己的 weight
那接下來呢
你再根據每一個 Gaussian 的 weight
去決定你要從哪一個 Gaussian sample data
然後，再從你選擇的那個 Gaussian 裡面 sample data
如果你選擇 1 這個 Gaussian的話
那你就從這個地方sample data
如果選擇 2 的話，就從這個地方
3 就從這個地方、4 就從這個地方、5 就從這個地方
以此類推
所以，怎麼從 Gaussian mixture model sample 一個 data 呢？
你就這樣做
首先，你有一個 multinomial 的 distribution
你從這一個 multinomial distribution 裡面
決定你要去 sample 哪一個 Gaussian
今天 m 代表第幾個 Gaussian
它是一個 integer
決定好你要從哪一個 m sample Gaussian 以後
你要從哪一個 Gaussian sample data 以後
決定哪一個 Gaussian 以後
每一個 Gaussian 有自己的 mean
μ(上標 m)，有一個自己的 variance, Σ(上標 m)
所以，你有了這個 m 以後
你就可以從，你就可以找到這個
mean 跟 variance
根據這個 mean 跟 variance
你就可以 sample 一個 x 出來
所以，今天整個 P(x) 寫成
summation over 所有的 Gaussian
那一個 Gaussian 的 weight, P(m) 再乘上
有了那一個 Gaussian以後
從那一個 Gaussian sample 出 x 的機率，P(x|m)
那在 Gaussian mixture model 裡面
有種種的問題，比如說你需要決定 mixture 的數目
但是，如果你知道 mixture的數目的話
接下來給你一些 data，x
你要 estimate 這一把 Gaussian
跟它的每一個 Gaussian 的 weight
跟它的 mean 跟 variance
其實是很容易的
你只要用 EM Algorithm 就好了
你不知道這個是甚麼沒有關係
反正這是很容易的事情
現在每一個 x
它都是從某一個 mixture 被 sample 出來的
這件事情其實就很像是
在做 classification 一樣
每一個我們所看到的 x
它都是來自於某一個分類
它都是來自於某一個 class
但是，我們之前有講過說
把 data 做 classification
做 clustering 其實是不夠的
更好的表示方式是用
Distributed 的 representation
也就是說，每一個 x
它並不是屬於某一個 class
某一個 cluster，而是它有一個 vector
來描述它的各個不同面相的 attribute
描述它各個不同面向的特質
所以 ，VAE 其實就是 Gaussian mixture model 的
distributive representation 的版本
怎麼說？首先呢
我們要 sample 一個 z
這個 z 是從一個 normal distribution sample 出來的
z 是一個 vector
它從一個 normal distribution 被 sample 出來
那這個 z 是一個 vector
這個 vector 的每一個 dimension
就代表了某種 attribute
代表你現在要 sample 的那個東西的某種特質
z 的每一個 dimension
就代表了它要 sample 的某種東西的特質
假設 z 它長這樣
假設 z 長這樣
它是一個 Gaussian distribution
現在我們在這個圖上呢
就假設它是一維的
但在實際上 z 可能是一個 10 維的、100 維的 vector
到底有幾維，是由你自己決定
假設現在 z 呢，就是一維的 Gaussian
接下來，你sample 出這個 z 以後
根據 z ，你可以決定 μ 跟 σ
你可以決定 Gaussian 的 mean 跟 variance
剛才在 Gaussian mixture model 裡面
你有 10 個 mixture，你就是 10 個 mean
跟 10 個 variance
但是，今天在這個地方
你的 z 有無窮多個可能
它是 continuous，不是 discrete
所以你的 z，有無窮多的可能
所以，你的 mean 跟 variance
也有無窮多的可能
那怎麼找到這個 mean 跟 variance 呢？
怎麼給一個 z，找 mean 跟 variance 呢？
你這一邊的做法就是
假設 mean 跟 variance 都來自一個 function
都來自一個 function
你把 z 代進產生 mean 的這個 function
它就給你 μ(z)
μ(z)代表說，現在，如果你的
你的 hidden 的東西
你的 attribute 是 z 的時候
你在這個 x 這個 space 上的 mean是多少
同理，σ(z) 代表說你的 variance 是多少
代表說，你現在如果從
這個 latent 的 space 裡面得到 z 的時候
你的 variance 是多少
所以，實際上
所以，實際上這個 P(x) 是怎麼產生的呢？
在 z 這個 space上面
每一個點都有可能被 sample到
只是在中間這邊呢
這個點被 sample 到的機率比較大
在 tail 的地方，點被 sample 到的機率比較小
當你 sample 出一個點以後
你在 z 的 space 上面 sample 出一個 point 以後
那一個 point 會對應到 一個 Gaussian
這一個點對應到這一個 Gaussian
這個點對應到這一個 Gaussian
這一個 點對應到這一個Gaussian，等等
每一個點都對應到一個 Gaussian
至於某一個點對應到什麼樣的 Gaussian
它的 mean 跟 variance是多少
是由某一個 function 所決定的
所以，當你用這一個概念
當你今天你的這個 Gaussian
是從一個 normal distribution 所產生的時候
現在你等於就是有無窮多的 Gaussian
原來 Gaussian mixture model 裡面
最多甚麼 512 的，那個太少
我們現在有無窮多個 Gaussian
那另外一個問題就是
我們怎麼知道每一個 z
應該對應到怎樣的 mean 跟 variance
這個 function 怎麼找呢？
我們知道說 neural network 就是一個 function
所以，你可以說我就是 traing 一個 neural network
這個 neural network input z
然後，它的 output 就是兩個 vector
第一個 vector 代表了 input 是 z 的時候
你 Gaussian 的 mean
這個 σ 代表了 variance
那 variance 時常說，它是一個 matrix
你可以說，你可以把 matrix 拉直當作它的 output
或者是你可以只 output diagonal 的地方
然後，假設 non-diagonal 的地方都是 0
這樣都是可以的
反正，我們有一個 neural network
它可以告訴我們說
在 z 這一個 space 上每一個點
它對應到 x space 的時候呢
你的這一個 distribution，mean 跟 variance 分別是多少
那現在這個 P(x) 的 distribution 會長什麼樣子呢？
這個 P(x) 的 distribution 呢
就會變成是 P(z)的機率
跟我們知道 z 的時候，x 的機率
再對所有可能的 z 做積分
這邊不能夠是相加
不能夠是 summation，必須要是積分
因為這個 z 是 continuous 的
那有人可能會有一個困惑就是
為甚麼這邊一定是這個 Gaussian 呢
為什麼這邊一定是 Gaussian 呢？
你可以不是 Gaussian 這個樣子
它可以是一種花的樣子
在文獻上確實有人會把它弄成一朵花的樣子
它可以是任何的東西
這個是你自己決定的
當然這個 Gaussian，說起來是合理的
你就假設說，每一個 attribute 它的分佈就是 Gaussian
比較極端的 case 總是比較少的嘛
比較沒有特色的東西總是比較多的嘛
然後，attribute 跟 attribute之間是 independent 的
這樣子的假設其實也是合理的
不過，這個形狀是你自己假設的
你可以假設是任何的形狀
你可以假設任何形狀
那現在這個
但是，你不用擔心說
你如果假設 Gaussian 會不會對 P(x) 帶來很大的限制
會不會說，如果假設 z 是 Gaussian distribution 的話
有一些 P(x)，你就沒有辦法描述
其實，你不用太擔心這個問題
你不要忘了這個 NN 是非常 powerful 的
NN 可以 represent 任何 function
只要 neuron 夠多，NN 可以 represent 任何 function
所以，今天從 z 到 x 中間的 mapping 可以是很複雜
所以，就算你的 z 是一個 normal distribution
最後這個 P(x) 呢
它也可以是一個非常複雜的 distribution
再來呢，所以我們現在的式子是這樣子的
我們知道 P(x) 可以寫成對 z 的積分
然後乘上P(z)，還有乘上 P(x|z)
P(z) 是一個 normal distribution
這個 x given z 呢
我們先知道 z 是什麼，然後我們就可以決定 x
它是從什麼樣子的 mean 跟 variance 的 Gaussian裡面
被 sample 出來的
但是這一個 function 有 z
它有什麼樣的 mean 跟 variance
它們中間的關係是不知道的
是等著要被找出來的
但是，問題是要怎麼找呢？
它的 criterion 就是要 maximize 我們的 likelihood
我們現在手上已經有一筆 data x
那你希望找到一組
找到一個 μ 的 function
找到一個 σ 的 function
它可以讓這個
你現在已經觀察到的 data
你現在手上已經有的 image
它的每一個 x 代表了一個 image
你現在手上已經有的 image
它的 P(x) 取 log 以後
它的值相加以後是被 maximize 的
這個就是 maximize 我們已經看到 image 的 likelihood
這邊只是複習一下這個 z 怎麼產生這個 μ 跟 σ 呢
它是透過了一個 NN
所以，我們要做的事情就是，調這個 NN 裡面的參數
調這個 NN 裡面每一個 neuron 的 weight 跟 bias
使得這個 likelihood 可以被 maximize
但是在這邊，等一下會引入另外一個 distribution
它叫做 q(z|x)
它跟這個 NN 是相反的
它是 given z，決定這個 x 的 mean 跟 variance
這邊是 given x，決定在 z 這個 space 上面的
mean 跟 variance
也就是說，我們有另外一個 NN
這邊寫成 NN'，你 input x 以後
它會告訴你說，對應的 z 的 mean
跟對應的 z 的 variance
你給它 x 以後
它會決定這個 z 要從什樣的 mean 跟甚麼樣的 variance
被 sample 出來
上面這個 NN，其實就是 VAE 裡面的 decoder
下面這個 NN，其實就是 VAE 裡面的 encoder
那我們現在先不要管 NN 這一件事情
我們現在就先只看 這個式子就好
P(x|z) 我們就先不要在意它是不是從 NN 產生的
反正這個就是一個機率，我們要去把它找出來
怎麼找呢？
這個 log P(x)
它可以寫成積分
over z 的積分，然後 q(z|x) * logP(x) dz這樣
我們想說為什麼是這樣呢？
因為 q(z|x)，它是一個 distribution
這個式子對任何 distribution 都成立
我們假設 q(z|x) 是一個從路邊撿來的 distribution
它可以是任何一個 distribution
那任何一個 distribution，你都可以寫成
寫成這個樣子
對不對？因為這個積分跟 P(x) 是無關的
所以，可以把 P(x)這一項提出 來
然後，積分的部分就會變成 1
所以，左式就等於右式
這個沒有什麼好講的，這式子什麼都沒有做
再來呢，也是一個其實什麼都沒有做的式子
這個 P(x) 可以寫成 P(z,x) / P(z|x)
那你把 P(z|x) 展開 一下就會發現說
這一項等於這一項，這也沒什麼好講的
那接下來又是一個甚麼都沒有做的式子
就是，本來我們把 P(z, x) / q(z|x)
然後，再把 q(z|x) / P(z|x)
左式也等於右式
因為這個 q 其實是可以消掉的
這個小學生應該就知道
這個式子也等於是什麼事都沒有做這樣子
接下來，這個東西被放在 log 裡面
我們知道 log 相乘等於拆開後相加
所以，log 這一項乘這一項
等於 log 這一項加 log 這一項
那接下來呢，觀察一下這兩項到底代表什麼事情
右邊這一項，它代表了一個 KL divergence
這個 P(z|x) 是一個 distribution
q(z|x) 是另外一個 distribution
現在 x 是給定的，所以你有兩個 distribution
當有兩個 distribution 的時候
你可以算一個東西，叫做 KL divergence
KL divergence 代表的是這兩個 distribution 相近的程度
它們兩個相近的程度
如果 KL divergence 它越大
代表這兩個 distribution 越不像
這兩個 distribution 一模一樣的時候， KL divergence 會是 0
所以，KL divergence 它是一個距離的概念
它衡量了兩個 distribution 之間的距離
這一項就是 KL divergence 的式子
這一項是一個距離
所以，它一定是大於等於 0 的
最小也是 0 而已
至於這個為什麼 KL divergence的這個
反正你就記起來就是了
那因為這一項一定是大於等於 0 的
所以，這一項會是
L 的 lower bound
L 一定會大於等於這一項
這一項你可以再拆一下，P(z, x) = P(x|z) * P(z)
所以，L 一定會大於這一項
那這一項就是一個 lower bound
我們叫它 L(下標 b)
那現在我們知道事情是這樣
這個 log Probability，就是我們要 maximize 的對象
它是由這兩項加起來的結果
Lb 它長成這個樣子
它長成這個樣子
在這個式子裡面
P(z)是 normal distribution，是已知的
我們不知道的是 P(x|z) 跟 q(z|x)
那我們本來要做的事情
我們本來要做的事情是要找 P(x|z)
讓這個 P，讓這個 likelihood 越大越好
現在我們要做的事情
變成找 P(x|z) 和 q(z|x)
讓 Lb 越大越好
我們本來只要找這一項，本來只要找這一項
現在順便也要找這一項
把這兩項合起來
我們希望同時找這兩項
然後去 maximize 這個 Lb
突然多找一項是要做什麼呢？
如果我們現在只找這一項的話
如果假設我們現在只找這一項
然後去 maximize Lb 的話
你如果 maximize 這一項
你如果調整這一項
你如果找這一項 P(x|z)
讓 Lb 被 maximize 的話
那因為你要找的這一個 log
你要找的這個 likelihood，它是 Lb 的 upper bound
所以，你增加 Lb 的時候
你有可能會增加你的 likelihood
但是，你不知道你的這個 likelihood
跟你的 lower bound 之間到底有什麼樣的距離
你想像你希望能做到的事情是
當你的 lower bound 上升的時候
你的 likelihood 是會比 lower bound 高， 然後你的 likelihood 也跟著上升
但是，你有可能會遇到一個比較糟糕的狀況是
你的 lower bound 上升的時候
likelihood 反而下降
雖然，它還是 lower bound，它還是比 lower bound 大
但是，它有可能下降 因為根本不知道它們之間的差距是多少
所以，引入 q 這一項呢
其實可以解決剛才說的那一個問題
為什麼呢？因為你看這個是 likelihood
likelihood = Lb + KL divergence
如果你今天去這個調這個 q(z|x)，調 q 這一項
去 maximize Lb 的話，會發生什麼事呢？
你會發現說，首先 q 這一項
跟 log P(x) 是一點關係都沒有的
對不對？log P(x) 只跟 P(x|z) 有關
這個 q 代什麼東西，這個值都是不變的
所以，這個值都是不變的
藍色這一條長度都是一樣的
但是，我們現在卻去 maximize Lb
maximize Lb 代表說你 minimize 了這個 KL divergence
也就是說你會讓
你的 lower bound 跟你的這個 likelihood
越來越接近
如果你 maximize q 這一項的話
所以，今天假如你固定住這個
假如你固定住這個 P，假如你固定住 P(x|z) 這一項
然後一直去調 q(z|x) 這一項的話
你會讓這個 Lb 一直上升，一直上升，一直上升
最後這一個 KL divergence 會完全不見
假如你最後可以找到一個 q
它跟這個 p(z|x) 正好完全 distribution 一模一樣的話
你就會發現說你的 likelihood 就會跟
lower bound 完全停在一起
它們就完全是一樣大
這個時候呢，如果你再把 lower bound 上升的話
因為你的 likelihood 一定要比 lower bond 大
所以這個時候你的 likelihood 呢
你就可以確定它一定會上升
所以，這個就是引入 q 這一項它有趣的地方
今天我會得到一個副產物
當你在 maximize q 這一項的時候
你會讓這個 KL divergence 越來越小
意謂這說，你就是讓這個 q 跟 P(z|x)
注意一下，這兩項是不一樣的
這個方向是不一樣的
你會讓這個 q(z|x) 跟 P(z|x) 越來越接近
所以我們接下要做的事情呢
就是找這一個跟這一個
然後可以讓 Lb 越大越好
讓 Lb 越大越好
就等同於我們可以讓 likelihood 越來越大
而且你順便會找到
這個 q 可以去 approximation of p(z|x)
那這一項 Lb 它長什麼樣子呢
這一項 Lb 我們剛才講過它就是長這個樣子
然後 log 裡面相乘，可以把它拆開
可以把它拆開
我們把 P (z) 跟 q(z|x) 放在一邊
把這一項放在另外一邊
那如果你觀察一下的話
會發現 P(z) 是一個 distribution
q(z|x) 也是一個 distribution
所以，這一項是一個 KL divergence
這一項是 P(z) 跟 q(z|x) 的 KL divergence
那如果複習一下，這個 q 是什麼呢
q 是一個 neural network
q是一個 neural network
當你給 x 的時候，它會告訴你說
q(z|x) 它是從什麼樣的
mean 跟 variance 的 Gaussian 裡面 sample 出來的
所以，我們現在如果你要
minimize 這個 P(z) 跟 q(z|x) 的 KL divergence 的話呢
你就是去調這個 output、這個 output
你去調你的這個 q 對應的那一個 neural network
你去調你的那個 q 對應的那一個 neural network
讓它產生的 distribution 可以跟這個
一個 normal distribution 越接近越好
這一件事情的這個推導呢
我們就把他放在，你就參照 VAE 原始的 paper
那 minimize 這一項
其實就是我們剛才說的這一項
剛才說的在 reconstruction error 外
另外再加的那一個，看起來像是 regularization 的式子
它要做的事情就是 minimize 這個 KL divergence
它要做的事情就是希望說
這一個 q(z|x) 的 output
跟 normal distribution 是接近的
那我們還有另外一項
另外一項是這樣子
另外一項是要這個積分
over q(z|x) * log[P(x|z)] 對 z 做積分
這一項的意思就是
你可以想像，我們有一個 log P(x|z)
然後，它用 q(z|x) 來做這個 weighted sum
所以，你可以把它寫成
[log P(x|z)] 根據 q(z|x) 的這個期望值
根據它的期望值
所以這一邊這個式子的意思呢
這一邊這個式子的意思就好像是說
我們從 q(z|x) 去 sample data
給我們一個 x 的時候
我們去計算，我們去根據這個 q(z|x)，這個機率分佈
去 sample 一個 data
然後，要讓 log P(x|z) 的機率越大越好
那這一件事情其實就 Auto-encoder 在做的事情
什麼意思呢？
怎麼從 q(z|x) 去 sample 一個 data 呢？
你就把 x 丟到 neural network 裡面去
它產生一個 mean 跟一個 variance
根據這個 mean 跟 variance
你就可以 sample 出一個 z
接下來，我們要做的事情
你已經做這一項了
這一邊就是這一項
你已經根據現在的 x sample 出 一個 z
接下來，你要 maximize 這一個 z
產生這個 x 的機率
那這個 z 產生這個 x 的 機率是甚麼呢
這個 z 產生這個 x 的機率
是把這個 z 丟到另外一個 neural network 裡面去
它產生一個 mean 跟 variance
要怎麼讓這個機率越大越好呢？
要怎麼讓這個 NN output
所代表 distribution 產生 x 的 機率越大越好呢？
假設我們無視 variance 這一件事情的話
後來在一般實作裡面
你可能不會把 variance 這一件事情考慮進去
你只考慮 mean 這一項的話
那你要做的事情就是
讓這個 mean 呢
讓你的這個 mean 跟你的 x 越接近越好
你現在是一個 normal
你現在是一個 Gaussian distribution
那 Gaussian distribution 在 mean 的地方機率是最高的
所以，如果你讓這個 NN
output 的這個 mean 正好等於你現在這個 data x 的話
這一項 log P(x|z) 它的值是最大的
所以，現在這整個 case 就變成說
input 一個 x，然後，產生兩個 vector
然後 sample 一下產生一個 z，再根據這個 z
你要產生另外一個 vector
這個 vector 要跟原來的 x 越接近越好
這件事情其實就是
就是 Auto-encoder 在做的事情
你要讓你的 input 跟 output 越接近越好
它就是 Auto-encoder 在做的事情
所以這兩項合起來
就是剛才我們前面看到的 VAE 的 loss function
如果你聽不懂的話也沒有關係
前面有提供了比較 intuitive 的想法
那其實 VAE 有另外一個是叫做 conditional 的 VAE
conditional VAE 這邊我們就簡單講一下概念就好
conditional VAE 它可以做的事情是說
比如說，如果你現在讓 VAE 可以產生手寫的數字
讓 VAE 可以產生手寫的數字
它就是看一個，給它一個 digit
然後，它把這個 digit 的特性抽出來
它抽出它的特性
比如說，它的筆劃的粗細等等
然後，接下來呢
你在丟進 encoder 的時候
你一方面給它
有關這一個數字的特性的 distribution
另外一方面告訴 decoder 說
它是什麼數字
那你就可以 generate 一大排
你就可以根據這一個 digit
generate 跟它 style 很相近的 digit
這個應該是在 MNIST 上面的結果
我的 reference 在下面，這是在 MNIST 上面的結果
這是在另外一個數字的 corpus 上面的結果
你會發現說
conditional VAE 確實可以根據某一個 digit
畫出其他的這個 style 相近的數字
這一邊是一些 reference 給大家參考
那 VAE 其實有一個很嚴重的問題
就是因為它有這問題，所以
之後又 propose 了 GAN
那 VAE 有什麼樣的問題呢？
VAE 其實它從來沒有去真的學怎麼產生一張
看起來像真的 image，對不對？
因為它所學到的事情是
它想要產生一張 image
跟我們在 data base 裡面某張 image 越接近越好
但是，它不知道的事情是
我們在 evaluate 它產生的 image
跟 data base 裡面的 image 的相似度的時候
我們是用，比如說，mean square error 等等
來 evaluate 兩張 image 中間的相似度
今天呢，假設我們這個
這個 decoder 的 output 跟真的 image 之間
有一個 pixel 的差距
它們有某一個 pixel 是不一樣的
但是，這個不一樣的 pixel，它落在不一樣的位置
其實是會得到非常不一樣的結果
假設這個不一樣的 pixel
它是落在這個地方
它落在這個地方
它只是讓 7 的筆劃比較長一點
跟它落在另外一個地方
它落在這個地方
對人來說 ，你一眼就可以看出說
這個是 machine generate，是怪怪的 digit
這個搞不好是真的
因為你根本看不出來跟原來的 7 有什麼差異
它只是稍微長一點，看起來還是很正常
但是，對 VAE 來說，都是一個 pixel 的差異
對它來說，這兩張 image 是一樣的好或一樣的不好
所以，VAE 它學的事情
只是怎麼產生一張 image 跟 data base 裡面的 image 一模一樣
它從來沒有想過說要
真的產生一張可以假亂真的 image
所以，如果你用 VAE 來做 training 的時候
其實你產生出來的 image
VAE 所產生出來的 image
往往都是 data base 裡面的 image
的 linear combination 而已
因為它從來沒有學過要產生新的 image
它唯一做的事情只有模仿而已
它唯一做的事情只有
希望它產生的 image 跟 data base 的某張 image 越像越好
它只是模仿而已
或者最多就是把原來 data base 裡面的image 做 linear combination
它做一些 combination，它沒辦法產生一些新的 image
所以，這樣感覺沒有非常的 intelligent
所以，接下來就有人 propose
有另外一個方法叫做 Generative Adversarial Network
Adversarial 是對抗的意思
它縮寫是 GAN
你會發現它是很新的 paper
它最早出現的時候 是 2014 年的 1 2月
所以，大概是兩年前的 paper
以下呢，我們引用了 Yann LeCun 對 GAN 的 comment
就是有人在 Quora 上面
問了說這個
Unsupervised learning 的 approach 哪一個是最有 potential 的
然後，Yann LeCun 他親自來回答，他說呢
Adversarial Training is the coolest thing since sliced bread.
since sliced bread，大家知道是什麼意思嗎？
我 google了一下，這是個片語
如果翻譯成中文的話，是有史以來的意思
since sliced bread 是什麼意思呢？
sliced bread 是切片麵包的意思
那這個片語的典故，好像是說
在過去麵包店是不幫你切麵麵包的
吐司麵包烤完之後，他是不幫你切的
所以你買回去之後，要自己切很麻煩
後來就人發明說，應該先切了以後再賣
然後，大家都很高興這個樣子
所以，since sliced bread
它的英文片語就是有史以來的意思
它說這是有史以來最強的、最酷的方法
他這邊還講了一些別的，他說
What's missing at the moment is a good understanding of it.
so we can make it work reliably.
It's very finicky.
Sort of like CovNet were in the 1990s,
when I had the reputation of being the only person who could make them work(which wasn't true).
這其實是 GAN 非常難 train
感覺好像只有 Ian 跟 Goodfellow
才 propose 他們可以做得起來
其他人做起來，你可以 google 一下那個 GAN 的 code
很多都在 MNIST 上面
他們產生的 digit，都不是很好看
我們用 VAE 隨便做都可以打爆這些東西
所以產生的 image 很怪
但是，你如果看 paper 的話
它的 performance 是滿好的
所以，它裡面還有很多不為人知的技巧
像過去大家相信說只有 Yann LeCun 可以 train 起來 CNN
不過其實不是這樣子
那其實我很無聊，我又收集了，找到另外一則這樣
就是有人問說
有沒有什麼最近的 breakthroughs 在 deep learning 裡面
然後，Yann LeCun 又來回答了，他說
The most important one, in my opinion, is adversarial training.
also called GAN.
This is an idea proposed by Ian Goodfellow.
他說這個是 the most interest idea
in the last ten years in ML
所以，你就來看這個十年來最有趣的想法到底是怎麼樣
這個 GAN 的概念
有點像似擬態的演化
比如說，這是一個枯葉蝶
這個是一個枯葉蝶
他長得就跟枯葉一模一樣
枯葉蝶是怎麼變的跟枯葉一模一樣呢？
怎麼變成這麼像的呢？
也許一開始他長的是這個樣子
然後呢，但是他有天敵
類似麻雀的天敵
比如像波波這樣子的天敵
天敵會吃這個蝴蝶
天敵辨識是不是蝴蝶的方式，就是他知道蝴蝶不是棕色
他就吃不是棕色的東西
所以蝴蝶就演化，他就變成是棕色的
但是，他的天敵也會跟著演化
波波就會變成比比鳥這樣
然後這個比比鳥知道說，蝴蝶是沒有葉脈的
所以，他會吃沒有葉脈的東西
他會 ignore 有葉脈的東西
所以，蝴蝶又再演化
就會變成枯葉蝶，他就產生葉脈
但是，他的天敵也會再演化
這個好像是神獸
這個好像不是波波演化來的，不過沒有關係
然後，他的天敵也還會再演化
所以，這兩個東西呢
天敵和枯葉蝶，他們就會共同的演化
所以，枯葉蝶就會長得越來越樣枯葉
直到最後沒有辦法分辨為止
所以這跟 GAN 的概念，是非常類似的
GAN 的概念是這個樣子
首先，有一個第一代的 Generator
第一代的 Generator 它很廢，它可能根本就是 random 的
它會 generate 一大堆奇怪的東西
看起來不像是真正地 image 的東西
假如我們現在叫它 Generate 4 個 digit
那接下來有一個的第一代 Discriminator
他就是那個天敵
Discriminator 做的事情是
他會根據 real 的 image
跟 Generator 所產生的 image
去調整它裡面的參數
去評斷說，一張 image 是真正的 image
還是 Generator 所產生的 image
接下來呢，這個 Generator 根據這個 Discriminator
等一下會講說 Generator
怎麼根據 Discriminator去演化
Generator 根據 Discriminator
他又去調整了他的參數
所以，第二代的 Generator
他產生的參數，他產生的 digit 就可能就更像真的
接下來，Discriminator 會再根據第二代的Generator
產生的 digit 跟真正的 digit
去 update 他的參數
接下來，有了第二代的 Discriminator
就會再產生第三代的 Generator
第三代 Generator 產生的數字又更像真正的這個數字
就是第三代 Generator 他產生的這些數字
可以騙過第二代的 Discriminator
第二代產生的這些數字，可以騙過第一代的 Generator
但是，這個第一代的 Discriminator
他產生的數字，可以騙過第二代的 Discriminator
但是，Discriminator 會再演化
可能又可以再分辨第三代 Generator 產生的數字
跟真正的數字之間的差距
你要注意一個地方就是，這個 Generator 啊
他從來沒有看過真正的 image長什麼樣子
Discriminator 有看過真正的 image 長什麼樣子
它會比較真正的 image 跟 Generator 的 output 的不同
但是，Generator 從來沒有看過真正的 image
他做的事情，只是想去騙過 Discriminator
所以，因為 generator 從來沒有看過真正 image
所以，Generator 他可以產生出來的那一些 image
是 data base 裡面從來都沒有見過的
所以，這比較像是，我們想要 machine 做的事情
我們現在看 Discriminator 是怎麼 train 的
這一邊是比較直覺的
這個 Discriminator 他就是一個 neural network
他的 input 就是一張 image
他的 output 就是一個 number
它的 output 就是一個 scalar
你可能通過 sigmoid function，讓他的值介於 0 到 1 之間
1 就代表說 input 這張 image 是真正的 image
假如你是要做手寫數字辨識的話
那 input image 就是真正的人手寫的數字
0 代表是假的，是 Generator 所產生的
那 Generator 是什麼呢？
Generator 在這一邊，其實
他的那個架構就跟 VAE 的 decoder 是一模一樣的
他也是一個 neural network
他的 input 就是從一個 distribution
他可以是某某 distribution 或是任何其他的 distribution
從某一個 distribution sample 出來的一個 vector
你把這個 sample 出來的 vector
丟到 Generator 裡面
他就會產生一個數字
產生一個 image
你給他不同的 vector
他就產生不同樣子的 image
那先用 Generator 產生一堆假的 image
然後，我們有真正的 image
Discriminator 就是把這一些 Generator
所產生的 image
都 label 為 0，也都 label 為 fake
然後，把這個真正的 image
都 label 為 1
也就是都 label 為 true
接下來，就只是一個 binary classification 的 problem
大家都很熟
你就可以 learn 一個 Discriminator
接下來，怎麼 learn 這個 Generator 呢？
Generator 的 learn 法是這個樣子
現在已經有了第一代的 Discriminator
怎麼根據第一代的 Discriminator
把第一代的 Generator 再 update 呢
首先，如果我們隨便給，輸入一個 vector
他會產生一張隨便的 image
那這一個 image 可能沒有辦法騙過這個 Discriminator
你把 Generator 產生的 image 丟到 Discriminator 裡面
他可能說，這有 87% 像這樣子
然後，接下來要做的事情是甚麼呢？
接下來，我們要做的事情是調這個 Generator 的參數
調這個 Generator 的參數
讓 Discriminator 會認為說 Generator
generate 出來的 image 是真的，也就是說
要讓 Generator generate 出來的 image
丟到 Discriminator 以後，Discriminator 的 output
必須要越接近越好
所以，你希望 Generator generate 是長這樣子的 image
他可以騙過 Discriminator
Discriminator output 是 1.0
覺得他是一個真正的 image
這件事情怎麼做呢
其實，因為你知道這個 Generator 是一個 neural network
那 Discriminator 也是一個 neural network
你把這個 Generator 的 output
丟到這個當作 Discriminator 的 input
然後，再讓他產生一個 scalar
這一件事情，其實就好像是
你有一個很大很大的 neural network
他這邊有很多層
他這一邊也有很多層
然後，你丟一個 random 的 vector
他 output 就是一個 scalar
所以，一個 Generator 加一個 Discriminator，他合起來
就是一個很大的 network
他既然乘起來是一個很大的 network
那你要讓這個 network
再丟進一個 random vector
他 output 1 是很容易的，你就做 Gradient Descent 就好
你就用 Gradient Descent 調參數
希望丟進這一個 vector 的時候
他的 output 是要接近 1 的
但是，你這邊要注意的事情是
你在調這個參數的時候
你在調這個 network 參數的時候
你在做 Backpropagation 的時候
你只能夠調整這個 Generator 的參數
你只能算 generator 的參數對 output 的 gradient
然後去 update Generator 的參數
你必須要 fix 住 Discriminator 的參數
如果你今天不 fix 住 Discriminator 的參數， 會發生什麼事情呢？
你會發生，對 Discriminator 來說
要讓他 output 1 很簡單阿
就他最後output 的時候，bias 設 1
然後其他都設 2，weight 都設 0，它 output 就 1 了
所以，Discriminator，你要讓這整個 network
input 一個 random 的 vector，output 是1 的時候
你要把 Discriminator 這個參數鎖住
Discriminator 參數必須要是 fix 住的
然後，input 一個 Generator，只調 generator 的參數
這樣 generator 產生出來的 image
才會像是，才是一個可以騙過 Discriminator 的 image
這邊有一個來自 GAN 原始 paper 的 Toy example
我們來說明一下，這個 Toy example 是什麼意思
這個 Toy example 是這樣子
他說，現在我們的這個 z space
也就是這個 decoder 的 input
我們知道 decoder 的 input 就是一個 z
就是一個 hidden 的 vector
hidden 的這個 vector
這個 z 他是一個 one dimensional 的東西
那他丟到 Generator 裡面
他會產生另外一個 one dimension 的東西
這個 z 可以從任何的 distribution 裡面 sample 出來
這邊在這個例子裡面
他顯然是從一個 uniform 的 distribution 裡面
sample 出來的
然後，你把這一個 z 通過 neural network 以後
每一個不同的 z，他會給你不同的 x
這個 x 的分布，就是綠色的這個分布
綠色這個分布，現在要做的事情是
希望這個 Generator 的 output 可以越像 real data 越好
他這一邊的 real data 就是黑色的這個點
假設有一組 real data 就是黑色的這個點
你要找的這個 distribution 是黑色的這個點
那你希望你的 Generator 的 output
也就是這個綠色 distribution
可以跟黑色的這個點，越接近越好
如果按照 GAN 的概念的話
你就是把這個 Generator 的 output x
跟這個 real 的 data
這些黑色的點，丟到 Discriminator 裡面
然後，讓 Discriminator 去判斷說
現在這個 value，其實現在這個 x
real data 都只是一個 scalar 而已
現在這個 scalar，他是來自真正的 data 的機率
跟來自於 Generator 的 output 的機率
如果他是真正的 data 的話就是 1
反之就是 0
Discriminator 的 output，就是綠色的 curve
那假設現在，Generator 他還很弱
所以，他產生出來的 distribution
是這個綠色的 distribution
那這個 Discriminator 他根據 real data
跟這個 Generator distribution 他的樣子呢
你給他這個 x 的值，他的 output
可能就會像是這一條藍色的線
這一條藍色的線告訴我們說
Discriminator 認為說，如果是在這一帶的點
他比較有可能是假的
他的這個值是比較低的
如果是落在這一帶的點
他比較有可能是從 Generator 產生的
落在這一帶的點
他比較有可能是 real data
接下來，Generator 就根據
Discriminator 的結果去調整他的參數
Generator 要做的事情是騙過 Discriminator
既然 Discriminator 認為
這個地方比較有可能是 real data
Generator 就把他的 output 往左邊移
他就把他的 output 往左邊移
那你說有沒有可能會移太多
比如說，通通偏到左邊去，是有可能的
所以 GAN 很難 train 這樣
這個要小心的調參數
小心的調參數，讓他不要移太多
這綠色的 distribution 就可以稍微偏一點
比較接近真正 real 的黑色的點的 distribution
所以，Generator 會騙過他
他就產生新的 distribution
然後，接下來 Discriminator
會再 update 綠色的這一條線
這一個 process 就不斷反覆地去進行
直到最後呢
Generator 產生的 output 跟 real data 一模一樣
那 Discriminator 會沒有任何辦法
分辨真正的 data
你有問題嗎？
其實這個就是現在 train GAN 的時候
所遇到最大的問題
你不知道 Discriminator 是不是對的
因為你說 Discriminator 現在得到一個很好的結果
那可能是 Generator 太廢
有時候 Discriminator 得到一個很差的結果
比如說，他認為說每一個地方
每一個地方他都無法分辨說
是 real value 還是 fake value
這個時候並不代表說 Generator generate 的很像
有可能只是 Discriminator 太弱了
所以，這是一個現在還沒有好的 solution 的難題
所以，真正在 train GAN 的時候，你會怎麼做呢？
你會一直坐在電腦旁邊，看他產生 image 這樣，你懂嗎 ?
因為你從 Discriminator 跟 Generator 的 loss
你看不出來他 generate 的 image 有沒有比較好
所以，變成說你 Generator 每 update 一次參數
Discriminator 每 update 一次參數
你就去看看他
你就拿 generated 的 image 看看有沒有比較好
如果變差以後
方向走錯了，再重新調一下參數這樣子
所以，這個非常非常的困難
非常非常的困難
我們這一邊其實有人在線上放了一個 demo
我們來看一下這個 demo
非常 realistic 的 image
這個是 OpenAI 產生的 image
如果我們問你說
你覺得左邊是 real image
還是右邊是 real image
你覺得左邊是電腦產生的 image 的同學舉手一下
有人，請放下
覺得右邊是電腦產生的 image 的同學舉手一下
好，手放下
其實他還是沒有辦法騙過人
你看這邊還有很多怪怪的的東西就是了
很多東西很像
這個馬還蠻像
這個有飛魚，有大嘴巴的貓
有很多怪怪的東西
所以，他其實沒有辦法騙過人
我覺得如果放單一一張，光看這個馬
他可能可以騙過人
OpenAI 他們有做那個實驗
好像有 21% 的 image
就有 21% machine generate 的 image 會被誤判成 real
所以，他其實是可以騙過部分的人
另外，這一邊又有一個很驚人的結果
在文獻上非常驚人的結果
就是說先拿很多房間的照片
讓 machine 去 train GAN
他可以 generate 房間的照片
那我們說，那個 Generator 就是你 input 一個 vector 給他
他就會 output 一張 image 給你
那你現在可以在那個
input 的 space 上調你的 vector，去產生不同的 output
所以，他說他先 random 找幾個 vector
random 找 5 個 vector
產生 5 張房間的圖
接著，再從這一個點移動你的這個 vector 到這個點這樣
所以，就發現說你的 image 逐漸地變化
逐漸的變化，然後跑到這個點
然後再逐漸的變化，再跑到這個點
你會發現一些有趣的地方，比如說，這邊有一個窗戶
它慢慢的就變成一個類似電視的東西
或是這邊有一個電視
它慢慢的就變成了窗戶這樣子
我覺得最驚人結果
是有人，有日本人他用 GAN
很神奇的，很神奇的東西
就傳說中，你只要能夠
一旦你能夠成功使用他
他就可以召喚不可思議的力量
但是，大部分的時候，你都沒有辦法成功的召喚它
它有點像是神之卡的感覺這樣
你只要能夠操控那個神，就可以獲得不可思議的力量
他大部分的時候你都無法操控他
昨天晚上我想說，我可不可以自己
generate 一些寶可夢
弄到 5 點我搞不起來，所以後來我想我還是去睡好了
就很麻煩
因為，它最大的問題就是你沒有一個很明確的 signal
它可以告訴你說，現在的 Generator
到底做的怎麼樣
沒有一個很明確的 signal 可以告訴你這件事
在一個 stander NN 的 training 裡面
你就看那一個 loss，loss 越來越小
代表說現在 training 越來越好
但是，在 GAN 裡面，你其實要做的事情是
keep 你的 Generator 跟 Discriminator， 他們是 well match 的
他們必需要不斷屬於一種競爭的狀態
他們必須要不斷處於可以
他們要像塔史亮跟進藤光一樣
不斷處於這種勢均力敵的狀態
他們必須要成為對手
那個第三堂課的時候
會請作業一、二、三做得特別好的同學來分享一下
他是怎麼做的
作業三就有人用 GAN
所以，代表是有人有做起來
那這很麻煩，因為在 GAN 裡面
你要讓 Discriminator 跟 Generator
他們一直維持一種勢均力敵的狀態
所以，你必須要用不可思議的平衡感
來調整這兩個 Discriminator 跟 Generator 的參數
讓他們一直處於勢均力敵的狀態
今天這個其實很像是在做 Alpha Go 一樣
你有兩個 agent
然後，你要讓他們一直是處於一樣強的狀態
當今天你的 Discriminator fail 的時候
因為我們最後 training 的終極的目標
是希望 Generator 產生出來的東西
是 Discriminator 完全無法分別的
就是 Discriminator 在鑑別真的或假的 image 上面
它的正確率是 0
但是，往往當你發現你的 Discriminator 整個 fail 掉的時候
並不代表說 Generator 真的 generate 很好的 image
往 你遇到的狀況是你的 Generator 太弱
那很多時候，我在 train 的時候還會遇到的狀況
就是 Generator 它不管 input 什麼樣的 vector
它 output 都給你一張非常像的東西
那一張非常像的東西
不知道怎麼回事就騙過了 Discriminator
那個就是 Discriminator 的罩門
它無法分辨那一張 image
那它整個就 fail 掉了，但並不代表說你的 machine
真的得到好的結果
我要說的大概就是這樣 後面是一些 reference 給大家參考
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
