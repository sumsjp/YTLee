今天這一堂課要講的是
假設我訓練不了人工智慧
那我如何訓練我自己
讓我在使用人工智慧的時候
他可以發揮更強大的力量
為了避免大家誤解這一堂課的內容
所以這邊要先強調一下
在本節課中沒有任何的模型被訓練
本節課的授課方向
並不是教你怎麼對特定的任務寫prompt
因為常常有同學問我說
我想做某件事
那老師可不可以教我怎麼寫這件事的prompt
那我必須要告訴你
其實針對特定任務寫prompt這件事
是不太需要學的
為什麼不太需要學呢
因為今天這些語言模型的能力都很強
你今天在下prompt在給他指令的時候
其實是不需要什麼特定格式的
各式各樣不同的問法
往往他都看得懂
而且按照今天這些語言模型的能力
通常你只要把你想要做的事情講清楚
你講到另外一個人看得懂
往往語言模型也就有機會可以看得懂
所以你今天其實可以把這些大型語言模型
就想像成是一個在線的新人助理
那為什麼要特別強調他是一個新人助理呢
首先這邊有人這個字
因為他跟一般人其實已經非常類似了
今天這些線上的大起源模型
往往有一般人的基本知識和一般人的理解能力
但是這邊要特別強調新這個字
因為他是一個新人
他第一天認識你
他對你的一切一無所知
所以假設有一些事情是隻有你自己才知道的
你的出生年月日
你的身份證字號
這些東西他是不知道的
他對於你的事情一無所知
他今天第一次認識你
所以有一些要求
你要求他他做不到
也許只是因為他對你的瞭解太少了
所以他的反應才會不是你所預期的
等一下我會告訴你說遇到這種狀況
有什麼樣可能的解決方式
那這一堂課呢
我打算從五個不同的面向來跟大家講
有哪些不訓練模型就強化語言模型的方法
第一個要跟大家分享的是
在這個世界上有一些神奇的咒語
這些神奇的咒語有可能可以強化模型的能力
但這邊要加一個免責聲明
這些神奇的咒語並不一定對所有模型
所有任務通通都適用
那有什麼樣的神奇咒語呢
最經典的一個咒語叫做Chain of Thought
它的縮寫是COT
那Chain of Soul是什麼意思呢
一句話就可以說完
就是叫模型思考
當你叫模型解一個比如說數學問題的時候
你多加一個指令叫他Let's think step by step
他突然能力就不一樣了
怎麼個不一樣法呢
這邊這個表格呢
是來自一篇2022年11月的論文
他說,在模型解數學問題的時候
如果你直接把問題丟給他
沒有給他額外的指令
他的正確率是17.7%
但一旦你跟他說Let's think
叫他想一想
突然正確率飆到57%
接下來你跟他講說Let's think step by step
一步一步的想
哇,正確率突然飆到78%
叫模型思考
居然可以強化它達數學的正確率
但是如同我剛才一開始就講的
其實這些神奇咒語
雖然那個農場文最喜歡講這種神奇咒語了
但神奇咒語的使用往往還是看模型的
所以這邊要強調一下
這邊的結果是用在一個比較舊的模型上
這個模型是IntructGPT Text DaVinci 002
這是一個在ChatGPT時代之前
在GPT-3.5之前就有了一個上一世代的模型
那叫他思考對他來說差別是蠻大的
那據說叫模型思考也對GPT-4看圖有幫助
那如果你看GPT-4的DEMO的話
他們有特別加think about it step by step
看起來這句話可能也對GPT-4看圖的能力是有影響的
但實際上有多大的影響
我手上並沒有數據可以作證
到底有多大的影響
另外呢
叫模型解釋一下自己的答案
往往也對他的能力是有幫助的
那這邊引用的這兩篇論文
其實是我們助教的論文
在這兩篇論文裡面
助教要做的事情是什麼呢
我們要做的事情就是
用大型語言模型來批改文章
就跟大家在作業2裡面做的事情
是一模一樣的
就給模型一個文章
然後呢
看看他能不能夠正確的給一個分數
跟人類老師給的能不能夠非常的接近
所以你知道這個
助教在用大型語言模型
批改文章上是非常專業的
他其實有兩篇這個Top Conference的paper
都是在講怎麼用大型語言模型來批改文章
那在我們的論文裡面有一個這樣的發現
我們發現說
如果叫模型解釋一下
為什麼他這樣批改
他批改的正確率會更高
所以剛才看到
助教在作業2的demo裡面
那些模型都是先長篇大論
解釋一下這篇文章好在哪裡不好在哪裡
最後才給一個批改的分數
那怎麼讓模型解釋自己的答案呢
其實也非常簡單
就是跟他講要解釋就結束了
跟他講說要解釋一下你的答案
他就會解釋答案
然後再把最終的答案輸出出來
那這件事情有什麼樣的幫助呢?
這邊是做在那個GPT 3.5上面的結果啦
那這個表格有點複雜
總之不同的欄位表示的是評估文章的不同的面向
那上面這個數字呢
代表的是模型評估的結果跟人之間的相似程度
那個數值呢是越大越好
那這一排結果是叫語言模型直接給答案
不解釋的時候
他跟人類老師評估出來的相似度
這邊的結果是叫模型先解釋一下
為什麼這樣評分
再給分數的時候
跟人類老師的相近程度
那你會發現說
如果模型有先解釋
為什麼他會這樣評分
他得到的評分結果
跟人類老師得到的評分結果
是更接近的
那還有什麼神奇的咒語呢
有一個是去年暑假有人發現的這個咒語
叫做情緒勒索
當你今天對模型說
這件事情真的對我的生涯很重要的時候
他的能力居然就不一樣了
那這篇論文做的實驗其實是蠻完整的
他不是隻做在一兩個模型上
你看他這一頁投影片裡面
他做了六個不同的模型
然後如果沒有對他情緒勒索的時候
在一些任務上他的正確率是這個樣子
那如果對他情緒勒索
告訴他說這件事真的對我很重要
他的正確率突然就提升了一個層次
所以看起來情緒勒索也對大型語言模型是有用的
好那還有更多更多這種神奇咒語啦
大家可以讀一篇paper叫做
Principle Instruction Are All You Need
這篇paper其實不太像是principle啦
這篇paper比較像是
他驗證了各式各樣的都市傳說
告訴你說 因為你知道網路上流傳的各式各樣
有關神奇咒語的都市傳說
那這篇論文
他們驗證了各式各樣的都市傳說
告訴你說哪一些傳說是真的
那這是一個非常新的論文
是去年12月才公開的論文
那細節就給大家自己讀
我就擷取幾個比較有趣的發現
第一個是對模型有禮貌是沒有用的
大家都會想說我對模型講話
也許應該有禮貌跟他說請幫我做什麼
然後跟他說謝謝
看看他的答案會不會更正確
這篇文章告訴你說
對他有禮貌是沒有用的
直接有什麼要求直說無妨
好那模型呢
你要跟他講要做什麼
不要跟他講不要做什麼
舉例來說你要希望他文章寫長一點
你就直接跟他說
要把文章寫長一點
但是不要反過來說
你如果說不要文章寫太短
他可能就沒有那麼厲害
所以直接明確的要求他對模型是比較有用的
然後再來跟他說如果你做得好的話
我就給你一些小費
這句居然是有用的
然後跟他說如果你做不好的話
你就會得到處罰
這句話也是有用的
或者是說
如果你跟他說你要保證你的答案是沒有偏見的
而且要避免使用任何的刻板印象
這句話對模型也是有影響的
那還有很多很多
總之大家可以自己讀這篇論文
看看有哪一些神奇咒語的都市傳說
是被驗證為確實有用的
那但是這些神奇咒語要怎麼被找出來呢
那通常這些神奇咒語怎麼發現
這個情緒勒索對打醒原模型有用呢
就是有人靈光乍現覺得說我們來情緒勒索一下大型語言模型
看看會不會有影響
試下去發現真的有影響
但有沒有更有效更系統化的方法來找這些神奇咒語呢
一個可能的思路是我們直接用增強式學習
這是某種機器學習的方法
那我們未來還會提到這邊就只先講的大概念
也許我們可以用增強式學習的方法來訓練另外一個語言模型
這個語言模型它做的事情就是專門下咒語
然後看看能不能夠訓練出一個擅長下咒語的語言模型
那一個語言模型要怎麼學會下咒語呢
那一開始這個語言模型可能根本不知道要怎麼下咒語
所以它下的咒語可能是亂七八糟的
可能對我們的目標語言模型沒有什麼影響
但是沒有關係
它就嘗試不同的咒語
得到不同的結果
得出來的結果呢
就想辦法用一個自動化的方法來評估好壞
然後呢
這個評估好壞的這個回饋
就可以給這個下咒語的語言模型進行學習
他可以從回饋中學習
期待他就可以學出更有效
更能控制目標語言模型的咒語
這邊有一個實際的例子
這個實際的例子呢
是希望找一個咒語
讓語言模型變成一個畫牢
讓他的回應越長越好
那這邊要強調一下這個實驗是做在GPT-3上面啦
就是雀GPT-3.5的上一個世代的模型
所以這個咒語對GPT-3有用
但我們試過了對GPT-3.5是沒有用的
那這邊的目標呢是希望
語言模型變成一個畫牢
他的回答越長越好
如果你今天是直接問他問題
沒有多做什麼樣神奇的Prompt
這個語言模型回應的長度
平均長度是18.6個字
如果你今天要求語言模型
他的答案要越長越好
他可以從18個字
扁到23.76個字
但是透過用AI來找神奇咒語
透過這個增強式學習的方法
來訓練另外一個語言模型來找神奇咒語
我們可以讓GPT-3回答的平均長度
扁到34個字
比你直接叫他回答長一點
還更加有效
那你可能會想說這是什麼樣的神奇咒語呢
這個咒語長這樣子
就叫他喂喂喂喂喂
然後不知道為什麼GPT-3的回答就變長了
那像這種神奇咒語啊
尤其這種人類看不懂的神奇咒語
通常對GPT-3比較有效
對前一個世代的ChatGPT比較有效
那今天ChatGPT都蠻厲害的
他們跟人類非常的接近
所以你給他亂七八糟的神奇咒語
他往往是跟你講我不知道你在說什麼
所以神奇咒語 尤其是人看不懂的那一種
對今天的GPT-3.5以上的模型
可能是沒有那麼大的用處
但是這邊是想要表示說
你確實可以有機會用增強式學習的方法
自動找出一些神奇的咒語
來讓模型做到你想要它做的事
來讓模型強化特定方向的能力
那今天這些語言模型都很厲害了
所以找咒語有了一個新的方法
什麼樣新的方法呢
你想問什麼樣的咒語
最能夠操控語言模型做某些事
比如說數學問題
你直接問他就好
你就直接問語言模型說
如果我今天要你解一個數學問題
你有什麼樣的咒語可以強化你的能力呢
語言模型就會提供一些candidate給你
提供一些可能給你
你就一個一個試
看看你能不能夠試出更強的咒語
很多人都採取了類似的方式
找出了更強的咒語
我們剛才看到
Let's think step by step 是一個很強的咒語
有人去直接問了大型語言模型說
你能不能給我一個更強的咒語
他給了一個更強的
Let's work this out in a step by step way
to be sure we have the right answer
正確率高達82%
所以大型語言模型可以自己想出更強的咒語
後來還有人發現說
Let's work this out 在新的這個資料集上也沒有很強
所以他叫大型語言模型再想一些更厲害的咒語
他找到一個咒語是take a deep breath
然後work on this problem step by step
叫大學模型先深呼吸一下
他的能力又會變得再更強一點
那你知道一般在比這個排行榜
leader board的時候呢
都是對模型做排名
但是其實我們也可以對
礦來做排名
那這邊呢是一個有趣的活動
是我們實驗室跟陳尚哲老師實驗室
和Intel Lab一起舉辦的一個活動
我們建立了一個平臺
在這個平臺上
我們想知道說對特定的任務而言
有沒有什麼礦
是可以特別強化
Lamma2這個Meta所試出來的
開源模型的能力的
就過去大家比的都是
同樣任務
不同模型間的能力
我們現在要比的是同樣任務
同樣模型
但是不同礦間的差異
看看有沒有人是這個大型語言模型催眠大師
可以想出神奇的Prompt
激發語言模型不可思議的能力
那其實我們在作業室也會做一模一樣的事情
在作業室就是我們會先把任務設定好
然後呢要大家想出Prompt
想出神奇的Prompt
看看這個班上有近千人修課
有沒有人可以想出神奇的Prompt
來讓大型語言模型發揮它不可思議的能力
那這邊要再強調一下
剛才講的神奇咒語雖然很有趣
我把這個部分放在最前面講
因為這個通常是大家會覺得最有趣的部分
但是神奇咒語並不一定對所有模型都有用
甚至對同一個模型
不同時間點的同一個系列的模型
可能也都不一定有用
這邊舉一個實際的例子
剛才你看到think about it step by step
叫模型思考
好像是一個非常強悍的咒語
但是他對GPT-3.5現在的版本
沒有太大的幫助
我們實際上試了一下
叫GPT-3.5解數學的應用問題
如果你用的是去年六月的舊版本的話
沒有神奇咒語的時候
模型的正確率是72%
跟他講Let's think step by step
叫他想一想正確率變88%
看起來神奇咒語非常有用
但是今天你用現在最新版本的GPT-3
沒有神奇咒語它正確率已經高達85%
加神奇咒語也只能進步到89%
看起來神奇咒語的影響力變得小很多
位子一想想這也是比較合理的
怎麼可以叫你思考的時候才思考呢
當然隨時問你問題你都要使出全力啊
沒有叫你思考的時候也應該要思考啊
所以GPT-3.5看起來變得是更厲害的
沒有叫他思考,沒有暗示他要think step by step
他也可以得到正確的答案
那至於為什麼叫模型思考會有用
等一下我們會看到,等一下會解釋給你聽
那像我剛才講說
要求模型做解釋可以強化他的能力
那這招也不是對所有模型都適用
比如說這邊引用一篇這個2022年5月的文章
他們也嘗試叫模型做解釋
但是發現說GPT-3
或者是更早的模型
如果你要求他做解釋的時候
他不一定可以得到更準確的結果
這個是第一部分
那接下來呢
第二部分是想要跟大家分享說
有時候如果模型沒有得到正確的答案
那是因為你給他的資訊不夠
如果你提供給他更多的資訊
他就可能可以給你更正確的答案
舉例來說
有時候模型沒辦法得到正確的答案
只是因為他不清楚你的前提是什麼
有的同學會這樣問ChatGPT
說NTU是什麼的縮寫
那你知道NTU其實既是臺灣大學的縮寫
也是南洋理工大學的縮寫
那更多的時候
你問隨便一個世界上的人
NTU是什麼
往往他會覺得是南洋理工大學的縮寫
我在前年去JHU兩個月
去短期訪問兩個月
到我快離開的時候
大家還是覺得這個人是新加坡來的
那對於大型語言模型來說
你直接問他
我這邊是直接問GPT-4
NTU是什麼的縮寫
他也覺得就是南陽理工大學的縮寫
那怎麼讓他知道
NTU是臺灣大學的縮寫呢
你直接跟他講
你是臺灣人
跟他講說你是一個臺灣人
用臺灣人的角度來思考
NTU是什麼的縮寫
他就知道是國立臺灣大學的縮寫
那如果再跟他說你是新加坡人
用新加坡人的角度來思考
他就覺得NTU是南洋理工大學的縮寫
所以有時候把前提講清楚
模型可以給你更精確的答案
那深層式AI它本身的知識還是有限的
未來我們會再講到說
講到說這些深層式AI它的知識是從哪裡學到的
但它並不是無所不知
所以很多時候它給你的答案不是正確的
是因為它根本沒有相關的知識
比如說我叫這個GPT-4整理一個表格
這個表格裡面要列出前幾代的GPT
GPT一代二代三代模型的參數量跟訓練資料
那其實現在這個大型語言模型都是會畫表格的
所以它畫出來一個表格
然後他整理出了GPT一代二代三代他的參數量
那這個數字呢算是蠻精確的
但是如果說訓練的資料用的是什麼
他就有點答不上來了
GPT-3他大概知道說用了多少的訓練資料
但是GPT跟GPT-2他就沒有答出來
他直接寫了一個Not Available
那怎麼辦呢
怎麼讓這個GPT-4可以正確的把前幾代模型的參數量跟訓練資料量都打出來呢
也許你可以先上網去做一下搜尋
搜尋一些相關的資料
把這些相關的資料直接拿給語言模型案
他有相關的資料自然可以正確的把表格畫出來
但是你可能會想說
我都上網搜尋到相關的資料了
我還需要他幫我畫表格嗎
我自己畫不是一樣快嗎
我告訴你,你可以更偷懶一點
直接把GPT一代二代三代的Paper輸入給他
直接叫他幫你讀Paper
你自己就不用讀了
他輸出來的結果是這個樣子的
他就開始說
我讀一下GPT1.pdf
然後再來讀一下GPT2的文章
接下來讀GPT3的文章
然後就畫出了一個表格
基本上這邊的每一個模型的參數量
還有訓練的資料量
都是蠻精確的
你可能想說這個GPT1
他沒講訓練的資料量啊
他只說用了哪一個訓練資料集
沒錯
因為我仔細讀過論文裡面確實沒有提到訓練的資料量
只說用哪一個Compass而已
所以模型的回答算是蠻精準的
那剛才講到你可以給前提
你可以給額外資訊
你還可以提供範例
假設你現在想叫語言模型做情感分析
你跟他說請幫我做情感分析
給他一個句子
希望他可以分析這個句子是正面還是負面的
那如果今天這個語言模型
他根本不知道情感分析是什麼意思的話
怎麼辦呢
也許我們可以提供給他一些範例
告訴他說今天天氣真好
你就要說是正面的
今天運氣真差
你就要說是負面的
這朵花真美
你就要說是正面的
我累了
就要說是負面的等等
給他一些例子
期待他讀過這些例子以後
他知道情感分析是怎麼回事
給他一個新的句子
他可以給出正確的答案
那這種提供範例
可以讓模型根據範例得到更準確的答案
這件事情叫做In Context Learning
那In Context Learning這個技術
老早在有GPT-3的時代
就已經被發現了
這邊引用的是GPT-3原始的論文
那你從這個網址裡面
可以看到說這篇論文他發表的時間
是2020年的5月
三四年前就生成式AI的領域來說
就是上古時代
上古時代就已經知道
in-context learning這個技術了
那這邊還是要強調一下
in-context learning這個字裡面
雖然有learning這個詞彙
但是這裡沒有任何模型真的被訓練
這個語言模型的參數是完全沒有改變的
唯一改變的是輸入改變了
所以in-context learning的意思是提供額外的範例
而並不是語言模型真的有被訓練
但是在2020年的時候
那時候人們都非常的懷疑
這些語言模型就是做文字接龍
它真的可以讀懂這些例子
然後影響它的輸出嗎
於是在2022年的時候
有一篇文章叫Rethinking the Role of Demonstration
決定要驗證一下這些模型
到底有沒有看懂這些例子
那怎麼驗證模型有沒有看懂這些例子呢
你故意給他錯的例子
你故意把本來應該回答是正面的這些句子改成負面
把本來應該回答負面的句子改成正面
當你把這些例子改的時候
改掉以後
就這些實際上是正面的句子
他的答案是負面的
實際上是負面的句子
答案是正面的
當你把這個範例的資料做修改以後
照理說語言模型
他的答案應該要跟著改變
如果他的答案仍然是正面
給他我感到非常高興這個例子
他的答案仍然是正面的話
代表他沒看懂範例
如果他答案改成負面
代表他真的有看懂範例
結果如何呢
結果這篇文章發現說
這些模型沒有真的看懂範例
故意給他錯的範例
對他的答案是沒有什麼影響的
所以在2022年那個時候呢
這篇文章的結論是
看來語言模型沒有真的讀懂範例
那為什麼提供給他範例往往還是有用呢
也許是因為當你給他提供這些範例的時候
你告訴他答案可能是負面或者是正面的
他讓他更清楚現在要做什麼樣的事情
他可以得到更精確的答案
但他並沒有真的仔細的讀懂了這些範例
但是這是2022年的結論
這邊有個很好的例子告訴你說
這個時代的變化很快
我們對於這些生成式AI能力的認知
也是不斷在改變的
過了一年之後
有另外一篇文章
這篇文章的標題叫做
Larger Language Model
Due in Context Learning Differently
這是2023年3月的文章
在這篇文章裡面
作者做了一些新的實驗
他們拿更強的語言模型
來測試他們讀範例的能力
這邊他測試了
PALM從8個Billion參數
到540個Billion參數的模型
他也測試了一系列
當時OpenAI用的這個
這個GPT-3
介於GPT-3到GPT-3.5之間的
一系列的模型
他發現什麼他發現這個橫軸啊
橫軸是錯誤的範例比例
從0%的錯誤一直到100%的錯誤
那發現說對一些最強的模型
這邊不同的顏色代表不同的模型
顏色越深代表這個模型的能力越強
對於這些最強的模型
比如說PALM
這是一個Google所開發的大型元模型
它有540個Billion的參數量
一個非常巨大的模型
這個巨大的模型在你給他錯誤的範例的時候
他真的會答錯
在這邊這個答錯呢
有用引號誇起來
這個答錯其實是答對
希望大家可以聽得懂這個意思
因為今天給模型錯誤的範例
所以他應該要跟著答錯
所以答錯反而是好的
答對反而是不對的
最理想的狀況
今天所有的範例都是錯的
100%的範例都是錯的
正確率應該要是0%才是最好的
當然這些模型能力看起來還是有一些極限啦
所以他們並沒有真的做到0%的錯誤率
但至少他的錯誤率是比隨機
這個這條曲線是隨機的
代表50%的錯誤率
至少他們的錯誤率是比50%還要低的
所以這篇論文的結論是
看起來最強的模型是真的讀懂了這些範例
那這邊我就想要做一個類似的小實驗
我們來看看GPT-4
如果我們給他的指令跟正常一般的認知不一樣的時候
他到底是會按照我們的指令來執行
還是他腦中的偏見太強了
他根本就不去讀那些指令呢
那記得我們在上週一開始的時候
就嘗試用ChatGPT來做新聞的分類
那個時候我們只提供了類別的名稱
然後給語言模型
一個新聞
他可以給我們
正確的分類
好 那我們現在給每一個類別一個描述來看看
它是不是真的可以按照我們的描述來進行分類
那這邊給他的描述呢 有一點怪怪的
我跟他說所謂政治類的新聞
是報導有關產業經濟的消息
例如企業投資金融不虧失
那如果是財經方面的新聞
是報導國內的政治消息
例如政府部門政黨選舉跟政策
GPT-4能不能夠看懂我提供給他的怪怪的類別指示呢
這邊給他一篇文章
這個是跟AI有關的新聞
照理說這應該是屬於
就如果你不看前面的這些說明的話
這顯然是財經類
但如果你有仔細讀一下這些說明
他應該是屬於政治類
因為這邊特別說了
跟產業經濟有關的消息算是政治類
結果如何呢
嗯 他說是財經類
我試了很多次 他的回答都是財經類
看來他沒有好好的讀這些類別
當這些類別跟我們一般預想的不一樣的時候
GPT-4並沒有辦法真的答對
但他的能力就僅限於此嗎
你知道有時候語言模型的能力
取決於你跟他說什麼
這邊暗示一下
這些類別的定義可能與一般的定義不同
他突然就懂了 他可以回答是政治類
所以提示一下好好讀這些說明
他其實能力又會不一樣
那講到這個讀範例的in-context learning
我就來提一下Gemini 1.5
大家知道說Gemini 1.5跟Solr是同一天發表
所以沒什麼人注意到這個Gemini 1.5
那在今天OpenAI發表任何東西的時候
都是故意跟Google選同一天
所以當我發現Sora發表的時候
我就想說Google一定有發表一個什麼東西
所以我就去找了一下 原來是發表了Gemini 1.5
那Gemini 1.5有非常強的in-context能力
在Gemini 1.5的技術報告裡面
他們提到了這樣一件事
他們想叫語言模型去翻譯一個叫做卡拉蒙的語言
這個語言是一個非常少人用的語言
根據Gemini的技術報告裡面說
這個語言的使用者大概只有200人而已
在網路上找不到這個語言的任何資料
所以這個語言模型顯然不懂卡拉蒙語
所以他根本沒有辦法把這個句子翻譯成卡拉蒙語
那接下來Google做了什麼事情呢
他們給大型語言模型非常大量的教科書
我們找了這個語言的文法書跟他的字典
合起來有25萬字左右
把這些大量的資料加上你的指令
一起丟給語言模型
這個時候它突然能夠翻譯卡拉蒙語了
那實際上的數值結果怎麼樣呢?
這個是來自於Gemini 1.5的技術報告
中間這個Column代表從卡拉蒙語翻成英文
右邊代表從英文翻成卡拉蒙語
那在完全沒有提供任何教科書
Zero Shark代表沒有提供任何額外的資訊
這些模型就是裸考
給他去卡拉蒙語教翻英文
給他英文教翻成卡拉蒙語
讓他直接翻譯
那如果不管是GPT-4
還是Claude 2
還是Gemini 1.5
通通沒有辦法做準確的翻譯
這邊的數值代表人類評估的結果
那人類評估的那個分數滿分是6分啦
所以0點多分是非常低的分數
那後面這個括號裡面的分數是拿模型去自動評比的結果
那我們這邊就先不看括號裡面的分數
那這個投影片上現在的分數是在沒有任何額外資訊的情況下
模型得到的答案
今天如果給他半本卡拉蒙語的教科書
或給他完整的一本教科書
他讀完以後再拿去做翻譯
這個時候 他的正確率 人類評估的分數變成4.36跟5.52
當然實際上跟人類的學習者還是有一段差距啦
你叫人類真的去讀那些教科書
讀完以後真的來做翻譯
人類是可以做得比機器更好的
但是這邊的實驗結果顯示說
機器他讀完了整本教科書以後
他真的有一點不一樣了
他可以翻譯一個他之前不會的語言
但是講到這邊有一個非常重要的觀念要考考大家
什麼樣重要的觀念呢
我們現在來講一個情境
剛才我們說如果讓語言模型直接去翻譯卡拉蒙語
他沒有辦法正確的翻譯
給他一本教科書叫他去翻譯卡拉蒙語
他先讀完教科書
接下來讀到這個要翻譯的指示
接下來做文字接龍可以接出正確的翻譯
給他看教科書
他突然會翻譯的
接下來
在下一次使用他的時候
再叫他翻譯卡拉蒙語的時候
他可以翻譯的出來嗎
給大家五秒鐘的時間想一下
如果你可以答對的話
那你就真的知道
這堂課在講些什麼
覺得答案是1的同學舉手一下
好,多數同學都覺得是1,覺得答案是2的同學舉手一下
好,幾乎沒有,好,看起來大家都聽懂了,非常棒
這個確實模型,你再一次叫他翻譯卡拉盟語的時候
他是沒有辦法被翻譯的,為什麼?
因為今天沒有任何的模型被訓練
模型的參數都是固定的
所以今天就算是他讀了這本教科書能夠翻譯
能夠翻譯那是在有這本教科書
作為文字接龍前面的句子的前提下
他可以做翻譯
下一次你在用他的時候沒有這本教科書
他就翻譯不了了
那大家都答對了
代表你真的瞭解了
in-context learning 的精神
好那我們剩下
15分鐘的時間
那剩下這個部分呢
我想是講不完了
也許我們先來回答一下
同學們的問題好了
