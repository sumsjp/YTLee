好,那接下來呢
我們之前有說，這學習的課程上課呢 就是以生成式學習作為主軸
那接下來，我們就要來多講一些生成式學習模型相關的技術
那這份投影片是要告訴大家說，生成式學習的模型可以分成兩大類
這兩大類分別對應到兩種生成的策略
那我這邊，那我這邊取了一個比較通俗的名字
這兩個策略，一個我叫「各個擊破」，另外一個叫做「一次到位」
那從它的名字，也大概可以猜得出說這兩種策略分別是怎麼進行的
好，那我們剛才有講過說 所謂的生成式學習就是要生成有結構的複雜物件
比如說文句，比如說影像，比如說語音
那這些複雜的物件通常是，由一些小的元素所構成的
舉例來說，構成文句的元素是什麼?
在文件上，我們叫做token
但你直覺想，你會說「欸，是字啊」
在文件上，你如果是讀這個ChatGPT相關的論文 它會告訴你說
文句所組成的元素叫做token
那這個token到底是什麼呢?
這個token在中文裡面，其實就是字啦
其實就是字
但它在英文裡面，它不是英文的詞
它是英文的word piece
是一個介於字母跟英文詞彙之間的東西
那什麼叫word piece呢?
直接舉一個例子
比如說unbreakable這個詞彙
你可能會猜成三個word piece，un、break跟able
就是我們高中的時候學過的這個字首字根的概念
那為什麼英文的token不是全部可能的英文的詞彙
而是用word piece呢?
因為你知道說，今天這個GPT它在生成東西的時候 它其實是一個分類的問題
它其實做的是一個選擇題
所以你要把所有的選項通通都列出來
而中文的字是比較有機會窮舉的
而英文的詞彙可以說是無窮多
每一個人名、每一個地名都算是一個新的詞彙
所以英文的詞彙，英文的word是無窮多的
所以在英文裡面，你不能把word當作選項 因為根本窮舉不了所有的word
所以在生成文句的時候，英文用的單位叫做word piece
就把word拆解成更小的單位，而這種單位是可以窮舉的
好，這個是文句的部分
那影像呢?影像構成的元素，這個大家就很容易想像了
就是pixel，就是像素
那語音呢?
這個語音，它是由一堆取樣點所構成的
舉例來說，16K sampling rate的聲音訊號，一般做語音辨識
你的聲音訊號通常是16K的sampling rate 意味著一秒鐘裡面
16K有16000個點 ，所以語音是很複雜的
好，那生成文句，這個大家都很熟悉了
就是ChatGPT，生成影像，這個大家都知道了
Midjourney、Dali都是生成影像的AI
那等一下我們再舉幾個大家可能不一定知道的例子
一個是生成影片
那今天生成影片在文件上看起來，其實也做得蠻成功的
那以下這個例子是來自於Google在去年10月發表的論文
它叫Image Video
Image Video可以做的事情就是
給一個句子，它按照你寫給它的句子生成對應的影片
跟它說，畫一個泰迪熊在洗碗
畫一個泰迪熊在洗碗
這是機器生成的影片，真的是蠻猛的
那我看到它的demo裡面最猛的是這個
它說有一大堆秋天的落葉落在一個平靜的湖上
這些落葉要拼成文字叫Image Video
它的輸出是這個樣子的
落葉落在湖面上，拼起來就是Image Video
哇，這個非常難以置信，這個是由機器自動生出來的影片
好，那這是生成影片的例子
那還可以生成語音
那生成語音對大家來說你可能不會覺得特別稀奇
Google小姐就是生成語音的一個應用
所以語音合成，其實生成是AI也不是最近才有的
你說生成語音這個技術發展的老成熟了
給一個文字，今天Google小姐都可以念出非常字正腔圓的句子
那現在不只是給文字生成句子
你還可以直接下指令 告訴它說生成的句子風格應該要是什麼樣子
而且風格的描述直接用自然語言處理直接描述就好
以下的Demo來自一篇騰訊的論文叫InstructTTS
那這是非常新的論文
這是今年一月放在arxiv這個平台上的論文
那我們來看看這個論文裡面的Demo，看它做得怎麼樣
那它這邊可以做的事情就是
你在語音合成的時候
直接下一個自然語言的指令告訴它說我要和什麼風格的句子
比如說，這邊是和語調高昂聲音洪亮內心非常憤慨的句子
聽起來像是這樣
學長今天還說他喜歡我呢
你不珍惜我，我就跟別人跑了
好，這個是聲音 然後接下來聲音高語氣嚴厲大聲斥責，聽起來是這樣
學長今天還說他喜歡我呢
你不珍惜我，我就跟別人跑了
接下來鎮定從容語氣平和語調穩定
學長今天還說他喜歡我呢
你不珍惜我
我就跟別人跑了
接下來是語氣中惆悵帶有一絲苦澀 這很複雜，聽起來像是這樣
學長今天還說他喜歡我呢
你不珍惜我
我就跟別人跑了
快哭了的感覺
聲音難過鬱鬱寡歡，輕速的語氣中透露出疲憊落寞的情感 聽起來像是這樣
學長今天還說他喜歡我呢
你不珍惜我
我就跟別人跑了
這個如果加到Midjourney加ChatGPT的遊戲裡面
讓遊戲裡面的角色可以說話，其實真的是非常帶感這樣子
還可以生成聲音，那生成語音跟生成聲音有什麼不同呢?
這個語音指的是人發出來的聲音
那聲音就不限於人發出來的聲音，可以是任何東西 比如說狗在叫啊，汽車跑過去啊等等
那現在你可以直接下文字產生對應的聲音
比如說，這邊下的文字是兩艘太空船在太空中戰鬥
機器合出來的聲音聽起來像是這樣
或者是你可以下更複雜的指令
比如說他想要產生海的聲音
但是作者又不知道怎麼描述海的聲音
所以直接問ChatGPT
產生叫它產生描述海的聲音，ChatGPT就說了一大串
再把這串文字丟到模型裡面 模型合出來的聲音聽起來是這樣
有海鷗的聲音，聽起來有點在海邊的感覺
或者跟它說產生pop music ChatGPT把pop music描述得更精確一點以後
丟到模型裡面，聽起來是這樣
我知道說音樂生成的AI其實很多啦，但這邊他想要展示的是說
你看，它什麼聲音都可以生成
叫它生音樂就生音樂，生海浪聲就生海浪聲
那在這個論文裡面，作者為了要展示說
他的模型是真的可以聽懂指令間微妙的差異
所以他舉了三個很像的輸入
一個是，有一個人在大房間裡面講話
一個是，有一個人在小房間裡面講話
一個是，有一個人在錄音室中講話
我們聽聽看有什麼不同
所以你完全可以感覺到就像是在一個大房間裡面講話
但這個人講的是什麼?不知道，聽不懂 不知所云，不是中文也不是英文
因為對這個模型來說
它可能沒有聽過太多語音 它只知道人的聲音大概聽起來長什麼樣子
但是要生出你聽得懂的，它可能沒有辦法
換小房間，聽起來是這樣
這是小房間，那如果在錄音室呢?
所以你比較錄音室跟小房間就會發現說，錄音室因為在錄音室裡錄的
所以聲音訊號就是比較乾淨的
而這也是非常新的論文
也是今年一月放到arxiv這個平台上的論文
好，那我們就來講生成的兩種策略
第一個策略叫做「各個擊破」
那ChatGPT採用的就是「各個擊破」的策略
就是給一個句子，如果我要生一個句子怎麼辦呢?
把句子裡面的元素
也就是一個一個的字，一個一個的生出來
那如果影像，影像的元素是像素
那如果你今天要叫影像生成的模型生一個影像，用「各個擊破」的方法
就是一次生一個pixel，一次生一個像素
那生成的過程可能像是這樣
先畫左上角第一個像素，那我假設只要生4x4大小的影像
那你可以想像說4x4大小的影像很小
今天都要生一個高清的圖才可以
但原理是一樣的
所以這個像素就一個一個生出來
一個一個生出來，一個一個生出來
最後就把整張圖片生出來
這是「各個擊破」的技術
那這種「各個擊破」的模型呢
這個專業的術語叫做Autoregressive，叫AR的模型
在過去2016年的機器學習課堂中
我其實是demo過 用「各個擊破」的方法產生寶可夢的圖
一次就只產生一個像素
把整張圖片的像素都生完，就變成一隻寶可夢
有興趣的同學，我把影片連結放在這裡給大家參考
那策略二呢，叫做「一次到位」
那這個「一次到位」的方法 它的英文叫做Non-autoregressive的Model，說寫是NAR
所以剛才一個一個「各個擊破」叫Autoregressive 那不是Autoregressive的方法
就是「一次到位」，叫Non-autoregressive的方法
那怎麼Non-autoregressive呢，怎麼「一次到位」呢?
其實就跟你直覺想的一樣 反正就是一次把所有文字產生出來
一次把所有的像素Pixel產生出來
那這邊會有一個問題 我們之前講「各個擊破」的方法的時候
機器要產生一個句子，它什麼時候會停下來?
產生代表結束的符號的時候會停下來
但是今天如果是「一次到位」的方法 所有的文字都要一次產生出來啊
機器怎麼知道要產生多長的句子呢?
這邊有幾個不同的可能性，一個可能性是
反正每次永遠都輸出固定長度 你就先設定好，不管問什麼問題，一概輸出512個字
那你說這個回答不就是都固定512個字嗎?也不會
你就看看回答裡面，有沒有輸出END這個符號
有輸出END這個符號，後面就直接丟掉
所以就使用者感覺上看起來， 每一次產生出來的答案，它的長度仍然是不一樣
那另外一個方法是
你先叫你文字生成的模型先輸出一個數字
比如說100 這個數字代表什麼?
這個數字代表我們接下來生出來的回應要有多少個字
所以先生出數字100 接下來就先決定好要生出100個字，再把100個字一次產生出來
所以就算是non-overregressive的方法 也有方法決定說生出來的回應應該要多長
那接下來我們就來比較「各個擊破」跟 「一次到位」這兩個方法有什麼樣的差異
先從生成速度上來看
「各個擊破」的方法 那這邊我就不說我們是在生影像還是生文字
反正概念都是一樣的
好，「各個擊破」的方法，一次只生一個元素
每一個元素生出來之前，都要等前面的元素生成
每一個字生出來之前都要等前面的字生成 每一個像素生出來之前都要等前面的像素生成
它顯然非常的花時間，而且沒有辦法平行化
「一次到位」，只要你有足夠的平行運算能力
你就可以把所有的元素一次都產生出來 它顯然是比較快的
所以這就解釋了為什麼你發現說 影像生成通常用的都是「一次到位」的技術
那為什麼會這樣呢?
那是因為相較於文字，影像的元素是更多的
像一段回應頂多五百個字一千個字 你就覺得回應的很長了吧
但是影像呢?一張圖片
光是100x100的圖片裡面的像素，就有100x100個就已經一萬個了
而且100x100的圖片還是很低畫質的圖片
所以高清的圖片那像素太多了
多到你根本沒有辦法考慮用 「各個擊破」一個一個像素產生的方法
這就是為什麼影像生成 你在文件上會發現說都跟用GPT不太一樣
它是用「一次到位」的方法
一次把所有的像素都產生出來的
好那接下來我們來討論生成品質
這兩個方法的生成品質有什麼樣的差異呢?
先講結論
通常「各個擊破」的生成品質比較好 「一次到位」生成的品質比較差
怎麼說呢?我們這邊直接舉個例子
假設你問一個產生文字的模型，比如說李宏毅的職業是什麼
你可以回答教授或者是老師
但是其實有別的答案 如果你Google李弘毅的話
其實你比較容易Google到的是這個人，對不對?
一個
這個演員啦，顯然就是個型男，跟我這種肥宅是不一樣的
所以其實如果你要問李宏毅的職業的話
可以回答演員或者是回答老師
可能都可以算是正確的答案
好那如果我們來比較一下 這個問題如果問「各個擊破」跟「一次到位」會有什麼樣的不同
如果是「各個擊破」，一開始機器會先產生
我們在講這個ChatGPT的時候告訴你說
機器實際上產生的是一個機率的分布
再從這個機率的分布裡面去做Sample
所以一開始問這個問題的時候 因為搞不清楚問的人是要問哪一個李宏毅啊
所以可能第一個答案「演」的機率也很高，「老」的機率也很高
然後接下來我們做Sample，用機率來決定要產生哪一個答案
假設現在Sample到的是「老」 因為Sample到的不一定是機率最高的嘛
所以有時候你會Sample到機率比較低的
那假設Sample到的是「老」
那接下來根據這一句話，還有已經Sample到「老」的這個部分
再去產生下一個分布的時候
「師」的機率就會很高了
因為「老」後面就是要接「師」 你已經不能夠接其他的選擇了
所以接下來可能產生的答案就是「老師」
但是如果是「一次到位」呢?
「一次到位」就是讓模型一次把所有的分布都產生出來
所以第一個產生詞彙的位置
第一個生成的位置 可能「老」的機率很高，「演」的機率也很高
第二個生成的位置 「師」的機率也很高，「員」的機率也很高
接下來再去做Sample 搞不好他第一次先Sample「老」，第二次Sample「員」
就產生「老員」，好像是一個猿猴的意思
不知道在說些什麼，總之就不是一個正確的答案
所以你會發現說在文獻上
往往「一次到位」的品質是比較差的
「各個擊破」的品質是比較好的
好，那我們現在來做成表格比較一下這兩種方法
「各個擊破」就是Autoregressive的Model 「一次到位」就是Not-autoregressive的Model
在速度上「一次到位」比較快
在品質上「各個擊破」比較好
那因為這個資料本身特性的問題，因為文字通常比較短
所以我們往往選擇「各個擊破」
影像因為他的像素實在是太多了，往往選擇「一次到位」
那講到這邊
你心裡一定會浮現的一個問題是
有兩個策略
這兩個策略能不能夠綜合使用，截長補短呢?
當然是可以的，有一系列的做法
把「各個擊破」跟「一次到位」綜合起來，截長補短
我們這邊舉兩個例子
第一個例子啊，在很早以前就已經被用在語音合成上了
算是語音合成蠻標準的一個解法
因為你要從文字合成出聲音訊號
這很複雜啊
這個聲音訊號 一秒鐘的聲音訊號裡面就有一萬六千個點
所以聲音訊號是非常複雜的
所以你要直接用「各個擊破」的方法產生這個聲音訊號
非常的困難
事實上，也有人試過用「各個擊破」的方法產生聲音訊號
有一個知名的network叫做WaveNet
它是在這個
我記得如果沒記錯的話，是2016年的時候發表出來的
那是最早直接用類神經網路生成語音的紀錄
那個時候WaveNet啊
它就是用「各個擊破」的方法，一次產生一個取樣點
所以你合個一秒鐘的聲音訊號，你可能要等個一個小時這樣
非常花時間
但是那個時候因為它合出來的聲音品質很好
雖然它很慢，但人們還是覺得很驚奇
沒想到用類神經網路真的可以產生一段聽起來品質很高的聲音訊號
這是WaveNet
那在實用上，這種模型就很難用啊
你很難 這個合一秒鐘的聲音就要等個一個小時
所以怎麼辦呢?
在語音合成領域 我們往往是把生成的過程拆成兩階段
先「各個擊破」，再「一次到位」
先用「各個擊破」產生一個中間產物
再用「一次到位」把中間產物變成16K的取樣頻率的Waveform
像這種中間產物啊
你就會讓它遠比這個聲音訊號還要更加的簡單
比如說，每秒只有100個向量
聲音訊號每秒有16,000的取樣點
但中間產物它是每秒只有100個向量
因為每秒只有100個向量 所以你可以用「各個擊破」的方法來產生出來
也許是你的computing power可以接受的
那先用「各個擊破」的方法先產生出中間產物
這些中間產物先決定好大方向
然後接下來再用「一次到位」的方法把結果產生出來
把比較複雜的聲音訊號產生出來
那因為我們知道說這種「一次到位」的方法 它壞就壞在哪裡
它雖然跑得比較快，它壞在說機器有時候會沒辦法決定一個單一的策略
沒辦法決定一個單一的方向
有時候你往左走也沒問題，往右走也沒問題 但是就是不左不右，就是會發生問題
所以先用「各個擊破」的方法先決定大方向 再用「一次到位」的方法根據大方向產生最終的聲音訊號
那這是把「各個擊破」的優勢跟「一次到位」的優勢把它結合在一起
那這個是在語音合成上非常常用的一個策略
還有另外一個策略
另外一個策略是把「一次到位」改成「N次到位」
那也等於是把「各個擊破」的方法融合進來了
比如說你要生成一張圖片
你用「一次到位」的方法產生出來的圖片可能會非常的模糊
那我們剛才是舉「一次到位」的缺點的時候
我們是舉文字上的例子 機器會把兩個答案綜合在一起
那在影像上面呢 把兩張可能產生出來的圖片綜合在一起
你往往得到的結果就是有點模糊的那種影像
所以「一次到位」產生的圖片通常沒有辦法非常的清晰
但怎麼辦?把「一次到位」改成「N次到位」
先用「一次到位」的方法呢
先生比較模糊的圖片
把模糊的圖片再通過「一次到位」的方法再變得更清楚一點
那因為大方向已經決定了
所以就不會有剛才那種
不知道要選哪一個答案 把兩個答案綜合在一起的問題
大方向已經決定得差不多
所以「一次到位」的模型就只有一個方向可以選
那可能可以產生比較好的結果
所以每次先產生模糊的圖片
再讓它越來越清楚、越來越清楚
把「一次到位」改成「N次到位」
假設你熟悉Diffusion Model的話 你可能會想到說
這不就是Diffusion Model嗎?
嗯，對，這就是Diffusion Model的基本精神
從這邊你可以了解說
為什麼Diffusion Model相較於過去其他的方法
它是一個今天特別好的模型
因為過去都是用「一次到位」的方法 那你生出來的影像往往比較模糊
而Diffusion Model把「一次到位」改成「N次到位」 它有機會產生更高清的圖片
那這邊是就概念上，講一下兩種不同的生成策略
日後我們都還會再詳細的跟大家剖析
