這邊是一個很快地介紹
Batch Normalization 這個技術
好 那你記得啊
我們之前才講過說
我們能不能夠直接改
error surface 的 landscape
我們覺得說 error surface 如果很崎嶇的時候
它比較難 train
那我們能不能夠直接把山剷平
讓它變得比較好 train 呢
Batch Normalization 就是其中一個
把山剷平的想法
好 那我記得我們在講 optimization 的時候
我們一開始就跟大家講說
不要小看 optimization 這個問題
有時候就算你的 error surface 是 confessed
它就是一個碗的形狀
都不見得很好 train
那我們舉的例子就是
假設你的兩個參數啊
它們對 Loss 的斜率差別非常大
在 W1 這個方向上面
你的斜率變化很小
在 W2 這個方向上面斜率變化很大
你今天如果是固定的 learning rate
你可能很難得到好的結果
所以我們才說你需要這個
adaptive 的 learning rate
你需要用這個 Adam 等等
比較進階的 optimization 的方法
才能夠得到好的結果
那現在我們要從另外一個方向想
直接把難做的 error surface 把它改掉
看能不能夠改得好做一點
那在做這件事之前
也許我們第一個要問的問題就是
欸 有這一種狀況
W1 跟 W2 它們的斜率差很多的這種狀況
到底是從什麼地方來的
那我們這邊就是舉一個例子
假設我現在有一個非常非常非常簡單的 model
它的輸入是 x1 跟 x2
x1 跟 x2 它對應的參數就是 W1 跟 W2
它是一個 linear 的 model
沒有 activation function
W1 乘 x1
W2 乘 x2 加上 b 以後就得到 y
然後會計算 y 跟 y hat 之間的差距當做 e
把所有 training data e 加起來呢
就是你的 Loss
那你希望去 minimize 你的 Loss
那什麼樣的狀況我們會產生像上面這樣子
比較不好 train 的 error surface 呢
我們來看一下這個 W1 啊
如果今天 W1 有改變的時候
它對 Loss 的變化我們要怎麼看呢
當我們對 W1 有一個小小的改變
比如說加上 delta W1 的時候
那這個 L 也會有一個改變
那這個 W1 呢
是透過 W1 改變的時候
你就改變了 y
y 改變的時候你就改變了 e
然後接下來就改變了 L
所以當 W1 改變的時候 L 就跟著改變了
那什麼時候 W1 的改變會對 L 的影響很小呢
什麼時候 W1 這邊的變化
它在 error surface 上的斜率會很小呢
一個可能性是當你的 input 很小的時候
假設 x1 的值都很小
假設 x1 的值在不同的 training example 裡面
它的值都很小
那因為 x1 是直接乘上 w1
如果 x1 的值都很小
W1 有一個變化的時候
它得到的
它對 y 的影響也是小的
對 e 的影響也是小的
它對 L 的影響就會是小的
所以如果 W1 接的 input 它的值都很小
那就會產生這邊這樣的 case
你在 W1 上面的變化對大 L 的影響是小的
反之呢
如果今天是 x2 的話
假設 x2 它的值都很大
那假設 x2 的值都很大
當你的 W2 有一個小小的變化的時候
雖然 W2 這個變化可能很小
但是因為它乘上了 x2
x2 的值很大
那 y 的變化就很大
那 e 的變化就很大
那 L 的變化就會很大
就會導致我們在 w 這個方向上
做變化的時候
我們把 w 改變一點點
那我們的 error surface 就會有很大的變化
所以你發現說
既然在這個 linear 的 model 裡面
當我們 input 的 feature
每一個 dimension 的值啊
它的 scale 差距很大的時候
我們就可能產生像這樣子的 error surface
就可能產生不同方向
它的斜率非常不同的
它的坡度非常不同的 error surface
所以怎麼辦呢
我們有沒有可能給不同的 dimension
feature 裡面不同的 dimension
讓它有同樣的數值的範圍
如果我們可以給不同的 dimension
同樣的數值範圍的話
那我們可能就可以製造比較好的 error surface
讓 training 變得比較容易一點
好 那怎麼讓不同的 dimension
有類似的 有接近的數值的範圍呢
其實有很多不同的方法
那這些不同的方法啊
往往就合起來統稱為
Feature Normalization
那我以下所講的方法只是
Feature Normalization 的一種可能性
它並不是 Feature Normalization 的全部
好 你可以怎麼做呢
你可以說假設 x1 到 xR
是我們所有的訓練資料的 feature vector
我們把所有訓練資料的 feature vector 呢
統統都集合起來
那每一個 vector 呢
x1 裡面就 x 上標 1 下標 1
代表 x1 的第一個 element
x 上標 2 下標 1
就代表 x2 的第一個 element
以此類推
好 那我們把同一個 dimension 啊
不同筆資料
不同 feature vector
同一個 dimension 裡面的數值
把它取出來
然後去計算某一個 dimension 的 mean
那我們現在計算的是第 i 個 dimension
而它的 mean 呢 就是 mi
那我計算的
我們計算第 i 個 dimension 的
standard deviation
我們用 Σ i 來表示它
那接下來我們就可以做一種 normalization
那這種 normalization 呢
其實叫做標準化
其實叫 standardization
不過我們這邊呢
就等一下都統稱 normalization 就好了
好 那我們怎麼做 normalization 呢
我們就是把這個 x 啊
把這邊的某一個數值啊
減掉這一個 dimension 算出來的 mean
再除掉這個 dimension
算出來的 standard deviation
減掉 mean
除掉 standard deviation
得到新的數值叫做 x tilde
然後得到新的數值以後
再把新的數值把它塞回去
我們等一下呢
都用這個 tilde 呢
來代表有被 normalize 後的數值
好 那做完 normalize 以後有什麼好處呢
做完 normalize 以後啊
這個 dimension 上面的數值就會平均是 0
然後它的 variance 呢 就會是 1
好 所以這一排數值
它的分布就都會在 0 上下
那你對每一個 dimension
每一個 dimension
都做一樣的事情
都做一樣的 normalization
把他們變成 mean 呢 接近 0
variance呢 是 1
那你就會發現說所有的數值
所有 feature 不同 dimension 的數值
都在 0 上下
那你可能就可以製造一個
比較好的 error surface
所以像這樣子 Feature Normalization 的方式
往往對你的 training 有幫助
它可以讓你在做 gradient descent 的時候
這個 gradient descent
它的 Loss 收斂更快一點
可以讓你的 gradient descent
它的訓練更順利一點
這個是 Feature Normalization
好 所以當然 Deep Learning
可以做 Feature Normalization
你可能會把 feature 做 normalize 以後
其實在助教的 code 裡面
我們都有對 feature 做 normalization
得到 x tilde 以後呢
這個是 x tilde 代表 normalize 的 feature 以後
把它丟到 deep network 裡面
去做接下來的計算
去做接下來的訓練
所以把 x1 tilde 通過第一個 layer 得到 z1
那你有可能通過 activation function
不管是選 Sigmoid 或者 ReLU 都可以
然後再得到 a1
然後再通過下一層等等
那就看你有幾層 network 你就做多少的運算
所以每一個 x 都做類似的事情
但是如果我們進一步來想的話
對 W2 來說
這邊的 a1 a3 這邊的 z1 z3
其實也是另外一種 input
如果這邊 x tilde
雖然它已經做 normalize 了
但是通過 W1 以後它就沒有做 normalize 啦
如果 x tilde 通過 W1 得到是 z1
而 z1 不同的 dimension 間
它的數值的分布仍然有很大的差異的話
那我們要 train W2 第二層的參數
會不會也有困難呢
所以這樣想起來
我們也應該要對這邊的 a 或對這邊的 z
做 Feature Normalization
對 W2 來說
這邊的 a 或這邊的 z 其實也是一種 feature
我們應該要對這些 feature 也做 normalization
但這邊有人就會問一個問題
應該要在 activation function 之前
做 normalization
還是要在 activation function 之後
做 normalization 呢
在實作上這兩件事情其實差異不大
你可以自己在作業裡面試試看
這兩件事情有沒有差異
它們的差異是不大的
所以你對 z 做 Feature Normalization
或對 a 做 Feature Normalization
其實都可以啦
那如果你選擇的是 Sigmoid
那可能比較推薦對 z 做 Feature Normalization
因為你知道 Sigmoid 是一個 s 的形狀嘛
那它在 0 附近斜率比較大
所以如果你對 z 做 Feature Normalization
把所有的值都挪到 0 附近
那你到時候算 gradient 的時候
算出來的值會比較大
那不過因為你不見得是跟 sigmoid 嘛
所以你也不一定要把 Feature Normalization
放在 z 這個地方
如果是選別的
也許你選 A 會
也會有好的結果
也說不定
總之
Ingeneral 而言
這個 normalization
要放在 activation function 之前
或之後都是可以的
在實作上
可能沒有太大的差別
好 那我們這邊呢
就是對 z 呢
做一下 Feature Normalization
好 那怎麼對 z 做 Feature Normalization 呢
那你就把 z
想成是另外一種 feature 嘛
我們這邊有 z1 z2 z3
我們就把 z1 z2 z3 拿出來
算一下它的 min
怎麼算 min 呢
這邊的 μ 啊
它是一個 vector
我們就把 z1 z2 z3
這三個 vector 呢
把它平均起來
得到 μ 這個 vector
那我們也算一個 standard deviation
這個 standard deviation 呢
這邊這個成 Σ
它也代表了一個 vector
那這個 vector 怎麼算出來呢
你就把 zi 減掉 μ
然後取平方
這邊的平方
這個 notation 有點 abuse 啊
這邊的平方就是指
對每一個 element 都去做平方
然後再開根號
這邊開根號指的是對每一個 element
向量裡面的每一個 element
都去做開根號
得到 Σ
反正你知道我的意思就好
就把這三個 vector
裡面的每一個 dimension
都去把它的 μ 算出來
把它的 Σ 算出來
好 我這邊呢
就不把那些箭頭呢 畫出來了
從 z1 z2 z3
算出 μ
算出 Σ
好 接下來呢
你就把這邊的每一個 z 啊
都去減掉 μ 除以 Σ
你把 zi 減掉 μ
除以 Σ
就得到 zi 的 tilde
那這邊的 μ 跟 Σ
它都是向量啦
所以這邊這個除呢
它的 notation 有點 abuse
我這邊的除的這個意思是說
element wise 的相除
就是 zi 減 μ
它是一個向量
所以分子的地方是一個向量
分母的地方也是一個向量
把這個兩個向量
它們對應的 element 的值相除
是我這邊這個除號的意思
這邊得到 Z 的 tilde
好 所以我們就是把 z1 減 μ 除以 Σ
得到 z1 tilde
同理 z2 減 μ 除以 Σ
得到 z2 tilde
z3 減 μ 除以 Σ
得到 z3 tilde
那就把這個 z1 z2 z3
做 Feature Normalization
變成 z1 tilde
z2 tilde 跟 z3 的 tilde
好 那接下來呢
要做什麼
接下來就看你愛做什麼 就做什麼啦
通過 activation function
得到其他 vector
然後再通過
再去通過其他 layer 等等
這樣就可以了
這樣你就等於對 z1 z2 z3
做了 Feature Normalization
變成 z1 tilde z2 tilde z3 tilde
在這邊有一件有趣的事情
這件事情是這樣子的
這邊的 μ 跟 Σ
它們其實都是根據 z1 z2 z3 算出來的
所以這邊 z1 啊
它本來
如果我們沒有做 Feature Normalization 的時候
你改變了 z1 的值
你會改變這邊 a 的值
但是現在啊
當你改變 z1 的值的時候
μ 跟 Σ 也會跟著改變
μ 跟 Σ 改變以後
z2 的值 a2 的值
z3 的值 a3 的值
也會跟著改變
所以之前啊
我們每一個 x1 tilde x2 tilde x3 tilde
它是獨立分開處理的
但是我們在做 Feature Normalization 以後
這三個 example
它們變得彼此關聯了
我們這邊 z1 只要有改變
接下來 z2 a2 z3 a3
也都會跟著改變
所以這邊啊
其實你要把
當你有做 Feature Normalization 的時候
你要把這一整個 process
就是有收集一堆 feature
把這堆 feature 算出 μ 跟 Σ 這件事情
當做是 network 的一部分
也就是說
你現在有一個比較大的 network
你之前的 network
都只吃一個 input
得到一個 output
現在呢
你有一個比較大的 network
這個大的 network
它是吃一堆 input
用這堆 input 在這個 network 裡面
要算出 μ 跟 Σ
然後接下來產生一堆 output
那這個地方比較抽象
不知道大家有沒有
希望你聽得懂
如果你覺得有困惑的話
你可以等一下詢問
或者是在討論版上發問
那這一段呢
我覺得
只可會意 不可言傳這樣子
不知道你聽不聽得懂這一段的意思
就是現在不是
一個 network 處理一個 example
而是有一個巨大的 network
它處理一把 example
用這把 example
還要算個 μ 跟 σ
得到一把 output
那這邊就會有一個問題了
因為你的訓練資料裡面
你的 data 非常多啊
現在一個 data set
benchmark corpus 都上百萬筆資料啊
你哪有辦法一次把上百萬筆資料
丟到一個 network 裡面
你這個 GPU 的 memory 根本無法
這個
這個沒電了
好 麥克風沒電了
好
好 那你那個 GPU 的 memory
根本沒有辦法
把它整個 data set 的 data 都 load 進去啊
所以怎麼辦
在實作的時候
你不會讓這一個 network 考慮
整個 training data 裡面的所有 example
你只會考慮一個 batch 裡面的 example
舉例來說
你 batch 設 64
那你這個巨大的 network
就是把 64 筆 data 讀進去
算這 64 筆 data 的 μ
算這 64 筆 data 的 σ
對這 64 筆 data 都去做 normalization
因為我們在實作的時候
我們只對一個 batch 裡面的 data
做 normalization
所以這招叫做 Batch Normalization
這個就是你常常聽到的
Batch Normalization
那這個 Batch Normalization
顯然有一個問題 就是
你一定要有一個夠大的 batch
你才算得出 μ 跟 σ
假設你今天
你 batch size 設 1
那你就沒有什麼 μ 或 σ 可以算
你就會有問題
所以這個 Batch Normalization
是適用於 batch size 比較大的時候
那我們因為 batch size 如果比較大
也許這個 batch size 裡面的 data
就足以表示
整個 corpus 的分布
那這個時候你就可以
把這個本來要對整個 corpus
做 Feature Normalization 這件事情
改成只在一個 batch
做 Feature Normalization
作為 approximation
好 那在做 Batch Normalization 的時候
往往還會有這樣的設計
你算出這個 z tilde 以後啊
接下來你會把這個 z tilde
再乘上另外一個向量
叫做 γ
這個 γ 也是一個向量
所以你就是把 z tilde 跟 γ 呢
做 element wise 的相乘
把 z 這個向量裡面的 element
跟 γ 這個向量裡面的 element
兩兩做相乘
再加上 β 這個向量
得到 z hat
而 β 跟 γ
你要把它想成是 network 的參數
它是另外再被認出來的
那為什麼要加上 β 跟 γ 呢
那是因為有人可能會覺得說
如果我們做 normalization 以後
那這邊的 z tilde
它的平均呢
就一定是 0
那也許
今天如果平均是 0 的話
就是給那 network 一些限制嘛
那也許這個限制會帶來什麼負面的影響
所以我們把 β 跟 γ 加回去
然後讓 network 呢
現在它的 hidden layer 的 output 呢
不需要平均是 0
如果它想要不平均是
平均不是 0 的話
他就自己去認這個 β 跟 γ
來調整一下輸出的分布
來調整這個 z hat 的分布
但講到這邊又會有人問說
欸 這個
剛才不是說做 Batch Normalization 就是
為了要讓每一個不同的 dimension
它的 range 都是一樣
我們才做這個 normalization 嗎
現在如果加去乘上 γ
再加上 β
把 γ 跟 β 加進去
這樣不會不同 dimension 的分布
它的 range 又都不一樣了嗎
有可能
但是實際上你在做的時候啊
你實際上在訓練的時候
這個 γ 跟 β 的初始值啊
你會把這個 γ 的初始值 就都設為 1
所以 γ 是一個裡面的值
一開始其實是一個裡面的值
全部都是 1 的向量
那 β 是一個裡面的值
全部都是 0 的向量
所以 γ 是一個 one vector
都是 1 的向量
β 是一個 zero vector
裡面的值都是 0 的向量
所以讓你的 network 在一開始訓練的時候
每一個 dimension 的分布
是比較接近的
也許訓練到後來
你已經訓練夠長的一段時間
已經找到一個比較好的 error surface
走到一個比較好的地方以後
那再把 γ 跟 β 慢慢地加進去
好 所以加 Batch Normalization
往往對你的訓練是有幫助的
好 那接下來就要講 testing 的部分了
剛才講的都是 training 的部分
還沒有講到 testing 的部分
testing 啊
有時候又叫 inference 啊
所以有人在文件上看到有人說
做個 inference
inference 指的就是 testing
這個 Batch Normalization 在 inference
或是 testing 的時候呢
會有問題啊
會有什麼樣的問題呢
在 testing 的時候
如果 當然如果今天你是在做作業
我們一次會把所有的 testing 的資料給你
所以你確實也可以在 testing 的資料上面
製造一個一個 batch
但是假設你真的有系統上線
你是一個真正的線上的 application
你可以說
我今天一定要等 30
比如說你的 batch size 設 64
我一定要等 64 筆資料都進來
我才一次做運算嗎
這顯然是不行的 對不對
如果你是一個線上的服務
一筆資料做進來
一筆資料進來
你就要每次都做運算
你不能等說
我累積了一個 batch 的資料
才開始做運算
但是在做 Batch Normalization 的時候
我們今天呢
一個 x tilde
一個 normalization 過的 feature 進來
然後你有一個 z
你的 z 呢
要減掉 μ 跟除 Σ
那這個 μ 跟 Σ
是用一個 batch 的資料算出來的
但如果今天在 testing 的時候
根本就沒有 batch
那我們要怎麼算這個 μ
跟怎麼算這個 Σ 呢
好 所以真正的
這個實作上的解法是這個樣子的
如果你看那個 PyTorch 的話呢
Batch Normalization 在 testing 的時候
你並不需要做什麼特別的處理
PyTorch 幫你處理好了
PyTorch 是怎麼處理這件事的呢
他說
在 training 的時候
如果你有在做 Batch Normalization 的話
在 training 的時候
你每一個 batch 計算出來的 μ 跟 Σ
他都會拿出來算 moving average
什麼意思呢
你每一次取一個 batch 出來的時候
你就會算一個 μ1
取第二個 batch 出來的時候
你就算個 μ2
一直到取第 t 個 batch 出來的時候
你就算一個 μ t
接下來你會算一個 moving average
也就是呢
你會把你現在算出來的 μ 的一個平均值
叫做 μ bar
乘上某一個 factor
那這也是一個常數
這個這也是一個 constant
這也是一個那個 hyper parameter
也是需要調的那種啦
那在 PyTorch 裡面
我沒記錯 他就設 0.1
我記得他 P 就設 0.1
好
然後加上 1 減 P
乘上 μ t
然後來更新你的 μ 的平均值
然後最後在 testing 的時候
你就不用算 batch 裡面的 μ 跟 Σ 了
因為 testing 的時候
在真正 application 上
也沒有 batch 這個東西
你就直接拿 μ bar 跟 Σ bar
也就是 μ 跟 Σ 在訓練的時候
得到的 moving average
μ bar 跟 Σ bar
來取代這邊的 μ 跟 Σ
這個就是 Batch Normalization
在 testing 的時候的運作方式
好 那這個是從 Batch Normalization
原始的文件上面截出來的一個實驗結果
那在原始的文件上還講了很多其他的東西
舉例來說
我們今天還沒有講的是
Batch Normalization 用在 CNN 上
要怎麼用呢
那你自己去讀一下原始的文獻
裡面會告訴你說
Batch Normalization 如果用在 CNN 上
應該要長什麼樣子
好 那這個是原始文獻上面截出來的一個數據
那這個橫軸呢
代表的是訓練的過程
縱軸呢
代表的是 validation set 上面的 accuracy
那這個黑色的虛線是
沒有做 Batch Normalization 的結果
它用的是 inception 的 network
就是某一種 network 架構啦
也是以 CNN 為基礎的 network 架構
總之黑色的這個虛線
它代表沒有做 Batch Normalization 的結果
然後如果有做 Batch Normalization
你會得到紅色的這一條虛線
那你會發現說
紅色這一條虛線
它訓練的速度
顯然比黑色的虛線還要快很多
雖然最後收斂的結果啊
就你只要給它足夠的訓練的時間
可能都跑到差不多的 accuracy
但是紅色這一條虛線
可以在比較短的時間內
就跑到一樣的 accuracy
那這邊這個藍色的菱形
代表說這幾個點的那個 accuracy 是一樣的啦
那紅色的
大概在一半以內的時間
相較於沒有做 Batch Normalization
只需要一半
或甚至更少的時間
就跑到同樣的正確率了
那這邊還有別的線啦
這邊有一個粉紅色的線
粉紅色的線是什麼呢
粉紅色的線是 sigmoid function
就 sigmoid function 一般的認知
我們雖然還沒有討論這件事啦
但一般都會選擇 ReLu
而不是用 sigmoid function
因為 sigmoid function
它的 training 是比較困難的
但是這邊想要強調的點是說
就算是 sigmoid 比較難搞的
加 Batch Normalization
還是 train 的起來
那這邊沒有 sigmoid
沒有做 Batch Normalization 的結果
因為在這個實驗上
作者有說
sigmoid 不加 Batch Normalization
根本連 train 都 train 不起來
好 那這邊還有這個藍色的實線
藍色的實線跟這個藍色的虛線呢
是把 learning rate 設比較大一點
乘 5
就是 learning rate 變原來的 5 倍
然後乘 30
就是 learning rate 變原來的 30 倍
那因為如果你做 Batch Normalization 的話
那你的 error surface 呢
會比較平滑 比較容易訓練
所以你可以把你的比較不崎嶇
所以你就可以把你的 learning rate 呢
設大一點
那這邊有個不好解釋的奇怪的地方
就是不知道為什麼
learning rate 設 30 倍的時候
是比 5 倍差啦
那作者也沒有解釋啦
你也知道做 deep learning 就是
有時候會產生這種怪怪的
不知道怎麼解釋的現象就是了
不過作者就是照實
把他做出來的實驗結果
呈現在這個圖上面
好 接下來的問題就是
Batch Normalization
它為什麼會有幫助呢
在原始的 Batch Normalization
那篇 paper 裡面
他提出來一個概念
叫做 internal 的 covariate shift
covariate shift 這個詞彙是原來就有的
internal covariate shift
我認為是
Batch Normalization 的作者自己發明的
他想了
他認為說今天在 train network 的時候
會有以下這個問題
這個問題是這樣
network 有很多層
x 通過第一層以後 得到 a
a 通過第二層以後 得到 b
那我們今天計算出 gradient 以後
把 A update 成 A′
把 B 這一層的參數 update 成 B′
但是作者認為說
現在我們在把 B update 到 B′ 的時候
那我們在計算 B
update 到 B′ 的 gradient 的時候
這個時候前一層的參數是 A 啊
或者是前一層的 output 是小 a 啊
那當前一層從 A 變成 A′ 的時候
它的 output 就從小 a 變成小 a′ 啊
但是我們計算這個 gradient 的時候
我們是根據這個 a 算出來的啊
所以這個 update 的方向
也許它適合用在 a 上
但不適合用在 a′ 上面
那如果說 Batch Normalization 的話
我們會讓
因為我們每次都有做 normalization
我們就會讓 a 跟 a′ 呢
它的分布比較接近
也許這樣就會對訓練呢
有幫助
但是有一篇 paper 叫做
How Does Batch Normalization
Help Optimization
然後他就打臉了
internal covariate shift 的這一個觀點
在這篇 paper 裡面
他從各式各樣的面向來告訴你說
internal covariate shift
首先它不一定是 training network 的
train network 的時候的一個問題
然後 Batch Normalization
它會比較好
可能不見得是因為
它解決了 internal covariate shift
那在這篇 paper 裡面呢
他做了很多很多的實驗
比如說他比較了訓練的時候
這個 a 的分布的變化 發現
不管有沒有做 Batch Normalization
它的變化都不大
然後他又說
就算是變化很大
對 training 也沒有太大的傷害
然後他又說
不管你是根據 a 算出來的 gradient
還是根據 a′ 算出來的 gradient
方向居然都差不多
所以他告訴你說
internal covariate shift
可能不是 training network 的時候
最主要的問題
它可能也不是
Batch Normalization 會好的一個的關鍵
那有關更多的實驗
你就自己參見這篇文章
好 那
為什麼 Batch Normalization 會比較好呢
那在這篇 How Does Batch Normalization
Help Optimization 這篇論文裡面
他從實驗上
也從理論上
至少支持了 Batch Normalization
可以改變 error surface
讓 error surface 比較不崎嶇這個觀點
所以這個觀點是有理論的支持
也有實驗的佐證的
那在這篇文章裡面呢
作者還講了一個非常有趣的話
他說他覺得啊
這個 Batch Normalization 的 positive impact
因為他說
如果我們要讓 network
這個 error surface 變得比較不崎嶇
其實不見得要做 Batch Normalization
感覺有很多其他的方法
都可以讓 error surface 變得不崎嶇
那他就試了一些其他的方法
發現說
跟 Batch Normalization performance 也差不多
甚至還稍微好一點
所以他就講了下面這句感嘆
他覺得說
這個
positive impact of batchnorm on training
可能是 somewhat
serendipitous
什麼是 serendipitous 呢
這個字眼可能可以翻譯成偶然的
但偶然並沒有完全表達這個詞彙的意思
這個詞彙的意思是說
你發現了一個什麼意料之外的東西
舉例來說
盤尼西林就是
意料之外的發現
大家知道盤尼西林的由來就是
有一個人叫做弗萊明
然後他本來想要那個
培養一些葡萄球菌
然後但是因為他實驗沒有做好
他的那個葡萄球菌被感染了
有一些黴菌掉到他的培養皿裡面
然後發現那些培養皿
那些黴菌呢
會殺死葡萄球菌
所以他就發明了
發現了盤尼西林
所以這是一種偶然的發現
那這篇文章的作者也覺得
Batch Normalization 也像是盤尼西林一樣
是一種偶然的發現
但無論如何
它是一個有用的方法
好 那其實 Batch Normalization
不是唯一的 normalization
normalization 的方法有一把啦
那這邊就是列了幾個比較知名的
