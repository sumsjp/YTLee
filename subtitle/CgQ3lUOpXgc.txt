有很多同學問我對於GPT-4o有什麼看法
所以我今天來講一下
GPT-4o背後可能的語音技術
這個影片的目標聽眾是
語音技術背景的聽眾
如果你已經是語音領域的專家的話
這部影片講的內容對你來說
可能就太科普了
先說明一下我假設聽這個影片的同學
是有修過生成式人工智慧導論的
如果你還沒有看過這堂課的影片的話
你可以在這個連結找到這堂課的影片
那如果你看過這堂課的影片的話
可以讓你更瞭解
我現在這部影片裡面
所要講的內容
那GPT-4o其中一項
特別令人
矚目的技術
就是語音互動的功能
Google
也釋出了Project ASTRA的結果
那在Project ASTRA裡面
也打造了語音互動的技術
那這個代表說現在語音互動的技術
是各個大廠都非常重視的一塊
那GPT-4O的語音模式Voice Mode
有什麼樣特別的地方呢
第一個是它有很豐富的語音風格
講到這個語音合成
大家往往想到的就是Google小姐
那Google小姐就是一種語調
但是GPT-4o你可以用文字的指令
要求他用不同的語調來說話
比如說叫他講快一點
叫他輕聲細語
或甚至叫他用唱的
把他想要講的事情表達出來
另外GPT-4o好像可以理解
語音內容以外的資訊
用比較白話的講法就是
他能夠察言觀色
舉例來說,他可能可以聽得到一個人的喘氣聲,知道一個人氣喘如牛
那他可以發出非語音性的聲音,例如笑聲
那如果你有看GPT-4o的Demo的話
你會發現這是一個非常愛笑的模型
他動不動就會笑
另外GPT-4o有自然而即時的互動
在其中的Demo裡面有一個人說
我們來做一個有趣的嘗試
話還沒有說完
GPT-4o居然說了一聲哇
然後那個人再說我要你去做某一件事情
如果還沒有看過OpenAI的Demo
你可以先看一下OpenAI的Demo
再來繼續這部影片
那首先我想要澄清一個
很多人對GPT-4o語音模式的誤解
那我這部影片呢
是在5月19號的晚上錄製的
其實到5月19號的晚上為止
多數人應該都是沒有GPT-4o的語音模式的
很多人會想說
我有啊
我可以用語音跟我的ChatGPT互動
那個語音互動
是ChetGPT手機版本來就有的語音介面
ChetGPT他的手機版本來就可以用語音互動
他的互動方式是說
這個是你的ChetGPT的頁面
那GPT 4.0已經釋出了
你可以在右下角點這個語音的這個鈕
然後你就會看到一個這樣子的畫面
那你確實可以透過這個介面
跟ChetGPT用語音溝通
但是如果你有實際嘗試的話
你會發現這一個語音的介面
並沒有你在GPT-4O語音模式的演示裡面
看到的種種神奇功能
比如說用不同的語氣講話
比如說你可以打斷他說話等等
那事實上呢
事實上大家可能都有收到一則OpenAI給的訊息
就是其實VoiceMode還沒有真的釋出
它只是coming soon
它就會在接下來的數週慢慢的被釋出來
所以大家所使用的手機版的語音介面
其實可能還不是GPT-4O的語音模式
那舊版的語音介面是怎麼做的呢
舊版的語音介面是先把語音訊號透過語音辨識變成文字
再把文字丟給語言模型
比如說 ChatGPT
然後語言模型會給一個回應
這個回應透過語音合成產生語音訊號
那其實如果語音辨識跟語音合成跑得夠快的話
其實這個介面也可以達到蠻自然即時的互動功能的
但是跟這個OpenAI GPT-4語音模式的demo比起來
還是有一段差距
比如說呢
這個如果是透過語音辨識把聲音訊號轉成文字
那語言模型可能會不知道語者的情緒
語音合成如果只是把語言模型的輸出轉成聲音訊號
那語音合成就只有某一種說話的風格而已
那我在看這個GPT-4-O的語音介面的Demo的時候
我其實在想它會不會只是把原來的系統
加上了一些額外的模組
比如說在語音辨識之外
我們可以加一些語音事件偵測或情緒辨認的模組
所以我們也有機會透過情緒辨認的模組
也有機會把聲音訊號的情緒把它偵測出來
那你可以把偵測出的情緒放在語音辨識的結果後面
再去丟給語言模型
那這樣語言模型也有機會知道這句話的情緒
那我這邊引用一些論文
這些論文是說你可以加一些額外的模組
來幫助語言模型瞭解現在輸入語音的情緒
那另外你也可以讓語言模型除了文字之外
多輸出一些符號
比如說語言模型在他的輸出的文具後面加一個括號笑
再丟給語音合成系統
搞不好語音合成系統就可以產生笑聲
你可能會問說這個語音合成系統可以看到這個括號笑就產生笑聲嗎?
有一些語音合成系統是真的可以的
比如說Zuno AI所開發的Bark
你在你的文具裡面打一個笑
這個語音合成模型
這個語音合成的結果是真的可以產生笑聲的
另外一方面,ChatGPT可能也可以產生一段指令
告訴語音合成系統說
現在應該要用什麼樣的語氣來進行合成
語音合成系統可以看得懂這些文字的指令嗎?
其實很多語音合成系統是可以看得懂文字指令的
舉例來說,一個具代表性的例子
就是Meta開發的AudioBox
所以用很多現有的技術串接起來
其實也不是沒有可能打造GPT-4O的語音介面
只是說把很多系統串接起來
那可能即時性不是非常的好
需要透過大量工程的手段
來加快模型運作的速度
來達到真正即時的回應
不過GPT-4O的Blog已經告訴我們
它的語音模式是一個end-to-end的模型
也就是他只有用一個模型來處理所有的事情
也就是說他用的不是像上一個投影片一樣
一個非常複雜的架構
而是只有一個單一的模型
這個單一的模型聽語音訊號作為輸入
然後產生對應的輸出
那這個模型其實是一個多模態的模型
也就是他是可以看圖片看影像的
但以下的討論,我們只集中在聲音的部分
這邊先說一個免責聲明
本影片主要的目的是討論技術
那沒有要對任何群體或個人造成傷害的意圖
那我目前還沒有GPT 4.0的Voice Mode
所以我對GPT 4.0的理解
是完全來自OpenAI的展示
那至於實際上真的Voice Mode釋出以後
有沒有展示那麼厲害
我們就拭目以待
那OpenAI到目前為止都沒有發表跟GPT-4O相關的論文或者是技術報告
以下的內容完全是根據我知道的技術進行臆測
所以以下我對於GPT-4O背後技術的猜想是完全沒有根據的
只是並沒有任何的論文可以佐證我的猜測
那我只是想要跟大家討論一下根據今天語音技術的發展
有什麼樣可能的技術可以達GPT-4O這樣的效果
如果日後發現實際的技術跟我的臆測有差異
還請大家見諒
好,那在開始講這個
End-to-End的語音模型之前
我們先來複習一下
這個文字的語言模型
是怎麼被訓練出來的
那我們知道文字的語言模型
它的訓練呢
有三個步驟
第一個步驟是Pretrain
用大量沒有標註的資料進行訓練
然後接下來做Finetune
用少量有標註的資料微調
然後做RLHF
用使用者回饋的資料進行微調
那Finetune跟RLHF合起來又叫做Alignment
那如果你對於這三個步驟還不熟悉的話
你可以看一下過去我上課的錄影
你也可以看一下我最近釋出的影片
講這個生成式人工智慧的生成策略
瞭解生成式人工智慧的基本原理
再進行以下的課程
好那語音版的語言模型
它可能的運作原理是什麼樣子的呢
我們知道語言模型就是做文字接龍
給他一個未完成的句子
他猜接下來應該輸出哪一個文字的token
語音版的語言模型可以聽一句話給你一個回應
他背後運作的邏輯可能也非常的類似
他做的事情可能就是聽一段聲音
然後去猜接下來應該產生什麼樣的聲音
也就是他做的事情是聲音接龍
雖然從表面上看起來語言模型跟語音版的語言模型
他們的運作可能非常的類似
但是語音版的語言模型有它獨特的挑戰
第一個挑戰就是這個語音相較於文字
它更為複雜
那對一段聲音訊號而言如果它的Sampling Rate是16kHz的話
意思就是一秒鐘的聲音訊號裡面
由16000個取樣點所構成
也就是一秒鐘的聲音訊號
是由一萬六千個數值所構成的
那所以聲音訊號非常的複雜
如果我們今天在做這個聲音接龍的時候
是讓聲音訊號一個一個取樣點跑出來
那你要產生一秒鐘的聲音訊號
那你就要跑一萬六千次的聲音訊號接龍
這顯然會非常的花時間
所以現在在做這種語音的接龍呢
通常不是直接在語音的訊號上面做接龍
比較常見的做法是
先把聲音的訊號呢進行壓縮
你會有一個編碼器
它的英文呢是encoder
那這個編碼器裡面呢
可能會有一個codebook
這個codebook裡面是一大堆的code
所謂的code每一個code代表某種類型的聲音
可能某一個code代表人類發的/b/這個聲音
那有個code代表人類的笑聲
有的code甚至代表不是人類的聲音
比如說狗叫聲等等
所以一段聲音訊號輸入給編碼器
編碼器會用它裡面的codebook
來表示這一段聲音訊號
所以一段聲音訊號會被表示成一個code sequence
會表示成一個code的序列
那這些code又被叫做speech unit
他們是可以看作是聲音的單位
那你還會訓練一個decoder
訓練一個解碼器
這個解碼器可以讀這些speech的unit
把這些speech的unit轉回聲音訊號
一般這些編碼器跟解碼器也是類神經網路
也是需要透過訓練資料來進行訓練的
如果你想要知道更多有關這種
把聲音訊號變成 Speech Unit
再把 Speech Unit 解回來相關的技術的話
可以閱讀一下這兩篇文章
那語音版的語言模型呢
是運作在這一些Speech Unit上面的
也就是說當一段聲音訊號進來的時候
語音版的語言模型並不是直接對這段聲音訊號去做語音接龍
這些聲音訊號會先通過Encoder
變成一串Unit
然後呢這些Unit會變成語音版語言模型的輸入
語音版語言模型要做的事情
是預測下一個Unit會是什麼
然後下一個Unit會再通過Decoder產生聲音訊號
所以語音版語言模型沒有必要產生複雜的聲音訊號
它只需要產生Unit
然後由Decoder把Unit轉回複雜的聲音訊號
所以語音版的語言模型是運作在Speech Unit
這種壓縮過的東西上面
針對語音訊號壓縮過的這些東西上面
那有很多具代表性的語音版的語言模型
比如說Meta的GSLM還有Google的Audio LN
那這其實不是非常新的技術
GSLM是在21年的年初的時候就已經有的
其實我們也可以把解碼器呢
看作是一種非常特殊的語音辨識
只是它的輸出不是文字而是speech的unit
我們可以把解碼器看成是一種特殊的語音合成
只是它的輸入不是文字而是speech的unit
它不是把文字變成語音
而是把speech的unit變成語音
那你可能會想說
那為什麼我們不直接套用語音辨識跟語音合成
當作編碼器跟解碼器呢
其實我們確實可以把語音辨識
想成是一種非常特別的壓縮方式
當我們把聲音訊號變成文字的時候
其實也是在做某種壓縮
文字本來就可以看作是語音的濃縮版
但是這邊的問題是
文字只代表了語音裡面某個面向的資訊
文字只代表了語音裡面內容的資訊
假設今天輸的聲音訊號是有一個人說好好笑
接下來哈哈哈哈了幾聲
語音辨識系統可能只能夠把
代表文字的好好笑辨識出來
那笑聲就不見了
接下來你用語音合成把好好笑還原回聲音的時候
其實笑聲的資訊就丟掉了
那用Speech Unit的好處是
它可能可以保留文字
所無法表達的資訊
但是另外一方面
用這種Speech Unit也有一個壞處
就是Speech Unit裡面
可能很多Unit它本來就是對應到
某個文字的token
那你等於是要這個編碼器呢
去重造輪子
本來就已經有文字
可以表示語音裡面的某些訊號
編碼器需要再用另外新的符號
來表示這些
我們本來就有文字符號
可以描述的語音資訊
那這樣感覺是重造了輪子
所以另外一個可能的做法是把編碼器
跟語音辨識結合
那我這邊把它叫做混合編碼器
把解碼器跟語音合成結合
這邊叫做混合的解碼器
也就是對一段聲音訊號而言
能夠做語音辨識的部分
能夠辨識成文字的部分
把它用文字來表示
那無法用文字來表示的部分
就用 Speech Unit 來表示
所以一段聲音訊號好好笑
後面接哈哈哈哈
那好好笑的部分
可能可以用文字來表示
那笑聲的部分就拿一個特殊的符號來表示
那混合解碼器可以看文字跟特殊的符號
把它轉回聲音訊號
我不知道GPT-4O會不會採取這樣混合的策略啦
也許GPT-4O也是使用編碼器而已
並沒有用語音辨識
那我只是覺得用混合的編碼器有它額外的好處
那這個部分我們等一下會再提到
那另外呢,我們這邊還需要一個
語者自動分段標記的技術
那這樣的技術呢,叫做Speaker Dialerization
如果你看GPT-4-O的Demo的話
當有兩個以上的人跟他說話的時候
其實他是知道的,他可以知道誰是誰
所以他應該需要用到Speaker Dialerization的技術
Speaker Dialerization的系統
語者自動分段標記的系統就給他一段聲音訊號
他可以知道哪些段落是某一個同一個語者講的
他可以知道說這一段跟這一段都是語者A講的
這一段是語者B講的
所以我猜測一段聲音訊號
對於一個語音版的語言模型來說
他的表達方式應該是這個樣子的
就可能同時使用了混合的編碼器
跟語者自動分段的標記
那有一段聲音訊號進來會標出說哪一個段落是語者A說的
他可能會用A冒號一個特殊的符號來代表這是A說的話
然後可以用文字表示的地方可能用文字表示
那不能用文字表示的地方可能就拿特殊的符號用SpeechUnit來表示
這個是對於語音版的語言模型看到一段聲音訊號的時候
它實際上輸入的猜想
那接下來怎麼訓練這種語音版的語言模型呢?
我們已經知道
這個語言模型
是用大量文字資料所訓練出來的
語音版的語言模型用同樣的道理
也可以用大量聲音的資料
來進行訓練
我們完全可以用大量的聲音資料
來訓練一個語音版的
語言模型
來做speech unit的接龍
那哪裡去找大量的聲音資料呢?
那這個想起來非常的直覺
網路上有這麼多的影音平臺
從這些影音平臺上就可以收集到大量的聲音資料
那OpenAI顯然有在做這件事情
因為在四月的時候紐約時報才說了
OpenAI用了超過一百萬小時的YouTube影片
來訓練他們的語言模型
那時候我剛看這個報導的時候就很困惑
為什麼要用YouTube的影片來訓練語言模型呢
難道現在文字的資料已經全部都用完了
真的沒有文字的資料可以用了嗎
但是如果OpenAI是在訓練語音版的語言模型
那用YouTube影片完全就說得通了
那你可能會想說
可是網路上的影片五花八門
又不是全部都是乾淨的語音
那些語音很多背後有是有音效有背景音樂的
那我們拿這些聲音訊號來訓練我們的語音版語言模型
這個語音版語言模型會不會把音效背景音樂通通都學進去了呢
非常有可能
我們來仔細聽一下
在GPT 4.0 Demo裡面
GPT 4.0的聲音
大家仔細聽一下GPT 4.0的聲音喔
有沒有發現,GPT-4-O他講話的時候,背景是有一個鋼琴聲的
當然我們不能排除現場正好有人在彈鋼琴的可能啦
但是如果你告訴我GPT-4-O講話的時候
就是有可能會產生一些音效或者是背景音樂
這我完全也不意外
那不過我覺得可以如果今天一個語音模型啊
他在講話的時候就是會自帶音效或自帶BGM
其實也是一個很酷的事情
他以後講話都自帶BGM
所以我們可以說這不是一個bug
這是一個feature
好那在demo裡面我們也可以看到說啊
GPT-4O他可以產生非常多樣化
非常戲劇性的聲音
這是怎麼辦到的呢?
有一些論文已經指出
這邊是引用了一個
Amazon在今年2月所釋出的論文
他們用了超過10萬個小時的語音訊號
來訓練他們的語音合成模型
這邊用的是一個參數有B.10億等級的模型
當然這樣大小的模型
在文字領域並不是很大
很大,但對於語音合成模型來說已經非常大了
他們發現是說過去語音合成系統
因為他用的資料不夠多
所以往往合出來的聲音非常的平淡
這個如果你有看動漫的話
你知道這就叫做棒讀
就是講話不帶情緒非常的平淡
但是現在如果有大量訓練資料的時候
這些語音合成的模型就可以理解要讀的內容
而且根據要讀的內容給予適當的變化
舉例來說
我們來看這個句子
注意一下這個句子裡面
有提到
Whisper
輕聲細語
輕聲這個詞彙
我們來看看語言模型
是怎麼合這個句子的
來聽一下這個聲音
根據這篇論文的講法,他們並沒有對輸入的文字做任何特殊的處理
模型就是直接讀輸入的文字
但他在讀到 whisper 輕聲這個字的時候
他自動知道他要用比較輕的聲音來唸這個句子
這是他自動根據資料學到的
當然這邊可能合出來的聲音沒有非常的戲劇化
我們今天看這個GPT-4的Demo
它的聲音的合成是可以更戲劇化的
你要叫它講輕聲的時候
它是可以講得更輕聲的
但也許把資料從十萬小時擴展到一百萬小時的時候
就是會有這種效果
當然因為我從來沒有訓練過這麼多
從來沒有用過這麼多資料訓練語音合成的系統
所以這只是個猜測
另外一方面呢
單單使用語音的資料來訓練
非常顯然是會不夠的
必須要利用文字的資訊
為什麼用語音資料訓練是不夠的呢
我們來想想看
一百萬小時的語音背後
可能有多少的文字
我們來算算有多少分鐘
一百萬小時是六千萬分鐘
一般人一分鐘可以大概講
一百多個文字的TOKEN
那六千萬分鐘
就可以講60億個文字的TOKEN
你可能想說60億
聽起來是一個蠻大的數字了
從60億個TOKEN
應該可以學會很多知識吧
但是不要忘了
LLaMA3
可是用了15兆個文字的TOKEN來訓練喔
所以一百萬小時的語音
只有LLaMA3
預訓練資料的250分之一而已 (勘誤：應該是 2500 分之一)
所以假設一個模型
他只聽了100萬小時的語音進行學習
也許他可以學到一些語音的資訊
但是他的知識可能會非常的不足
所以模型不能單單只用語音聲音的資料訓練
還得利用文字的資訊
那怎麼利用文字的資訊呢
一個可能是語音版的語言模型是以原來的語言模型作為初始化的
那OpenAI早就打造了很多很厲害的語言模型
他們可能不會浪費這些語言模型已經學到的知識
所以他們可以用原來已有的語言模型作為初始化
去打造語音版的語言模型
或者是用通俗的講法就是
我們可以去教原有的語言模型
讓他聽懂語音
那麼剛才有說
語音其實會被表示成一堆Unit
而這些Unit對於語言模型來說
就是一個全新的語言
他有一套自己獨特的符號系統
所以當我們要教一個原來的語言模型
聽懂語音的時候
其實對語言模型來說
他就是在學一個全新的語言
當然另外有一個可能是把語音的符號
跟文字全部倒在一起訓練
總之單憑語音資訊來訓練一定是不行的
一定要想些辦法利用文字的資訊
那我覺得這個地方可能就可以展現混合模式的好處
因為假設我們今天表示一段聲音不是隻用unit
而是有用到文字的話
那對於語言模型來說,它學習起來可能會更為容易
因為這些文字符號的意思,它早就知道了
它只需要花力氣去了解這些新的符號是什麼意思就好
當然這只是一個猜測
如何利用語音資訊其實並不是隻有固定一種方法而已
還有很多其他讓語音版語言模型利用文字資訊的方法
到這邊就列舉幾篇論文給大家參考
好,那剛才講的是Pretrain
那我們都知道說光是有pre-trained是不夠的
文字的語言模型如果只有做pre-trained
只有做預訓練的話
他根本沒辦法好好的回答問題
所以需要做alignment
需要根據有標註的資料進行微調
那在文字模型上
你需要收集一些這樣對話的資料
人說你是誰
模型就要回答我是人工智慧
人說教我駭入鄰居家的Wi-Fi
模型就要回答我不能教你等等
你需要蒐集這樣的資料
來微調你的模型
讓它能夠好好跟人說話
那對於語音的模型
我們可能就需要蒐集語音的對話來訓練它
所以就要蒐集語音的對話
然後在這個對話裡面
你讓某一個人去當作使用者
某一個人就說他是扮演AI的角色
然後你就可以微調語音版的語言模型
讓他聽這個人說話
然後要產生這個人的回應
但光是這樣可能還不夠
大家如果聽那個DEMO
那個DEMO是固定某一個語者的聲音
所以假設要讓我們的語言模型講出話來
都是某一個人的聲音
比如說現在ChatGPT的語音頁面裡面
你也是可以選語者的
那比如說你選Sky這個人
所以他就只能用Sky這個人的聲音來講話
那所以說可能在做alignment的時候
假設你想讓模型學會多用Sky的聲音來講話
那你可能需要收集大量Sky跟其他人的對話
那可能有某一個聲優
他就代表了Sky
收集很多這個聲優跟其他人的對話
你需要這樣子的語音對話來訓練模型
那有人可能會想說
那是不是需要錄很多Sky跟其他的對話呢
也許不一定需要
因為記不記得我們在講文字模型的時候
我們有說其實Finetune往往不需要太多資料
模型在Pretrain的時候
已經擁有非常豐富的知識
Finetune只是畫龍點睛
那今天會不會做完語音的pre-train以後
模型已經很會模仿各種不同的人說話
他只要聽過幾句sky的話
就可以很好的學會sky說話的方式了
所以也許一個有大量資料pre-train的模型
它是它在fine-tune的時候
在做alignment的時候
只需要少量的資料
那另外一個可能就是
就算沒有很多sky對話的錄音
也許可以用語音轉換的技術
語音轉換的技術就是把某一個人的聲音轉成另外一個人的聲音
也許可以用語音轉換的技術
把對話中各種不同人的聲音都轉成sky的聲音
那你就有大量sky跟其他人對話的錄音檔
可以拿來訓練你的模型
但講到目前為止
看起來這個語音的語言模型
跟原來文字的語言模型非常的類似
但是其實語音跟文字
還是有很多
本質上的區別
舉例來說
我們今天用文字來跟
AI互動的時候
你有很明確的開始跟結束
今天
你跟ChatGPT互動的時候你打一句話你打完以後
你會按Enter
ChatGPT就知道該輪到他講話了
他不用猜什麼時候
他該開始講話
結束也是一樣
你今天不想讓ChatGPT講話了
右下角有一個停止的鈕你按下去
他就很明確的知道他不會再生成更多的文字
但是語音介面不一樣
當有一個人對AI說我們來做一件有趣的事
稍微停頓一下
這個時候AI必須要猜
他到底應該接話說什麼事
還是這個人還沒有講完
我們應該要等他繼續講完
再進行答覆
所以跟文字不一樣
在文字的介面上
AI很清楚什麼時候輪到他講話
但是如果是一個語音的介面
那AI就必須要猜
他什麼時候可以開始講話
另外一方面
也許今天有人叫語言模型講一個故事
他開始講一個無聊的故事
山上有座廟
廟裡有個老和尚
這個故事會不斷的循環下去
是一個無限循環的故事
那人聽了就很厭煩
跟他說停停停
這不是我要聽的
那語音版的語言模型
他知不知道要停下來呢
那也許你說這是什麼問題
反正只要人說話
我們就停下來
這個不是最好的解決問題的方法
因為也許這個人講話
並不是要語言模型停下來啊
也許現在人正好在跟這個AI合唱
所以並不是只要聽到人的聲音
就必須停下來
所以假設今天要讓一個語音版的語言模型
跟人有自然的互動
要讓模型可以同時聽跟說
但原來的文字接龍的方式
聽跟說是切開的
模型在說的時候
它就很難繼續進行聽這件事情
那一個可能的解決方法
是把聽跟說這兩件事情分開來
那這個技術呢過去也是有過的
有一個模型叫做Dialogue GSLM
這個是22年就已經有的模型
所以等一下講的東西
如果你聽不太懂的話
你可以參考這一篇論文
怎麼讓模型同時聽跟說呢
就是要把聽跟說分成兩個不同的頻道
所以今天有一個聽的頻道
那這個是模型的麥克風
他會在聽人在說什麼
外界發生了什麼聲音
那另外模型會記錄
自己發出過什麼樣的聲音
但這兩個頻道應該要是分開的
而不是直接被混在一起的
我們來看一下模型在這個狀況
要怎麼運作
語音版的語言模型就是同時
聽這兩個頻道的內容
包括現在人在說什麼
他之前講過什麼
現在人說了個我
那感覺這句話還沒有說完
所以語音版語言模型就輸出 silence
輸出安靜
安靜可能也是有某一些特殊的speech unit
代表安靜
就不說話
然後接下來呢
人就會繼續說
也許人說到某一個段落
那語音版的語言模型
根據人現在說的話
覺得那他說完了
輪到我說了
人說我們來做個有趣的嘗試
那語言模型也許就輸出一個 wow
代表說他很興奮
可以做這個嘗試
那接下來呢
假設人仍然是沒有聲音的
人是安靜的
語言模型就知道說
那他可以繼續說
也許他就會說我好期待
但如果人仍然有發出聲音
就我要你做某件事情
那語音版語言模型知道說
人還沒有說完
也許他就會輸出
代表安靜的符號
繼續保持安靜
當然這並不是唯一讓語言模型
可以同時聽跟說的方法了
但可能還有其他的解法
我這邊只是講了某一個可能的解法而已
另外一方面也許語言不只要同時聽跟說
還要同時聽加說加看
因為你看這個GPT-4ALL的DEMO
模型是可以一邊看著東西一邊說話的
所以可能有不只有聽跟說兩個頻道
還有一個視覺的頻道
專門讀取外界的影像輸入
舉例來說,在這個OpenAI的Demo裡面
有一個Demo就是
這個語言模型正在描述房間中的燈光
講得非常的奇跡
這時候有一個人跳出來
比了一個Yeah的動作
語言模型無視這個影像
繼續談他的燈光
不知道他有沒有看到這個比Yeah的人
這時候人就打斷語言模型說
等一下,你有沒有看到什麼奇怪的東西
語言模型顯然是有看到比Yeah的這個人的
只是在講燈光的時候他講得太爽了
所以就沒有覺得需要對這個人發表
比Yeah的人發表任何的評論
但是當有人問他說
你有看到什麼怪怪的東西的時候
這個時候語言模型會做的事情是
他會對每一個Channel
不只是他現在生成的Channel
還有他聽的Channel
還有他看的Channel
通通去做Attention
所以他會收集所有的資訊
然後得到一個答案
那可能就會知道說有人比了一個頁
那其實在Google Project Astra裡面的Demo
也有一個非常類似的例子
看起來大家都覺得需要有這種例子來展現模型的能力
他們的例子是
有一個人拿著手機隨便走隨便拍
然後中間拍攝的過程中有看到了一個眼鏡
但是那個時候語言模型並沒有對眼鏡做出任何評論
人也沒有提到跟眼鏡有關的事情
然後人就跟語言模型聊了一下
我們現在在哪裡
語言模型說在王十字車站附近
然後接下來人問說有沒有看到我的眼鏡
這個時候眼鏡已經不在畫面裡面了
但是語言模型會對過去的資訊去做Attention
從過去看到的影像
從過去聽到的聲音收集一下資訊
他知道他有看到眼鏡
眼鏡是在桌上
這是一個蠻自然的互動的模式
其實技術的部分就講到這邊
如果大家想知道更多有關語音版語言模型的論文
我這邊附了一個連結
裡面收集了很多語音版語言模型相關的論文
給大家參考
謝謝大家謝謝
