臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
deep learning 現在非常的熱門
所以，它可以用在甚麼地方
我覺得真的還不需要多講
我覺得大家搞不好都知道得比我更多
我相信如果你隨便用 deep learning 當作關鍵字
胡亂 google 一下
你就可以找到一大堆的、exciting 的 result
所以，我們就直接用這個圖呢
來簡單地 summarize 一下這個趨勢
這個圖呢，是 google 的 Jeff Dean
它在 sigmoid 的一個 keynote speech 的一張投影片
那這個圖想要表達的事情是這樣
橫軸代表時間
從 2012 的 Q1 到 2016
縱軸，代表說在 google 內部
有用到 deep learning 的 project 的數目
那可以發現說，這個趨勢
是從幾乎 0 到超過 2000
所以，這個使用 deep learning 的 project 數目呢
是指數成長的
那如果你看它的應用的話
它有各種不同的應用，涵蓋幾乎你可以想像的領域
比如說，Android, Apps, drug discovery
Gmail, Image understanding, Natural language understanding
Speech, 各種不同的應用
通通有用到 deep learning
那 deep learning 可以做的應用實在是太多了
我們這邊就不要花時間來講這些東西
如果要講這些東西的話呢
再用兩、三堂課，其實也是講不完的
那這個隨便 google 就有的東西呢，我們就不要再講了
我們來稍微回顧一下
這個 deep learning 的歷史
在歷史上呢，它是有經過好幾次的沉沉浮浮
首先，在 1958 年
有一個技術叫做 Perceptron 被提出來
Perceptron 這個技術，它也是一個 linear 的 model
它非常非常像
我們在前一堂課講的 Logistic Regression
它只是沒有 sigmoid 的部分
但它還是 linear 的 model
那一開始，有人提出這個想法的時候
提出 Perceptron 這個想法的時候呢
大家非常非常的興奮
這個是 Frank Rosenblatt
在海軍的 project 裡面提出來的
他一開始提出來的時候，大家覺得非常非常興奮
那個時候要做 Perceptron 的運算呢
如果你看 Bishop 的教科書，裡面有一張的機器呢
看起來是房間那麼大的機器
那個時候，New York Times 據說還寫了一個報導說
從此以後，人工智慧就要產生了
電腦可以自己學習，就像現在這個樣子
可是後來呢
MIT 有一個
有人呢，就寫了一本教科書
這本教科書的名字就叫做 Perceptron
這本教科書裡面，他就指出了 linear 的 model
是有極限的，就像我們在上一張投影片講的
linear model 有很多事都辦不到
就像是我們剛才舉的那麼簡單的 example
它都辦不到
然後大家希望就破滅了
然後，當時在 1958 年，剛提出 Perceptron 的時候呢
有各種驚人的 application
就像現在 deep learning 一樣，有人就說
我用了 Perceptron
結果我可以分辨說，給我一張照片
那照片裡面有坦克，或是一般的卡車
那它可以正確地分辨說，那些照片裡面是坦克
那些照片裡面是卡車
就算那些坦克呢，被藏在叢林裡面
就算被樹木蓋住呢，也偵測得出來
那其他人就覺得說，太厲害了，這就是人工智慧
可是既然 Perceptron 有這樣的
後來大家就發現說 Perceptron 其實很多 limitation
那怎麼可能分辨卡車跟坦克，這麼複雜的 object 呢？
所以，有人就去把那個 data，再拿出來看了一下
就發現說，原來卡車跟坦克的照片
是在不同的日子所拍攝的
所以，一天是雨天，一天是晴天
所以，這個照片本身的亮度就不一樣
所以，Perceptron 它唯一抓到的東西
就只有亮度而已
所以大家就崩潰了，這個方法的名字就臭掉了
後來就有人想說，既然
一個 Perceptron 不行，我能不能連接很多個 Perceptron
就像我們剛才講的，Logistic Regression
都接在一起，它應該就很 powerful
這個就叫做 Multi-layer 的 Perceptron
事實上，1980 年代
Multi-layer 的 Perceptron 它的技術
基本上在 1980 年代的時候呢
就都開發得差不多了
那個時候已經開發完的技術
其實就跟今天的 deep learning
是沒有太 significant 的差別的
然後，有一個關鍵的技術是 1986 年的時候
Hinton propose 的 Backpropagation
那其實同時也有很多人 propose Backpropagation
只是大部分人把這個 credit 歸給 Hinton 的 paper
他那篇 paper 是比較著名的
但是，在這個時候遇到的問題就是
通常超過 3 個 layer 的 neural network
你就 train 不出好的結果
通常一個還可以，再多你就 train 不出好的結果了
那後來在 1989 年
有人就發現了一個理論
就是說，一個 hidden layer
其實就可以
model 任何可能的 function
了解嗎？只要一個 neural network 有一個 hidden layer
它就可以是任何的 function
它就已經夠強了，所以根本沒有必要
疊很多個 hidden layer
所以，這個 Multi-layer 的 Perceptron 的方法，又臭掉了
然後，大家就比較喜歡做 SVM
那這個方法就臭掉了，據說那一陣子呢
Multi-layer 的 Perceptron 這個方法
也有人叫它 neural network 這個方法
它就像是一個髒話一樣
寫在 paper 裡面，那個 paper 一定被 reject
後來呢
有人就想到一個突破的點
這個突破的點，關鍵的地方就是
把它改個名字，這個方法已經臭掉了
只好改一個名字，把它改成 deep learning
整個就潮起來了
所以我講說，改名字其實是有很大的力量的
大家現在都不想念博士
我們把博士改個名字，大家都想唸了
下次跟系主任建議一下
很多人覺得說，有一個關鍵的技術是
應該在 2006 年提的
用 Restricted Boltzmann Machine 做 initialization
很多人覺得說，這是個突破這樣
甚至有一陣子呢
大家的認知是，到底 deep learning 跟
1980 年代的 Multi-layer Perceptron
有何不同呢？它的不同之處在於
如果你有用 RBM 做 initialization
你在做 Gradient Descent 的時候，不是要找一個
初始的值嗎？如果你是用 RBM 找的，叫 deep learning
你沒有用 RBM 找的，是傳統的、 1980 年代的 Multi-layer Perceptron
後來呢，大家逐漸意識到說
因為 RBM 這個方法呢
非常的複雜，假設沒有
假設你是 machine learning 的初學者的話
你看這個 paper，我相信你是看不懂的
如果我們今天要講 RBM
要從現在開始
假設用我們上課那些知識，講到讓你聽懂
要再另外多講 3 周這樣子
它是有用到一些比較深的理論
大家覺得說，哇！這個這麼複雜
一定就是非常的 powerful 阿
而且它不是 Neural network base 的方法
它是 graphical model
它這個大家覺得說，這個這麼複雜，我們看都看不懂
這個一定是有用的
後來大家逐漸試來試去以後，就發現說
這招其實沒什麼用這樣子
你可以發現說，如果你讀 deep learning 的文獻
現在已經不太有人用 RBM 做 initialization 了
因為這一招帶給我們的幫助呢
並沒有說特別大
連 Hinton 自己都知道這點， 他在某篇 paper 裡面提過這件事情
只是大家可能都沒有注意到那篇 paper 就是了
所以說，但是他有個最強的地方
它最強的地方就是它讓大家
重新、再一次對這個 model 有了興趣
因為它很複雜，所以大家就會開始想要研究
deep learning 是什麼樣的東西，花很多力氣去研究
所以，其實我聽過有一個
google 的人對 RBM 的評論
他說這個方法就是石頭湯裡面的石頭
石頭湯的故事大家聽過嗎？
就有一個人說，我要煮一碗石頭湯
有一個士兵，他到一個村莊裡面借宿
然後，他說：我要煮一碗石頭湯
然後，大家都說：你要怎麼煮一碗石頭湯？
我用石頭就可以了
就用石頭煮一碗湯，如果再加點鹽就更好了
就加點鹽
再加點米就更好了，再加點米
再加點菜就更好了，再加點菜
然後，就煮了一鍋湯
大家就覺得好好喝，用石頭就可以煮湯這樣
但是，那個石頭其實並沒有什麼作用
所以，其實 RBM 就類似這種東西
然後，我覺得有一個關鍵的突破是在 09 年的時候
我們，知道要用 GPU 來加速
這件事情還頗關鍵的，過去如果你做 deep learning
train 一次，一周就過去了
然後，結果實驗又失敗，你就不會想做第 2 次了
其實，我小時候，其實也不是小時候啦
我多年前有跟一個學弟試著想要做 deep learning
可是，那個時候還不知道疊很多層
那個時候大家都疊一層
直覺想法就知道說，一定要疊很多層阿
怎麼會只有疊一層？
所以想說，我們來把它疊很多層吧
然後，我們就開始做，train 一次要一周
train 完以後，結果沒有比較好
就沒有人想要再繼續做下去了
本來想找專題生做，專題生也都不想做
就沒有人要做，這個題目就沒有人要做了
那個時候我們把他好好做出來的話，我們現在就發了
不過那時候，我們不知道要用 GPU
所以，train 一次要一周，想要做出來也是很難
現在有 GPU 以後，本來要一周的東西
你可能幾個小時，就可以馬上看到結果了
那在 11 年的時候
這個方法被引入到語音辨識裡面
語音辨識發現說，這招果然很有用
然後，開始瘋狂地用 deep learning 的技術
到 12 年的時候呢
這個 deep learning 的技術
贏了一個很重要的 image 的比賽
所以，在 image 那邊的人 也瘋狂地用 deep learning 的技術
其實，deep learning 的技術並沒有
很複雜，它其實非常簡單
我們之前講說，machine learning 就是 3 個 step
那其實 deep learning 也一樣就是，這 3 個 step
講說 deep learning 就這 3 個 step
就好像是在說，把那個大象放進冰箱一樣
大象放進冰箱，大家知道嗎？
把門打開，把大象趕進去
把冰箱門關起來，就把大象放進冰箱了
3 個 step 聽起來就像這樣子
deep learning 是很簡單的
你其實可以非常快的了解，其實你可以秒懂它
其實在 deep learning 裡面
我們說，在 machine learning 裡面第一個 step
就是要 define 一個 function
這個 function，其實就是一個 Neural network
那這個 Neural network 是
是甚麼呢？我們剛才已經講說呢
我們把這個 Logistic Regression 呢
前後 content 在一起
然後把一個 Logistic Regression 稱之為 Neuron
整個稱之為 Neural Network
我們其實就得到了一個 Neural Network
那我們可以用不同的方法
來連接這些 Neural Network
我們用不同的方法來連接這些 Neural Network
我們就得到了不同的 structure
在這個 Neural Network 裡面
我們有一大堆的 Logistic Regression
每個 Logistic Regression，它都有自己的 weight
跟自己的 bias
這些 weight 跟 bias 集合起來
就是這個 network 的 parameter
我們這邊用 θ 來描述它
那這些 Logistic Regression
或這些 Neuron
我們應該怎麼把它接起來呢
有各種不同的方式
怎麼連接，其實是你手動去設計的
是你手動去連接的
最常見的連接方式呢？
叫做 Fully Connected Feedforward Network
在 Fully Connected Feedforward Network 裡面呢
你就把你的 Neuron 排成一排一排
這邊有 6 個 Neuron，就兩個兩個一排
然後，每一組 Neuron，它都有一組 weight
都有一組 bias
那這個 weight 跟 bias 是根據 training data 去找出來的
假設上面這個藍色 Neuron 它的 weight 是 1, -2，它的 bias 是 1
下面呢，它的 weight 是 -1, 1，它的 bias 是 0
假設我們現在的輸入
是 1 跟 -1
那這兩個藍色的 Neuron 它的 output 是甚麼呢？
這個做一下小學生會做的運算，你就可以得到答案
1*1 + (-1)*(-2)，再加上 bias 1
通過 sigmoid function 以後
你得到的結果呢，就是 0.98
那下面呢，你把 1*(-1)、(-1)*1 + 0
再通過 sigmoid function 以後，你就得到 0.12
接下來，假設這一個 structure 裡面的每一個 neuron
它的 weight 跟 bias，我們都是知道的
那我們就可以反覆進行剛才的運算
1 跟 -1 通過這兩個 neuron，變成 0.98 跟 0.12
再通過這兩個 neuron，變成 0.86 跟 0.11
再通過這兩個 neuron，得到 0.62 跟 0.83
所以，輸入 1 跟 -1
經過一串很複雜的轉換以後呢
就得到 0.62 跟 0.83
那如果你輸入是 0 跟 0 的話
你輸入 0 跟 0 的話，你得到的 output 就是
經過一番一模一樣的運算，你得到的是 0.51, 0.85
所以，一個 neural network 你就可以把它看作是
一個 function
如果一個 neural network 裡面的參數
weight 跟 bias 我們都知道的話
它就是一個 function
它的 input 是一個 vector，它的 output 是另一個 vector
舉例來說，我們剛才看到說 input 是 [1, -1]
output 是 [0.62, 0.83]
input 是 [0, 0]，output 是 [0.51, 0.85]
所以，一個 network，如果我們已經把參數設上去的話
它就是一個 function
如果我們今天還不知道參數
我只是定出了這個 network 的 structure
我只是決定好說，這些 neuron 間
我們要怎麼連接在一起
這樣子的一個 network structure
它其實就是 define 了一個 function set，對不對？
我們可以給這個 network 設不同的參數
它就變成不同的 function
把這些可能的 function 通通集合起來
我們就得到了一個 function set
所以，一個 neural network 你還沒有認參數
你只是把它架構架起來
你決定這些 neuron 要怎麼連接
你把連接的圖，畫出來的時候
你其實就決定了一個 function set
這跟我們之前做的東西都是一樣的
我們之前也是做 Logistic Regression, Linear Regression
我們都是也決定了一個 function set
那這邊呢，我們也只是換一個方式，來決定 function set
只是如果我們用 neural network 決定 function set 的時候
你的 function set 是比較大的
它包含了很多原來你做 Logistic Regression
做 Linear Regression 所沒有辦法包含的 function
那剛才講的是一個比較簡單的例子
在這個例子裡面呢
我們把 neuron 分成一排一排的
然後說每一排的 neuron 都兩兩互相連接
藍色 neuron 的 output 都接給紅色
紅色的都接給綠色
綠色後面沒有別人可以接了， 所以它是整個 network 的輸出
藍色前面沒有其他人了， 所以它就是整個 network 的輸入
那 in general 而言呢
我們可以把 network 畫成這樣
你有好多好多排的 neuron
你有第 1 排、第 2 排.......到第 L 排
很多排 neuron，每一排 neuron 它裡面的 neuron 的數目
可能很多，比如 1000 個、2000 個阿，這個 scale
那這邊每一顆球，代表一個 neuron
在 layer 和 layer 之間的 neuron，是兩兩互相連接的
layer 1 它的 neuron 的 output 會接給
每一個 layer 2 的 neuron
那 layer 2 的 neuron 的 input 就是所有 layer 1 的 output
就是所有 layer 1 的 output
因為 layer 和 layer 間，所有的 neuron 都有兩兩連接
所以它叫 Fully Connected 的 Network
那因為現在傳遞的方向是從 1 到 2
從 2 到 3，由後往前傳
所以它叫做 Feedforward Network
那整個 network 呢，需要一組
需要一個 input，這個 input 就是一個 vector
那對 layer 1 的每一個 neuron 來說
每一個 neuron，它的 input
就是 input layer 的每一個 dimension
那最後第 L 的那些 neuron
它後面沒有接其他東西了
所以，它的 output 就是整個 network 的 output
假設第 L 排有 M 個 neuron 的話
它的 output 就是 y1. y2 到 y(下標 M)
這邊每一個 layer，它是有一些名字的
input 的地方，我們叫做 input layer
嚴格說起來，input 其實不是一個 layer
它跟其他 layer 不一樣，它不是由其他 neuron 所組成的
但是，我們也把它當作一個 layer 來看
所以，叫它 input layer
output 的地方，我們叫它 output layer
其餘的部分，就叫做 hidden layer
那所謂的 deep 是什麼意思呢？
所謂的 deep 就是有很多 hidden layer
就叫做 deep
那有人就會問一個問題，說要幾個 hidden layer
才叫做 deep learning
這個就很難說了
有人會告訴你說，要 3 層以上才叫做 deep
有人會告訴你說，要 8 層以上才叫做 deep
所以就看每一個人的定義，都不一樣
本來沒有 deep learning 這個詞的時候
大家都說：我在做 neural network
通常都只有一層，自從有 deep learning 這個詞以後
有一層的人，都說他在做 deep learning 這樣
所以，現在基本上只要是 neural network base 的方法呢
大家都會說是 deep learning 的方法
那到底可以有幾層呢？
在 2012 年的時候
參加 ImageNet 比賽得到冠軍的 AlexNet
它有 8 層，它的錯誤率是 16.4 %
大家可能都知道說，它比第二名的好非常多
第二名的 error rate 是 30%
到 14 年的時候，VGG 有 19層
它的 error rate 降到 7%
GoogleNet 有 22 層，它的 error rate 降到 6.7%
但是，這個都還不算甚麼
Residual Network 152 層這樣
如果它跟 GoogleNet, VGG, AlexNet 比起來
它大概是長這個樣子
它的 error rate 是 3.57%
如果你看 Benchmark Corpus 的話，其實
它這個 performance 是比人在同一個 test 上做的還要好
你可能會很懷疑說
人怎麼可能會在影像辨識上輸給機器呢？
因為那個 test 其實還滿難的
它給你一張圖，一張狗的圖
你光回答狗並不是正確答案，你要回答哈士奇這樣子
那個 test 其實還蠻難的，但其實為了要公平的比較
當時，在人跟機器比較的時候
那些人是有事先看過 training data 的
也就是說，你有先讓它訓練辨識不同狗的種類
不同花的種類、不同豬的種類
只是經過訓練以後，它還是沒有機器那麼強就是了
那這個是跟 101 做一下比較
101 在這邊
這個我們之後再講，那 Residual Network 呢
它不是一般的 Fully Connected Feedforward Network
如果你用一般 Fully Connected Feedforward Network
搞在這個地方，其實結果是會有問題的
它不是 overfitting，而是 train 都 train 不起來
所以，你其實要特別的 structure
才能搞定這麼深的 network
這個我們之後再講
那 network 的運作呢
我們常常會把它用 Matrix Operation 來表示
怎麼說呢，我們舉剛才的例子
假設第一個 layer 的兩個 neuron
他們的 weight 分別是 1, -2, -1, 1
那你可以把 1, -2, -1, 1 排成一個 matrix
把這個 1, -2, -1, 1 排成一個 matrix
那當我們 input 1, -1 要做運算的時候
我們就是把 1*1、(-1)*(-2)、1*(-1)、(-1)*1
所以我們就可以把 1 跟 -1 當做一個 vector 排在這邊
當我們把這個 matrix 跟這個 vector
做運算的時候，我們算 1*1 + (-2)*(-1)
就等於是做 1*1、(-1)*(-2)
當我們做 1*(-1)、(-1)*1 的時候
就等於是做 1*(-1)、(-1)*1 這樣
接下來呢，有 bias 1 跟 0
所以，我們要在後面把 bias 排成一個 vector
再把這個 vector 加上去
這個結果，算出來就是 [4, -2]
就是通過 activation function 之前的值，就是 4, -2
然後，通過這個 sigmoid function
然後在這個 neural network 的文件裡面
我們把這個 function 稱之為 activation function
事實上，它不見得是 sigmoid function
因為到現在，大家都換成別的 function
如果你是從 Logistic Regression 想過來的話
你會覺得它是一個 sigmoid function， 但現在已經較少用 sigmoid function
假設我們這邊用的是 sigmoid function 的話呢
我們就是把 4, -2 丟到 sigmoid function 裡面
接下來算出來，就是 0.98, 0.12
所以，一個 neural network，一個 feedforward network
它的一個 layer 的運算，你從 1, -1 到 0.98, 0.12
你做的運算就是把 input 1 跟 -1
1 跟 -1 乘一個 matrix
加上一個 bias 所成的 vector
再通過一個 sigmoid function
得到最後的結果
所以，以 general 來說
一個 neural network
假設我們說第一個 layer 的 weight
全部集合起來，當作一個 matrix, W^1
這個 W^1 是一個 matrix
把它的 bias 全部集合起來
當作一個 vector, b^1
把第二個 matrix 的 weight 全部集合起來
當作 W^2
把它的每一個 neural 的 bias 集合起來，當作 b^2
到第 L 個 layer，所有的 weight 集合起來，變成 W^L
bias 集合起來，變成 b^L
那你今天給它一個 input x 的時候
output 的這個 y 要怎麼算出來呢？
我們假設我們把 x1, x2, 到 xN，接起來，變成一個 x
那這個 output 的 y 要怎麼把它算出來呢？
你就這樣算
x * W^1 + b^1
再通過 activation function
然後，你就算出第二排這些 neuron 的 output
我們稱之為 a^1，你把 x * W^1 + b^1，就得到 a^1
x * W^1 + b^1
再通過 activation function 以後，就得到 a^1
接下來呢，你做一樣的運算
把 a1 * W^2 + b^2
再通過 activation function 以後，就得到 a^2
然後，就這樣一層一層的做下去到最後一層
你把 a^(L-1) * W^L + b^L 通過 activation function 以後
得到這個 network 的最終 output y
所以，整個 neural network 的運算其實就是
一連串 matrix 的 operation
就是這個 function
x 它的 input, output，x 跟 y 的關係
x 跟 y 的關係，他們是怎麼樣的關係呢？
你把 x * W^1 + b^1，再通過 activation function
再把這個 output 再乘上 W^2，加 b^2
再通過 activation function，再通過 sigmoid function
最後，到乘上 W^L，加上 b^L
再通過這個 sigmoid function
就得到最後的 y
所以，一個 neural network
實際上做的事情就是一連串的
vector 乘上 matrix ，再加上 vector
就是一連串我們在線性代數就有學過的：矩陣運算
那把這件事情寫成矩陣運算的好處就是
你可以用 GPU 加速
實際上呢，現在一般在用 GPU 加速的時候
這個 GPU 的加速並不是
真的有去對 neural network 做甚麼特化
是說，現在有一些特別的技術有做特化
但是，如果你 general 是買那種玩遊戲的顯卡的話
那它是沒有對這個 neural network 做甚麼特化
那你實際上拿來加速的方式是
當你需要算矩陣運算的時候
你就 call 一下 GPU，叫它幫你做矩陣運算
這會比你用 CPU 來算還要快
所以，我們在寫 neural network 式子的時候
我們習慣把它寫成 matrix operation 的樣子
那裡面如果有需要用到矩陣運算的時候
就 call GPU 來做它
那這整個 network，我們怎麼看待呢？
我們可以把它到 output layer 之前的部分阿
到 output layer 之前的部分
看作是一個 feature 的 extractor
這個 feature extractor 就 replace 我們之前
要手動做 feature engineering
做 feature transformation 這件事情
所以，你把一個 x input，通過很多很多 hidden layer
在最後一個 hidden layer 的 output
每一個 neuron 的 output，x1, x2, 到 xK
你就可以把它想成是一組新的 feature
那 output layer 做的事情呢？output layer 就是
一個 Multi-class 的 Classifier
這個 Multi-class Classifier，它是拿前一個 layer 的 output
當作 feature
這個 Multi-class Classifier， 它用的 feature 不是直接從 x 抽出來的
它是經過很多個 hidden layer，做很複雜的轉換以後
抽出一組特別好的 feature
這組好的 feature 可能是能夠被 separable
經過這一連串的轉換以後呢
它們可以被用一個簡單的 layer 的
Multi-class Classifier 就把它分類好
那我們剛才其實有講過說
Multi-class Classifier 它要通過一個 sigmoid function
而不要通過一個 Softmax function，對不對？
要通過一個 Softmax function
因為我們把 output layer
也看作是一個 Multi-class Classifier
因為我們把 output layer 也看作是 一個 Multi-class Classifier
所以，我們最後一個 layer 也會加上 Softmax
一般你在做 neural network 的時候呢
是會這樣做
那舉一個不是寶可夢的例子
不是寶可夢的例子，我之後會示範一下做這個例子
就 input 一張 image，它是一張手寫的數字
然後 output 說這個 input 的 image，它對應的是甚麼？
那在這個問題裡面，你的 input 是一張 image
但對機器來說，一張 image 就是一個 vector
假設這是一個解析度 16*16 的 image
那它有 256 個 pixel
對 machine 來說，它就是一個 256 維的 vector
那在這個 image 裡面呢
每一個 pixel 就對應到其中的 dimension
所以，右上角這個 pixel 就對應到 x1
這個就對應到 x2，右下角的就對應到 x256
如果你可以說，有塗黑的地方就是 1
那沒有塗黑的地方，它對應的數字就是 0
那 output 呢？
neural network 的 output，如果你用 Softmax 的話
那它的 output 代表了一個 probability distribution，對不對？
所以，今天假設 output 是 10 維的話
就可以把這個 output 看成是
對應到每一個，你可以把這個 output 看成是
對應到每一個數字的機率
y1，代表了 input 這張 image
根據這個 neural network 判斷，它是屬於 1 的機率
代表它是屬於 2 的機率，代表它是屬於 0 的機率
那你就實際上讓 network 幫你算一下說
每一個 input 加上數字屬於 image 的機率是多少？
假設屬於數字 2 的機率最大是 0.7
那你的 machine 就會 output 說
這張 image 它是屬於數字 2
那在這個 application 裡面， 假設你要解這個手寫數字辨識的問題
那你唯一需要的，就是一個 function
這個 function input 是一個二維的 vector
output 是一個 10 維的 vector
而這個 function 就是 neural network
所以，你只要兜一個 neural network
你可以用簡單的 feedforward network 就好了
兜一個 neural network， 它的 input 有 256維、是一張 image
它的 output 你特別設成 10 維
這 10 維裡面，每一個 dimension
都對應到一個數字
如果你做這樣的設計，input 是 256 維
output 固定是 10 維的話
那這一個 network，它其實就
代表了一個可以拿來做手寫數字的 function
這個 function set 裡面呢
這個 network 的 structure 就 define 了一個 function set
這個 function set 裡面
每一個 function 都可以拿來做手寫數字辨識
只是有些做出來結果比較好
有些做出來結果比較差
那接下來你要做的事情就是
用 Gradient Descent 去找一組參數
去挑一個最適合拿來做手寫數字辨識的 function
那在這個 process 裡面呢
我們需要做一些 design
之前在做 Logistic Regression
或是 Linear Regression 的時候
我們對 model 的 structure 是沒有甚麼好設計的
但是，對 neural network 來說
我們現在唯一的 constraint 只有 input 要是 256 維
output 要是 10 維
但是中間要有幾個 hidden layer
每一個 hidden layer 要有多少的 neuron
是沒有限制的
你必須要自己去設計它
你必須自己去決定說，我要甚麼 layer
每個 layer 要有多少的 neuron
那決定 layer 的數目
和每一個 layer 的 neuron 數這件事情
就等於是決定了你的 function set 長甚麼樣子
你可以想像說，如果我今天決定了一個差的 function set
那裡面沒有包含任何好的 function
那你之後在找最好的 function 的時候，就好像是
大海撈針，結果針不在海裡這樣
怎麼找都找不到一個好的 function
所以，決定一個好的 function set
其實很關鍵，你就決定這個 network 的 structure
是很關鍵的
講到這邊，總是會有人問我一個問題
假設我們今天讓 machine 來聽我的 talk
然後，叫它 predict 之後會有甚麼問題的話
它一定可以預測接下來的問題
那我們到底應該要怎麼決定 layer 的數目
還有每一個 layer 的 neuron 的數目呢
這個答案，就是我不知道這樣子
這個問題很難，就好像問說怎麼成為寶可夢大師一樣
這只能夠憑著經驗和直覺這樣
這個 network structure 要長甚麼樣子
就是憑著直覺還有多方的常識阿
然後，後續想辦法找一個最後的 network structure
找 network structure 這件事情，並沒有那麼容易
它有時候是滿困難的
有時候甚至需要一些 domain knowledge
所以，我覺得從非 deep learning 的方法
到 deep learning 的方法
我並不認為machine learning 真的變得比較簡單
而是我們把一個問題轉化成另一個問題
就本來不是 deep 的 model
我們要得到好的結果
你往往需要做 Feature Engineering
也就是做 Feature Transform，然後找一組好的 feature
但是，如果今天是做 deep learning 的時候
你往往不需要找一個好的 feature
比如說，做影像辨識的時候
你可以直接把 pixel 丟進去
過去做影像辨識的時候，你需要對影像抽一些 feature
抽一些人定的 feature
這件事情就是 Feature Transform
但是有 deep learning 之後，你可以直接丟 pixel 硬做
但是，今天 deep learning 製造了一個新的問題
它所製造的新的問題就是， 你需要去 design network 的 structure
就你的問題變成本來抽 feature
轉化成怎麼抽、怎麼 design network structure
那我覺得 deep learning 是不是真的好用就 depend on
你覺得哪一個問題比較容易
我個人覺得如果是
語音辨識或是影像辨識的話
design network structure 可能比 Feature Engineering 容易
因為，雖然說我們人工會看、會聽
我們自己都做得嚇嚇叫
但是，這件事情它太過潛意識了
它離我們意識的層次太遠
我們其實不知道
我們無法意識到，我們到底是怎麼做語音辨識這件事情
所以，對人來說你要抽一組好的 feature
讓機器可以用很方便的用 linear 的方法做語音辨識
這件事對人來說很難，因為 根本不知道好的 feature 長甚麼樣子
所以，還不如 design 一個 network structure
或是嘗試各種 network structure
讓 machine 自己去找出好的 feature
這件事情反而變得比較容易
我覺得對影像來說，也是一樣
那對其他 case 來說，我覺得就是 case by case
比如說，你有聽過一個說法是
deep learning 在 NLP 上面
覺得 performance 沒有那麼好
有聽過這個說法的同學舉手一下
好，沒關係，手放下
好像沒有太多人聽過這個說法
那這件事情是這樣
如果你開語音辨識跟影像辨識的文件
語音辨識跟影像辨識這兩個 community
是最早開始用 deep learning 的
一用下去，進步量就非常驚人
比如說，電視的錯誤率相對下降了 20% 以上
那如果是 NLP 的話
你就會覺得說，它的進步量
似乎沒有那麼驚人
甚至很多 NLP 的人，現在仍然認為說 deep learning 不見得那麼 work
那我覺得這個，我自己的猜想是
這個原因就是，人在做 NLP 這件事情
文字處理來說，人是比較強的
比如說，叫你說
叫你設計一個 rule detect 說
一篇 document 它是正面的情緒，還是負面的情緒
你可以說我就列表
列一些正面情緒的詞彙跟負面情緒的詞彙
然後看這個 document 裡面， 正面情緒的詞彙出現百分之多少
你可能就可以得到一個不錯的結果
所以，NLP 這個 task
對人來說，你比較容易設計 rule
所以，你設計的那些 ad-hoc 的 rule
我好像可以給你一個還不錯的結果
這就是為甚麼 deep learning 相較於 NLP 傳統的方法
覺得進步沒有那麼地顯著
但它其實還是有一些進步的
只是(覺得沒有)其他的領域
沒有語音和影像處理看起來那麼地顯著
但還是有進步的
那我覺得就長久而言呢
因為文字處理其實也是很困難的問題
你沒有很多幽微的資訊，可能是人自己也不知道的
所以，就長久而言呢
deep learning，讓 machine 自己去學這件事情
還是可以佔到一些優勢
只是一下子，眼下看起來進步沒有那麼顯著
它跟傳統的方法比起來的差異就沒有那麼驚人
但是還是有進步的
這是我一些想法
那再來就是有人會問說
能不能夠自動學 network 的 structure
其實是可以的，你可以去問余天立老師
就是在基因演算法那邊，有很多的 technique
是可以讓 machine 自動的去找出 network structure
不過這些方法，目前還沒有非常的普及
你看到那些非常驚人的應用
比如說，AlphaGo 什麼的阿
都不是用這些方法所做出來的
那還有一個常問的問題就是
我們能不能自己去設計 network 的 structure
我可不可以不要 fully connected
我可不可以說，第一個連到第三個，第二個連到第四個
就自己亂接，可以
一個特殊的接法就是 Convolutional Neural Network
這個我們下一堂課再講
那接下來，第二步
第二步跟第三步真的很快，我們等一下就秒講這樣子
第二步是甚麼呢？
要定義一個 function 的好壞
在 Neural Network 裡面
怎麼決定一組參數它的好壞呢？
就假設給定一組參數
我要做手寫數字辨識
所以，我有一張 image，跟它的 label
然後，把這張 image 呢，這個 label 告訴我們說
因為你現在是一個 Multiclass classification 的問題
所以，今天這個 label 1，告訴我們說
你現在的 target 是一個 vector
你現在的 target 是一個 10 維的 vector
只有在第一維，對應到數字 1 的地方
它值是 1，其他呢，都是 0
那你就 input 這張 image 的 pixel
然後，通過這個 neural network 以後呢
你會得到一個 output
那這個 output，我們就稱之為 y
我把我們的 target，稱之為 y\head
接下來你要做的事情就是
計算這個 y 跟 y\head 之間的 cross entropy
就跟我們在做 Multiclass classification 的時候
是一模一樣的
我們就計算 y 跟 y\head 之間的 cross entropy
然後，接下來我們就是要調整 network 的參數
去讓這個 cross entropy 越小越好
當然你整個 training data 裡面， 不會只有一筆 data，你有一大堆的 data
你有第一筆 data
那它所算出來的 cross entropy 是 C^1
第二筆 data 算出來是 C^2
到第 N 筆 data 算出來是 C^N
那你會把所有 data 的 cross entropy 全部 sum 起來
得到一個 total loss，L
然後，你接下來要做的事情就是
在 function set 裡面，找一個 function
它可以 minimize 這個 total loss
或者是找一組 network 的 parameter
現在寫成 θ*
它可以 minimize 這個 total loss
怎麼解這個問題？
怎麼找一個 θ* minimize 這個 total loss 呢？
你用的方法就是 Gradient Descent
Gradient Descent 大家已經太熟了，沒有甚麼好講的
實際上，在 deep learning 裡面用 Gradient Descent
跟 Linear Regression 那邊沒有甚麼差別，就一模一樣
所以，你要做的事情，只是 function 變複雜而已
其他東西都是一樣的
也就是說
你的 θ 裡面是一大堆的參數
一大堆的 weight 跟一大堆的 bias
你先 random 找一個初始值
random 給每一個數字一個初始值
接下來呢，去計算一下它的 Gradient
計算每一個參數，對你 total loss 的偏微分
那把這些偏微分全部集合起來呢
叫做 Gradient
有了這些偏微分以後，你就可以更新你的參數
你就把所有的參數，都減掉一個 learning rate
這邊寫成 μ，乘上偏微分的值
你就得到一組新的參數
這個 process 就反覆進行下去
你有了新的參數，再計算一下它的 Gradient
然後，再根據你的 Gradient
再更新你的參數，你就得到一組新的參數
按照這個 process，繼續下去
這個都是我們講過的東西
你就可以找到一組好的參數
你就做完 neural network 的 training 了
所以，其實就這樣子啦
deep learning 的 training 就這樣子
就算是最潮的 AlphaGo
也是用 Gradient Descent train 的啦！
所以，大家可能會想像說
如果是 deep learning 的話
machine 的 learning 應該是這樣子
其實上，我們知道說
Gradient Descent 就是玩世紀帝國，所以其實是這個樣子
然後，希望你不要覺得太失望
然後，你可能會問說，那這個 Gradient Descent
的 function 式子長甚麼樣子呢？
之前我們都手把手的把那個算式算出來給大家看
但是，在 neural network 裡面
因為 function 比較複雜
所以，如果我們要手把手的算出來給大家看
是比較難，需要花一些時間
其實，在現代這個時代
我還記得幾年前，在做 deep learning 很痛苦
因為你要自己 implement Backpropagation
那現在呢，你應該已經沒有人 自己 implement Backpropagation 了
因為有太多太多太多的 toolkit
可以幫你算 Backpropagation
那在作業三，我們要做 deep learning，為了容許你用
這些 toolkit
所以，你就算不會算這個微分
Backpropagation 就是算這個微分的一個比較有效的方式
因為參數非常多嘛
你有 million的參數
你沒有辦法為每個 million 的參數 都算微分，這太花時間了
Backpropagation 是一個比較有效率的算這個微分的方式
如果你想要更知道詳情的話
我之前的 deep learning 課呢
內容是有錄影的，你可以聽一下這個上課的內容
如果你不想知道的話呢，其實也沒什麼關係
我聽過一個傳聞是說
就是說有某個公司，它在應徵 deep learning 的人
我就問他說，你們要應徵 deep learning 的人， 會問他什麼問題
他說，我問她說如何算微分
75%、號稱是 deep learning 專家的人，其實不會算微分
所以，大家已經太習慣用 toolkit 了
也不太會算微分了
那 toolkit 很多啦，這邊就列一些出來，給大家參考
那最後，在請助教講作業二之前
我們有一個最後的問題
為甚麼我們要 deep learning？
你可能直覺地說這個答案很簡單啊
因為越 deep，performance 就越好
這個是一個很早年的實驗
這個是 2011 年，Interspeech 裡面的某一篇 paper
他做的是 word error rate
word error rate 是越小越好，你會發現
1 個 hidden layer
每個 hidden layer，2k 個 neuron，word error rate 是24.2
那越來越 deep 以後
它的 performance，error rate 就越來越低
但是，如果你稍微有一點 machine learning 的常識的話
這個結果並沒有讓你太 surprise 阿，因為
本來，model 越多的 parameter
它 cover 的 function set 就越大
它的 bias 就越小，對不對？
你今天如果有越多的 training data
你有夠多的 training data 去控制它的 variance
一個複雜的 model，一個參數比較多的 model
它 performance 比較好，是很正常的阿
那變 deep 有甚麼特別了不起的地方？
有一個理論是這樣告訴我們
這個理論是這樣說的
任何連續的 function
假設任何連續的 function， 它的 input 是一個 N 維的 vector
它 output 是一個 M 維的 vector
它都可以用一個 hidden layer 的 neural network 來表示
只要你這個 hidden layer 的 neuron 夠多
它可以表示成任何的 function
那既然一個 hidden layer 的 neural network
它可以表示成任何 function
而我們在做 machine learning 的時候， 我們需要的東西就只是一個 function 而已
既然 hidden layer 就可以表示任何 function
那做 deep 的意義何在呢？
沒有甚麼特別的意義阿
所以有人說，deep learning 就只是一個噱頭而已
就是做 deep 感覺比較潮
如果你只是把它變寬，變成 fat neural network
那感覺就太虛弱了
所以它沒有辦法引起大家的注意
所以，我們要做 deep learning
真的是這樣嗎？這個我們在往後的 lecture
再來告訴大家
最後就是列一些 reference 阿
如果你對 deep learning 很有興趣的話，你可以先看一下
那還有我上學期講 deep learning 的錄影呢
在這邊也找的到
那另外呢，我有一個 6 小時的 tutorial 的 slide
那你在這個連結上，也可以找到
這個 tutorial ，我 11 月還會在新竹再講一次
這或許是我人生中最後一次講這個 tutorial
千萬不要來聽，因為
為甚麼呢？因為在 tutorial 講的東西
在未來的課程裡面，都會涵蓋，它就只是一個 subset 而已
那接下來來我們就先休息 5 分鐘， 然後趕快就助教來換場一下
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
