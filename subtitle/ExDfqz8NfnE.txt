then we move to the next talk so uh the
third speaker is tang and Dona is a PhD
student at the Chinese University of
Hong Kong supervised by Professor Helman
currently his work focused on audio Tex
unit audio and today he's going to share
with us the challenges in developing
this kind of uh AIO Foundation models
okay don't tell now the stage is yours
okay uh thank you thank you everyone and
I'm very happy to have this chance to
share our uh our development in the
Universal Audio models and I'm my name
is Don and a p student in the Chinese
University of Hong Kong and my professor
is Halon and in this talk I will
introduce from four four
part and firstly and in the past few
years and we have focusing on design and
different tasks for different uh model
audio modalities and for example for
speech we we have developed such as text
to speeech voice conversion speech
enhancement and speaker extraction uh
and a lot of tasks and and also for the
audio and and S modality we we also
develop a lot of tasks such as task to
music Tas to sand generation and the
so so for traditionally uh for for for
those different tasks we try to handle
them one by one and uh actually uh we
have get a very good performance with
those special models however uh we also
find that developing different tax tax
model for different tasks is very uh
costly uh we need to cost a lot of
training and development and the team
cost and also uh we I also fail to
leverage the shared knowledge among
different taxs and also inspired uh by
the successful the m in the lp domains
and the gbt stre models have changed the
the NP do domains and so we also have a
question how about our W domain so
whether we can develop a what fundament
Foundation model to solve different
tasks in a unified
ways and so uh toward building our
universal audio fun models we have two
important problem the first is to find a
unified audio modality representation
for Speed s music and sound uh that's
because the audio signal is very complex
and and and diversity we need a compact
complete and unified reputation because
it's more suitable for the modeling
after that we also need to find a
unified audio reputation modeling
strategy that is we need to find a
simple and effective modeling strategy
to use the scale data set how to help us
to get to get a very good
performance so in the following we will
discuss two aspect the first is the
discret audio reputation we will discuss
which types of audio CCT is more
suitable as the tooner for the audio um
and we we also discuss the audio
modeling strategy and that is it's to
explore uh whether it's possible to
build a un audio function model with the
parad and also we have point out the
what is the biggest biggest challenge in
training based audio fun
models and as we can as we know the
neural audio Cod such as Sun stream and
inod have have been wildly used as the
discret audio rations and it has been uh
widely used in different audio TXS such
as TTS s s generation or music
Generations so uh all of us want to uh
ask a question which type of audio Cod
is more suitable for as the cler for the
audio arm before that we we can we can
review the literature and have two OB
observation the first is uh high high
bit rate codak model bring a challenge
for generating models uh for example the
long it will result the long sequence
problem and the next operation is the
progress work such AUD arm show that
semantic information is more easily to
model than the acoustic information by
the language
models So based on this two operation a
lot of work has begin to towards
building a low bit low bit rate and a
semantic R audio codak and recently many
many R work have been have been proposed
to the low beit rate or the semantic
rate audio
codak so uh uh in samary to to realize
uh low beit rate target PR work try to
uh uh first is try to reduce the VQ
layers such as using a single VQ layer
or fsq to replace the rvq and the second
strategy is to reduce the F rate as for
example uh can pass the one second audio
into to about 12 fors and and S as a
Sound Stream just comprise one one
second audio into 50
frames here I we propose the first
question uh if a what codak is a low B
rate and has a very good reconstruction
performance does it indicate that is a
good uh audio for language
model uh our answer is is maybe not and
we conduct a simple experiment for two
type of audio CCT the first is aicle
based model to use a three aric layer
another another contact is a one one
layer with
fsq and uh and the and and from the
Reconstruction performance we can see
the PQ shows that just using one uh One
V one one one layer can get a very high
reconstruction performance and is a very
a low bit rate but uh dur train we find
the the token prediction accuracy is
very is very is very low and the
performance of their TTS model get a
very high
W the W year have more than the TW 20%
and so so it means that only consider
the Reconstruction performance may not
get uh a good audio for the audio
arm so the next question is the the SE
the semantic information is really
important and our finding is s to is
more easy to fit bym and we and we
design a simple tax to do the audio
continuous that is we use semantic token
and acoustic to train the audio arm and
we found that and the the semantical is
easy to to f to to f by fight by the by
by the language model and we can easily
to realize a audio continuous tax but
for the we if we only use a CO token so
that is very hard to realize this
Target in the same data
set so if we have know the SE semantic
information is very very important so
the question is how to obiz the semantic
information for audio
codu care uh progress work shows uh a
lot of ways in suar the first is
directly to quanze the SSL models such
as H and or we to re B models by VQ or k
but the disadvantage is we need to tr
another decoder to recover those tokens
into uh into the wi
form the second way is try to distill
the uh semantic information from the
from the SL model into the first layer
of the audio coduct uh the rep Network
can be such as the SE spe spe Conor
semantic codak and MIM
codak and the Third Way is try to making
the audio codak and M injoy the same
vocabulary space that is our designed am
codak and and in the next I will give a
introduction about the am codak
so the Cod uh terms to mapping the audio
modality into the token space of of the
front of such as l two and our
motivation is uh uh our current audio
tokens do not have any uh connection
with the text token so it cannot be
directly combined with the p Tred text
um so we wanted to directory mapping the
audio modality into the token space of
for of and using the M vocabulary to
denote the audio tokens so that each
audio will be can be mapping into a
sequence of words so that M can
understand as a new audio language by
several demonstration with its in
contast
line and here we give a detailed uh
analysis to train as model uh we also
use a Styles but we force the different
layer to the different the different
informations for example for the first
vle we we expect it can incl inod the
semantic information so we to distill
the semantic information from and
similar the second V use more token more
tokens to incode the the theost
information and the last year and the
last vular to to ensure a good speed
quality and the important thing is the
way
we we set a different scale for
different layers for example the CTIC
layer we we think we we can use a few
semantic tokens to repr each but for the
acoustic tokens we will use more
acoustic tokens and also we initialize
the V code book with ARs vocabulary and
fix this code book the train so that so
that our what tokens can be easily to
map to the to the to the MS World
vocabulary
so after this screen and we can simply
uh use it for combine with and arm will
be fixed and do not do any or or or
other things and we just need to write a
instruction and then we quantize our
audio into the word sequence so the word
sequence can be easily input to the L so
after for example we can use a vis short
larning to to that the model to learn uh
to learn the representation of of of the
audio so that we can uh after
demonstration we can ask a new example
to ask the model to decide which type of
emotion of this uh audio
includes and similarly we can also do
another audio understanding TX with
visual such as accent classification B
dection and language identification and
spe command recognition and so on and
similar we can also use them to solve
sever simple audio generation tasks for
example we can add to generate the
answer of audio of one 1+
one and uh a simple summary a directly
align AIO token space and the text token
was earlier attempt it Prov a new inside
to integrate the semantic information
into the audio
codak and in the next part I want to to
discuss about the audio modeling
strategies and
uh uh noway there are three commonly
used audio modeling strategy in
generation tax that is the first is AR
based generation uh generative models
and that is transfer the generation tax
as the next token prediction tax and the
second way is latent def based
generative models and uh and and we we
modeling is the continuous latest Bas
with diffusion or flow magine based
model and the third is the disc diff
model that is such as the sand storm try
to predict the discrete token in a
uh STS and in this P we mainly discuss
the based Generation Um models that's
that's this talk will F
Focus so the first problem is uh it is
possible to build a universal audio F
model with um
problem and our our answer is sure and
as we described in progress with the
help of audio tation we can transfer the
audio data into a unified display l
space and then we can observe the text
formulation for different audio related
text and we found that we can easily
formul any text as a sequence to
sequence problem and the both condition
and a Target can be effectively
represented by the
tokenization so that we can design a
unified framework to building to
building this sequence to sequence
problem uh by the decoder only the
model so what is the big biggest
challenge in training on based audio
function moduls and our answer is how to
effective modeling the long audio
sequence U that is which means when we
use our VQ based codak it will doing a
uh long
sequence uh the long sequence problem
for example for a 20 second audio it
will produce a very long sequence that
is uh that is not that that that is very
difficult to directly modeling those
very long
sequence so the problem is how to
effectively modeling the long audio
sequence and we have two observation the
first is the long sequence problem
mainly from the application of rvq and
and we also find that the a token
sequence exist the cority that means the
The Ice token only related to the
previous i i i minus one tokens such
part is very suitable for for the for
the auto regressive Auto regressive
predictions so we develop a high Ral
modeling strategy and we try to use a
global Transformer to modeling the
temporary information between different
parts and local Transformer to modeling
the uh the the the detailed acoustic
information and uh and and we also found
the local Transformer is is far far
small than the global Transformer which
means that uh it will do not bring a a a
very a big INF
cost and we also fin with the help of
this model strategy the ru the a problem
is will not be a problem for the Aron
based
models and K we also compare with the
progress modeling strategy such as the
value St that is to train two separate
models and the flatten that is proposed
in music am but it is very it has a very
high performance but not suitable for
the long audio also we have another
another Strat as the parallel and that
is such as used by the audio G but the
performance is not very good and uh and
also in the music I they propos the
delay the delay pattern this is better
than the parallel but still poor than
the flaton so and use the
Hier mod strategy we can enjoy a very
high performance and uh and it's very
suitable for the uh long audio and as
and as you know as the the pr such as
mus also adopt this similar modeling
strategy to to building their conversion
models
so the the six question is uh to BU
building a universal audio F will be
bring the performance improvement over
the single tax models and our answer is
yes and uh by bring by building the
universal audio fance model and we find
it can bring a better performance than
the pr single Tas models and we also
show that scaling with more data will
bring a better performance in the based
audio fun
models and next we uh give the
conclusion for this
talk uh first for the audio tokenization
and and then we have found the audio
techn plays a very important role in
building the universal audio Foundation
models and the Compact and complete and
semantical re reach audio Cod is always
our desired toer and uh and also we find
exploring a more advanced Cod is also is
the future
directions for the audio modeling
strategies and we have showed based
generative MTH uh is is very effective
and uh other but we can also to try to
explore other generative M such as diff
based P or combine both the based and
diff based model is also uh a a good
direction to exploring
uh that that that conclude my
presentation thank you for the time and
attention so any questions from the
audience any
questions okay while people are think uh
thinking question I have one so you know
in the first talk in the end of the talk
Neil said uh they do not observe the
emerging capability of audio language
which model so in your experiment do you
have any observation of the emerging
capability of the models or do you have
any idea how to make the model have the
emerging
capability uh yeah actually uh in in my
experiment I think when the data scaling
uh is enough and then we can find a uh
get a very good performance but for the
Imaging ability I think uh uh in count I
uh maybe my data is not uh enough enough
large so I do not observe such a very
good Imaging ability yeah yeah thank
you
great thank you D nice talk um in in
your conclusion slides uh in the future
work you mentioned that um one potential
work uh is to uh develop better uh more
Advanced Audio codex as one of future
work um so can you give some insights um
in how to designing better audio code
books to preserve the Paran linguistic
information like emotion um accents and
things like
that uh yeah I think it's a good
question and uh so to I think if we uh
directed between a codak so all of the
linguistic information will be Pro
preserved but the problem is how to
better uh present it and uh so it can be
easily mod by the language model so I
think one of uh direction is maybe we
can direct to I think maybe a lot of
work have tried to do direct to uh quize
the SSL representation so and and add a
a a a decoder to recover this
information uh to the wave form but the
problem is direct uh during your your
try to VU the uh the SSL how to how to
make sure the those information do not
lost so maybe you need to add a lot of
supervised loss to make sure those
information will not be uh discarded in
the during in between between your V mod
no okay and okay we have a question
behind hi don't thanks for the great
talk I have a quick question because for
I want to ask is um for speech we have
semantic token and acoustic tokens and
semantic token is a single layer so
which is good to to to change Timbo
information these kind of things but do
do you think for audio or for music and
we have similar and sematic and acoustic
tokens in the future yeah or a single
layer is good good good enough thank you
yes yes yes I think it's a good problem
so I I think for the even even even for
the music we also have the we can also
build the the cment token such as the
music AR they also use the train uh
SE of of SSL model using their music dat
so they they can also get S for the
music for the music data so I think uh
to building a universal uh semantic
token is also
important and any question from the
audience okay if not let's thank stona
again for his very insightful talk thank
you
