在座的同學，線上的同學，大家好
我們今天就是繼續來講大型語言模型的故事
今天要講的是人類對大型語言模型 在歷史上有兩種不同的期待
這兩種不同的期待其實導致了非常不一樣的結果
這兩種不同的期待對應到兩種不同的技術
一個叫Fintune，一個叫Prompt
語言模型是我們已經介紹過的東西
我們已經跟大家講過說GPT就是一個語言模型
GPT在做的事情就是文字接龍，這是大家都已經很熟悉的事情
語言模型除了GPT以外，另外一個特別知名的叫做BERT
BERT是2018年年底的時候 Google所釋出的一個大型的語言模型
BERT做的事情跟GPT其實略有不同
BERT做的事情是文字填空
文字填空跟文字接龍有點像又有微妙的差異
文字填空做的事情是給機器一個句子
它要做的事情是句子裡面有一些部分是鏤空的
它要把鏤空的部分填回來
所以這個GPT是做文字接龍，把不完整的句子補完
BERT是做文字填空，把一個句子裡面挖空的地方把它補齊
為了講解使用大型語言模型的意象 這邊特別用Midjourney生的一個圖
這個圖的Prompt就是有一隻老鼠騎在大象的頭上 還用韁繩來操控大象
這個是Midjourney畫的圖
這個大象就是大型的語言模型
大型的語言模型在網路上讀過很多資料
它知道很多事情，但是它不知道自己要做什麼
它空有一身力量，但是站在那裡不知道要做什麼
它需要有人來指引它的方向
這個指引它方向的人也許不用花很大的力量
只需要用一點點的力量 就可以引導大象走向我們要它走的方向
所以大象指的是大型語言模型
而大象頭上的這個老鼠 指的是使用這個語言模型的人類
這邊我是要叫Midjourney畫一個老鼠
但不知道它怎麼畫，都畫不出一隻老鼠
它畫出來的老鼠都是像是米老鼠這個感覺，有一點人的樣子
這個大象也有點失敗了，不知道為什麼它有兩個鼻子就死了
發現嗎?它有兩個鼻子，看起來很正常 這在乍看之下你還沒發現有什麼問題
但它其實是鼻子跟腳融合在一起的大象
人們對大型語言模型有兩種不同的期待
第一個期待是 我們期待大型語言模型成為解決某一種問題的專才
它可以解決某一種自然語言處理的特定任務
舉例來說，我們把大型語言模型做一些調整以後
叫它專門做翻譯，你給它什麼句子，它都把中文翻成英文
我給它一個句子，它專門做摘要 看到一段文章作為輸入，它就是會把文章變短
這個是讓大型語言模型變成專才
第二個期待呢 是期待大型語言模型變成一個通才，變得無所不能，什麼都會
舉例來說，你給它一個句子以後 它因為什麼都會，所以如果你只給它一個句子
它不知道它要得到什麼樣的輸出 所以你需要對它用人類可以看得懂的語言
直接下指令，你告訴大型語言模型說，現在把這句話做翻譯，期待它就產生英文的句子
同樣的句子，如果你下不同的指令說要做摘要 它就把你輸入的文字進行摘要
而這個額外所下的用人類語言來描述的指令 期待今天機器可以看得懂人下的指令
這個人下的指令叫做Prompt
大家都知道說，今天ChatGPT走的就是期待二的路線
希望我們的大型語言模型變成一個通才
你今天要叫它做什麼事情，直接用人類的語言去操控它
其實讓語言模型成為一個通才這件事，並不是一件非常新鮮的點子
很早以前就已經有人提出過類似的想法
一個例子是有一篇2018年的文章，大概5年前的文章
它入文的標題叫做Natural Language Decathlon
自然語言處理的十項全能競賽
它要做的事情是什麼呢?在這邊文章裡面
就是講說所有自然語言處理的問題都是問答的問題
什麼叫問答的問題?
問答的問題就是給機器一篇文章
然後問它一個問題，叫它根據你給的文章產出答案
所有自然語言處理的問題，都可以把它看作是問答的問題
怎麼說呢?
假設你今天要做翻譯，就是給機器一段文字
然後你問它說要怎麼把這句話從英文翻成德文 期待它輸出一個德文給你
或者是你給機器一篇文章
你現在的問題是這篇文章的摘要是什麼 期待機器輸出一個摘要
或者是你想做情緒辨識，做Sentiment Analysis
給機器一篇文章，你問它說這篇文章內容是正面還負面的
希望機器可以得出正確的答案
所以在Natural Language Decathlon這篇文章裡面
就已經有說過說，所有的問題都是問答的問題
其實這篇文章裡面所謂的Question，所謂的問題
在今天看起來就是Prompt
只是在2018年的時候，Prompt這個詞彙還不流行
所以在Natural Language Decathlon裡面說
操控自然語言處理模型，做各種不同事情的指令，叫做Question
其實這篇文章也不是最早提出這樣概念的文章
我在更早2015年，Deep Learning剛開始紅起來的時候
就有人提過這樣的概念了，這篇文章叫做Ask Me Anything
從文章的標題就可以知道說，作者有一個很大的野心
他相信所有自然語言處理的問題，通通是問答的問題
他期待有一個模型，問他什麼問題都可以回答
當然在2015年的時候，看起來這個就是天方夜譚
大家會覺得說 嗯，這個有趣的點子，但應該...
一百年內都不太可能實踐吧 但今天回頭看起來不就是ChatGPT嗎?
所以我想要講的就是，讓機器變成通才這件事情
也不是最近才有的點子，是人類一直以來的夢想
文為通才跟專才其實各自有利有弊
那成為通才的好處比較容易想像
我們先來講講成為專才的好處
成為專才的好處就是
這個模型專注在一個任務上面，其他什麼事都不想
所以它比較有機會在單一任務上面擊敗同才
這邊舉一個例子 有一個今年1月的文章叫做《Is ChatGPT a good translator?》
這個是騰訊的文章，他想知道說
ChatGPT可以做翻譯，但是跟現在的商用翻譯軟體
比如說Google Translator比起來，結果怎麼樣呢?
但是ChatGPT你要對它做翻譯之前 你要先對它下指令，要先對它做Prompt
所以這篇文章的起手式是 因為我們並不知道要下什麼樣的Prompt
才是最適合把ChatGPT變成翻譯軟體的Prompt
所以他先問ChatGPT問題 我應該要下什麼樣的Prompt才能夠讓你做翻譯呢?
然後ChatGPT給了十個建議
這群作者再根據這十個建議做一些修改
得到一個他們覺得最適合把ChatGPT當作翻譯軟體的Prompt
結果怎麼樣呢?這是文章中的結果，他們試了好幾種不同的語言
像這邊Zh-En的意思就是 左到右就是把Zh轉成En，就是把中文翻成英文
就是把中文翻成英文，右到左就是把英文翻成中文
這邊就做了十二個不同的翻譯任務
這邊試了Google、DeepL和騰訊的翻譯系統 就是有三種不同的商用翻譯軟體
跟ChatGPT來做PK
這邊的數值是一個叫做BLEU Score的數值
BLEU Score這個數值在作業五我們還會看到 它是通常拿來評量翻譯系統的一種指標
總之數值越大，代表翻譯的品質越好
細節我們就不再講，其實在作業五我們會用到BLEU Score這個評比的標準
你可以很明顯地看得到說，在多數的情況下 其實ChatGPT都是輸給商用系統的
商用系統只單一做一件事情，當然可以比ChatGPT做得好
ChatGPT還要另外做一萬件事情呢，所以翻譯並不是它特別專長的事情
剛才那是騰訊的文章，騰訊的文章是比較少開始做的
他用的是12月版本的ChatGPT，而且他們只試了50個句子
這是有點被人詬病的，因為那時候ChatGPT還沒有API 所以他們應該是人手輸進去的有點麻煩，所以只試了50個句子
在幾週之前，Microsoft本身出了另外一篇文章
就是《How Good Are GPT Model as Machine Translation?》
想要知道GPT系列當作一個翻譯軟體的時候 跟現有最強的翻譯模型比起來怎麼樣
這邊一樣試了各種不同語言間的翻譯，總共試了8個不同的狀況
這邊比較的基準是WMT best
你可以想像在某一個比賽裡面，WMT其實是一個翻譯的比賽
WMT best就是在某一個翻譯比賽裡面，最強的那一個模型
這邊比了ChatGPT，還有Davinci-002、 Davinci-003就是之前的GPT
你就會發現說這個ChatGPT
如果跟WNT best，跟最好的翻譯模型比起來，還是沒有辦法贏的
在多數的狀況下，在WNT比賽裡面最好的模型，還是贏過ChatGPT的
這是專才的好處，假設你只在意一個任務 那你其實應該專門為這個任務打造一個系統
那成為通才的好處是什麼呢?
其實我覺得成為通才最大的好處就是
這個才符合人類對人工智慧的想像
這樣才潮、才上得了新聞，才會被討論
你只專做一個任務，現在沒有什麼特別厲害的地方
沒有人會討論這個軟體
那成為通才的好處除了潮以外
另外一個好處就是，未來要開發新的功能就很快了
從現在開始，你要開發自然語言處理的任務
過去開發新的自然語言處理的任務，是要寫程式
從今天開始，使用ChatGPT，你要開發任務，你不再是寫程式了
你是用人類的語言，直接來操控一個現有的通才的模型
讓它做你想做的事
操控機器，不再是用程式語言，而是用人類的語言。
舉例來說，假設如果你對模型下的指令是請幫我做摘要
但是模型輸出的摘要還是太長了，怎麼辦?
以前你要改程式，很麻煩
現在直接改指令說我要100字以內的摘要
ChatGPT八成看得懂，八成可以符合你的要求
按照你的要求，輸出你指定長度的摘要
所以我們就講了兩種對於語言模型不同的期待
一個是希望它只做一件事情，一個是希望它成為通才
而這兩種期待導致 兩種不同類型的使用與大型語言模型的方式
我們先來看期待一有什麼樣的大型語言模型的使用方式
BERT這個模型，我們剛才提到BERT就是一個做文字填空的模型
這個做文字填空的模型，通常我們在使用它的時候
都是在期待一的情境下來使用它
我們有一個語言模型 我們要這個語言模型成為各種不同任務的專才之前
我們需要先對它做一些改造
什麼樣的改造呢? 這個改造分成兩個面向
一個面向是你需要加一些外掛
第二個面向是你需要對它的參數做一些微調 它才能夠變成專精於某一個任務的專才
那為什麼需要加一個外掛呢?
你想想看，BERT相較於GPT系列
有一個先天的劣勢，什麼樣的劣勢?
它不會講話
它當初學習叫它做的事情就是文字填空
你給它一個句子，中間有一些鏤空
它把蓋起來的地方，把鏤空的地方，把它補上去
這是它會做的事情，但它不會生出一個完整的句子
所以如果你今天的任務是要BERT去產生某一種樣子的答案
或是要它做翻譯，生成一個完整的句子
你是沒辦法直接使用BERT這種模型的
你需要加上外掛
那怎麼幫BERT加上外掛呢?
在過去的上課錄影裡面，我們是有講的，所以我們今天就不細講
基本上有幾個固定的套路，本身在原始的BERT文章裡面 就已經告訴你說有幾個固定的套路
你可以根據你的需求來改裝這個BERT
所以BERT不是一出場就可以使用的，它要做一些改裝 加上一些額外的模組，才能夠做你想做的任務
如果不喜歡看影片的話，這邊有一個寫得很清楚的Blog文章 我把連結也放在投影片上面給大家參考
這邊講的是加外掛
另外一個要做的事情就是，你需要去微調語言模型內部的參數
微調這件事情的英文叫做Finetune
微調的概念是什麼呢?
一個語言模型拿來你要叫它做翻譯，它是不會做翻譯的
所以怎麼辦?
你要先準備一些成對的資料
告訴它說，如果有人跟你輸入Good Morning，你就要輸出早安
然後你就去微調語言模型內部的參數
讓它變成一個翻譯的專才，有人跟它說Good Morning的時候 它可以輸出早安
我剛才有講過說 如果你今天用的語言模型是一個BERT，它沒事是講不了一句話的
所以你一定要加額外的外掛，它才講得了一句完整的話
而外掛是要另外訓練出來的
你還是需要一些標註的資料，才能夠把外掛訓練出
這邊所謂的Finetune的意思，講得更具體一點
就是大家都已經知道的，如果你要調整一個類神經網路的參數 你要怎麼做呢?
你要跑gradient descent
這邊所謂的Finetune，其實就是跑gradient descent的意思
就是你把語言模型原來的參數當作訓練的初始化參數 當作訓練的initialization
之前在作業裡面train模型的時候
你的deep learning的network，你的類神經網路的參數
都是隨機初始化的
但是今天你有一個語言模型了，你要做翻譯
你不需要隨機初始化你的參數，不要從頭訓練起
把語言模型當作初始化的參數，然後用翻譯的資料去做一點微調
你去做幾步的gradient descent 那這個模型可能就有很好的翻譯的能力
這個就是微調的概念，你把原來語言模型裡面的參數
當作初始化的參數，用gradient descent去微調模型裡面的參數
還有另外一個技術叫做adapter
adapter的意思是說，這個語言模型的參數我們都不去動它
我們在語言模型裡面再插入額外的模組，講插入額外模組
講插入額外模組這件事有一點抽象
下一頁投影片會讓你更清楚所謂的插入額外的模組是什麼意思
我們就在大型語言模型裡面插入額外的插件 這個插件可能就是新增加的一個layer等等
有了這個額外的插件以後
今天在fine tune的時候，假設我們要讓這個語言模型可以做翻譯
我們有一些翻譯的資料，在fine tune的時候
語言模型本身是不動的，你只去微調adapter裡面的參數
所以語言模型本身不動，但語言模型加上adapter
加上額外的插件，合起來
它們可以做翻譯，合起來變成一個有翻譯專長的模型
adapter這個插件實際上長什麼樣子呢?
有千千百百種各式各樣不同的變形
你可以在網路上找到一個東西叫adapter hub
一個adapter的倉庫裡面有各式各樣的adapter
Adapter hub它的logo長得像是這個樣子 把BERT加上外掛，讓它變成一個機械BERT
adapter有各式各樣不同的插法，這邊就是列了幾種常見的插法
一種是Bitfit，這邊就很快的帶過去，你有興趣再自己去研究文獻
Bitfit就是把所有的bias當作額外的插件
今天在fine tune的時候，只fine tune那些neuron的bias
weight都不去動它，這是一種做法
一種叫Houlsby，Houlsby就是一個transformer encoder的layer
中間包括了attention，包括了Feed-forward network
Houlsby就在Feed-forward network後面 再接一層Feed-forward network
到時候做fine tune的時候只fine tune額外插入的這一層
AdapterBias是跟Feed-forward的模組平行的
它會對Bitfit forward的output做一些修改 把Bitfit forward的output做一下平移
然後還有這個Prefix
這個Prefix tuning是去改attention
還有一個最近因為跟Diffusion model結合 突然變得很熱門的方法叫做LoRA
LoRA也是去改attention
總之這個插件可以插在各式各樣的地方
那至於插在哪裡好，你得自己試
比如說LoRA在NLP上表現得很好
但是我們實驗室有篇文章是把LoRA是在語音上表現就特別差
講的是全部的adapter裡面最差的
所以一個adapter要插在哪裡才會好
我覺得其實取決於你的應用，這個是你要自己去嘗試的
我只想告訴你說，世界上有各式各樣的adapter 這個插件可以插在各式各樣的地方
那變成adapter有什麼樣的好處呢?
我們剛才講說，如果沒有用adapter的話 我們就要微調整個模型所有的參數
這邊在投影片上是列舉了三個專長
但是我們期待一個機器可以做的事情不止三個任務
比如說有一百個任務，有一千個任務
如果今天要微調整個模型，意味著每一個任務
你都有一組全新的模型，你都有一組全新的參數
而今天的語言模型都很大 什麼GPT-3，175、176個billion的參數
它參數量太多了，這些模型實在是太巨大了
一百個任務就要存一百個模型，一千個任務就要存一千個模型
這個可能在實作上是難以承受的
所以怎麼辦?adapter的方法是，語言模型本身就不懂 我們只插入額外的插件
到時候你真正實際要存的模型參數
其實只有大型語言模型本身GPT-3
只需要存一個，它有176個billion的參數
但是每一個任務，其實你只存了adapter
一百個任務，你就只存一百個adapter就好
adapter通常都很小，它們可能只是一個layer的參數而已
它們是很少的參數，所以存一百個，也許你還是可以接受的
這個是adapter的優勢，就為什麼大家會想要使用adapter這樣的技術
接下來，我們剛才講的是期待一 就是大家通常使用BERT這個模型的方式
接下來我們就要進入期待二，怎麼把模型變成一個通才
也許在講這邊之前，我們也可以看看大家有沒有什麼問題要問的
以後應該要講到一個段落，就停下來看看大家有沒有問題想要問的
我們可以看看線上有沒有同學
好，沒有的話，我們就講……請說請說
我以後都附述一下同學的問題
同學的問題是說 有了adapter，是不是就不用fine tune了?
是，就是你不去fine tune那個大的language model
你只去fine tune那個adapter裡面的參數
所以說，你還是有fine tune，但只fine tune adapter
其實像adapter這種技術 現在又有一個新的名詞叫做efficient fine tuning
fine tune整個model是一般的fine tune
adapter也有fine tune，但它不是fine tune整個模型
所以它有另外一個名詞叫efficient fine tuning 更有效率的fine tuning
接下來要講的就是，怎麼讓機器進一步成為一個通才呢?
