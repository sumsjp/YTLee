臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw
為甚麼我們要用 deep 呢？
接下來就是要講我們用 deep 的理由
那要用 deep 的理由一點都不是新鮮的東西
非常早以前就有人知道了
我記得我昨天跟助教講說，我明天要來
證一下 deep 是不是比較好的，助教就說
這個不是三年前就講過一模一樣的東西了嗎？
三年前聽你上課的時候，第一堂就講了類似的東西
但這跟三年前講的東西是不一樣的
三年前講的時候，我們只有直覺而已
但是，現在在三年後，deep learning 發展得很快
不只有了直覺，還有了更多的理論的基礎
所以，跟之前講的東西是不一樣的
那其實 deep 比較好這一件事情
在很早以前就有這樣子的猜想
舉例來說，你看那個
Yann Lecun 他們在 09 年的時候，寫的時候
為甚麼我們要用 deep network 的時候
裡面就有各式各樣的猜想告訴我們說
deep 是比較好的
只是在過去，並沒有太多的理論的證明
但是，現在已經有了很多理論的證明了
那我把相關理論的 paper 都附在這個投影片的後面
你有興趣的話再參考
今天所講的內容並不是
base on 某一篇 paper 的理論講的
你也知道上課嘛，上課要講的東西是
還是希望你們可以聽得懂這樣子
所以稍微做了一些簡化，希望大家是可以聽得懂的
那為甚麼我們要用 deep，我們都知道說
雖然 shallow 的 network 可以表示任何的 function
你用 shallow network 去 fit
任何的 **** function 到任何的精準度
只要 neuron 夠多
但是，沒有告訴你的是
今天所謂的 neuron 夠多
到底要多到甚麼樣的地步
那我們剛才已經講說，其實你 neuron 的數目是 L / ε
neuron 的數目是 L / ε
接下來要看的就是說，如果我們用 deep 的
加一個 *****，是兩倍
那為甚麼剛才沒有加兩倍呢？是因為
其實你永遠可以想到更好的方法來 fit 這個東西
也就是說，我剛才是用比較笨的方法
是用兩個 neuron 才知道一條直線
其實你仔細想一想，應該用
一個 neuron 就可以知道一條直線了
所以，比較好的寫法就是放一個 big O 這樣子
O(L/ε)，意思就是說前面
不知道 O 是什麼的人，也就是說
這前面有一個常數，我們不太確定這個常數是什麼
那我們等一下就會講說，deep 可不可以做得比這個好
可不可以做得比這個好
但是在講理論的部分之前
我們先回顧一下，在過去十年來
人們是怎麼講的
過去的時候，通常不是理論的證明，就是舉個例子
舉個例子，就是打個比方說
為什麼我們需要 deep，舉例來說我們在寫程式的時候
其實你知道，任何的演算法
都可以用兩行程式就 implement
就好像說，將大象塞進冰箱就只要三個步驟一樣
其實任何演算法都可以用兩行程式 implement
怎麼 implement 呢？舉例來說，假設今天演算法
是 sorting 的演算法
你就把所有 sort 前的字串通通找出來
窮舉所有可能 input
把每一個 output 的可能通通是先算好
當作它的 value
所有可能 input 都是 key
所有可對應的 output 就是 value
A 是一個數字的 sequence
A' 是他 sorting 好的結果
假設你要 implement 一個 sorting 的演算法的話
存一個巨大的 table，把它存好
接下來程式就這樣寫
input 一個 sequence 叫做 K
然後，第一行就是查表，call 一個 function 叫做
match key，看看這邊有沒有那個 key 出現
然後，把那個 key 的
落在第幾個 row 的那個 row 的 number
row 的 id 回傳回來
然後再看那個 row 的 id 對應到的 value 是什麼
把那個 value 吐出去
你就 implement 完任何演算法了，就這樣
所以我們知道說，其實任何演算法
都可以用兩行程式來把它完成
但是，沒有人用這樣的方法
來 implement sorting 這樣
其實，講這樣子的方法
你仔細想想看，他有點像是 SVM 加上 kernel
對不對？如果大家熟悉 SVM 的話
SVM 加上 kernel 是怎麼 implement 的
你有一筆 data, x 進來
然後這邊的 n 代表說
所有的 training data
你的 training data 有 n 筆，從 x^1 到 x^n
你把每一筆 data
input 的 data, x 跟每一筆 training data, x^n
都去計算他們的相似度
K(x^n, x) 代表他們之間的相似度
只是不同的 kernel 代表你使用了不同的相似度
x 跟 x^n 計算了相似度之後
然後再乘上 αn，最後就得到他的 output
所以，計算相似度這件事啊
其實就是 key matrix
乘上 αn，再輸出
其實就是你得到 row id 以後，把你的結果輸出來
只是如果 SVM with kernel 的話
他不是只抽一個 key 而已
他會算跟不同的 key 有不同的 match 程度
然後把你的 value 做 weighted sum
但是，其實 SVM 加上 kernel 他在本質上
他在做這個，解這個 machine learning 的問題的時候
他在描述一個 function 的時候
就很像是這種兩行的演算法
但是我們知道說，你不會用
兩行 code 來 implement algorithm
你會用好多個 step 來 implement algorithm
為什麼？因為這樣是比較有效率的方式
因為你並不需要存一個碩大無朋的 table
那這個是一個比喻
還有另一個比喻是用這個 circuit
來做比喻，假設你是電機系的同學的話
一定修過邏輯電路設計，這個是必修
那在邏輯電路裡面，我們學到說
所有的邏輯電路都是由 gate 所構成的
就好像 neuron，neural network 都是由 neuron 所構成的
那我們知道只要兩層的邏輯閘
就可以組成任何的 boolean function
我們都知道說
只要一個 hidden layer 的 network
一個 hidden layer network 其實也是兩層
hidden layer 跟 output layer 也是兩層
可以表示任何的 continuous function
但是你不會用兩層的邏輯閘來設計電路
因為設計出來的電路太過龐大
如果你有用 hierarchical 的結構的話
你有用 multi-layer 的話，設計出來的電路
會是比較精簡的
所以，如果你用 deep 的結構來設計電路
你的電路裡面，邏輯閘是有好幾層的
那你設計出來的電路會比較精簡
你要做同樣的事情，只需要比較少的 gate
那對 network 來說，在過去就是
也不怎麼證明，就打個比方這樣子
就說 A 是這個樣子，B 應該也是一樣吧
如果有很多個 layer 的話
就有很多個很多個 layer 的 neuron
那你要描述一個 function 的時候，應該比較容易
所以你只要用 deep 的架構，只需要比較少的 neuron
就可以描述同樣的 function
那這樣可能會被反駁說
A 很像 B，並不代表他們的性質就是一樣的
但他們確實是很像的，理論可能是相同的
那下一頁這個例子
我打算把它略過，你可以自己看一下
當我們今天講到 network 的架構
把 network 的架構跟 deep 的 network
勾在一起的時候，通常就舉這個例子，告訴你說
在電路裡面
如果你要 implement 一個叫 parity check 的電路
你可以用 shallow 的 network 來 implement
但是，如果你 implement 的長度是 d 的話
你要 O(2^d) 的 gate
但是，假設你今天用一個 deep 的架構來 implement
實際上的細節就不管了，反正就弄成這個樣子
deep 的架構，那其實你只需要
O(d) gates，就可以做到這件事了
我想這個是大家都知道的事情
但這就只是打個比方，而不是一個證明
接下來，我們要講說
deep 為什麼他有潛能
有機會可以比 shallow 的 network 更好呢？
在早年，所謂早年是指大概
14年(2014)，15年(2015) 那個時候
這個東西是 Ian Goodfellow 教科書裡面都有寫的
所以他是比較早的東西，我們就知道說
假設一個 network，它是 Shallow 跟 wide
跟假設另外一個 network，他是 deep 而 narrow 的
在他們有差不多參數量的情況下
這個時候，shallow 而 wide 的 network
他可以產生出來的 piecewise linear function 的這個
piece 是比較少的
而 deep, narrow 的架構
他可以產生出來的 piece 是比較多的
如果你今天有一個
shallow 跟 deep 的 network，在相同參數量的情況下
shallow 的 network 可以產生出來的 piece 是比較少的
deep 的 network 可以產生出來的 piece 是比較多的
怎麼說呢？
我們先來看看，假設給你一個 network 的架構
就隨便拿一個 ReLU 的 network 給你
這個 network 會有多少的 piece 呢？
他是 piecewise linear 的 function 嘛
他會有多少個 linear 的片段呢？他會有多少的 piece 呢？
我們先來看一下他的 upper bound
就是最多會有多少
怎麼想他的 upper bound 呢？
我們大家都對於 ReLU network 都很熟嘛
你知道在 ReLU network 裡面呢
每一個 neuron 有兩個 operation 的 region，對不對？
一個 operation 的 region 是
這個 neuron 的 output 是 0
另外一個 operation 的 region 是
input = output
那我們之前在 machine learning 的課裡面
我們都講過 ReLU 的 network
那時候會告訴你說，0 的那些
就把它拿掉，他就好像不存在一樣
剩下的部分，就好像是一個 linear 的 function
這個時候很多人就會問我說
其實 ReLU 只是 linear 的 function，那不是很弱嗎
但他不完全是一個 linear 的 function，他是一個
piecewise linear 的 function
當今天你的 neuron 都作用在同樣的 region 的時候
他是一個 linear function
但是，他今天換了，有一些 neuron 的 region
他的 operation 的狀態換了的時候
他就進入了另一個 linear 的 region
這樣大家應該知道我的意思吧，就是
這個是你的 input, x，這個是你的 output, y
在某種 activation function 的 mode 下面
他是一個 linear 的 function
但是你看今天 input 超過某個範圍
某一個 neuron 的 operation 的 mode
換掉了，就本來他可能是
output 都是 0，現在變成 input = output
或他本來 input = output，現在變成 output 都是 0
那你就變成另外一個 linear 的 function
所以，今天我們來分析一個 ReLU 的 network
他有幾個 piece，他的 upper bound 就是
看你有幾個 neuron
每一個 neuron 他都可以是
作用在兩個 mode 的其中一個
所以，所有的 neuron 的 mode 的可能性
我們就這邊叫他 activation 的 pattern
所以 activation pattern 的意思就是
某一種 neuron 的 mode 的組合，比如說這邊是
linear, linear, 0, 0
0, linear, 0, linear
這個叫做一種 pattern
你也可能有別種 pattern
總共有多少種 pattern 呢？
這個是國小數學，每一個 neuron 有兩種可能性所以
假設有 n 個 neuron，就是有 2^n 個
可能的 activation pattern
所以有 N 個 neuron， 就有 2^N 個可能的 activation pattern
8 個 neuron，就有 2^8 個可能的 activation pattern
每個 activation pattern 都只造了一個 linear function
所以，想起來好像是有 N 個 neuron
有 2^N 個 activation pattern
那我們用這一個 network 架構所訂出來的 function
應該要有 2^n 個 linear 的 pieces
有 2^n 個片段
這個其實是一個 upper bound
這個是一個 upper bound
這個是一個最佳的狀況，你不可能
事實上，為什麼這是一個最佳的狀況呢？因為
為什麼這個狀況不一定能夠達到呢？
因為有些 pattern
可能是永遠沒有辦法出現的
有些 pattern 是不可能出現的
舉例來說，我們舉一個這樣子的例子
這是一個非常簡單的 ReLU 的 network
只有一層，只有兩個 neuron
我們剛才說，每一個 neuron 有兩個 mode
所以今天，按照這個 network，他 activation 的 pattern
其實應該要有 4 種，對不對，就是
他的 activation 的 pattern，其實應該要有 4 種
但是，你實際上去想一想
你會發現說，這一個 network 的架構
他只定義得出，這三個 pieces 而已
他定義不出 4 個 pieces，對不對？因為
有一些 operation 的狀態
是不合理的，是沒有辦法呈現的
我不知道大家能不能夠接受這個想法
你可以回去想一下，你把這一個 network 的架構
你把他的參數試不同的參數
你挪來挪去，你可能就只造得出三個 piece 而已
因為有一些，你只能產生三種 activation pattern
你沒有辦法產生四種 activation pattern
其實，事實上呢，今天假設我們有一個 network
他是只有一個 hidden layer，有 n 個 neuron
本來，按照 Upper Bound 來想
應該要有 2^n 個 activation pattern
他應該可以產生 2^n 個 piece
但是，如果你仔細想一下的話，你會發現說
今天只有一個 hidden layer 的 network
他假設那個 hidden layer 的 neuron 是 n
其實他弄得出來的 pice 的數目
其實只是 O(n) 而已，可能是 n+1 這樣
我們沒有辦法弄出 2^n 個不同的 pieces
這邊我們剛才講的是 upper bound
所以從 Upper bound 看起來
雖然我們知道說，那個 upper bound
實際上可能是達不到的
就是我們說，今天給我們一個 ReLU 的 network
他最多可以製造出來的 function
會有幾個 linear 的 piece 呢？
最多是 2^n 個 piece
但這是一個 upper bound，很有可能，你怎麼調
都達不到那一個 piece 的數目
接下來，我們要講的是
那 lower bound 呢？
那你要講 lower bound 很簡單，就是
兜一個 network，看看我們可以製造出多少個 piece
那個 piece 就是我們可以製造出來的
piece 的數目的 lower bound
所以，我們就是講一個 network
找一個 ReLU 的 network
然後，分析一下說
我們根據這個 ReLU 的 network
我們可以製造出多少的 pieces
那在講這一段之前呢
我們先製造一個特別的 activation function
這個特別的 activation function 叫做
取絕對值的 activation function
比如說，你的 input 是 x
我們把 x 乘上 w，再加上 bias, b
通過這個 activation function 以後呢
會取絕對值
怎麼 implement 這種取絕對值的 activation function 呢
其實你可以把兩個 ReLU 的 activation function
組合起來，就可以變成一個
取絕對值的 activation function
怎麼做？我們今天有兩個 ReLU 的 neuron
我們有兩個 ReLU 的 neuron
今天 x 進來
走上面這個 ReLU 的 neuron 的時候
他乘以 w，再加上 bias
走下面這個 ReLU 的 neuron 的時候
他乘上 -w，再減掉 bias, b
今天，這兩個都是
這兩個都是 ReLU 的 activation function
所以，如果今天，(w*x + b) > 0 的話
那就是拿這邊的輸出，這邊的輸出就是 0 嘛
如果 (w*x + b) < 0 的話
就是拿這邊的輸出，這邊就是 0 嘛
所以，仔細想一下就會知道說
用這個方法，你用 ReLU 的 activation function
你可以製造一個取絕對值的 activation function
你可以去製造一個
像這樣子取絕對值的 activation function
那接下來呢
我們要講的事情是
如果我們現在有這樣子的 activation function 的話
像這樣子的 activation function
我們就放一個這樣子的 neuron
然後，我們的 input 是 x
我們的 output 是 a1
我們 input 的 x 跟 output 的 a1 間
可能會有什麼樣的關係呢？
他們的關係可能是
這個樣子，可能是這個樣子
在 0 到 1/2 中間
他的值是從 1 逐漸下降到 0
從 1/2 到 1 中間
今天 output a1 的值是從 1/2 逐漸上升到 1
那我們假設說
第一個 hidden layer 做的事情就是這樣
那我現在假設我們加上第二個 hidden layer
第二個 hidden layer 就是把 a1 吃進去
然後，變成 a2 吐出來
我們假設
第二個 hidden layer 跟第一個 hidden layer 做的事情
其實是一樣的
a1 跟 a2 的關係
和 x 和 a1 的關係，其實是一樣的
當 a1 的變化從 0 到 1/2 的時候
a2 從 1 下降到 0
當 a1 從 1/2 變到 1的時候
a2 從 0 上升到 1
x 和 a1 的關係和 a1 和 a2 的關係，其實是一樣的
那我們知道 x 和 a1 的關係
我們就知道 a1 和 a2 的關係
那這整個 function 長什麼樣子呢？
也就是說，x 跟 a2 之間的關係
像是什麼樣子呢？
我們來想一想
input 是從 0 到 1
我們現在就是要看說，x 從 0 到 1 的時候
a2 是怎麼樣變化
是怎麼樣變化
那今天這個 a2 呢
在 a1 是 1/2 的地方
會有一個轉折的點
所以我們在考慮的時候，畫一個 a1 = 1/2 的線
然後，從 0 到 1 之間
我們分成四個區段來考慮
我們考慮 0 到 1/4
考慮 0 到 1/4
1/4 到 1/2
1/2 到 3/4
3/4 到 1，分成四個片段來考慮
如果考慮第一個片段
考慮第一個片段，x 從 0 變到 1/2 的時候
x 從 0 變到 1/2 的時候
a1 從 1 變到 1/2
那 a1 從 1 變到 1/2 的時候，a2 呢？
他是從 1 變到 0
所以今天 x 從 0 到 1/4 這段距離
當從 0 到 1/4 這段距離有變化的時候呢
a2 是從 1 變到 0 的
這個是第一段
那如果你分析第二段從 1/4 到 1/2 的時候
分析從 1/4 到 1/2 的時候
x 的變化分從 1/4 到 1/2
a1 的變化是從 1/2 一直跑到 0
它從 1/2 一直跑到 0
這個其實非常難想像啦
我不知道大家聽不聽得懂這樣子
如果你聽不懂的話，你就自己回去
你就自己仔細想一想，這個畫圖也不知道要怎麼畫才好
這段 x 是從 1/4 到 1/2
那 a1 是從 1/2 變到 0
所以 a1 是從 1/2 變到 0
a2 是從 0 變到 1
所以， x 從這邊到中間
從 1/4 到 1/2 的時候
a2 是從 0 變到 1 的
所以是這個樣子
再講下去你可能就會覺得有點無聊了
所以，今天 x 從 1/2 變到 1/4
那 a2 會有什麼樣的變化呢？他的變化是怎麼樣
x 從 3/4 變到 1
a2 的變化是這個樣子，所以他畫了一個
w 的形狀，他畫了一個 w 的形狀
所以，今天我們知道說
a2 的輸出是這個 function
有兩個 neuron 的 function，他是 w 的形狀
那我們再加上第三個 neuron 呢？
如果再加上第三個 neuron 的話
會發生什麼樣的事情呢？
我們假設第三個 neuron
跟前面兩個 neuron 做的事情是一模一樣的
只是 input 是 a2
output 是 a3，那 a2 跟 a3 的關係
長得是這個樣子
當我們加上這個紅色的線的時候呢
你就會發現說
你加這個紅色線以後
接下來，你就分析一下這個 x 從 0 變到 1
你要分成 8 個區間去考慮
8 個區間去考慮
你要從 0 分析到 1/8
就 x 從 0 變到 1/8 的時候
a2 是從 1 跑到 1/2
是從 1 跑到 1/2
所以 a3 會從 1 變到 0
然後你就把每一個
piece，每一個小段啊
這個 x 跟 a2，a2 到 a3 的關係，通通畫出來
看起來就是這個樣子，所以
本來是一個 w 的形狀
現在變成兩個 ｗ 的形狀
變成兩個 ｗ 連在一起，變成很多鋸齒的形狀
所以，現在你就會發現說
本來 a1 他是長這個樣子
就 a2 跟 a3 的關係和 x 跟 a1 的關係長得是一樣
他們長這樣
他總共有兩個線段，就 2^1 個線段
到 a2 的時候，他總共有
4 個線段，2^2 個線段
到 a3 的時候，它就變成有
2^3 個線段
所以，你會發現說呢
你會發現說呢，今天當我們用 deep structure 的時候
每次我多加了一個 layer
其實我的每個 layer 只有兩個 neuron
我每次多加了兩個 neuron 的時候
我們可以產生的 linear 的 region
可以產生出來的線段的數目
就會變成兩倍，就每多加兩個
你的線段的數目就會 double
所以，如果我們今天比較
shallow 的 network 跟 deep 的 network
在講 shallow 的 network 的時候
我們每次兩個 neuron 組合起來才產生一個線段
所以今天如果你要產生 100 個線段
你就要 200 個 neuron
但是，今天如果是 deep 的 structure
每次多加了兩個 neuron ，這邊這個
取絕對值的這個 neuron
就是他是兩個 ReLU 所組成的
每次多加一個 layer
那個 layer 就只有兩個 neuron
你都可以讓你的線段的數目多增加兩倍
所以，你今天
舉例來說，你要產生 100 個線段
100 個線段是多少？
我只要 2 的 7 次方
對不對，我只要 7 個 neuron
我就可以做到那件事情了
結果發現，你要產生有比較多片段的
你要產生有比較多片段的
function 的時候，用 deep 是比較有效率的
如果你仔細回想一下
為什麼 deep 可以產生比較多的線段
他做的事情比較像是摺紙一樣
對不對？或者是他其實是把同樣的 pattern
反覆的出現，就本來你只有一個 v
然後第一層這個 v
第二層再把兩個 v 接起來變成 w
第三層則是把兩個 w 接起來
變成有兩個 ｗ 拼在一起
接下來，下一層就把兩個 w
再 double，變成有 4 個 w
他是這樣的，他是把原來你的 piece 不斷的反覆產生
有點像這個，不知道大家能不能體會，他就像是
那個雪花結晶的結構這樣
他不斷地把原來的 pattern，不斷地複製
他可以產生很多個 piece，但那些 piece 是有規則的
那講了這麼多以後呢
其實你可以非常輕易地
就證明說，假設你現在的 network
寬度是 k，深度是 h
那你可以製造 k^h 個片段
我們剛才是寬度是 2
我們可以製造出 2^h 個
就寬度是 2，深度是 h
我們可以製造 2^h 個片段
那現在你可以輕易地自己想出來說，假設現在
寬度不是 2 而是 k
那深度是 h，你可以製造出
k^h 個片段
其實你可以非常輕易地想出
找到一個 network 可以做一件事情
那這個東西就是這個 network 的架構
可以產生的 piece 的數目的 lower bound
所以，今天這個結果告訴我們什麼
告訴我們說，你可以產生的片段的數目啊
是 k^h 次方
所以，h 也就是深度，是放在指數的地方
所以，當你增加你的深度的時候
你可以非常快的增加 piece 的數目
如果你想要知道更多更細的證明的話
下面列了一大堆 paper 這樣
舉例來說，第一篇這個
前面兩篇是這個 Benjo 的 paper
其實是最早做這樣子分析的 paper
那在裡面呢，因為我們今天
在上課的時候，我們都假設 input 只有一個 dimension
那他們不是這樣，他們會假設比較複雜的 case
就 input 是第一個 dimension
那這個時候狀況就比較複雜了
沒有今天得到的式子那麼單純
原來在 ICLR, 2014 的 paper 裡面，他們
得到了一個 lower bound
後來呢，他們又在
下一篇 NIPS, 2014 裡面
就 improve 了那一個 lower bound
後來也有很多人做類似的嘗試去做
去繼續 improve 那個 lower bound
那就把這個文獻列在這邊給大家參考
那我想要講的是最後一篇
在最後一篇除了有理論的證明之外
他還做了一些實驗
因為我們剛才講的只是一個
理論上是這個樣子
就我們用了一個很奇怪的方法
兜出了一個 ReLU 的 network
然後分析說，嗯，這個 ReLU 的 network
他有很多很多的 piece
可是你可能就覺得說，欸
這個會不會是一個非常非常 specefic 的 case
也許在一個正常的狀況下，你 train 一個 network
你根本就不會產生那麼多的 piece
所以，他就做了一個
做了一些實驗來 verify 這些事
他有一個實驗是做在 MNIST 上
在 MNIST 上面
你這個有不同的
我們先看第一個圖
第一個圖的橫坐標代表的是
network 的深度
2 層, 4 層, 6 層, 8 層, 10 層, 12 層等等
那不同的顏色代表不同的寬度
50 個 neuron, 100 個 neuron
500 個 neuron, 700 個 neuron 等等
然後這個 scl 啊，大家不用太在意他
他是 train network 的時候不同的 initialization 的參數
接下來這邊他做的實驗就是
他把這個 network 先拿出來
然後再看說， input 從
某一點到某一點的時候
output 總共通過了多少個 piece
了解嗎？network 就是一個 function 啊
就是一個 function ，piecewise linear function
然後他就算說，從某一點
到某一點總共經過了多少個 pieces
那今天這個圖啊
縱軸就是 pieces 的數目
那注意一下這個縱軸
他是 exponential 的
所以今天這個直線的上升
其實是 exponential 的上升
所以今天固定你的 network 的 size
固定你的 network 的寬度，不是 size
固定 network 的寬度
增加深度的時候
你會發現，這個時候你產生 pieces 的量
就算在實際的 case
他也是 exponential 增加的
剛才講說，那個 upper bound 就是
寬 k 的深度的 h 次方
那這是一個理論上的
想像，但是在實際上
在實際的 application 上
你可以觀察到這樣的現象
當你的 depth 增加的時候
你產生的 piece 的數目是 exponential 增加
另外，這個實驗是
layer 的數目固定，但是橫軸是改變 layer 的寬度
有 200 個 neuron, 400 個 neuron
600 個 neuron, 800 個 neuron 等等
縱軸，一樣是 exponential
不同的顏色代表不同的深度
兩層、四層、六層等等
你會發現說，今天如果是固定你的深度
但是，只改變你的寬度的時候
對產生的 piece 的數目會影響有多大？這個線呢
基本上看起來像是直線一樣
這個是第一個實驗
他做了另外一個有趣的實驗是說
他假如他這樣做
就拿出一個 network
他 paper 裡面其實沒有講得很清楚
這個 network 是哪來的
拿出一個 network 有很多層
那他在 input 的這個 space 上面啊
畫一個圈圈，假設 input 是二維的，畫一個圈圈
那通過第一個 hidden layer 以後
那些 neuron 不是會有 output 嗎？
不是會變成一個 100 維的 vector 嗎？input 二維
假設你的第一個 hidden layer 是 100 維
他會變成一個 100 維的點嘛
但是你在 input 的時候，你是畫一個圈圈
那在高維的空間中
你也是走了某一個軌跡，對不對？
假設你的 hidden layer size 是 100 維
那在 100 維的空間中，你也是走了某一個軌跡
只是它不見得是圓的
他就把這個 100 維的軌跡
100 維空間中的軌跡 project 到二維，他說
看起來像是這個樣子，這是第一個 layer
第二個 layer 看起來像是這個樣子
第三個 layer 看起來像是這個樣子
第四個 layer 看起來就是這樣
就越來越複雜，就本來你在 input 的時候
你只是畫了一個圈圈
但是，通過很多個 layer 以後
這個軌跡，在 nerwork 的 output 看起來
越來越複雜
這個軌跡越來越複雜，直到最後變得非常的複雜
你會發現說，這個複雜的結構裡面
其實，如果你仔細看一下，他是有 pattern 的
他並不是一個完全隨機的複雜的結構
而是有某一些的對稱性這樣
這邊有一些，某一些有趣的對稱性
就好像是，雪花那個樣子
所以，就呼應我們剛才講的
說 network 他可以產生很多的 piece
那產生的這些 piece 中間，他是有某一種 pattern 的
他是把 v 變成 w，再把兩個 w 接起來這樣
他是有一個固定的 pattern，他不是隨機的
產生那些 piece
那個 paper 還有另外一個實驗， 這個實驗想要驗證的是說
low layer 的參數
就比較靠近 input 的那個 layer 的參數
相較於比較靠近 output layer 的參數
他是比較重要的，所以比較靠近 input 的那些參數
是比較重要的，因為在直覺上
想起來顯然是有道理的，因為
我們今天在做 deep learning 的時候
就好像是在折紙一樣
前面的 layer 做的事情就是去摺紙
那在折紙的時候，第一次對折是最重要的
對不對，你第一次對折就歪掉了
下面你再折就通通是歪的
所以，這一個實驗想要驗證的是
low layer 的參數是比較重要的
那怎麼驗證呢？他先做了一下 CIFAR-10
他在 CIFAR-10 上面拿出一個正確率非常高的 network
然後，在 network 的參數上面
加一些 node，那分別加在
第一層、第二層，一直加到第七層
那發現說，假設現在
那些 noise 是加在最後第七層的話
對結果幾乎沒有影響
下面這個是 noise 越加越大
noise 越加越大
對縱軸這個正確率，本來正確率是 100%
但是，你加一些 noise 幾乎沒什麼影響
但是，如果你加在第一層
一樣的 noise，加在第一層，整個結果就壞掉了
整個結果就突然爆炸這樣子
顯示說，第一層的 network 是非常 sensitive 的
你只要稍微加一點 noise，他就會壞掉了
另外這個實驗，如果沒記錯，應該是做在 MNIST 上面
縱軸是正確率，這個實驗是這樣子的
這個實驗是說，拿一個 network 出來
我們只 train 某一個 layer
其他 layer fix 住都是 random 的
只 train 一個 layer，其他都 fix 住
這邊的顏色跟這邊是一樣的
所以，這個紫色就是這邊的紫色
假設我們只 train 第一層
第二層到最後一層通通是 error，通通是 random 的話
其實你也可以得到大概 90% 的正確率
其實這沒有很高，因為 MNIST
MNIST 胡亂做都是 98% 這樣
所以 90% 是很差這樣子，但是
神奇的就是說，我只 learn 了第一層
後面都是 random
還是有 90%，顯示第一層非常的重要
但是，假設我們是說
前面都是 random
只 learn 最後一層
結果就很爛這樣子
所以這告訴我們說
deep network 裡面，前面的 layer 是 比較 sensitive，比較重要的
我們在這邊休息 10 分鐘，等一下再回來
剛才講的是用 shallow 的 network
來 fit 某一個 function
剛才又講了說假設比較 deep 和 shallow network 的話
deep 的 network 可以製造出比較多的片段
但是，這並沒有完全的
deep 的 network 可以製造出比較多片段這件事情
跟我們要 fit 某一個 function
並沒有直接的相關
我們現在真正關心的問題是
如果用 deep 的 structure 來 fit 某一個 function 的話
會是什麼樣子？
那我們假設我們現在要 fit 的 function 呢
是一個 比較簡單的 function
從這個比較簡單的 function 討論起
這個 function 是 f(x) = x^2
他長得就是這個樣子
x^2 在 0 到 1 區間呢
長的是這個樣子
現在就算是
我們不管是用 shallow 的 structure
還是 deep 的 structure
我們製造出來的 function
都是 piecewise linear 的
所以在討論怎麼用一個 shallow 的 network 或者甚至是
deep 的 network 來 fit 某一個 function 之前
我們要討論第一件事情
都是怎麼用一個 piecewise 的 linear 的 function
去 fit 我們現在的 target function
我們現在 target function 是 x^2
怎麼用一個 piecewise linear function
去 fit 這一個
f(x) = x^2 這個 function 呢
我們現在定義另外一個 function 呢
叫做 fm(x)
那這個 fm(x) 他總共會有 2^m 個片段
那 f1(x) 長的是這個樣子
長得是這樣
這個 f1(x) 怎麼來呢？你就把這個
你就取 0.5 的地方
然後跟頭接起來
跟尾巴接起來
就得到兩個片段
所以今天 f1(x) 呢
總共有 2^1，也就是兩個片段
接下來 f2(x) 呢？f2(x) 有 4 個片段
這 4 個片段怎麼產生呢
你就把 x 軸，橫軸啊
橫軸等於 0 到 1/4 的地方接起來
x 軸等於 1/4 到 1/2 的地方接起來
1/2 到 3/4 的地方接起來
3/4 到 1 的地方接起來
我知道你在台下其實很難看得清楚這樣
但是畫出來就是這個樣子，我也沒有辦法這樣
所以 f2(x) 他有 4 個片段
2^2，總共 4 個片段
他是從 0 到 1/2（此處口誤，應為 1/4）
0 到 1/4，1/4 到 1/2，1/2 到 3/4，3/4 到 1
總共四個片段
那你會發現說其實
f2(x) 其實跟 x^2 其實也滿接近的，所以
你有點看不清楚 f2(x) 在哪裡
哪如果你畫 f3(x) 的話
他就有八個片段
畫 f4(x) 的話，他就有 16 個片段
以此類推
接下來，我們問的問題是
這個 m 的值到底要到多少的時候
就是我們先給定這個 ε
m 的值要到多少的時候
我們才能夠讓 f(x)
跟 fm(x) 他們之間的最大的差
小於等於 ε 這樣
當然我們知道說，m 越多
fm(x) 跟 f(x) 之間的差異就越小
你這邊畫越多的片段
那你畫圖做出來的 fm(x) 跟 f(x)
也就是 x^2，他們就越接近
但是，假設今天他們的差距啊
不可以超過 ε，要小於等於 ε 的話
這個 m 應該要有多少呢？
你實際上算一下的話，這邊就省略掉計算過程
你就回去怒算一波這樣子
如果我算出來有錯你再告訴我這樣子
你就回去怒算一波，你就知道說
今天要如何讓，m 要多少
才能夠小於等於 ε 呢？
才能夠讓他們差距小於等於 ε 呢？
這個 m 要大於等於 -1/2log2(ε)-1 的時候
這個 m 就會小於等於 ε
你說怎麼算哦
你就把 fm(x) 的式子列出來
然後你就可以算他跟 x^2 的差距
這個計算並不困難，他只是有點繁瑣
然後，你就可以算出說
怎麼樣可以讓他們最大的差距小於等於 ε
總之，你算出來就是這個樣子
你說，欸這邊有一個負號
那是不是這一項是負的呢？不是啊
ε 是小於 1 的
ε 是一個很小的值
懂嗎？ε 是一個很小的值
所以 log2(ε) 是負的
負的東西乘上負的東西是正的
然後，再減掉 1 這樣
ε 可能是一個很小的值
比如說，2^(-7)
log 這邊就是變成 -7
(-7) * (-1/2) 變成正的
所以 m 要大於等於這個數值
才能夠讓它小於等於 ε
當然今天 ε 的值
越小，m 就要越大，這樣
那 m 是這個樣子，需要多少個片段呢？
那你就這邊取 2^m
這邊取 2 的這一項次方
所以，你今天需要的片段的數目
至少要大於等於 1/2 * 1/sqrt(ε) 個片段
這個值怎麼來的呢？
你就是取 2 的這個次方
你就把這個東西呢
放在指數項，2 的這個次方
就是 1/2 * 1/sqrt(ε)
講到這邊，假設你沒有跟上的話
你只要知道說，現在有一個
x^2 的 function
我們今天，如果要用
2 的 m 次方個
一樣寬的片段
去 fit x^2 這個 function
那我們的 2^m 的數目
一定要大於 1/2 * 1/sqrt(ε)p[ieces
那你會發現說，這個東西算起來
其實是比剛才，我們在第一堂課裡面得到的結果
l/ε 還要小的，對不對？
你想想看，這邊他是除 ε
這邊是除根號 ε
所以，如果你算他的 big O 的話，這一項
是比較小的
那這個結果其實也很合理的，因為
因為在第一堂課我們討論的是 general 的 case 嘛
任意的 function
這邊我們只討論 x^2
那你需要比較少的，根據 x^2 的特性
所以，你需要比較少的 function
你需要比較少的片段就可以
fit 他，這個結果也是頗為合理的
那現在，我們剛才講過說
假設是一個 shallow network 的話
你要兩個 ReLU
才能夠產生一個 piece
但其實更少一個
有時候一個 ReLU 就可以產生一個 piece
無論如何，你需要的 neuron 的數目
就是 O(1/sqrt(ε))
你需要至少這麼多的 neuron
就假設你是 shallow 的 network
你需要至少這麼多的 neuron
才可以 fit f(x) = x^2
這個是 shallow 的狀況
那我們來看一下 deep 的狀況
我們來看一下 deep 的狀況
deep 他的厲害的地方就是
他要製造出這麼多 pieces
其實她並不需要這麼多的 neuron
如果我們要用 deep 的 network 來產生 f(x)=x^2
那要怎麼做呢？你看哦
你要產生 f1(x) 的話
你要產生 f1(x)
你其實只需要把一條
斜率是 1 的斜線
減掉這一個東西
他就是 f1(x)
這樣大家可以接受嗎？
想想看哦，這個是，我們先把
x^2 畫出來
x^2 假設畫出來是這個樣子
把 f1(x) 畫出來這樣
假設 f1(x) 畫出來是這個樣子
換藍色好了，是這個樣子
那 f1(x) 跟這一個斜率是 1 的直線
他們中間這邊差多少
假設這邊是 0.5 的話
這邊的高是 (0.5)^2 是 0.25
這邊的高
這是一個斜率是 1 的直線
我知道我畫的很不標準，但是這邊的高
是0.5，所以這邊的差距是 0.25
所以，你把這一個斜線
減掉這中間的差
這邊最寬最高的地方是 0.25
後面會慢慢變小這樣
這邊是 0，他頭尾的地方都是 0
中間差距最大是 0.5
就會變成這個 f1(x)
這一個 piecewise linear 的 function
這樣大家可以接受嗎？所以其實你把
這一個藍色的斜線
減掉這一個三角形
這個三角形就是這邊這一塊
雖然看起來不像，但他就是啊
減掉這一塊現在塗顏色的地方
他就變成了 f1(x) 這樣子
接下來怎麼
所以我們現在可以用這兩個 function 相減
製造出 f1(x)
怎麼製造出 f2(x) 呢？
你就再產生這樣子、這樣子
有兩個鋸齒的形狀
第一個鋸齒減在這邊
第二個鋸齒減在這邊
你就產生 f2(x) 了，這樣
這樣 ok 嗎？就是
講到這邊大家還有問題要問的嗎？
你說
這兩個三角形嗎？
他們高度是一樣的
你可以自己 check 一下他們高度是一樣的
那這個，我們就不要畫圖好了，這個
這個圖其實是很難畫的
你如果不相信就回去 check 看看這樣子
總之，你把這個斜線
減掉第一個三角形
你就得到藍色這條線
然後再減掉第二個這樣子的，兩個鋸齒的三角形
減掉兩個鋸齒的三角形
這滑鼠很難指
兩個鋸齒的三角形
你就得到 x2 這樣
你就得到 f2(x)
所以今天呢，如果我們要
產生，所以我們知道說
這個一個斜線減掉這個就是 f1(x)
一個斜線減掉這個，再減掉這個就是 f2(x)
如果我們今天要產生 fm(x) 的話
那我們知道 ｍ 的值要大於等於這一項
如果我們要產生 fm(x) 的話
那我們做的事情就是把這條斜線
減掉一個三角形，再減兩個三角形
減四個三角形，減...
對，沒錯，減一個三角形
減掉一個三角形，兩個三角形
四個三角形，八個三角形
一直減下去，直到減到
2^m 個三角形這樣子
那他們的高度你要考慮一下
第一個三角形他的高度是 1/4
兩個三角形的時候，高度是 1/16
2^m 個三角形的時候，他的高度是 1/(4^m)
接下來呢，你要做的事情就是
產生這一連串的三角形這樣子
那怎麼產生這一連串的三角形呢？
其實，就跟我們在上一堂課講的
我們不是說，用這個
V 字型的，用這個
取絕對值的 neuron 其實就是兩個 ReLU
我們在第一層就可以製造出
一個 v 的形狀
再把 v 的形狀乘上一個負號，就是一個三角形了
那第二層你可以製造出一個 w
把 w 的形狀乘上一個負號
就是變成一個 ｍ 的形狀，就變成兩個三角形了
一直到你總共有 m 層
到第 ｍ 層的時候
你就可以製造這個有 2^m 個三角形的鋸齒狀的圖
所以今天，假設你要製造這些 function
我們假設你要製造這些 function
其實你只需要一個
一個有 m 層的 ReLU 的 network
其實就可以辦到了
所以，如果我今天要製造這個 fm(x)
其實我只需要產生這個東西，那這個東西很簡單
even 不需要 activation function 就可以製造
就 input = output 嘛
就 input = output，就原來的 x
然後再減掉 a1
再減掉 a2
再一直減到 am，只是你要稍微註明一下這中間的 scalar
你其實就製造出 fm(x) 了
所以，假設你要製造 fm(x) 的話
你只需要 m 個 layer
你只需要 O(m) 個 neuron
然後，總之 O(m) 個 layer
就可以製造這種 activation function
如果你把這個 ｍ 代 -1/2 log2(ε) - 1 的話
那這個 -1/2
這個可以提到 log 裡面啦，變成
log2(1/sqrt(ε))
所以你只需要 log2(1/sqrt(ε)) 個 neuron
然後 log2(1/sqrt(ε)) 個 layer
其實就可以產生、就可以去逼近
y = x^2 這樣的 target function
那你可能會想說，好像只討論了
就假設剛才的東西你都沒跟上的話，你就只要知道說
如果我們今天用 deep 的 network
因為 deep 的 network 可以輕易地產生這種鋸齒的形狀
所以我們就可以輕易地 fit y = x^2
比 general 的，用 shallow 的 network
還要的 neuron 少很多
那為什麼我們在意 y = x^2
你可能覺得說只考慮 y = x^2 非常的 limited
但其實不會， y = x^2
他有很多的妙用
怎麼用 y = x^2 呢？
我們現在知道說
我們只要 O(log2(1/sqrt(ε))) 個 neuron
我們就可以製造一個 Square Net
這個 Square Net 他做的事情
不像現在 R Net 都做一些很複雜的事情
他就是乘平方
然後他的誤差會小於等於 ε
那我們能夠製造 Square Net 以後
我們就可以製造一種特殊的 network 叫做
Multiply Net
他 Multiply Net 做的事情就是給他 x1, x2
他給你 output 把 x1, x2 相乘
怎麼從 Square Net
製造 Multiply Net 呢？
因為 y = x1x2 = 1/2[(x1+x2)^2 - x1^2 - x2^2]
你只要把 x1, x2 的平方展開
減 x1 平方、減 x2 平方，再乘以 1/2
那你就得到了 x1, x2
所以，今天我們只要會做 Square Net
你接下來就可以用三個 Square Net
拼出一個 Multiply Net
怎麼拼呢？我們先把 x1, x2 加起來
丟到 Square Net 裡面
然後把 x1 獨自丟到 Square Net 裡面
把 x2 獨自丟到 Square Net 裡面
然後這邊就是算出 (x1 + x2)^2
這邊算出 x2^2
這邊算出 x1^2
這三項就是這邊的這三項
把它算出來都乘上 1/2
加起來就得到 x1*x2
所以，今天我們能夠用
這麼多的 neuron 做出 Square Net
我們就是用三倍的 neuron 就可以做出 Multiply Net 了
那 O 的這個 complexity 是不變的
因為你只是在前面乘上一個常數而已
能夠做 Multiply Net 以後呢？
接下來你就可以做 Polynomial
因為你就可以做，至少你可以先做 y = x^n
怎麼做 y = x^n 呢？
其實很簡單，你就用一個 Square Net，把 x 變成 x^2
接下來我們會做 Multiply Net
你就會把 x^2 跟 x 乘起來變成 x^3
你還可以把 x^3 跟 x 乘起來就變成 x^4
所以今天要產生 x^n
沒有問題，你可以用一堆的 Multiply Net
就可以做到這件事情
但這不是唯一的方法
你永遠可以想一下別的方法，舉例來說
你也可以只用 Square Net 就算出 x^4 等等
總之你有 Square Net、Multiply Net，你就可以算 y = x^n
而這邊的每一個 block
需要的 neuron 的 complexity 啊
是這麼多，是 log2(1/sqrt(ε))
那接下來，你可以算 y = x^2
你就製造一個東西叫做 Power(n) Net
這後面有一個 n
就是他可以把 input 的 x
乘一乘就變成 x^n，就叫他 Power(n) Net
你有 Power(n) Net 以後呢，你就可以
就可以產生 Polynomial 了
你就可以產生 Polynomial，怎麼就產生 Polynomial
你用 Power(n) Net 產生 x^n
前面再乘 an
然後，你用 Power(n-1) Net，產生 x^(n-1)
前面再乘 a(n-1)
把他們通通加起來
你就可以產生任何你想要的 Polynomial 了
那你說 Polynomial 不夠 general
其實 Polynomial 就夠 general 了
你就可以用 Polynomial 去 fit 其他 continuous function
就結束了
所以，我們現在可以用 deep structure
我們會做 x^2
x^2 只要 O(log2(1/ε)) 個 neuron
接下來，最後就可以產生 Polynomial
我們就可以用 Polynomial 的 function
去 fit 其他的 function
我們就知道說，怎麼用 deep network
的架構去 fit 其他的 function
當然，你可能會說
在實作上，那個 network 不是這樣
這個 network 的參數不是像你這樣手設出來的啊
那個是 learn 出來的啊
我們現在還沒有討論那個問題
我們只是想一下，討論說
假設你要 fit 某一個 function，有沒有辦法做到
實際上你找不找得到
那個 function，那是 optimization 問題
那是我們之後才要討論的問題
不是我們今天要討論的問題
所以，我們現在得到這樣的結果
我們要 fit y = x^2
如果是 shallow 的 network
你的 neuron 的數目是 O(1/sqrt(ε))
那我們就把 1/sqrt(ε) 畫出來
橫軸是 ε
然後把 1/sqrt(ε) 的線畫出來
那 deep network 是 log(1/sqrt(ε))
所以你發現說，他們中間的差距
是有一個 log 項
他們中間的差距有 exponential 那麼多
他們有 exponential 的差距
deep 和 shallow，你要達到同樣的 error 的時候
同樣的這個 error 的時候
deep 他需要的 neuron
是 shallow 需要的 neuron 再取 log
或者是說，deep 可以用某個數量的 neuron
達成某個 accuracy
那 shallow 的 network 要 exponential 多的 neuron
才可以達到那個 accuracy
但是，這樣子的討論呢
是不足夠的
你覺得不足在哪裡呢？
你仔細想想看
你回憶一下列人第 20 集，比司吉告訴我們的
比司吉說，假設有很多的
每個人的能力範圍其實都是一個 range
今天我們拿 C 跟 A 來比較
然後 A，我們找他的一個 case
正好找到他，比如說，狀況特別差的一個 case
Ｃ我們找到一個 case
正好找到一個狀況特別好的 case
我們就會覺得說 C 可以贏過 A
但其實，那只是因為 A 正好選到狀況特別差的 case
也許 A 在最佳狀態的時候
他是可以打爆 C 的
所以今天的狀況也是一樣
我們剛才說 shallow
需要這麼多的 neuron 只是說
我們想了一個方法需要這麼多的 neuron
並不代表說那是最佳的 solution 啊
對不對？我們只是說
如果有這麼多 neuron
我可以 fit y = x^2
但是並不代表說，我一定要那麼多
才能 fit y = x^2，也許需要
也許只要比較少的 neuron
就可以辦到這件事情
也說不定，也許
這是 shallow network 他在很糟的狀態
他其實是 A，他在這邊，在很糟的狀態
也許他竭盡全力的時候是這個樣子
他可以打爆 deep
只是我們不知道而已
所以，接下來要問的下一個問題就是
假設我們讓
所以下一段要講的就是
假設我們現在讓 shallow 的 network
竭盡全力，他能不能夠打爆 deep 呢？
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw
