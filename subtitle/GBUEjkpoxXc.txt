臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
接下來，我們要講的是 Neighbor Embedding
我們在這個作業，最後一個作業的投影片裡面啊
助教有提到一個東西呢
叫做 t-SNE
t-SNE 我相信你在很多不同的場合也常常聽到
但是如果有人問你說，
t-SNE 是什麼的時候
大家往往都答不出來
現在我們要來講一下 t-SNE 是什麼
那 t-SNE 的 “ＮＥ” 其實就是 Neighbor Embedding 的縮寫
那我們現在要做的事情呢
其實就是我們之前講過的降維
只是我們要做的事情是「非線性」的降維
我們想要做到的事情是說
我們知道說 Data Point
他可能是在高維空間裡面的一個 Manifold
也就是說， Data Point 的分佈 他其實是分佈在一個低維的空間裡面
只是被扭曲了塞到一個高維空間裡面
那，講到 Manifold 的時候
常常舉的例子就是這個「地球」這樣子
地球的表面就是一個 Manifold
他是一個二維的平面
但是被塞到了一個三維的空間裡面
那，在一個這樣子的情況下呢
在一個 Manifold 裡面呢
只有，這個，很近的距離的點
在很近的距離的情況下
Euclidean distance 才會成立
如果今天距離很遠的時候
這個，歐式幾何他就不一定成立
也就是說，假如我們在這個 Ｓ 形的這個空間裡面
取某一個點放在這邊
那我們比較，我們用 Euclidean distance 比較
這個點
跟這個點之間的距離
比較他跟他之間的距離 和他跟他之間的距離
或許這件事情是 make sense 的
他跟他比較不像 他跟他比較像
但是，如果今天是距離比較遠的時候
你要比較，這個點
跟這個點的相似程度
跟這個點跟這個點的相似的程度
你在高維的空間中
直接算他們的 Euclidean distance
就變得不 make sense 了
因為，如果根據 Euclidean distance
他跟他比較近，他跟他比較遠
但是，實際，實際上呢
很有可能，他跟他是比較近的
他跟他是比較像的
他跟他是比較不像
所以， Manifold Learning 要做的事情
是把 Ｓ 形的這塊東西把它展開
把塞在高維空間裡面的低維空間呢 「攤平」
那攤平的好處就是
現在，如果我們把這個低維空間攤平以後
我們做降維
然後，把這個塞在高維空間裡面的 Manifold 攤平以後
那我們就可以在這個 Manifold 上面
用 Euclidean distance來
我們就可以在這個平面上用 Euclidean distance
在降維以後我們就可以用 Euclidean distance
來計算點和點之間的距離
這會對接下來如果你要做 clustering
或者是，你要做接下來的 supervised learning
都是會有幫助的
那類似方法有很多
那我們今天就很快的介紹幾種方法
在最後，講一下t-SNE
那有個方法叫做 Locally Linear Embedding
這個方法意思是說
在原來的空間裡面 你的點的分佈是長這個樣子
那有某一個點，叫做 xi
然後我們先選出這個 xi 的 neighbor
然後我們叫做 xj
那接下來呢，我們要找 xi 跟 xj的關係
那 xi 跟 xj 的關係，我們寫做 “ wij ”
wij 代表 xi 和 xj的關係
這個 wij 是怎麼找出來的呢？
wij 是這個樣子
我們假設說
每一個 xi，都可以用 他的 neighbor
做 linear combination 以後
組合而成
那這個 wij 就是
拿 j 去組合 i 的時候
wij 就是拿 xj 去組合 xi 的時候的 linear combination 的 weight
那要找這組wij怎麼做呢
也就是說，我們現在要找一組 wij
這組 wij 對 xi 的所有 neighbor xj 做 weighted sum的時候
他可以跟 xi 越接近越好
或者是 xi 減掉
summation over 所有的 j wij 乘以 xj
他的 two norm 是越接近越好的
xi 跟 xj 的 linear combination 他們是越接近越好
然後 summation over 所有的 data point i
然後呢，接下我們就要做 dimention reduction
把原來所有的 xi 跟 xj 轉成 zi 和 zj
但是，現在這邊的原則就是
從 xi 跟 xj 轉成 zi 跟 zj
他們中間的關係 " wij "，是不變的
這個東西，就是那個
白居易的長恨歌裡面講的這句話這樣子
我想那句話叫什麼
這句話是出自長恨歌的結尾，他是
臨別殷勤重寄詞，詞中有誓兩心知。 七月七日長生殿，夜半無人私語時。
在天願作比翼鳥，在地願為連理枝。 天長地久有時盡，此恨綿綿無絕期。
並不是我國文特好，謝謝謝謝～
你不要誤會可能是我國文特別強，我其實沒有很強
為什麼我會背，其實我可以從頭背到尾，為什麼呢
因為小時候我不知道犯了什麼錯被老師懲罰背長恨歌
然後我就就整首背起來了
你小時候背的東西，到長大其實不會忘
這個是什麼意思呢，所謂的
在天就是 xi 跟 xj，在原來的 space 上面
比翼鳥就是 wij
在地就是把 xi 跟 xj transform 到另外一個 space
就是 zi 跟 zj
連理枝就是比翼鳥就等於連理枝
所以他們關係還是 Wij
所以 LLE 他做的事情是這樣的
首先 wij 在原來的 space 上面找完以後
就 fix 住他，不要去動它
接下來，你為每一個 xi 跟 xj
找另外一個 vector
我們現在要做 dimension reduction
所以你新找的 vector 要比原本的 dimension 還要小
原本 100 維降維後要比較小，10 維 2維之類的
zi 跟 zj 是另外的 vector
那我們要找這個
zi 跟 zj 他可以 minimize 什麼呢
他可以 minimize 下面這個 function
也就是說原來這個 xi 他可以做 linear combination 產生 x
原來這些 xj 可以做 linear combination 產生 xi
這些 zj 也可以用同樣的 linear combination產生 zi
這些 zj 也可以用同樣的 linear combination 產生 zi
我們就是要找這組 z 可以滿足 wij 給我們的 constrain
所以現在，在這個式子裡面
wij 變成是已知的
但是我們要找一組 z，讓 zj 透過 wij
做 weighted sum 以後，他可以跟 zi 越接近越好
zj 用這組 weight 做 linear combination 後他可以跟 zi 越接近越好
然後 summation over 所有的 datapoint
那這個 LLE 你要注意一下，其實它並沒有一個
明確的 function
告訴你說我們怎麼做 dimension reduction
不像我們在做 auto encoder 的時候
你 learn 出一個 encoder 的 network
input 一個新的 datapoint，然後可以找到他 dimension 的結果
今天在 LLE 裡面
你並不會找到一個很明確的 function，沒有一個 function 告訴我們說
怎麼從 x 變到 z
z 就是完全憑空找出來的
其實如果你用 LLE 這種方法
如果用 LLE 或其他類似的方法會有一個好處
就算是你原來這個 xi xj 你不知道
你只知道 wij
你不知道 xi xj 要用什麼 vector 來描述他
你只知道他們的關係
你其實也可以用 LLE 這種方法
那 LLE 呢，你其實需要好好的調一下你的 neighbor 選幾個
你 neighbor 選的數目要剛剛好才會得到好的結果
那這邊，是從原始的 paper 裡面截出來的圖
原始的 paper 他的題目實在是太潮了
他是 “Think Globally, Fit Locally” 這樣子
太精彩了，很好的一個題目
然後呢，他調了不同的 K
如果你今天，K 太小
你得出來的結果會不太好
K 太大，得到的結果也不太好
為什麼 K 太大得到的結果不太好呢
因為我們之前的假設
就是每一個這個 Euclidean distance
只在很近的距離之內，可以被這樣想
所以，這個點和點之間的
data point 和 data point 之間的關係
在 transform前後可以被 keep 住
只有在距離很近的時候
才能夠成立
當你 K 選很大的時候
你會考慮一些距離很遠的點
你會考慮一些 transform 以後
relation 沒有辦法 keep 住的點
有一些關係太弱了，transform 以後沒有辦法 keep 住
那他不是這個比翼鳥或連理枝的關係
他太弱了，無法 keep 住
但是你不應該把他考慮進來
所以你 K 呢，要選一個適當的值
那另外一個方法叫 Laplacian Eigenmap
他的想法是這樣子
我們之前有說呢
我們之前在講這個 Semi-supervised learning 的時候
我們有講過 smoothness assumption
我們有說，如果你想要比較這個點跟這個點之間的距離
只算他的 Euclidean distance 是不足夠的
你要看的是他們在這個 high density 的 region 之間的 distance
如果兩個點之間
他們有 high density 的 connection
那他們才是真正的接近
那這件事情
你可以用一個 graph 來描述這件事情
也就是說你把你的 data point
construct 成一個 graph
你算你的 data point 兩兩之間的相似度
如果相似度超過一個threshold
就把他們 connect 起來
而建一個 graph 的方法有很多
有很多不同的方法
總之你就把比較近的點
把他們連起來變成一個 graph
那你把點變成這個 graph 以後
你考慮這個 smoothness 的距離
就可以被這個 graph 上面的 connection 來 approximate
那我們之前在講 semi-supervised learning 的時候
我們是這樣說的
如果今天 x1 跟 x2 在 high density 的 region 上面
他們是相近的
那他們的 label
y1 hat 和 y2 hat，很有可能是一樣的
雖然說 semi-supervised learning 的時候
我們可以這麼做
我們有一項是考慮有 label 的 data 的項
有另外一項是跟 label data 沒有關係的
我們只可以利用這個 unlabel 的 data
那這一項的作用呢
他是要考慮說我們現在得到的 label 是不是 smooth 的
他的作用很像一個 regularization 的 term
那這個 S 這一項啊
這個 S 這一項啊
他等於 yi 減 yj 的這個
他跟 yi 跟 yj 的這個距離
然後乘上 wi, j
也就是說，那個 wi, j 是什麼呢
wi, j 是說，如果今天兩個 data point xi 跟 xj
他們在圖上是相連的
那 wi, j 就是他們的相似成度
如果在圖上是沒有相連的
他就是 0
然後我們說
如果今天wi 跟 wj 他們很相近的話
那 wi, j 就有一個很大的值
就是希望yi 跟 yj 越接近越好
反之如果 wi, j 是 0
那 yi 跟 yj 要是什麼值都可以
那這個 S 呢
就是evaluate 說你現在得到的 label 有多麽的 smooth
那麼還說
這個 S 可以寫成一個 y 的 vector 乘上 L 再乘上 y
這個 L 就是 graph 的 laplacian
L 等於 E 減 W
這個大家回去 check 一下之前的投影片就知道了
那同樣的道理可以被 apply 在完全 unsupervised 的 test 上面
我們可以說
如果 x1 跟 x2 在 high density 的 region 他們是 close 的
那我們就會希望， z1 跟 z2 他們也是相近的
所以我們可以一樣把剛才那個 smoothness 的式子寫出來
我們可以寫說
我這邊寫平方可能是比較不好啦
我這邊應該寫 Euclidean distance 比較好
或者寫他的這個 two norm distance 比較好
因為我只是從前面投影片 copy 過來
只是把 y 改成 z 而已
就忘了把他改過來
好沒關係，那今天這個把 x1 跟 x2 變成 z1 跟 z2 以後
你的這個 z1 跟 z2 應該是長什麼樣子呢
你今天應該是這樣子
如果今天 xi 跟 xj
如果這個 i 跟 j 的這兩個 datapoint 他們之間的 wi, j 很像
那 zi 跟 zj 做完 dimension reduction 以後他們的距離就很近
反之呢，如果他們的 wi, j 很小
那他們的距離要怎樣就都可以
你覺得，這樣 ok 嗎？
我們就找一個 zi 跟 zj minimize 這個 S 的值
你不覺得這麼做是有問題的嗎
給大家五秒鐘想想看這個問題出在哪裡
其實，這個問題是這樣子
你不需要告訴我 wi, j 是什麼
我一秒就可以告訴你說
要 minimize x 的時候，我應該選什麼值
心算就可以得到了
選什麼值呢？
我把所有的 zi 跟 zj 通通設一樣的值就好了
結束
把所有 zi 跟 zj 我通通都設一樣的值，比如說通通都設0
那 S 就等於 0
這個問題就結束了
所以，光是有這個式子是不夠的
那你可能說剛剛在 semi-supervised learning 的時候 你怎麼不講這句話呢
之前在 semi-supervised learning 的時候
我們還有 supervised learning，"supervise" 那個 label data給我們的那一項
所以，如果你把所有的 y，所有的 label 都設成一樣的
那你在 supervise 那一項，你得到的 lost 就會很大
那我們要同時 balance supervise 那一項 跟 semi-supervise 那一項
我們要同時 balance supervise 那一項的 cost 跟 regularization 的 term
所以你不會選擇讓所有的 y 通通都是一樣的
但在這邊少了 supervise 的東西
所以變成選擇所有的 z 都是一樣
反而是一個最好的 solution
所以這件事情是行不通的
所以怎麼辦呢？
你要給你的 z 一些 constrain
什麼樣的 constrain 呢？
會給的 constrain 是這樣
如果今天 z 他降維以後的空間是 M 維的空間
是 M 維的空間，比方說 M 是 2 之類的
那你不會希望說
你的 z 他還分佈在一個比 M 還要小的 dimension 裡面
因為我們現在要做的事情
是希望把高維空間中塞進去的低維空間展開
那我們不希望說我們展開以後
其實展開的結果他在一個更低維的空間裡面
所以，今天假如你的 z 的 dimension 是 M 的話
你會希望你找出來的那些點
假設現在總共有 N 個點
z1 到 zN
他們做 span 以後，會等於 RM
也就是說 z，他不是活在一個比 M 維更低維的空間裡面
他就會佔據整個 M 維的空間這樣子
那其實呢，如果你要解這個式子的話
你解一解就會發現說
你解出來的這個 z 跟我們前面看到的那個 graph laplacian L 是有關係的
他其實就是 graph laplacian 的 eigenvector
對應到比較小的 eigenvalue 的那些 eigenvector
這個大家再自己 check 一下文件就好
所以他叫做 Laplacian Eigenmap
因為我們找的是那個 laplacian matrix 的 eigenvector
好那如果你今天先找出 z 以後
你再去做 cluster
你先找出 z，再用 K-means 做 clustering 的話
這ㄧ招有一個很潮的名字，叫做 spectral clustering
那接下來呢，我們就要講 t-SNE
那 t-SNE 他是 T-distributed Stochastic Neighbor Embedding 的縮寫
那 t-SNE 他解決什麼樣的問題呢
前面那些問題啊，有一個最大的問題就是
他只假設相近的點應該要是接近的
但他們沒有假設說，不相近的點沒有要接近
他沒有假設說，不相近的點要分開
所以比如說你用 LLE 在 MNIST 上
你會遇到這樣的情形
他確實會把不同的點呢
他確實會把， 這個不同的顏色代表不同的 digit 不同的 class，
他確實會把不同的 class都塞在一起
然後他確實會把同個 class 的點都聚集在一起
但他沒有防止說不同 class 的點不要疊成一團
如果他們都疊成一團的時候
沒有這個顏色
這些點還是擠在一起，你還是沒有辦法分開
這是另外一個例子
做在這個 COIL-20上面
COIL 是一個 image 的 corpus
他裡面的 image 就是某一個圖
比如說一個汽車、一個玩具車
然後把它轉一圈，然後拍很多張不同的照片這樣子
好那這邊同樣顏色代表同一個 object
你會發現說呢他找到一些圈圈
這個圈圈代表什麼意思呢？
這個圈圈代表，同一個 object 在做旋轉的時候
他每一張圖都很像
但是，旋轉的時候每一張圖都很像
但是你看一個東西的正面和側面，他卻是非常不一樣
所以這個圈圈就顯示你把某一張圖做旋轉以後
你所得到
在做 dimension reduction 以後
你所得到的結果
但是一樣
你會發現說，不同的 object 其實他們是擠成一團的
沒有辦法分開
所以如果做 t-SNE的話，做 t-SNE 是怎樣呢？
做 t-SNE 我們一樣是要做降維
把原來的 data point x
變成比較 low dimension 的 vector z
那在原來的 x 這個 space 上面
我們會計算所有的點的 pair，xi 和 xj 之間的 similarity
這裡先寫成 S(xi, xj)
接下來，會做一個 normalization
我們會計算一個 P( xj | xi)
這個 P( xj | xi)，他是從 xi 跟 xj 的 similarity 來的
我們等下會說這個 similarity 要怎麼算
這個 P( xj | xi)，在分子的地方是 xi 跟 xj 的 similarity
然後分母的地方
就是 summation over 除了 xi 以外，所有其他的點
和 xi 之間所算出來的距離
所以你會發現說
xi 對其他所有的 data point
他所算出來的這個 P( xj | xi)
他的 summation 應該要是 1
那另外假設我們今天已經找出了一個 low dimension 的 representation 就是 zi 跟 zj 的話
我們已經把 x 變成 z 的話
那我們也可以計算 zi 和 zj 之間的 similarity
這邊 similarity 我們寫成 S'
那一樣你可以計算一個 Q( zj | zi)
他的分子的地方就是 S'( zi, zj)
分母的地方就是 summation over zi 跟所有 database 裡面的 data point zk 之間的距離
那今天有做這個 normalization 其實感覺是必要的
因為如果你又做這個 normalization 的話
因為你不知道在高維空間中算出來的距離
S( xi, xj) 跟 S‘( zi, zj) 他們的 scale 是不是一樣的
如果你今天有做這個 normalization
那你最後就可以把他們都變成機率
那這個時候他們值都會介於 0 到 1 之間
他們 scale 會是一樣的
那接下來我們要做的事情就是
我們現在還不知道 zi 跟 zj 他們的值到底是多少
這是我們要被找出來的
那麼希望找一組 zi 跟 zj
他可以讓這一個 distribution 跟這一個 distribution
越接近越好
我們要讓原來在這個
根據 similarity 在 S 這個原來的 space 算出來 distribution
跟在這個 dimension reduction 以後的 space 算出來的 distribution
越接近越好
那怎麼衡量兩個 distribution 之間的相似度呢？
可以很直覺的衡量兩個 distribution 之間的相似度的方法
就是我們之前看到的 KL divergence
所以，我們今天要做的事情
就是找一組 z
他可以做到，這個
xi 的這個 distribution，跟 xi 對其他 point 的 distribution
跟 zi 對其他 point 的 distribution
這兩個distribution之間的KL divergence越小越好
然後summation over 所有的 data point
然後你要使得這一項，他的值越小越好
然後我把這個值就寫在這邊
這件事情要怎麼做呢？
要怎麼做
其實這件事情呢，實際上並沒有很困難
實際上你就是用 gradient descent 做的
你想想看，假設你知道這個 similarity 的 matrix
他這個式子長什麼樣子，然後這個式子長什麼樣子
那你把這些式子帶進去，你把這些式子帶進去
然後接下來你只要對 z 做微分，然後做 gradient descent
就搞定這個問題，就結束了，ok
好那今天有一個問題就是你在做 t-SNE 的時候
他會計算所有 data point 之間的 similarity
所以他運算量有點大，所以 t-SNE 有點麻煩
如果 data point 比較多的時候呢，你這電腦會跑不動
一個常見的做法是你會先做降維
比如說你原來的 dimension 很大
你不會直接從很高的 dimension 直接做 t-SNE
因為你這樣子計算 similarity 的時間太長
你通常會先做，用比較快的方法比如說 PCA
先用 PCA 做降維，比如說先降到 50 維
然後再用 t-SNE 從 50 維降到 2 維
這個是比較常見的做法
那其實像我們今天講的這些方法啊
他們你會發現說
比如說像 t-SNE，如果你給他一個新的 data point
他是沒辦法做的，對不對
他只能夠，你給他一大群
已經先給他一大堆 x，他幫你把每一個 x 的 z 都找出來
但你找完這些 z 以後
再給他一個新的 x
你要重新跑一遍這一整套演算法，很麻煩
所以，一般你不會
一般 t-SNE 的作用比較不是用在這種 training testing 的這種 base 上面
通常比較常用的做法是
拿來做 visualization
如果你已經有一大堆的 x，他是 high dimensional 的
那你想要 visualize 他們在二維空間的分佈上是什麼樣子
你用 t-SNE
那 t-SNE 往往可以給你不錯的結果
那 t-SNE 現在可能是最多人使用的一種選擇
好那我們再來要講 t-SNE 一個非常有趣的地方
就是他的這個 similarity 的選擇是非常的神妙的
我們在這個原來的 data point 的 space 上面
你的 similarity 的選擇
它是選擇 RDF 的 function
就選擇說，我們要讓 xi，
我們這個 evaluate similarity 的方式
是計算 xi 跟 xj 的 euclidean distance
然後取一個負號，再取 exponential
那我們之前有說如果你要在 graph 上算 similarity 的話
用這種方法比較好
因為它可以確保說只有非常相近的點才有值
那 exponential 他掉得非常快
所以只要距離一拉開，similarity 就會變得很小
那在 t-SNE之前有一個方法叫做 SNE
就把前面的 t 拿掉
SNE 就是一個很直覺的想法
在 data point 原來的 space 上用這個 evaluation 的 measure
當然在新的 space 上你用同樣的 measure 就好啦
選不同 measure，呃，你直覺就選一樣的 measure 嘛
那 t-SNE 神妙的地方就是
他在你 dimension reduction 以後的 space
他選的 measure 跟原來的 space 是不一樣的
他在 dimension reduction 以後選的 space
是這個 t-distribution 的其中一種
t-distribution 裡面有參數你可以調他
可以產生很多種不同的 distribution
t-distribution 的其中⼀一種
他是 1 除以 1 加 Euclidean distance 的平⽅
那你可能問說，為什麼要這樣做呢
我可以提供⼀個很直覺的理由
假設橫軸代表了在原來 space 上的 Euclidean distance
或者是
做 dimension reduction 以後的 Euclidean distance
那這個紅⾊這條線，是這⼀項
藍色這條線，是這⼀項
紅色這條線是 RBF function
藍色這條線是 t-distribution
你會發現說呢
如果原來在這個點跟這個點
之後做 dimension reduction 以後
要怎麼才能維持它原來的 space 呢?
那你就把它變成這個樣⼦
它就可以維持它原來的 space
如果你要維持他們原來之間的距離
如果你今天要維持他們原來的機率的話
要維持原來他們之間相對的關係的話
那你就把他變成這樣
那你會發現說
如果本來距離比較近，他們的影響是比較⼩的
如果本來就已經有一段距離
那從原來的這個 distribution 變到 t-distribution 以後
他會被拉得很遠
t-distribution 他的尾巴特別長
所以如果你本來距離比較遠的話
變到 t-distribution 以後，他會變得更遠
也就是說，原來在高維空間裡面
如果距離很近
那做完 transform 以後他還是很近
如果原來就已經有一段距離了，有⼀個 gap
那做完 transform 以後他就會被拉得很遠
所以你會發現說 t-SNE，他畫出來的圖往往長得像這樣
他會把你的 data point 聚集成一群一群一群
因為你的 data point 之間本來只要有一個 gap
做完 t-SNE 以後他就會把 gap 強化，gap 就會變得特別明顯
所以這是 t-SNE 做在 MNIST 上面的結果
那其實這個不是直接做在 MNIST 的 pixel 上⾯
直接做在 pixel 上面的 performance 看起來沒有這麼好
這是先對 pixel 做 PCA 降維以後，再做在 MNIST 上⾯
那不同顏色代表的是不同的數字
那你會發現，不同的數字會變成一群一群的
那 t-SNE 在 COIL-20 上的結果，其實還蠻驚人的
這邊的每一個圈圈就代表一個 object，他只是轉了一圈以後的結果
你會發現這邊有一些被扭曲的圈
這扭曲的圈圈是什麼意思呢?
那是因為有⼀些東西比如說杯⼦
他的這個⾯長這樣
他轉 180 度過來以後，他還是長得一模⼀樣
所以你把那個杯子轉 360 度轉一圈的時候呢
你就會發現說他在中間某一個地方是很像的 image
所以你會看到說這邊有很多看起來很像 8 的符號
那這邊這很多條線
其實好像是說有 4 部都是⼩汽⾞
4 部小汽車看起來是蠻像的
所以這邊有四條線是被 align 在一起的
這邊有一個從網路上找到的 t-SNE 動畫
他長得是這個樣⼦
你看，因為他是用 gradient descent train 的
所以你會看到，隨著 iteration 的 process
點會被分得越來越開
然後不同的點之間呢
這其實不是做在 MNIST 上，是做在另外⼀個手寫數字辨識的 corpus 上⾯
你會發現說不同的 digit 之間呢，他是會被分得很開的
那 t-SNE 的地方呢，我們就講到這邊
臺灣大學⼈⼯智慧中心 科技部⼈工智慧技術暨全幅健康照護聯合研究中⼼ http://aintu.tw
