hi everyone today i want to introduce
this project to you superb superb this
is a self-supervised learning project
for voice
in this superb project what are we going
to talk about first let's start with how
to apply self-supervised learning on
voice-related task first
i believe that self-supervised learning
should be familiar with how it works it
if you have taking my machine learning
lesson you must have been heard of bert
burt is trained on text
self-supervised learning model
if you are doing something related to
computer vision then you must have heard
sim clr
sim-clr is a self-supervised learning
model on image
so how is that progress in voice data
today i want to share with you
the latest development self-supervised
learning model on voice
before we get stated i will simply
review self-supervised learning
framework we will use voice data as ours
example
nowadays when we dueling with machine
learning problem we often separate it
into two stage
the first stage is pre-training
in pre-training we have a lots of
unlabeled data
in voice data as an example
you can imagine that we clawed a brunch
of voice data from the internet like we
download from youtube for those youtube
voice data that didn't have any subtitle
as our label what can we do to make use
of them we will use those in labeled
data to train a upstream model
training this upstream model does not
require any label data the purpose of
that upstream model is to input a voice
data it will output a representations
that is vectorizing the voice data
about how to train a upstream model due
to the time limit we will just roughly
go through the main idea there are a
variety of methods without labeled data
contraina upstream model
for example ucam mask and audio segments
let upstream model 2 recover the masked
audio segment during training this is
likely what we do in nlp bird is using
this kind of training method
another way for example contrastive
learning sim clr on image is
called contrastive contrastive learning
it can also be done on a voice domain in
a lot of different ways
we will not list here
the most interesting part is that when
we do this pre-training for downstream
task the pre-training is task agnostic
which means that when we are doing the
pre-training we did not know this
upstream we'll be fine-tuned on what
kind of downstream task that is task
agnostic during training we only use a
brunch of unlabeled data to train
upstream model how will that upstream
model
to be used in other applications next we
do not know
in today's talk we will apply the model
which is trained on phase 1 called the
upstream model you may notice that it
has many names
on other literature such as
self-supervised model and there is a
popular word recently is called
foundation model also refers to the
upstream model
so in voice processing this kind of
upstream model we already got several
well-known model for that it's a bert
model in voice
in nlp we have bert and gpt invoice
processing actually has a series
of upstream models that are ready to use
in pre-training stage we already use a
lots of unlabeled data to train a
upstream model we can further go to
stage 2. in stage 2 you will have a
specific problem to solve that
particular problem for example in voice
application you may immediately comes
out
speech recognition task to transfer
audio signal to text
of course voice recognition is not the
only voice-related task
we will introduce variety of downstream
tasks in the following part
back to speech recognition task we have
small amount of label data for speech
recognition it's a pairwise data with a
voice signal and its corresponding
sentence
you would use this small amount of label
data to train a downstream model
that downstream model is often be much
simpler compared to the upstreams
we usually only need a simple downstream
model if the upstream model is well
enough all you need will be a simple
downstream model it can solve the
downstream task you want in case of
voice recognition the downstream model
will take a voice signal as an input
output the speech recognition result you
will use a small amount of labeled data
to train this downstream model
some literature may purpose that we can
also use those labeled data to fine-tune
the upstream model
here i use a dashed line to indicate
that it is not necessarily to fine-tune
the upstream model
when we already have a bunch of upstream
pre-trained model
the literature results of those models
showing that they have an excellent
performance on voice recognition
there is the question
are these models only specialist for
asser or they are universal on all
voice-related tasks
these models have been proved in the
literature that they perform well on
speech recognition are they only able to
work on speech recognition or it can
also work well on different
voice-related tasks
are these model customized for speech
recognition or it can also work well on
any voice-related task you can use any
of the upstream models which is trained
on large amount of unlabeled data to
handle any voice-related task
to be more specific we have introduced
that we can apply downstream model right
after the upstream model with small
amount of labeled data speech
recognition labeled data set to train
the downstream model it can have good
performance
for the same upstream model we can also
apply it to other downstream tasks if we
are not doing speech recognition this
time we trying to do speaker
identification instead you will take
some of the speaker identification data
to train another downstream model this
downstream model can also be attached
after upstream model will that model
also have an impressive result
if you ask me these models are universal
or specialists a few months before i
probability tells you these models are
more likely to be specialists
why do i think so because you know that
speech recognition and speaker
identification they are actually very
different tasks
in speech recognition we want our model
to ignore the differences between
speaker
because different people speaks on same
word their pronunciation is not the same
but you want the voice recognition
system to ignore the differences between
speaker
but if you want to do speaker
identification who's talking now is the
most important thing you want to know
the system needs to be able to find
differences between speaker in conclude
voice recognition is to ignore the
difference between speaker speaker
identification is to find differences
between speaker these two tasks these
two goals are conflicting to each other
if you do well in voice recognition you
may not perform well on the other tasks
although i reasoned that these models
may be specialists we can still give it
a shot to see how these models perform
outside the speech recognition task
this is the idea of superb superb is a
speech processing universal performance
benchmark in short the member in this
project not only from national taiwan
university but also from the cmu
mit johns hopkins facebook ai and lxt
in this year's inter speech we publish a
paper about this benchmark if you have
any further question about the following
experiment you can check our paper
published in
interspeech the concept of superb is
like we already have these upstream
model
we take them to the same arena
evaluating their performance
superb will be like
the olympic on voice domain
self-supervised model they do not just
require doing well on a single task
they should be doing well on a decathlon
we want justify the model performance on
different tasks
here is the model we use on the
competition some basic information that
you can see from this table tells that
these models are different from each
other for example the way of
pre-training we listed the pre-training
on each model we use a notation to
represent different pre-training way
here we have at least
six different pre-training methods with
different kind of combinations
okay let's introduce our category first
of all we have sum
content content-related tasks we have
phoneme recognition giving the machine
voice signal and then identified phoneme
sequence
if you do not know what phoneme is you
can treat it as something like kk
phonetic
for keyword spotting giving the machine
voice signal it can tell us which one
keyword is used
of course there will be speech
recognition giving the machine voice
signal
identified the corresponding text
sequence
we also have query by example what is
query by example do
query by example is trying to let's say
there is a long audio files for example
a news report or your class recordings
you now want to search whether it
contains
something inside this recording how to
search this answer when the input will
be the question you spoke for example
you want to know this one hour long news
report whether it mentioned kovid 19 or
not you would say kovid 19 and then the
machine turn this voice signal with its
corresponding txt into account to check
whether it exists on this news report or
not
after that the machine will
automatically mark all the appear
keywords in every part of the news
report that is query by example
for speaker dependent tasks we have
speaker identification what speaker
identification for is to input a voice
signal it needs to determine who the
speaker is
we also have speaker verification task
speaker verification is to input two
pieces of voice
machine needs to decide whether they
came from the same person or not
although speaker identification and
speaker verification sounds similar but
in fact these are two different tasks
as well as speaker diarization speaker
diarization is too input a voice into
the machine the voice may be a meeting
recording that have multiple speakers in
the same time they may interrupt each
other
or even talking at the same time so your
machine should identify
which part comes from speaker a
which part is from speaker b and which
part is speakers a and b talking at the
same time
that is speaker diarization
for the tasks related to semantics
we have intent classification of voice
the input will be a voice signal
model needs to understand what is
intention behind
we have slot filling in audio version
input a voice signal it can captures the
important information from it let's say
in a booking system somebody wants to
buy a ticket from taipei to new york
machine needs to figure out taipei as
the departure
and new york is the destination
for those semantic-related tasks the
traditional way is to do voice
recognition first turning the voice
signals into text sentence
semantic understanding will be on text
level but in here that will be an end to
end model means that the machine does
not do speech recognition first and then
text understanding next end to end model
will not split it into two stages
instead we will use a unified model that
is an end to end model end to end model
will take a voice signal as an input and
directly output the text understanding
result
as well as emotion recognization input a
voice signal to the machine it needs to
judge what is that voice signal talking
about what kind of emotion it is
to sum up we will have 10 different
tasks in total
we already introduced the participate in
the competition also all the tasks in
the competition let's begin the first
round of the competition
in the first round of that competition
we set a relatively stringent
restrictions
how stringent that restrictions is the
upstream model should be fixed and we
will not take any label data to
fine-tune the upstream model
it will only include a brunch of
unlabeled data in pre-training after
pre-training it will be fixed no matter
what downstream is
maybe you have tried some model
for example bert in nlp you will
fine-tuning that model on different
tasks
you may want to ask in voice processing
why don't we fine-tune the upstream
model like bert
fine-tune upstream model should give us
better result in fact that we have done
related experiment on it
fine-tuning upstream model can give us
better result however in the first
section we want to give a greater
restrictions on these models we want to
increase their difficulty
to see what kind of performance under
this situation
we give a limitation of the upstream
model upstream model cannot be changed
on different tasks its parameters must
be fixed
we'll take the last layer of upstream
model use it into a different downstream
model on different tasks for the
downstream model the network
architecture is predetermined for
different upstream model it will attach
the same architecture of downstream
model the principle of designing
downstream model is that we want these
downstream model as simple as possible
the parameters should be as less as
possible
assume that a task can be solved using
linear layer we will use layer linear
but some tasks are more complicated such
as voice recognition slot filling it
cannot be solved with just linear layer
then we take one or two layers of lstm
to solve it the principle is that
downstream model should be as simple as
possible
you might ask why we have to set up such
constraint the reason is that because of
those limitation then we can just use
one upstream model to solve all tasks
also these downstream model are
deliberately designed to be very simple
if downstream model perform well on
different tasks under this setting what
does that mean it means that upstream
model has to be impressive in the
pre-training stage upstream models
should learn how to extract universal
feature in some kind that universal
feature can be applied on all of the
downstream tasks maintain a good
performance in the same time to remind
that upstream model is fixed parameter
all we need is one model for 10
different tasks if a fixed model can
solve 10 different tasks represented of
that this is a powerful model it s
feature can be used on all tasks
moreover those downstream are very weak
to solve the task can't align on
downstream model only downstream model
is not useful we have to count on
upstream models feature under the
setting we
introduced today and what we just talked
about we can have good results on 10
different tasks it means that we have a
universals feature
for any kind of new task in the feature
we do not need to tune the upstream
model directly use this model for the
new task we have that upstream model to
generate universal feature for new
voice-related tasks in the future you
will be no worry directly use the same
model upstream to solve this new task
course this is an ideal situation that
we have not seen
upstream model on voice their
performance on the 10 tasks if they
perform well on 10 tasks it means that
these models are very powerful in the
future have a new task directly apply
these models do not need to do anything
like purpose a new design can it be
possible let's check how is the result
in first round
here is the first round result for each
column it represents a task there will
be 10 column in total for 10 tasks four
content-related tasks three
speaker-related tasks two semantically
related tasks 1 emanation related tasks
for each row what is that for every row
represents a different upstream model
first row is a traditional upstream
model without using self-supervised
learning self-supervised learning pumped
directly feech past this fbank common
voice in this field re
direct pumping of the fbank feature to
use it in the top 10 downstream task
that we have try a variety of different
upstream models to see how is that 10
perform on tasks
ok i in that chart it is not easy to
interpret because there are a lot of
numbers and for different tasks the
standard of appraisal is different some
tasks are larger is better some tasks
are lower is better it is not easy to
fully understand this table in short
period of time we will sum up all of the
result here
for those model behave worse than fbank
we will mask out the result in black
blackpart means that the performance was
worse than fbank
self-supervised representatives is not
come in handy when upstream model is not
come in handy in some task what does
that mean the first conclusion is that
in this form the non-masked out place
is much more than the masked one it
means that upstream models
will come in handy in many cases you
will find that some of the tasks are
not doing well such as asvsd slot
filling
to models to those upstream model it is
not easy to perform well
you may find out that are wave 2 vec and
hubert wave 2 vec 2.0 and hubert all we
use are based model we didn't not use
large model why we not taking large
model into account we discover some
strange issue these large model
performance and competition are very
very miserable even worse than fbank
baseline we can't not even put it into
the table
why these large model perform so bad
maybe that is because in the first round
of the competition we restrict too much
so we now enter the second round of the
competition in the second round of the
competition we relax the restrictions of
the game what kind of restrictions we
relaxed in the first round of
competition
we only use the last lay in the game
in the second round we relax this
restriction other limitations are still
the same we still using a simple
downstream model fixed upstream model
parameters the only adjustment is when
we use this upstream model we are not
only taking the upstream model final
layer of representation but let the
downstream model to decide which layer
to use in upstream model
for fine tuning
the reason of such design is because we
notice that in a self-supervised model
each layer may contain different
information the best information the
most important information for task not
always store inside the final layer it
may be in the middle layer
so we want to make downstream models
have opportunity to choose which layer
it want for upstream model the
parameters is still fixed
how to let downstream model to decide
which layer to use our solution is we
extract each layer of the upstream model
to perform weighted sum representation
of each layer after weighted sum process
output a new representation such
representation after weighted sum will
be the input of downstream model for the
weight of weighted sum
of each layer it will be trained
together with downstream model so you
can think of the weight of weighted sum
is the part of downstream model
downstream model needs to learn by
itself to chose which upstream layer to
use
next we will take a look at the second
round result you find in the second
round has masked out part become even
less
indicate that weighted sum can let
downstream model to choose which
upstream layer for the task in this way
can achieve better result you can also
find that there are several models which
are much more better than fbank in most
of the tasks include npc deco ar 2.0
wave 2 vec 2.0 series and hubert series
they can both perform better performance
than the fbank baseline so there are a
lot of self-supervised learning models
can do well on all tasks
that this is today's conclusions it is a
slap in the face for me assume that you
ask me today whether these models are
universal or specialists
i will tell you that amazing thing is
that these models threw a lot of
unlabeled data training they actually
are universal at least in 10 different
speech-related talk
they have a better performance than
fbank we use in the past about what is
next in our study how those model
achieve that performance in pre-training
stage we do not know what will be in
downstream training how those upstream
to learn universal feature that can be
the problem we can study in the future
after all we've been though if you find
that you're trained
voice self-supervised model does not
appear in this competition what should
you do it doesn't matter you can upload
your own
self-supervised learning model to superb
benchmark
if you want to know more about the
further detail you can check the qr code
superb official website i left it on the
left side in this slide in this time the
public leaderboard are already online
you can submit the results to
the public leaderboard after mid of
october there will be hidden dataset
online on leaderboard you can submit on
it after october the hidden data set of
leaderboard
that is all for our superb plan if you
are working on the
self-supervised leering model you want
not one to miss the aaai 2022
self-supervised learning for speech and
audio processing workshop submission
deadline will be november 12th the
workshop website qr code
will be in the right-hand side in the
slide
if you are doing self-supervised
learning related to voice you don't want
miss this
special issue of ieee jstsp
there is a special issue in ieee jstsp
about self-supervised learning for
speech and audio processing deadline
will be the last day of this year the
relevant information qr code i have
posted on the bottom right of slide
to sum up today i want to tell you is
if you are studying voice-related
self-supervised learning task to the end
of this year there are three things that
you cannot miss to participate the
superb challenge upload your results to
the superb leader board second is
attending the next year's aaai workshop
deadline is 12th of november the third
is submit your work to ieee jstsp
special issue
deadline is the last day of this year
that all we want to tell
thank you very much thank you
