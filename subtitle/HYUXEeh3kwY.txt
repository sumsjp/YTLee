好 那上週呢
我們講到了critical point的問題
接下來呢 我要告訴大家說
critical point其實不一定是
你在訓練一個Network的時候
會遇到的最大的障礙
今天呢這一份投影片裡面
要告訴大家的是一個叫做
Adaptive Learning Rate的技術
也就是我們要
給每一個參數不同的learning rate
那為什麼我說這個critical point
不一定是我們訓練過程中
最大的阻礙呢
往往同學們
在訓練一個network的時候
你會把它的loss記錄下來
所以你會看到說
你的loss原來很大
隨著你參數不斷的update
橫軸代表參數update的次數
隨著你參數不斷的update
這個loss會越來越小
最後就卡住了
卡住的意思就是
你的loss不再下降
那多數這個時候
大家就會猜說
那是不是走到了critical point
因為gradient等於零的關係
所以我們沒有辦法再更新參數
但是真的是這樣嗎
當我們說 走到critical point的時候
意味著gradient非常的小
但是你有確認過
當你的loss不再下降的時候
gradient真的很小嗎
其實多數的同學可能
都沒有確認過這件事
而事實上在這個例子裡面
在今天我show的這個例子裡面
當我們的loss不再下降的時候
gradient並沒有真的變得很小
下面是gradient的norm
也就是gradient的向量
gradient是一個向量嘛
gradient這個向量的長度
隨著參數更新的時候的變化
你會發現說雖然loss不再下降
但是這個gradient的norm
gradient的大小並沒有真的變得很小
事實上在最後訓練的最終結的時候
loss已經幾乎沒有在動了
loss幾乎沒有在減少了
但是gradient卻突然還上升了一下
這是怎麼一回事呢
這樣子的結果其實也不難猜想
也許你遇到的是這樣子的狀況
這個是我們的error surface
然後你現在的gradient
在error surface山谷的兩個谷壁間
不斷的來回的震盪
這個時候你的loss不會再下降
所以你會覺得
看到這樣子的狀況
你的loss不會再越來越小
但是實際上它真的卡到了critical point
卡到了saddle point
卡到了local minima嗎
不是的
它的gradient仍然很大
只是loss不見得再減小了
所以你要注意
當你今天訓練一個network
train到後來發現
loss不再下降的時候
你不要隨便說
啊我卡在local minima
我卡在saddle point
有時候根本兩個都不是
你只是單純的loss沒有辦法再下降
就是為什麼你在在作業2-2
會有一個作業叫大家
算一下gradient的norm
然後算一下說
你現在是卡在saddle point
還是critical point
因為多數的時候
當你說你訓練卡住了
很少有人會去分析卡住的原因
為了強化你的印象
我們有一個作業
讓你來分析一下
卡住的原因是什麼
那講到這邊
有的同學就會有一個問題了
如果我們在訓練的時候
其實很少卡到saddle point
或者是local minima
那這一個圖是怎麼做出來的呢
你還記得這個圖嗎
我們上次有畫過這個圖是說
我們現在訓練一個Network
訓練到現在參數呢
在critical point附近
然後我們再來
根據eigen value的正負號
來判斷說這個critical point
比較像是saddle point
還是local minima
那如果實際上在訓練的時候
要走到saddle point
或者是local minima
是一件困難的事情
那這個圖到底是怎麼畫出來的
那這邊告訴大家一個秘密了
這個圖你要訓練出這樣子的結果
你要訓練到
你的參數很接近critical point
用一般的gradient descend
其實是做不到的
用一般的gradient descend train
你往往會得到的結果是
你在這個gradient還很大的時候
你的loss就已經掉了下去
這個是需要特別方法train的
所以做完這個實驗以後
我更感覺你要走到一個critical point
其實是困難的一件事
多數時候training
在還沒有走到critical point的時候
就已經停止了
那這並不代表說
critical point不是一個問題
我只是想要告訴你說
我們真正目前
當你用gradient descend
來做optimization的時候
你真正的魔王
你真正應該要怪罪的對象
往往不是critical point
而是其他的原因
為什麼
如果今天critical point不是問題的話
為什麼我們的training會卡住呢
我這邊舉一個非常簡單的例子
我這邊有一個
非常簡單的error surface
我們只有兩個參數
這兩個參數值不一樣的時候
Loss的值不一樣
我們就畫出了一個error surface
這個error surface的最低點呢
在X X這個地方
事實上呢
這個error surface是convex的形狀
如果你不知道convex是什麼
沒有關係
總之它是一個
它的這個等高線是橢圓形的
只是它在橫軸的地方呢
它的gradient非常的小
它的坡度的變化非常的小
非常的平滑
所以這個橢圓的長軸非常的長
短軸相對之下比較短
在縱軸的地方gradient的變化很大
error surface的坡度非常的陡峭
那現在我們要從這個地方
這個地方當作初始的點
然後來做gradient descend
你可能覺得說
這個convex的error surface
做gradient descend
有什麼難的嗎
不就是一路滑下來
然後可能再走過去嗎
應該是非常容易
你實際上自己試一下
你會發現說
就連這種convex的error surface
形狀這麼簡單的error surface
你用gradient descend
都不見得能把它做好
舉例來說這個是我實際上
自己試了一下的結果
我learning rate設10⁻²的時候
我的這個參數在峽谷的兩端
你可以想像說
這邊是一個山壁
然後另外一邊
在這個圖片之外這個山壁
我的參數在山壁的兩端不斷的震盪
我的loss掉不下去
但是gradient其實仍然是很大的
那你可能說
就是因為你learning rate設太大了阿
learning rate決定了我們
update參數的時候步伐有多大
learning rate設太大 顯然步伐太大
你沒有辦法慢慢地滑到山谷裡面
這個是咎由自取
只要把learning rate設小一點
不就可以解決這個問題了嗎
事實不然
因為我試著去
調整了這個learning rate
就會發現你光是要train
這種convex的optimization的問題
你就覺得很痛苦
???心累你知道嗎
覺得非常痛苦阿
我就調這個learning rate
從10⁻²
一直調到10⁻⁷
調到10⁻⁷以後
終於不再震盪了
終於從這個地方滑滑滑
滑到山谷底終於左轉
但是你發現說
這個訓練永遠走不到終點
為什麼
因為我的learning rate已經太小了
那這個很斜的地方
因為這個坡度很陡
gradient的值很大
所以還能夠前進一點
這個地方坡度已經非常的平滑了
這麼小的learning rate
根本沒有辦法再讓我們的訓練前進
事實上在這個地方啊
你看 看到這邊一大堆黑點
烏漆抹黑的
這邊有多少個點呢
這邊有十萬個點
這個是張遼八百衝十萬的那個十萬
知道嗎 這個是十萬個點
但是我都沒有辦法靠近
這個local minima的地方
所以顯然就算是
一個convex的error surface
你用gradient descend也很難train好
有同學可能會想說
這個convex的optimization的問題
應該有別的更好的方法可以解吧
也不一定要用gradient descend
這麼土的方法
也確實有別的方法可以解
但是你想想看
如果今天是更複雜的error surface
你真的要train一個
deep network的時候
gradient descend是你
唯一可以仰賴的工具
但是gradient descend這個工具
連這麼簡單的error surface都做不好
一室之不治 何以天下國家為
對不對
這麼簡單的問題都做不好
那如果難的問題
它又怎麼有可能做好呢
所以我們需要
更好的gradient descend的版本
那怎麼把gradient descend做的更好呢
在之前我們的gradient descend裡面
所有的參數
都是設同樣的learning rate
這顯然是不夠的
learning rate它應該要為
每一個參數客製化
所以接下來我們就是要講
客製化的learning rate
怎麼做到這件事情
那我們要怎麼客製化learning rate呢
我們不同的參數到底
需要什麼樣的learning rate呢
從剛才的例子裡面
其實我們可以看到一個大原則
這大原則是
如果在某一個方向上
我們的gradient的值很小
在某一個方向上非常的平坦
那我們會希望learning rate調大一點
如果在某一個方向上非常的陡峭
某一個方向上坡度很大
那我們其實期待
learning rate可以設得小一點
那這個learning rate要如何自動的
根據這個gradient的大小做調整呢
我們要改一下
gradient descend原來的式子
這邊我們等一下在講解的時候
我們只放某一個參數update的式子
我們之前在講gradient descend
我們往往是講
所有參數update的式子
那這邊為了等一下簡化這個問題
我們只看一個參數
但是你完全可以把這個方法
推廣到所有參數的狀況
我們只看一個參數
這個參數叫做θᵢ
這個θᵢ在第t個iteration的值
減掉在第t個iteration
這個參數i算出來的gradient
這個gᵢᵗ代表在第t個iteration
也就是θ等於θᵗ的時候
參數θᵢ對loss的微分
我們把這個θᵢᵗ減掉learning rate
乘上gᵢᵗ會更新learning rate到θᵢᵗ⁺¹
這是我們原來的gradient descend
我們的learning rate是固定
現在我們要有一個
會客製化的learning rate
隨著參數客製化的learning rate
要怎麼做呢
我們把原來learning rate η這一項呢
改寫成η除以σᵢᵗ
這個σᵢᵗ你發現它有一個上標t
有一個下標i
這代表說這個σ這個參數
首先它是depend on i的
它是參數dependent的
不同的參數我們要給它不同的σ
同時它也是iteration dependent的
不同的iteration我們也會有不同的σ
所以當我們把我們的learning rate
從η改成η除以σᵢᵗ的時候
我們就有一個
parameter dependent的learning rate
接下來我們是要看說
這個parameter dependent的
learning rate有什麼常見的計算方式
那這個σ有什麼樣的方式
可以把它計算出來呢
一個常見的類型是算
gradient的Root Mean Square
什麼意思呢
這個是我們
現在參數要update的式子
我們從θᵢ⁰
我們等一下就不把i唸出來了
大家知道我的意思
這邊是考慮某一個參數
所以有一個下標i
但是等一下為了方便講課
我就不把i唸出來了
θᵢ⁰初始化參數減掉gᵢ⁰
乘上learning rate η除以σᵢ⁰
就得到θᵢ¹
這個σᵢ⁰怎麼算出來的呢
在第一次update參數的時候
這個σᵢ⁰是(gᵢ⁰)²除以開根號
這個gᵢ⁰就是我們的gradient
就是gradient的平方除以開根號
那這一項呢
其實就是gᵢ⁰的絕對值對不對
所以你把gᵢ⁰的絕對值代到這邊
這個gᵢ⁰跟這個gᵢ⁰
它們的大小是一樣的
所以這邊呢 這一項只會有一個
要嘛是正一 要嘛是負一
就代表說我們第一次在update參數
從θᵢ⁰update到θᵢ¹的時候
要嘛是加上η 要嘛是減掉η
跟這個gradient的大小沒有關係
是看你η設多少
這個是第一步的狀況
這一個地方沒有特別重要
所以第一步的地方如果你聽了
有一點模模糊糊就算了
重點是接下來怎麼處理
那θᵢ¹它要一樣
減掉gradient gᵢ¹乘上η除以σᵢ¹
現在在第二次update參數的時候
是要除以σᵢ¹ 不是σᵢ⁰ 是σᵢ¹
那這個σᵢ¹是怎麼被算出來的呢
這個σᵢ¹就是我們過去
所有計算出來的gradient
它的平方的平均再開根號
我們到目前為止
在第一次update參數的時候
我們算出了gᵢ⁰
在第二次update參數的時候
我們算出了gᵢ¹
所以這個σᵢ¹就是(gᵢ⁰)²
加上(gᵢ¹)²除以½再開根號
這個就是Root Mean Square
我們算出這個σᵢ¹以後
我們的learning rate就是η除以σᵢ¹
然後把θᵢ¹減掉
η除以σᵢ¹乘以gᵢ¹ 得到θᵢ²
同樣的操作就反覆繼續下去
在θᵢ²的地方
你要減掉η除以σᵢ²乘以gᵢ²
那這個σ是什麼呢
這個σᵢ²就是過去
所有算出來的gradient
它的平方和的平均再開根號
所以你把gᵢ⁰取平方
gᵢ¹取平方 gᵢ²取平方
的平均再開根號
得到σᵢ²放在這個地方
然後update參數
所以這個process這個過程
就反覆繼續下去
到第t次update參數的時候
其實這個是第t + 1次啦
第t + 1次update參數的時候
你的這個σᵢᵗ要怎麼設定呢
它就是過去所有的gradient
gᵢᵗ從第一步到目前為止
所有算出來的gᵢᵗ的平方和
再平均 再開根號得到σᵢᵗ
然後在把它除learning rate
然後用這一項當作是
新的learning rate來update你的參數
那這一招被用在一個叫做
Adagrad的方法裡面
為什麼這一招是有用的呢
為什麼這一招可以做到我們剛才講的
坡度比較大的時候
learning rate就減小
坡度比較小的時候
learning rate就放大呢
你可以想像說
現在我們有兩個參數
一個叫θᵢ¹ 一個叫θᵢ²
θᵢ¹坡度小 θᵢ²坡度大
θᵢ¹因為它坡度小的關係
所以你在θᵢ¹這個參數上面
算出來的gradient值都比較小
因為gradient算出來的值比較小
然後這個σ是gradient的
Root Mean Square
gradient的平方和取平均再開根號
所以既然gradient小 算出來的σ就小
σ小 learning rate就大
反過來說θᵢ²
θᵢ²是一個比較陡峭的參數
在θᵢ²這個方向上loss的變化比較大
所以算出來的gradient都比較大
算出來的gradient比較大
你的σ就比較大 你的σ比較大
你在update的時候 你的step
你的參數update的量就比較小
所以有了σ這一項以後呢
你就可以隨著gradient的不同
每一個參數的gradient的不同
來自動的調整learning rate的大小
那這個並不是
你今天會用的最終極的版本
剛才那個版本還有什麼樣的問題呢
就算是同一個參數
它需要的learning rate
也會隨著時間而改變
我們剛才的假設
好像是同一個參數
它的gradient的大小
就會固定是差不多的值
但事實上並不一定是這個樣子的
舉例來說我們來看
這個新月形的error surface
如果我們考慮橫軸的話
考慮左右橫的水平線的方向的話
你會發現說
在這個地方坡度比較平滑
你會發現在這個地方坡度比較陡峭
所以我們需要比較小的learning rate
但是走到了中間這一段的時候呢
坡度又變得平滑了起來
這邊坡度比較陡峭
這邊坡度變得平滑了起來
所以我們在這個地方
就需要比較大的learning rate
所以就算是同一個參數 同一個方向
我們也期待說
learning rate是可以動態的調整的
所以怎麼辦呢
就有了一個新的招數
這個招數叫做RMS Prop
RMS Prop這個方法有點傳奇
它傳奇的地方在於它找不到論文
找不到論文這個方法出自於哪裡了
非常多年前應該是將近十年前
Hinton在Coursera上
開過deep learning的課程
那個時候他在他的課程裡面
講了RMS Prop這個方法
然後這個方法沒有論文
所以你要cite的話
你要cite那個影片的連結
這是個傳奇的方法叫做RMS Prop
RMS Prop這個方法是怎麼做的呢
它的第一步跟剛才講的
算Root Mean Square
也就是那個Apagrad的方法
是一模一樣的
所以第一步我們就不要看了
我們看第二步
第二步有什麼不同呢
一樣要算出σᵢ¹
只是我們現在算出σᵢ¹的方法跟剛才
算Root Mean Square的時候不一樣
剛才在算Root Mean Square的時候
每一個gradient都有同等的重要性
但在RMS Prop裡面
它決定你可以自己調整
現在的這個gradient
你覺得它有多重要
所以在RMS Prop裡面
我們這個σᵢ¹它是之前的σᵢ⁰
前一步算出來的σᵢ⁰裡面就是有gᵢ⁰嘛
所以這個σᵢ⁰就代表了gᵢ⁰的大小
所以它是(σᵢ⁰)²
乘上α加上(1-α)
乘上現在我們剛算出來的
新鮮熱騰騰的gradient就是gᵢ¹
那這個α另外一個參數
就像learning rate一樣
這個你要自己調它
它是一個hyperparameter
你要自己調它
那你可以想像說
如果我今天α設很小趨近於0
就代表說我覺得gᵢ¹相較於
之前所算出來的gradient而言
比較重要
我α設很大趨近於1
那就代表說我覺得現在算出來的gᵢ¹
比較不重要
之前算出來的gradient比較重要
所以同理在第三次update參數的時候
我們要算σᵢ² 那怎麼辦呢
我們就把σᵢ¹拿出來取平方再乘上α
那σᵢ¹裡面有gᵢ¹跟σᵢ⁰ σᵢ⁰裡面又有gᵢ⁰
所以你知道σᵢ¹裡面它有gᵢ¹有gᵢ⁰
然後這個gᵢ¹跟gᵢ⁰呢他們會被乘上α
然後再加上1-α乘上這個(gᵢ²)²
好所以這個α就會決定說gᵢ²
它在整個σᵢ²裡面佔有多大的影響力
那同樣的過程就反覆繼續下去
σᵢᵗ等於根號α乘上(σᵢᵗ⁻¹)²
加上(1-α) (gᵢᵗ)²
你用α來決定現在剛算出來的gᵢᵗ
它有多重要
好那這個就是RMSProp
那RMSProp我們剛剛講過說
透過α這一項你可以決定說
gᵢᵗ相較於之前存在
σᵢᵗ⁻¹裡面的gᵢᵗ到gᵢᵗ⁻¹而言
它的重要性有多大
如果你用RMS Prop的話
你就可以動態調整σ這一項
我們現在假設從這個地方開始
這個黑線是我們的error surface
從這個地方開始你要update參數
好你這個球就從這邊走到這邊
那因為一路上都很平坦
很平坦就代表說g算出來很小
g算出來很小
就代表說這個σ算出來很小
σ算出來很小就代表說
現在update參數的時候
我們會走比較大的步伐
接下來繼續滾 滾到這邊
滾到這邊以後我們gradient變大了
如果是原來的 不是RMS Prop
原來的Adagrad的話它反應比較慢
但如果你用RMS Prop
然後呢你把α設小一點
你就是讓新的
剛看到的gradient影響比較大的話
那你就可以很快的讓σ的值變大
也可以很快的讓你的步伐變小
你就可以踩一個煞車
本來很平滑走到這個地方
突然變得很陡
那RMS Prop可以很快的踩一個煞車
把learning rate變小
如果你沒有踩剎車的話
你走到這裡這個地方
learning rate太大了
那gradient又很大
兩個很大的東西乘起來
你可能就趴的一下你就飛出去了
就很快就飛出去了就飛到很遠的地方
好 那如果繼續走走走
又走到平滑的地方了
因為這個σᵢᵗ 你可以調整α
讓它比較看重於
最近算出來的gradient
所以你gradient一變小
σ可能就反應很快
它的這個值就變小了
然後呢你走的步伐就變大了
這個就是RMS Prop
那今天你最常用的
optimization的策略
有人又叫做optimizer
今天比較常用的optimization
最常用的optimization的策略
就是Adam
Adam就是什麼呢
Adam就是RMS Prop加上
我們上週講過的Momentum
那Adam的演算法跟原始的論文
我們就列在投影片上 我們就不細講
你可能說不細講的話
我怎麼知道怎麼做這個Adam呢
今天你不用擔心
這個pytorch裡面
都幫你寫得好好的了
或者是你打開助教的程式來看
搜尋Adam
裡面就有個的地方是Adam
所以這個你今天
不用擔心這種optimization的問題
optimizer這個deep learning的套件
往往都幫你做好了
然後這個optimizer裡面
也有一些參數需要調
也有一些hyperparameter
需要人工決定
但是你往往用預設的
那一種參數就夠好了
你自己調有時候會調到比較差的
往往你直接copy
這個pytorch裡面
Adam這個optimizer
然後預設的參數不要隨便調
就可以得到不錯的結果了
關於Adam的細節
就留給大家自己研究
我們剛才講說這個簡單的error surface
我們都train不起來
現在我們來看一下
加上Adaptive Learning Rate以後
train不train得起來
那這邊是採用
最原始的Adagrad那個做法啦
就是把過去看過的
這個learning rate通通都
過去看過的gradient
通通都取平方再平均再開根號
當作這個σ 那做起來怎麼樣呢
做起來是這個樣子的
你先不要管這邊
我知道你看到這個
一定覺得大吃一驚啊
這個到底是怎麼回事
這個黑壓壓的到底是什麼東西
是不是打翻墨水了
先不要管這邊先不要管這邊
你這個走下來沒有問題
然後接下來在左轉的時候啊
在左轉的時候
剛才我們update了十萬次
這邊也是update了十萬次
剛才update了十萬次
只卡在這個地方
那現在有Adagrad以後
你可以再繼續走下去
走到非常接近終點的位置
為什麼Adagrad可以繼續走下去呢
因為當你走到這個地方的時候
你因為這個左右的方向的
這個gradient很小
所以learning rate會自動調整
左右這個方向的
learning rate會自動變大
所以你這個步伐就可以變大
就可以不斷的前進
接下來的問題就是
為什麼走到這邊突然爆炸了呢
你想想看 我們在做這個σ的時候
我們是把過去所有看到的gradient
都拿來作平均 所以這個縱軸的方向
在這個初始的這個地方
感覺gradient很大
可是這邊走了很長一段路以後
這個縱軸的方向
gradient算出來都很小啊
所以縱軸這個方向
這個y軸的方向就累積了很小的σ
因為我們在這個y軸的方向
看到很多很小的gradient
所以我們就累積了很小的σ
累積到一個地步以後
這個step就變很大
然後就爆走就噴出去了
那噴出去以後沒關係
有辦法修正回來 為什麼
因為噴出去以後
就走到了這個gradient比較大的地方
走到gradient比較大的地方以後
這個σ又慢慢的變大
σ慢慢變大以後
這個參數update的距離
Update的步伐大小就慢慢的變小
你就發現說走著走著
突然往左右噴了一下
但是這個噴了一下不會永遠就是震盪
不會做簡諧運動停不下來
這個力道慢慢變小
有摩擦力 讓它慢慢地慢慢地
又回到中間這個峽谷來
然後但是又累計一段時間以後 又會噴
然後又慢慢地回來 怎麼辦呢
有一個方法也許可以解決這個問題
這個叫做learning rate的scheduling
什麼是learning rate的scheduling呢
我們剛才這邊還有一項η
這個η是一個固定的值
learning rate scheduling的意思就是說
這個η它要是跟時間有關的
我們不要把它當一個常數
我們把它跟時間有關
learning rate怎麼讓它跟時間有關呢
最常見的策略叫做Learning Rate Decay
也就是說呢 隨著時間的不斷地進行
隨著參數不斷的update
我們這個η讓它越來越小
那這個也就合理了
為什麼會這樣想呢
為什麼要讓learning rate越來越小呢
因為一開始我們距離終點很遠
隨著參數不斷update
我們距離終點越來越近
所以我們把learning rate減小
讓我們參數的更新踩了一個煞車
讓我們參數的更新能夠慢慢地慢下來
所以剛才那個狀況
如果加上Learning Rate Decay
有辦法解決
剛才那個狀況
如果加上Learning Rate Decay的話
我們就可以很平順的走到終點
因為在這個地方
這個η已經變得非常的小了
η變得非常的小
雖然說它本來想要左右亂噴
但是因為乘上這個非常小的η
就停下來了 就可以慢慢地走到終點
那除了Learning Rate Decay以外
還有另外一個經典 非常常用的
Learning Rate Scheduling的方式
叫做Warm Up
那我本來很猶豫
要不要講Warm Up這件事啦
但是我們有一個跟BERT有關的作業
你可能需要用到Warm Up
才能夠得到好的結果
所以我們還是
把Warm Up拿出來講一下
Warm Up這個方法
聽起來有點匪夷所思
這Warm Up的方法是說
我們這個learning rate
要先變大後變小這樣
你會問說 變大要變到多大呢
變大速度要多快呢 變小速度要多快呢
這個也是hyperparameter
你要自己用手調的
但是大方向的大策略就是
learning rate要先變大後變小
那這個方法聽起來很神奇
就是一個黑科技這樣
這個黑科技出現在
很多遠古時代的論文裡面
這個warm up
最近因為在訓練BERT的時候
往往需要用到Warm Up
所以又被大家常常拿出來講
但它並不是有BERT以後
才有Warm Up的
Warm Up這東西遠古時代就有了
舉例來說
Residual Network裡面是有Warm Up的
這邊是放了Residual network
放在arXiv上面的文章連結啦
今天這種有關machine learning 的
文章往往在投conference之前
投國際會議之前
就先放到一個叫做arXiv的網站上
把它公開來讓全世界的人都可以看
你其實看這個arXiv的網址
你就可以知道
這篇文章是什麼時候放到網路上的
怎麼看呢 arXiv的前四個數字
這15代表年份
代表說residual network這篇文章
是2015年放到arXiv上面的
後兩個數字代表月份
所以它是15年的12月
15年的年底放在arXiv上面的
所以五六年前的文章
在deep learning變化
這麼快速的領域裡面
五六年前就是上古時代
那在上古時代
這個Residual Network裡面
就已經記載了Warm Up的這件事情
它說我們用learning rate 0.01
取Warm Up
先用learning rate 0.01
再把learning rate改成0.1
用過去我們通常最常見的train
Learning Rate Scheduling的方法
就是讓learning rate越來越小
但是Residual Network
這邊特別註明它反其道而行
一開始要設0.01 接下來設0.1
還特別加一個註解說
一開始就用0.1反而就train不好
不知道為什麼 也沒解釋
反正就是train不好
需要Warm Up這個黑科技
而在這個黑科技
在知名的Transformer裡面
我相信很多同學
可能都聽過Transformer這個技術
這門課也會講到Transformer這個東西
這個黑科技在Transformer裡面
也用一個式子提了它
它這邊有一個式子說
它的learning rate遵守這一個
神奇的function來設定
它的learning rate
這個神奇的function
乍看之下會覺得 哇 在寫什麼
不知道在寫些什麼
這個東西你實際上
把這個function畫出來
你實際上把equation畫出來的話
就會發現它就是Warm Up
learning rate會先增加
然後接下來再遞減
所以你發現說Warm Up這個技術
在很多知名的network裡面都有
被當作一個黑科技
就論文裡面不解釋說
為什麼要用這個
但就偷偷在一個小地方
你沒有注意到的小地方告訴你說
這個network要用這種黑科技
才能夠把它訓練起來
那為什麼需要warm Up呢
這個仍然是今天
一個可以研究的問題啦
我並不認為為什麼要用warm Up
這件事已經獲得了完全的解答
但是這邊有一個可能的解釋是說
你想想看當我們在用Adam RMS Prop
或Adagrad的時候
我們會需要計算σ
這個σ是怎麼來的
它是一個統計的結果對不對
就σ告訴我們說
某一個方向它到底有多陡
或者是多平滑
那這個統計的結果
要看得夠多筆數據以後
這個統計才精準
所以一開始我們的統計是不精準的
一開始我們的σ是不精準的
所以我們一開始不要讓我們的參數
走離初始的地方太遠
先讓它在初始的地方呢
做一些像是探索這樣
所以一開始learning rate比較小呢
是讓它探索 收集
一些有關error surface的情報
先收集有關σ的統計數據
等σ統計得比較精準以後
在讓learning rate呢慢慢地爬升
所以這是一個解釋
為什麼我們需要warm up的可能性
那如果你想要學更多
有關warm up的東西的話
你其實可以看一篇paper
它是Adam的進階版叫做RAdam
裡面對warm up這件事情
有更多的理解
那有關optimization的部分
其實我們就講到這邊啦
所以我們從最原始的gradient descent
進化到這一個版本
這個版本裡面有什麼東西呢
第一個 我們有Momentum
也就是說我們現在
不是完全順著gradient的方向
現在不是完全順著這一個時間點
算出來的gradient的方向
來update參數
而是把過去
所有算出來gradient的方向
做一個加總
當作update的方向
這個是momentum
接下來應該要update多大的步伐呢
我們要除掉
gradient的Root Mean Square
那講到這邊可能有同學會覺得很困惑
這一個momentum是考慮
過去所有的gradient
這個σ也是考慮過去所有的gradient
一個放在分子一個放在分母
都考慮過去所有的gradient
不就是正好抵銷了嗎
但是其實這個Momentum跟這個σ
它們在使用過去所有gradient的方式
是不一樣的
Momentum是直接
把所有的gradient通通都加起來
所以它有考慮方向
它有考慮gradient的正負號
它有考慮gradient
是往左走還是往右走
但是這個Root Mean Square
它就不考慮gradient的方向了
它只考慮gradient的大小
記不記得我們在算σ的時候
我們都要取平方項
我們都要把gradient取一個平方項
我們是把平方的結果加起來
所以我們只考慮gradient的大小
不考慮它的方向
所以Momentum跟這個σ
算出來的結果並不會互相抵銷掉
那最後我們還會加上
一個learning rate的scheduling
那這個是今天optimization的
完整的版本了
這種Optimizer
除了Adam以外
Adam可能是今天最常用的
但除了Adam以外
還有各式各樣的變形
但其實各式各樣的變形都不脫
就是要嘛不同的方法算M
要嘛不同的方法算σ
要嘛不同的
Learning Rate Scheduling的方式
那如果你想要知道更多
跟optimization有關的事情的話
那有之前助教的錄影
給大家參考到這裡
影片蠻長的大概兩個小時
所以你可以想見說
有關Optimizer的東西
其實是還有蠻多東西可以講的
所以時間的關係我們就不講下去
到目前為止呢 我們講的是什麼
我們講的是
當我們的error surface非常的崎嶇
就像這個例子一樣非常的崎嶇的時候
我們需要一些比較好的方法
來做optimization
前面有一座山擋著
我們希望可以繞過那座山
山不轉路轉的意思這樣
你知道這個gradient
這奇怪的error surface
會讓人覺得很痛苦對不對
???那再來會怎麼樣呢
再來就是要用神羅天征
把這個炸平這樣子
所以接下來我們會講的技巧
就是有沒有可能
直接把這個error surface移平
我們改Network裡面的什麼東西
改Network的架構activation function
或者是其它的東西
直接移平error surface
讓它變得比較好train
也就是山擋在前面
就把山直接剷平的意思
