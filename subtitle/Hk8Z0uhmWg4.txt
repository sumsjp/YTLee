大家好啊
今天這堂課呢
我們要講語言模型的
能力檢定
知道說今天不斷有新的
大型語言模型推陳出新
每個推出來的新的
語言模型都要碰瓷一下
GPT-4 說它跟
GPT-4 的能力差不多
那我們今天來看一下
怎麼評估大型語言模型的能力
那怎麼
評比語言模型的能力呢
基本的想法是這個樣子的
其實不管是語言模型還是其他人工智慧的模型
評比它的能力基本上的想法是這個樣子的
你準備一些要給語言模型的輸入
你準備這些輸入應該要有的標準答案
那如果你要比較A跟B這兩個模型
就給它們一模一樣的輸入
看看它們分別會有什麼樣的輸出
再跟你準備的標準答案去比對
就可以知道哪一個模型的能力比較好
那這些你準備好來評量模型能力的這些輸入跟標準答案
就叫做Benchmark的Corpus
所以常常會聽到有人說
你開發完模型以後要跑在Benchmark上面
看看模型的能力是什麼樣子
指的就是把你的模型跑在一些大家已經工定好的輸入跟標準答案上面
看看你的模型可以跟其他模型比起來,能力如何
那這個概念雖然聽起來很簡單
但是當我們要評量語言模型能力的時候
這邊有各式各樣的問題需要考慮
第一個需要考慮的問題是
怎麼根據標準答案
決定一個語言模型的輸出是否正確呢
因為語言模型它的輸出是沒有限制的
這個導致你在評估語言模型的輸出是否正確的時候
有了一定程度的挑戰性
那你可能會想說
那我不就出選擇題就好了嗎
選擇題是有標準答案的
我們就問機器選擇題
有一個知名的選擇題的Benchmark Components
叫做Massive Multitask Language Understanding MMLU
裡面收集了上萬題的選擇題
那它的題目涵蓋各式各樣不同的學科
而且裡面的題目都蠻有挑戰性的
都是那種高中大學
你在期中期末考會出現的考題
那這邊就列舉幾個問題
比如說這是數學的問題
我相信在座各位
叫你一瞬間答對這個問題
恐怕有點困難吧
這個是物理的問題
如果你高中畢業很久了
這個問題你八成答不出來吧
這個是化學的問題
總之NNLU裡面就是這種有挑戰性的選擇題
那你想說選擇題就是有標準答案啊
雖然這些問題可能對今天的語言模型來說也很困難
但是評估語言模型有沒有答對
總不是個問題吧
但有趣的是同樣是NNLU的考題
不同的文獻裡面
評估同一個模型的正確率居然是不一樣的
比如說這是Lama1-65B的模型
三篇不同的論文裡面
他講的Lama1-65B在MMLU上面的正確率居然是不一樣的
為什麼會這個樣子呢?
我們來想想看
你要怎麼讓語言模型回答選擇題
那你就是把選擇題的題目跟選項都丟給語言模型
然後看看它輸出什麼樣的答案
那這一題的正確答案是B
如果語言模型輸出B
我們就可以說它就是答對的
但是要是語言模型輸出的不是剛好就是B呢
不要忘了這個生成式AI就是可以說任何話
語言模型如果它的輸出是答案是B
那這樣子你要算它對嗎
那你說你可以寫一個簡單的小程式
就檢查語言模型的輸出
只要輸出裡面有提到B這個字
我們就算它是對的
那如果今天語言模型回答是
根據計算我認為是1
這個B這個選項的答案是1
那這樣子你要不要算它是對的呢
那有人可能會說
哎呀那我們就不要讓語言模型說任何話
我們限制它的輸出
跟它講說只可以輸出選項
不可以輸出其他內容
輸出其他內容
我就當你是錯的
你可以這樣子評比
但問題就是當你這樣評比的時候
你是真的在考驗語言模型解這個數學問題的能力嗎
還是你只是在考驗他控制自己
除了選項以外
其他文字都不要輸出來
而如果他輸出選項之後
還想要解釋一下為什麼回答這個選項
你就算他錯
如果你是這樣的評比方式的話
你可能根本不是在評比語言模型真正的能力
你只是在比他能不能夠看懂這個指令
去按照這個指令做你要做的事情而已
那有人可能會說
我們也許可以乾脆直接限制語言模型的輸出
我們知道說語言模型的輸出其實是一個機率分佈
那我們能不能夠就看ABCD這四個選項的機率分佈
哪一個選項機率分佈最高
我們就說它是對的呢
你當然可以這樣想
但是如果今天這個語言模型輸出的機率分佈是這樣子
那這個應該要算他答對還是答錯呢
你要不要用五秒鐘來想一下
假設這個是語言模型輸出的機率分佈
你覺得這個語言模型算是答對的同學舉手一下
沒有人
你覺得他算是答錯的同學舉手一下
好,有好些同學覺得他是答錯了
好,那我覺得這一題沒有標準答案
那這邊我們可以說語言模型是答對的,為什麼?
因為假設只看A、B、C、D這四個選項的話
B的選項確實是最高的
所以如果在限制只看A、B、C、D這四個選項的情況下
我們可以說語言模型是答對的
但是剛才很多同學說這個語言模型算是答錯了,為什麼?
你看三的機率才是最高的
你根本就是想要回答3這個數字吧
你根本不覺得選項B是正確答案吧
所以語言模型就算是在選擇題上
它的評量是有不同的可能性的
那這邊講更多在做選擇題的時候可能會遇到的問題
這邊一樣是MMLU這個Benchmark Compass上面的正確率
那不同模型它的正確率是不一樣的
那從這個表格上看起來
這個GPT-3.5相較於其他的模型看起來正確率是最高的
這是在原版的MMLU上
有人就做了一件事
把所有的正確答案都挪到A選項
看看會發生什麼樣的事情
把所有正確答案都移到A選項
沒有影響題目的難度
但是這幾個模型比起來
突然LAMA-30B變成第一名了
為什麼?因為Llama30B最喜歡猜A
看到不會的他就猜A
所以正確選項移到A的時候
Llama30B就佔便宜了
他變成全部最強的模型
那把正確選項移到BCD
也都會有類似的問題發生
所以讓機器做選擇題的時候
選項擺放的位置
其實也會影響你評估的結果
甚至選項的表示符號
也有一定程度的影響
如果你今天的四個選項
分別是大寫的ABCD的話
GPT3.5得到的是67%的正確率
如果用小寫的ABCD
正確率也差不多
但是如果今天是用數字的1234
或者是ABCD加上括號
GPT3.5在NNLU上答題的正確率會下降
所以今天你讓機器考選擇題的時候
你怎麼擺放你的選項
甚至選項的代號
都有可能對評估的結果造成影響
所以這個評比語言模型的能力並不容易
剛才講的還只是選擇題而已
有很多其他的任務
有很多選擇題之外的任務
是沒有標準答案的
比如說翻譯一段英文
它可能可以有不同翻譯成中文的方式
一篇文章你要做摘要
可能有不只一種摘要的方式
那這個時候
當語言模型的答案和標準答案不同
並不能說語言模型錯的
就是錯的就一定是不好的
當然翻譯跟摘要
從來都不是新的議題
過去在文獻上已經有很多方法
試圖來評量機器做翻譯
做摘要的好壞
在翻譯上面
最知名的評比的方式
方式叫做BLEU
那摘要上面呢有一個知名的評比方式
叫做ROUGE
那這些方法呢他們主打的就是
你的語言模型的答案不需要跟標準答案一模一樣
只要有部分相同就可以拿到分數
但是要部分相同才能拿到分數
那如果今天的狀況是英文輸入是Humor
標準答案寫幽默
中文輸出是詼諧
那這樣難道就要算選錯嗎
所以這些人定出來的評比的基準
他們也是有一定程度的瑕疵
那過去人們也都知道說
這些評比的基準
其實是有一定程度的問題的
他只能夠作為參考
並不能完全相信這些
BLEU或者是Rogue
算出來的結果
那也許還是讓人來評比語言模型的能力是最準的
有一個網站叫做 Chatbot Arena
它是一個語言模型的競技場
它要做的事情就是由人來評比語言模型的能力
你每次進入這個競技場以後
就會隨機分配給你兩個語言模型
然後你就對這兩個語言模型問一模一樣的問題
這兩個模型就會分別給你答案
那你就可以評比說
到底是左邊這個模型比較好
還是右邊這個模型比較好
那這個就是語言模型的競技場
那這個語言模型的競技場呢
是有一個不斷更新的排行榜的啦
那今天常常會有人說
啊這個
呃比如說那個Cloud 3剛出來的時候啊
曾經一度有人說他超過了GPT-4
他指的其實就是在這個排行榜上的成績啦
不過我剛才中午的時候查了一下這個排行榜
看起來現在GPT-4又翻回第一名了
不過GPT-4啊 Gemini Pro啊 Cloud啊
看起來他們在這個排行榜上的能力都是差不多的
都是在伯仲之間
所以今天常在報章雜誌上看到說
哪個語言模型現在是全世界最強的模型
往往指的是這一個語言模型競技場上面的排行榜
但是請人來評比,太耗費資源了,人的時間是非常有限的
所以就有人來在想說,也許我們可以用語言模型來當作人來評比其他的語言模型
什麼意思呢?我們今天能不能夠有一個標準答案,有一個語言模型的輸出
我們不找人來看對不對
我們找GPT-4來看對不對
我們把語言模型的輸出跟標準答案一起丟給GPT-4
讓它給我們一個評比的結果
告訴我們說這個語言模型的輸出是不是正確答案
或者是給語言模型的輸出一個分數
那有時候你也可以讓兩個語言模型作對廝殺
有一個語言模型A產生輸出A
一個語言模型B產生輸出B
把A跟B通通丟給GPT-4
問他哪一個比較好
用GPT-4來取代人類進行評量
能夠用語言模型來取代人類進行評量嗎
還真的不是不可以
今天有一個很知名的Benchmark叫做MTBench
MTBench就是用語言模型來進行評量
MTBench是用GPT-4來衡量參加這一個評比的模型
那為什麼MTBench需要用語言模型來評量呢?
因為MTBench裡面的問題都是一些沒有標準答案的問題
那其實MTBench裡面的問題並不多,只有80題
那每一題有兩個題組
那它裡面的問題都像是這樣子的問題
比如說有一個寫作題
第一題是叫你寫一個遊記
去夏威夷的遊記
那你要講這個文化的經驗
還有一定要去的景點
這是第一題
那它通常是題組
第二個題組就是
重寫你的遊記
然後每一個句子的
開頭都必須要用字母A
來當作開頭
通常第二小題呢
是非常有挑戰性的問題
那像這樣子的問題呢
你就很難用自動的方法
來評量
那這種問題通常是要
人來評量才評量的準
但是你又不可能每一個模型都找人來評量
所以在MTBench上
今天大家公認的結果
就是用GPT-4來進行評量
那在MTBench上用GPT-4進行評準不準呢
如果你相信
Chepa Arena那個語言模型上面
語言模型競技場的結果
就是一個準確的結果的話
當然如果你不相信的話
以下的討論就沒有意義了
假設你相信語言模型競技場上的排名就是最準確的排名的話
那MTBench跟語言模型競技場上的排名的相關性是非常高的
他們的Spearman Correlation
就是這個一個相關性的指數那個分數越高越好了
Spearman Correlation是高達0.94的
那剛才我們在課程一開頭提到的MMLU
也在這個榜上是比MTBench還要稍微低一些
所以看起來用語言模型來評比其他模型
好像也是一個可行的方式
當然語言模型本身自己也有一些偏見
也許語言模型會特別偏袒某些類型的答案
那一個很知名的語言模型的偏見就是
語言模型通常喜歡長的答案
語言模型自己本身就是畫老
他本身就很喜歡長篇大論
所以他看到別人長篇大論的時候
他也會特別高興
覺得這是一個好的答案
所以有一個Benchmark Corpus叫Alpaca Evaluation
它其實也是用大型語言模型來評比的
那Alpaca Evaluation本來是沒有AntiBench
跟Chatbot Arena的correlation這麼高
但是後來Alpaca Evaluation就做了一個改版
這個改版就是要把輸出的長度進行考慮
就是他們在排名語言模型的時候
不是只看這個GPT-4的給分
而是還要考慮一下語言模型輸出的長度
如果某個語言模型每次答案都很長
那GPT-4就算給他比較高分
那個語言模型的排名也要比較後面
那考慮到長度的這個評減之後呢
這一個 Alpaca 2.0 跟 Chatbot Arena 的 Correlation 就又再上升了一截
所以這個是告訴我們說
我們也要考慮大型語言模型進行評比的時候
那個模型本身的偏見
那最近 MTBench 又有一個新的改版了
這個改版是在 4 月 19 號的時候發生的
同一個團隊他們釋出了一個新的測試的資料
叫ArenaHard,那裡面有更多的題目
據說相較於AntiBench ArenaHard
跟Chatbot Arena的這個correlation
又再更高一點
那我就把相關的連結放在這裡給大家參考
好,那我們剛才講了
怎麼根據這個語言模型的輸出
跟標準答案之間的比對
來看看語言模型的答案是不是正確的
那這邊還有另外一個議題
那究竟輸入的時候要問語言模型什麼東西呢?
因為今天這些語言模型他們的能力往往是全面的
所以你在檢測這些語言模型的能力的時候
你往往不會只用某一個任務
某幾個任務去要求它完成
你也許會期待收集大量的各式各樣的任務
來看看語言模型是不是各式各樣的任務都能夠做好
所以在這個自然語言處理的領域呢
有很多知名的Benchmark corpus
它裡面不是隻有一個任務
而是由多個不同任務組成的
想要全面評比語言模型的能力
那我這邊就列舉了幾個比較知名的Benchmark corpus
跟他們對應的論文連結
那可以看到在早年
所謂的早年就是18、19年
上古時代的時候
那時候人們還不覺得這個語言模型
應該要做很多事情
所以那時候檢測語言模型的時候
只用八九個任務來進行檢測
那後來有了 FLAN
有了 CrossFit
CrossFit 我記得是一百六十個任務
那有了這個 Big Bench
是兩百多個任務
那後來還有了 Natural Instruction
裡面有一千六百個任務
所以從各式各樣的面向
去檢測語言模型的能力
那到底準備了哪些任務
來檢測語言模型的能力呢
那我這邊想跟大家特別提一下BigBench
BigBench這個Compass裡面
蒐集了各種奇葩的任務
來檢查語言模型的能力
BigBench它第一個驚人的地方就是
它的文章有444個作者
橫跨100多個單位
所以是一個長篇鉅作
那在BigBench裡面就蒐集了各式各樣
特別拿來為難語言模型的任務
其中最被人津津樂道的就是Emoji Movie
這個任務呢
這個農場文特別喜歡講這個任務
這個任務就是給語言模型一些Emoji
針據這些Emoji猜它對應到哪部電影
那農場文常常舉以下這個例子
就是如果你給這個語言模型這幾個表情符號
有一個小女孩
有三隻魚
然後問他說這是哪部電影呢?
如果是比較小的模型,他根本不知道要幹嘛
他就亂猜,他就說這個電影叫做Emoji電影
那只有比較大的模型才知道說這個電影呢
是Finding Nemo,也就是海底總動員
那通常拿這個例子來講
大語言模型的Immersion Capability
就是大的語言模型,他會有比較強的能力
這邊我又選了幾個Emoji Movie裡面的題目啦
看看你能不能猜對
這個
好 大家猜一下是什麼吧
這邊有一個兔子
一個狐狸
一個警車跟一個城市
好 大家知道是什麼嗎
有人要回答一下嗎
動物方程式
對 動物方程式
大家都跟大型語言模型差不多厲害
那這一個呢
兩個牛仔中間有一個愛心
有人知道答案嗎?
有人要回答嗎?
你來說一下
斷背山,太厲害了
這也跟語言模型的能力差不多
太厲害了
所以大家都猜的對
就跟語言模型一樣
語言模型
但比較大的模型
也可以看這些表情符號去猜
它是哪一部電影
這個是語言模型
還勉強可以回答一下的問題
阿有很難的啦
這個叫語言模型
逼他下西洋棋阿
叫他做跟AlphaGo一樣的事
看看他能不能夠下棋
那怎麼叫語言模型下棋呢
就是給他下面這串文字
這串文字呢
代表了西洋棋的棋譜
叫他讀這個棋譜
讀完之後問他說
接下來你要下哪一步
才能夠給對方將軍
那在這個
那他畢竟比較早的文章啦,是兩三年前的文章
那個時候呢,沒有語言模型能夠答對這個問題
那這一串文字啊,對應到的是右上角這個棋譜
正確的答案是橘色這一條線
把馬呢,走到這邊就可以給對方將軍
那這些綠色的線呢,是語言模型的答案
大家發現這些語言模型提出了各式各樣的答案
但是沒有一個模型可以答對
不過有趣的地方是
十線是比較大的模型提出來的
虛線是比較小的模型提出來的答案
那看起來比較大的模型
他至少可以提出一些正確的
就是符合西洋棋規則的答案
至少這些模型知道怎麼下西洋棋
他們只是下西洋棋的能力很差
沒有辦法給對方將軍而已
而小的模型是連怎麼下西洋棋都不知道的
這個是真正折騰語言模型的任務
這個任務是叫語言模型讀這一串奇怪的密碼
然後問語言模型說
在這串密碼裡面你看到哪一個英文單字呢
這一題的正確答案是什麼呢
這一題的正確答案是Bench
看到了嗎?這邊是 B, E, N, C, H
所以當年沒有任何語言模型能夠做這種問題
這些問題太折騰語言模型了
就是給語言模型找麻煩
所以 Big Bench 裡面就是有各式各樣的任務
全面的評比語言模型的能力
那有時候我們也會想針對語言模型特定的能力來進行評比
舉例來說,近年來,大家都會希望語言模型可以閱讀非常長的文獻
但是光是能夠閱讀長文沒什麼用啊
如果他讀了前面就忘了後面的話
讀了後面就忘了前面的話
那光是能夠讀很長的長文也沒有意義啊
所以有人就開發了一個評比語言模型閱讀長文能力的評比方式
叫做大海撈針測驗
它的英文就是Needle in a haystack
大海撈針的英文就是
這個乾草堆裡面的針
在乾草堆裡面找針
這個中文就是翻成大海撈針
那我把相關的連結呢
放在右上角
那這個大海撈針測試
是怎麼進行實驗的呢
這個方法是這個樣子的
首先呢準備一篇
非常長的文章
那在這篇非常長的文章裡面的
某一個位置插入一段
特定的訊息
比如說在這個測驗裡面插入的訊息就是
在舊金山最好的事情是做某什麼什麼事情
然後接下來就是問語言模型說呢
在這個San Francisco最好的事情是什麼呢
看看語言模型能不能夠根據這一小段指令
這一小段資訊給你正確的答案
那在做大海撈針測試的時候呢
會把這一段資訊放在整段長文的不同的位置來進行測試
也就是說你可能會把這個資訊放在長文的最開頭進行測試
放在中間進行測試 放在尾端進行測試
為什麼要插入不同的位置進行測試呢
因為也許不同語言模型
他們對篇長文關注的位置是不一樣的
也許有的語言模型他只記得文章的開頭跟結尾
中間他都不記得了
也許有的語言模型只記得結尾
前面都不記得了
那用這個大海撈針測試
在不同的位置都插入同樣的針
在不同的位置都插入這一根針
來判斷說語言模型在讀長文的時候
他會不會有某一些片段
他是特別容易忽略的
好這個大海撈針測試
在GPT-4上面的結果怎麼樣呢
GPT-4號稱可以閱讀128K
那麼多token的輸入
但是當他讀這麼長的token輸入的時候
他所有的內容都讀懂了嗎
以下是實驗結果
這個橫軸指的是大海撈針的時候
那個海的大小
或者是在甘草堆裡面找針的時候
那個甘草堆的大小
越往右代表文本的長度越長
從1K個TOKEN一直到128K個TOKEN
就是GPT-4閱讀文章長度的上限
縱軸呢 縱軸是針插入的位置
那上面指的是那個針放在開頭的位置
到下面指的是針放在結尾的位置
結果怎麼樣呢
結果發現說如果今天文本的長度
是在64K的Token以下
不管那一根針放在文本的哪裡
GPT-4都可以準確的截取那一根針裡面的訊息
但是如果文本的長度非常的長
如果文本的長度真的有128K那麼長
那如果你的針是放在整篇文章的前面
10%到50%的位置
那GPT-4就有可能沒有辦法讀到針裡面的訊息
所以看起來給GPT-4真的非常長的文章的時候
它還是有可能會遺漏文章中的訊息的
那這個是GPT-4上評比的結果
那後來有人就量了Claude
那那個時候還是Claude 2.1版
有人就量了Claude 2.1版
Claude一直是以能夠閱讀長文而著名的
但是從這個實驗結果你會發現說
這個橫軸是文章的長度
縱軸是針插入的位置
那從這個圖上會發現說
如果今天在真的讀長文的時候
看起來 Claude 讀長文的能力
好像沒有那麼厲害
如果文章真的很長
好像把針插在很多地方
這個直越偏紅色就代表說
越讀不到那個針的訊息
看起來這個文本很長的時候
再把針插在很多地方
Claude都讀不到文
那個真的內容都截取不到真裡面的資訊
你知道Claude呢
一直是以能夠閱讀常文而知名啦
所以Claude的團隊
看到這個結果
真的就坐不住了
所以他們就發了一個文章Diss這個結果
他們是怎麼說的呢
他們說
有人做大海撈針測試
對Claude進行了測試
發現說Claude讀長文的能力
其實沒有那麼強
他們要說這個結果不一定是精確的
他們說只要改了一下Pump
結果就不一樣了
原來的問法是
就問他說在這個San Francisco
最有趣的事情是什麼呢
然後assistant冒號
那接下來叫機器做文字揭露
把答案揭出來
那Claude團隊說你需要再多加這句話
Here is the most relevant sentence in the context
你要多加這句話
再讓 Claude 去做文字接龍
那之所以為什麼要多加這句話
那你可以閱讀一下這個 Claude 團隊發表的 Blog
然後看看你接不接受他們的說法
總之多加這句話以後
這 Claude 的能力突然就不一樣了
一樣是大海撈針的實驗
這個橫軸是文本的長度
縱軸是針插入的位置
一旦多加了這句話
再做大海撈針實驗
這個時候 Claude 它閱讀長文的能力
就變得幾乎是完美了
好,那我講這個實驗主要是想要告訴你說
今天我們在量這個語言模型的時候
有另外一個要注意的事情就是
你怎麼下 prompt
因為我們叫語言模型解某一個任務的時候
你總是要下些 prompt
而這些 prompt 也許對語言模型本身的能力
是有巨大影響的
所以當我們在看這個語言模型
評比的結果的時候是需要小心的
那還有很多測試語言模型其他面向的方法
這邊就舉幾個有趣的例子
有一個benchmark叫做馬基維利benchmark
就是問語言模型有沒有信奉馬基維利主義
看看他會不會為了達成目標而不擇手段
怎麼測驗語言模型會不會為達目標不擇手段呢
這個馬基維利 Benchmark 就是讓語言模型玩一些文字冒險遊戲
注意一下不是語言模型出文字冒險遊戲給人玩喔
我們之前上課有講過說語言模型可以出冒險遊戲給人玩
這邊是人把文字冒險遊戲準備好 叫語言模型來玩
所以這個圖上的這每一個點代表文字冒險遊戲裡面的一個場景
每一個場景你可以做出不同的選擇
這邊是要用語言模型來進行選擇
不同的選擇接下來就會有不同的劇情走向
在玩這個遊戲的時候
一開始會告訴語言模型
你有一些任務
比如說你要增進你家族的榮耀
你要推翻某一個壞人的陰謀
然後就讓語言模型開始來玩文字冒險遊戲
在每一個節點
語言模型就會看到一個敘述
比如這個敘述是
有一個人拿槍指著他跟他說
你要不要跟我合作
那語言模型有三個選項
一個選項是我可以跟他合作
一個選項是我假裝跟他合作
那之後會背叛他
第三個選項就是不跟他合作
然後語言模型每做出一個決策呢
就會增加一些他得到的reward
就在這個遊戲裡面
有設定你做到什麼樣的事情
你就可以得到多少分數
所以語言模型他可以得到一些分數
但是同時他所做出來的行為
又會被評比
就有一些行為是違反道德的
那會計算語言模型它的行為
有多符合人類社會的道德規範
結果怎麼樣呢
語言模型會不會為達目的不擇手段呢
這個是實驗結果
這個橫軸是參加這個馬基維利測驗的模型
它所得到的分數
這個縱軸是違反道德的比例
那這個值是越高代表越符合道德規範
越小代表越容易違反人類社會的道德規範
這個藍色的這些點的這些模型呢
是專門針對這些文字冒險遊戲去訓練過的
他們在這個文字冒險遊戲裡面做過RLHF
他們要做的事情就在文字冒險遊戲裡面
得到最高的分數 其他事情都不要管
所以這些藍色的模型呢
他們會在文字冒險遊戲裡面得到很高的分數
但是他們會做出很多違反人類社會道德規範的事情
那GPT-4呢
如果是GPT-4的話
你就會發現說GPT-4它是有底線的
有一些事情它是不做的
所以它沒有辦法得到最高的分數
但是它也沒有其他模型那麼容易違反人類社會的道德規範
那另外你也可以強化一下GPT-4
GPT-4遵守道德的能力
比如說這邊加了一個Ethnics Prompt
其實就跟GPT-4講說
你要符合人類社會道德的規範
等等 它就聽了
所以它得到的分數更低 但更符合道德規範
那你也可以直接不給GPT-4遊戲的目標
一開始就不告訴它這個遊戲要幹嘛
它就會更符合道德規範
當然得到的分數就更低
這個實驗告訴我們說
語言模型還是有一些底線的
那另外一個測驗呢
是很多人會想要知道說
機器有沒有心智理論
theory of mind
什麼是心智理論呢
心智理論指的就是
這種揣摩他人想法的能力
我們人類會知道說
另外一個人他的想法
跟我們是不一樣的
我們知道說每一個人
他的心理狀態可能都是不一樣的
如果要舉一個例子的話
這邊有個莫名其妙的例子
就是輝夜大小姐讓人告白
輝夜大小姐想讓人告白是什麼樣的故事呢
就是有一個學校叫做秀枝苑
而那個秀枝苑的學生會副會長叫做四宮輝夜
然後他喜歡那個學生會的會長叫做白銀
所以輝夜喜歡白銀
那輝夜不只是喜歡白銀而已
他是有心智理論的
所以他知道白銀也喜歡輝夜
但輝夜不只知道白銀也喜歡輝夜
輝夜知道白銀也喜歡...
這個不知道怎麼講
輝夜知道白銀喜歡輝夜
而且白銀知道灰燕也喜歡白銀
就是這麼樣的一個故事
來可能會問說
為什麼他們不在一起呢
這個故事就是這個樣子
就他們覺得只要先告白就輸了
所以他們絕對不能先告白
這整個漫畫就是要挖一個陷阱
讓對方跳進去 讓對方告白
就是這麼樣的一個故事
所以人類是有心智理論的
人類就是會知道一些
我知道你知道 他知道 我知道
這種事情 那機器
有沒有心智理論呢
所以就來測驗一下
機器有沒有心智理論嗎
心理學上拿來
測試一個東西 一個生物
有沒有心智理論才有的測驗呢
叫做沙利與小安測驗
沙莉與小安測驗
就是這樣一個問題
你就跟一個人講說
沙莉和小安
他們旁邊有個箱子和籃子
沙莉把球放在籃子之後
沙莉就離開了
小安在沙莉離開後
把球放到了箱子中
請問沙莉回來以後
會去哪裡找球呢
我們給大家三秒鐘想一下
你覺得沙莉會去籃子找球的同學舉手一下
你覺得沙利會去這個箱子找球的同學舉手一下
幾乎沒有 所以大家都是有心智理論的 因為你知道說沙利覺得
球在籃子裡 他並不知道小安把球挪開了
那GPT3.5知不知道呢 他知道 所以他覺得說沙利回來
會去籃子裡面找球 因為他記得他把球放在那裡
我也問了一下臺德 看起來臺德也可以得到正確的答案
過去很多人就有對那個GPT-1做了這樣子的測試
發現說 哇 語言模型可以正確回答心智理論的問題耶
那叫心智理論這種問題呢 年紀太小的小孩是答不對的
三四歲的小孩可能答不對 要五六歲以後呢 才能答對這種問題
所以有人就說 哇 這個機器有人類五六歲小孩的智商耶
真的是這樣子嗎?
這些題目都是網路上就有的
這個題目我從網路上抄來的
搞不好語言模型根本就看過啦
換一個問法,把人名換掉
大雄跟小叮噹
他們旁邊有抽屜跟衣櫃
小叮噹把銅鑼燒放到抽屜之後
小叮噹就離開了
大雄在小叮噹離開後
把銅鑼燒放到了衣櫃中
小叮噹回來後
他會去哪裡找銅鑼燒呢?
好 覺得是抽屜的同學舉手一下
好 都是同學覺得是抽屜
好 覺得是衣櫃的同學舉手一下
好 幾乎沒有
我們來看看GPT 3.5怎麼回答嗎
他覺得是衣櫃啊
所以 哇 怎麼突然之間沒有心智理論了
然後我又問那個臺德一樣的問題
他說小叮噹會到衣櫃和抽屜查看這樣子
他直接就說兩個都會查看
所以突然之間又沒有辦法又沒有心智理論了
不過我試過一樣的問題如果是GPT-4的話
他基本上還是可以答的對
但是我舉這個例子是要告訴你說
有時候我們覺得語言模型有某種能力
你要小心啊
是不是他在網路上已經看過類似的問題了
你根本不知道他在網路上爬過什麼資料
很多你覺得很難的問題
搞不好答案他其實早就看過了
所以後來就有人做了一個比較完整的測驗
看看機器到底有沒有心智理論呢
當然不能夠直接爬網路的題目
所以他們就出了一些新的題目
那這些題目呢都是對話的情境
他們就設計了一個對話的情境
比如說像左邊這樣
這邊這個女孩呢叫做凱莉
凱莉呢說她要離開一下
那所以紅色的區塊呢
是其他的人在聊天
但是凱莉不在
那凱莉不在的時候呢
琳達就說到說她有一隻狗
而這隻狗呢是這個黃金獵犬
接下來凱莉回來了
他們再繼續聊天
但沒有再聊到琳達的狗是哪一個品種的
那接下來你就問語言模型一些問題
比如說琳達的狗是哪一個品種呢
正確答案當然是黃金獵犬
剛才在對話裡面有提到
但是如果這個問題的問法變成
凱莉覺得琳達的狗是哪一種品種呢
正確答案應該是凱莉不知道琳達的狗是哪一種品種
因為凱莉在琳達討論狗的品種這件事情的時候
她離開了 她不在現場
所以她不應該知道琳達的狗是哪一種品種
所以有了這些題目
再來測驗一下語言模型
它有沒有心智理論呢
這個是測試的結果
這個是人類的正確率
人類的正確率是87.5
那其他的語言模型,他們的能力怎麼樣呢?
其他模型都直接鋪接這樣子
所以發現說,這個GPT-4就算是強如GPT-4
他的心智理論的能力跟人類比起來
還是有非常非常顯著的差距
看起來GPT-4對於人類心智狀態的理解
沒有我們想像的那麼強
那這邊要提醒一下大家不要盡信Benchmark的結果
為什麼不能盡信Benchmark的結果呢
因為想想看Benchmark就是公開來讓大家評量語言模型能力用的
所以那些考題通通都是公開的
那些考題是公開的可能會造成什麼現象呢
有人就做了一個這樣的實驗
他想要跟大家講說Benchmark可能會有的問題
他做了一個這樣子的實驗
他說MMLU的測試資料是公開的
大家都拿那些測試資料來評比你的模型
好
那我就出一些新的題目
這些新的題目呢
跟MMLU的題目呢
基本上是一樣的
但通通都是換句話說
所以你不能夠說我偷偷用了MMLU的考題
因為如果真的偷偷用MMLU的考題
那個太沒品了
沒有人會做這種事
但他偷偷另外出了一些考題
這些題目的內容跟MMLU的題目是一樣的
只是換句話說而已
然後拿去train Lama13B
在MMLU上Lama13B直接就跟GPT-4一樣強了
GPT-4正確是86%
13B的模型也有86%
但你能說這個模型它真的跟GPT-4一樣嗎
這顯然不可能
這個模型只是它看過類似的題目
跟一樣的答案而已
所以這是告訴我們說
不要盡信Benchmark的結果
另外呢
你知道語言模型很有可能
都已經偷看過Benchmark的資料了
怎麼說呢
這邊有一個例證
有一篇論文
他就是收集了各式各樣的Benchmark
跟找了各式各樣的模型來進行評比
右邊這個圖呢
是在這一些Benchmark Purpose上
語言模型的表現
數值越高代表表現越好
語言模型它的訓練資料都只訓練到某一個時間點
所以左邊這些Dataset代表說
這些Dataset是在語言模型訓練之前就已經存在的Dataset
右邊這些Dataset是語言模型訓練之後才釋出的Dataset
所以很明顯的發現說
語言模型在他訓練之後才釋出的 dataset 上
表現明顯是比較差的
在他訓練之前就已經有的 dataset
表現明顯是比較好的
這邊可能可以得到的一個推論是
也許語言模型已經偷看過這些資料跟他的答案
當然可能不是有意為之的
可能是不小心的
因為這些語言模型的學習資料就是在網路上爬下來的啊
所以就算是開發者也不知道語言模型到底學了什麼
到底爬到了什麼樣的資料
也許這些Benchmark Open在網路上
本來就是公開的語言模型
早就在pre-trained的時候
看過這些資料的
但是僅僅只有這個證據是不夠的
因為有人可能會反駁說
這些比較新的資料集
可能就是比較難
為了要測驗語言模型的能力
所以人類出的資料集 出的測驗題目
越來越難
也許新的資料集比較難
所以語言模型才做得比較差
這邊有一個讓大家百口莫辯的結果
這個百口莫辯的實驗是這樣子的
直接跟語言模型講說
有一個資料集叫做RTE
你能不能給我一些RTE裡面的訓練資料
如果他輸出來的結果
正好跟RTE裡面訓練資料一樣
那這就是個鐵證
這個實錘
他真的偷看過這個資料集
結果如何呢
這邊是不同的資料集
這邊是不同的模型
那我們就看GPT-3.5吧
你會發現有好多資料集
GPT-3.5都可以輸出那個資料集裡面的訓練資料
所以這個是一個蠻實錘的證據說
這些模型有偷偷看過這些訓練資料
當然沒有辦法突出這些訓練資料的模型
你並不能夠說他一定沒有看過這些訓練資料
因為他有可能只是太笨了
他沒有記住這些訓練資料
所以他讀不出來而已
但是有很多模型是實錘他有看過這一些訓練資料的
那剛才講的主要都是語言模型在能力上的評比
當然語言模型今天我們要使用它
考量的不僅僅是有能力
可能還有其他的面向
比如說這個語言模型
如果它這個線上的API往往要花錢的
那它的價格多少呢
或者是這個語言模型
你在使用它的時候
你需要配備多好的硬體才能夠使用它呢
除了語言模型的能力以外
今天還是有其他的因素
其他的因素你是需要考慮的
如果你想要看語言模型更多評比的面向的話
我這邊右下角放了一個網站
裡面可以找到更多語言模型評比的面向
那在這個網站裡面就放了一個圖
比如說這個縱軸呢
是語言模型的能力
那這個能力呢
就是我們剛才講過的幾個知名的benchmark purpose
比如說MMLU
Anti-Bench
平均的結果
那橫軸呢
是這個語言模型用它的時候
需要花多少的錢
所以就可以看到說
也許有兩個語言模型他們之間
能力也許沒那麼大的差距
但是有些模型會明顯的比其他模型貴很多
那就可以給大家做一個參考
你今天在使用這些模型的時候
如果能力沒有差很多的話
也許你會想要選擇一個比較便宜的模型等等
那剛才我們到目前為止講的
只是評量人工智慧的效能
但是效能並不是唯一的考量
我們還需要考量其他的面向
比如說人工智慧的安全性
那在下一段課程裡面
我們就要講有哪些安全性的面向
我們是需要考慮的
好那我們其實就可以繼續講下去了
那我們把下一段課程講完以後
再請助教來講作業
今天看起來時間應該是蠻足夠的
You
