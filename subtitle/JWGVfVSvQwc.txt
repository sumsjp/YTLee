hello my name is D and this is the title
of our inter speech 2020 paper B QBC
plus one voice conversion by
vectorization and unit
architecture and this is our alline I
will first introduce what is voice
conversion and introduce some basic idea
or basic framework of voice conversion
and then I will introduce some related
work and next I will introduce our
propos approach VVC plus and then is the
experiment details and the experiment
result in the final I will play out
demo for conversion means we want to
change the speaker Identity or absence
of original auter while preserving the
content information some general
applications like customization of text
speech or personalized human computer
interface and so
on in general we divide the VC system
into par data and unpar
data the property of per data means the
speakers in the data set says the same
set of the sentence this kind of dat set
is very hard to collect however it is
very easy for model training on the
other hand on data means that we don't
have any c on what speaker should said
then there are generally two types of
method to tackle the data for C direct
transform and feature disentangle releas
some papers of D transform and in this
talk I will focus on the fangle base
model the basic idea of fangle base
model is he want to separate the
original utterance into contain B as
edding and we need a decoder which can
combine these two edding and revert back
to the original audio if we have these
three component contain Bing speaking B
decoder The Voice conversion is
fulfilled you can just F the contain
Bing of the AL of s speaker and the
speaker embedding of the out of Target
speaker into the decoder then the voice
conversion is fulfilled however one of
the problem of fure disle based model is
how can we make y encoder for content
and one encoder for
speaker there are various method fortain
Bing we can use ination or vectorization
or PR as a system and for speaker eding
we can use to CH speaker verification
model like I Vector X vector or D vector
or we can use adaptive ination which is
first using the image transfer power
in our previous work we find that iniz
plus vectorization can eliminate speaker
information from
containg second we found that speaker
embedding can be represented by the
difference between Contex before and
cont after
record in details if V is the output of
encoder that our contain B in C is V
passing through internalization and
vectorization our speaker ining s is the
difference between V and c and mean over
the
time this architecture can fulfill voice
conversion however the sub quality of
converted utterance seems not good we
see this is the side effect of
vectorization vectorization can well
separate the speaker and content
information however it decine the audio
quality this also means that the decoder
can now Rec contract the original audio
well one straightforward solution is to
increase the number of codes however dis
link the speaker information into the
contact Bing so there's trade off
between the information bottle neck and
the Recon eror so this is the main
motivation of this work you want to
resolve the problem by architecture
design I will introduce some related
work which is used in our architecture
first I will introduce the unit
architecture the major design of Ure is
ski condition module ski condition is
first using the image segmentation
problem it pass the output of encoder
through the corresponding decoder input
it wants to capture both the F and
Global Information of the image
so we hope that this architecture maybe
can help us improve the audio
quality second it wants to introduce the
progressive growing architecture the
basic idea of this architecture is it
generate the low resolution Image First
and the generation of high resolution
image is based on the low resolution one
for example if we want to generate a
real image we first generate a 4x4 pixel
image
and then generate by 8 by a image the
generation of 8 by a image is based on
the 4x4 and so
on this architecture is also very useful
in the audio senses it is showing mailet
and seeing voice
generation so next is our propose
approach here we just specify our
notation here VQ done conclusion is just
same with the encoder of VVC which
comprise of the conclusion base encoder
ination and vectorization and get the as
which is same as
thetion and in our this approach we
enlarge our model architecture and make
the whole model get
deeper and then add the speed commition
from each encode and corresponding
decoder third we add progressing going
architecture at the output of the deod
layer one of the difference between our
model and the original progressing
architecture is Progressive architecture
train the different resolution
image progressively however in our
architure we train them
together because we use the Progressive
architecture so we should expand the
time and frequency Channel using the
interpolation and conation so this is
the architecture of our V uption
model so we do the experiment using VK
Copus which compris of9 speakers and
there are Vol sentence for each speaker
and there are 44 hours in total we
randomly select 20 speakers of our
testing datet and other are our training
data
set the first experiment we want to show
is the disentangle performance the trend
a speaker discriminator count edding a
speaker edding using our test data set
which is comprised of 20
speakers for Content Bing because we
hope there is no speaker information
content Bing so the accuracy of speaker
discriminator L is better on the other
hand the accuracy of speaker Bing is
better here is our experiment result I
will first specify some notations here
Q32 me the C size in V layer is2 I only
me that there's no vectorization
there find that if we enlarge the C size
one Lo here is a recontruction arror
decrease this means that if we use more
code then the model can Rec Contra the m
speci
better however if we use the more code
the disentangle performance also
decrease there are only 20% for Q32 in
c0 is the first sk connection layer and
there are about
35% speaker accuracy for C
256 we also find that if the model is Go
deeper then the speaker verification
accuracy is also Bel
lower here the speaker verication
accuracy of C for I model is 70% this
means that this model cannot do the
voice conversion the only thing it can
do is the rec contraction next is the
subjected
evaluation we compared our model with
Joe and auto
VC here Blue Bar means the subject
prefer the result of our model and the
red bar is subject prefer the result the
other one here u means the voice
conversion result of the unse speaker s
is the voice conversion result of the
seeing speaker in the
CH we compared our model with the other
one on both natur and similarity and we
show that our model performs better on
this bothos one and both on S and S
voice conversion
test so this is our
demo this is a s speaker was not hard to
feel some sympathy for back there a
Target speaker whole thing was just
unreal and this is a result
conversion
believe and this is the S speaker
Washington is consumed by the
crisis we are really impressed
Washington is consumed by the
crisis the result here is using the mil
to confir the m back to the
W so this is our conclusion unit and
progress going architecture can improve
the audio quality second talization can
necessary to the in voice Carion tests
thank you very much
