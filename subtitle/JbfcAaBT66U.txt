好,那我們來講一下今天鼎鼎大名的Stable Diffusion吧
那其實今天比較好的影像生成的模型
就算它不是Stable Diffusion
它的套路其實也跟Stable Diffusion差不多
所以我們今天就是要來介紹這個
今天最好的State of the Art的影像生成模型
它背後的套路長什麼樣子
今天最好的影像生成模型
基本上它內部就是有三個元件
第一個元件是一個text的encoder
是一個好的文字的encoder
會把一段文字的敘述把它變成一個一個的向量
然後接下來你會有一個generation的model
那今天大家都用Diffusion model
但用別的也是可以的
這個generation的model就是吃一個雜訊
我這邊用這個粉紅色的向量
帶粉紅色的矩陣代表輸入的雜訊
輸入雜訊跟文字的encoder產生一個中間產物
這個中間產物等一下再細講
它可以是一個人看得懂的
只是比較小的比較模糊的圖片
它也可以甚至是人根本看不懂的東西
那這個中間產物是一個圖片被壓縮以後的結果
所以這是第二個模組
從文字encoder的輸出產生一個中間產物
代表圖片被壓縮以後的版本
接下來直接套一個decoder
這個decoder的作用是從壓縮後的版本還原回原始
我們剛才講說就是有三個模組
一個文字的encoder
一個生成的模型
今天用diffusion model
還有一個decoder直接從生成模型輸出的圖片的壓縮版本
還原回原來的圖片
通常三個模組是分開訓練
然後再把它組合起來的
今天你看到的比較好的文字生影像的模型
都是差不多的套路
比如說這是Stable Diffusion論文裡面的一張圖
你就看到它先有一個encoder可以處理輸入的東西
但它輸入不是只有文字啦
它還有別的可能的輸入
但是反正輸入文字你需要一個encoder來處理它
然後你需要有一個生成的模型
那這邊Stable Diffusion內部當然用的就是Diffusion的model
那你需要有一個decoder
把Diffusion model生出來的中間產物
一個圖片壓縮後的版本
還原回原來的圖片
所以就是我剛才講的三招三個component
那DARLIE系列其實從一開始就也是用同樣的套路
你需要有一個文字的encoder
先對文字進行理解
接下來你要有一個生成的模型
那其實在DARLIE裡面它是有兩個生成的模型
你可以用Auto Regressive的model
因為今天已經先做了一些處理以後
現在你要產生的並不是一張完整的圖片
如果你要生成完整的圖片的話
Auto Regressive的model運算量太大了
但是如果生成的只是一個圖片的壓縮的版本
也許用Auto Regressive的方法還可以
所以可以用Auto Regressive
也可以用Diffusion model
生成圖片壓縮的版本
用個decoder還原回原來的圖片
這是DARLIE系列的做法
Google也有一個影像生成的模型叫Imagen
也是吃文字
吃一段文字的敘述
產生一張圖片
這個套路也是一樣的
先有一個好的文字的encoder
然後你要有一個影像生成的模型
從文字去生出圖片壓縮後的版本
那在Imagen裡面它生出來的東西
就是人看得懂的東西
它生成出來的東西就是一張比較小的圖片
最終目標是要生1024x1022的大圖
但是Diffusion model只幫我們生
先生這個64x64的小圖
然後接下來再有一個decoder
不過它的decoder也是一個Diffusion model
它的decoder把小圖再生成大圖
這個是Imagen
現在我們就來介紹
整個framework裡面的三個component
介紹完我們就下課了
第一個要介紹的是文字的encoder
文字的encoder其實也不用再多做介紹
就是我們之前講的
在幾堂課之前講的那些東西
比如說你可以拿GPT當作你的encoder
更古早時代你也可以拿BERT當作你的encoder
那這邊秀這個實驗結果是要告訴你說
這個encoder這個文字的encoder
其實對結果的影響是非常大的
那這個結果是來自於Google的Imagen那篇paper
那在Google他們用的那個encoder
是一個叫做T5的encoder
總之就是一個文字的encoder就對了
他們試了不同的版本由小到大
那在這個圖上
他用兩個不同的measure來衡量圖片的好壞
一個是FID
那等下下一頁投影片我會跟大家講FID是什麼
總之FID的值越小代表你的圖片生出來越好
那還有另外一個東西叫做Crit Score
那這個Crit Score呢它是值越大越好
那在下一頁投影片
在下下一頁投影片也會告訴你
Crit Score是什麼東西
總之越往右下角越好
那你可以很明顯的看到說
隨著用的這個encoder越來越大
圖片生成的品質是越來越高的
所以我們知道說
文字的encoder對結果其實是非常重要的
那這個其實也是可以想像的
因為對於這個影像生成的模型而言
它用的
如果你今天沒有用額外的文字的encoder
它讀過的文字就是那50億張圖片所帶有的caption
那它讀的文字量還是很有限
可能有很多奇奇怪怪的東西
它是不知道是什麼的
那所以你有一個好的文字的encoder
可以幫助它去process去處理那些
它在影像跟文字成對的資料裡面
沒有看過的
比如說新的詞彙
所以這個文字的encoder
對結果的影響其實是很大
那相對而言
這個Diffusion Model的大小
似乎就沒有那麼重要
這邊Unit Size其實指的是那個Diffusion Model
裡面的那個Noise Predictor的大小
那在Imagine這篇paper裡面
就嘗試了用了不同大小的Diffusion Model
發現真大Diffusion Model
對結果的幫助是比較有限的
所以看起來文字的encoder
其實對於今天這些模型
可以有這麼好的表現
其實是非常重要的
那什麼是FID呢?
FID的意思是這樣
因為你怎麼量一張圖片的好壞
今天我們在做這種影像生成的時候
一個難題就是
你怎麼評估這個影像生成的模型
到底做得好還是做得不好
那在做每一段文字的時候
你根本沒有標準答案
或者說假設你準備了標準答案
你可以說你的模型產生出來的結果
跟標準答案不一樣
就說它是錯的嗎?
你也不能這麼說吧
所以今天你需要有一些特別的方法
來評估影像生成模型的好壞
那其中一個今天很常用的
叫做FID
FID是什麼呢?
FID就是你得先有一個
Pre-trained好的CNN Model
你得先有一個Pre-trained好的
影像分類的Model
你把手上的圖片
不管是機器生成的
還是真實的圖片
通通丟到這個CNN裡面
得到CNN的Latent Representation
這個其實就是大家在作業三的時候
拿出來的那個Representation
對不對?
作業三不是說拿一個Representation嗎?
那其實就是這樣的東西
你拿出一個Representation以後呢
你把真實的影像的Representation
生成的影像的Representation畫出來
在這個圖上
藍色的點代表的是生成的圖片
通過這個影像分類的Module以後
所產生的Representation
那紅色的圖片代表的是真實的影像
產生這個圖片分類系統
得到的Representation
那如果這兩個Representation
它這兩組Representation越接近
就代表說生出來的影像
跟真實的影像越接近
如果這兩組分得很遠
就代表說真實的影像跟生成的影像
它們非常的不像
那怎麼算兩組Representation之間的距離呢?
FID這邊用的是一個
其實很粗糙但是有用的方法
它的假設很簡單
假設這兩組Representation
都是Gaussian Distribution
那我知道這邊有一萬個可以吐槽的地方啦
但是反正就是假設它們是Gaussian Distribution
然後算這兩個Gaussian Distribution的Distance
那這邊算的是一個叫做Fresh Height的Distance
就結束了這樣
那這個方法看起來有點粗糙
但是它的結果看起來是好的
跟人類的評估是滿一致的
所以FID現在仍然是一個非常常用的做法
就算是今天這個ImageN這麼新的模型
還是用FID作為評估的基準之一
那因為它算的是兩組Distribution之間的距離
所以這個分數是越小越好
距離越小代表你的模型生成出來的結果越好
FID有一個問題就是
你需要Sample出很多的Image
你不能只Sample幾張Image就量FID
你要Sample大量的Image才能量FID
所以像前一夜頭影片
不是有一個10K嗎
10K是什麼意思
10K就是Sample了10K張Image來量FID的意思
那CLIP Score是什麼呢
我們現在講CLIP
CLIP是Contrastive Language Image Pre-training的縮寫
那CLIP是用400個Million的Image跟Text Pair的Data
所訓練出來的一個模型
這個模型做的事情就是
它裡面有一個Image的Encoder
有一個Text的Encoder
那Text的Encoder讀一張文字作為輸入
產生一個向量
Image的Encoder讀一張圖片作為輸入
產生一個向量
那如果這兩個圖片跟這段文字
它們是Pair的
這段文字是在描述這個圖片
那這個當然是有人標好哪段文字描述哪個圖片
如果它們是成對的
那這兩個向量就要越近越好
如果它們不是成對的
這兩個向量就要越遠越好
那CLIP Score簡單來說就是
把你機器產生出來的圖片丟進去
然後把你當時要讓機器產生圖片的那段文字的敘述也丟進去
然後看看CLIP算出來的向量的距離
算出來的向量它們的這個像不像
如果很像就代表Paceful高
就代表說你的模型生出來的那個圖
跟那個文字是有對應的關係的
這個就是CLIP Score
好那剛才講的是Text Encoder
接下來我們先講Decoder
那這個Decoder做的事情是什麼呢
這個Decoder它一般在訓練的時候
就不需要影像跟文字成對的資料
你要訓練中間的這個Generation的Model
比如說Diffusion的Model
讓它可以持文字的Embedding在產生中間產物
這需要Pair的Data
需要影像跟文字的對應關係
那影像跟文字對應的資料
雖然今天你說可以收集到很多張
可以收集到什麼五個Billion那麼多的
影像跟文字成對的資料
但是沒有跟文字成對的影像是更多更多的
而這個額外的Decoder它的好處就是
它的訓練是不需要文字資料的
你可以單憑著大量的影像的資料
就自動把這一個Decoder訓練出來
好那這個Decoder是怎麼訓練的呢
如果你今天你的Decoder的輸入是
你的這個中間產物就是一張比較小的圖
那Decoder的訓練非常簡單
你就把你手上可以找得到的影像都拿出來
然後把它們做Downsampling變成小圖
你就有成對的資料
你就可以訓練Decoder把小圖變成大圖
就結束了
好那如果我們的中間產物不是小圖
而是某種Latent Representation呢
那我們怎麼訓練一個Decoder
它可以把Latent Representation當作輸入
把這些Latent Representation還原成一張圖片呢
那你就要訓練一個Autoencoder
這個Autoencoder做的事情
就是有一個Encoder輸一張圖片
變成一個Latent Representation
然後把Latent Representation通過Decoder還原原來的圖片
然後你要讓輸入跟輸出越接近越好
那像這樣的套路
我們剛才在前一堂課已經講過了
好總之你就訓練這個Encoder跟Decoder以後
訓練完就把這個Decoder拿出來用
這個Decoder就可以吃一個Latent Representation
還原一張圖片
這個是中間產物
是Latent Representation的case
那像Imagent用的就是前面
把小圖當中間產物
那像Stable Diffusion還有Dali
就是用這張圖片的方法
把Latent Representation當作中間產物
好那通常這個中間產物長什麼樣子呢
如果講得更具體一點
通常的做法是這個樣子
假設你輸入的圖片是hxwx3
這個h是它的高
w是它的寬
3是每一個像素
其實是RGB三個數字所表示
那通常你的這個Latent Representation
你可以寫成小hxwxc
所以你要把它看作是一張小圖也可以
只是這張小圖是人類看不懂的小圖
這個h跟w
就是大h跟w做Dunsampling的結果
比如說這個小h可能是大h除10
小w可能是大w除10等等
然後這個c呢就代表的是Channel
就代表說在這小圖上每一個位置
是用多少個數字來表示
如果c設10就代表
每一個位置是用10個數字來表示
好那最後呢
就來進入Generation Model的部分
那Generation Model的作用
就是要吃文字的Representation
產生一個壓縮的結果
產生一個中間產物圖片壓縮的結果
那怎麼做呢
那我們已經講過Diffusion Model的概念了
我們現在唯一不一樣的地方是
剛才在做Diffusion Model的時候
你的Noise是加到圖片上
在做Diffusion Process的時候
你的Noise是直接加在圖片上
但是現在我們要Diffusion Model
產生出來的東西已經不是圖片了
那怎麼辦呢
我們的這個Noise要加在中間產物上
或加在Latent Representation上
所以假設我今天是拿Latent Representation
當作中間產物
這邊的做法就是
你先拿個Encoder出來
Encoder吃一張圖片
然後呢產生Latent Representation
接下來你的雜訊是加在Latent Representation上的
你會Sample一個雜訊
這個雜訊的Dimension
跟這個Latent Representation的Dimension是一樣的
你Sample一些雜訊出來
加到Latent Representation上
然後就稍微編一點
再Sample一些雜訊
又再編一點
以此類推
直到最後你加了夠多的雜訊
然後你的Latent Representation
變成純粹從雜訊裡面Sample出來的樣子
然後接下來呢
你就是Train一個Noise Predictor
這個跟一般的Diffusion Model
是完全一模一樣的
加入雜訊以後的Representation
當作輸入
那你也需要現在是第幾個Step
當作輸入
然後你也需要文字當作輸入
那現在文字呢
是用這個Latent Representation
也是一排向量來表示
所以你就把這個
加入Noise以後的Latent Representation
通過文字Decoder得出來的
這個文字的Encoder
Understanding這段文字以後的結果
跟現在是第幾個Step
丟到Noise Predictor裡面
希望它就可以把Noise Predictor出來
就這樣
好 然後今天在生圖的時候啊
你的做法就是
你有一個純粹從
Normal Distribution裡面
Sample出來的
Latent Representation
它的大小像是一個小圖的樣子
但是它是從Normal Distribution裡面
Sample出來的
然後呢
你把這個東西
加上這段文字
丟給一個Denoise的Module
然後Denoise的Module就去掉一些Noise
然後這個步驟就反覆繼續下去
再通過Denoise的Module
再去掉一些Noise
然後呢 反覆繼續下去
直到你覺得產生出
直到你通過一定次數的Denoise之後
產生出來的結果夠好了
丟給Decoder就可以生圖了
那所以你會發現
你在用那個Me Journey的時候啊
它背後是Diffusion Model啦
但是你看它生圖的時候
它有一個生圖的過程喔
從模糊慢慢到清楚
那如果是Diffusion Model的話
你應該看到的是一開始是
完全Random的雜訊
就白噪音就跟那個電視機壞掉一樣
然後那個白噪音在越來越少
越來越少
然後你的圖才生出來
但是其實你用那個Me Journey的時候
它生圖不是這個樣子
你會發現說它的圖是
從一張模糊的圖
只看得出輪廓
然後接下來越來越清楚
越來越清楚
然後中間有時候
本來中間產生某個物件
到時候畫到最後還會不見這樣子
那為什麼這樣
那就是因為
它其實是把這一個process裡面
每一次產生出來的
這些Latent Representation
通過Decoder以後
拿出來給你看
然後所以雖然這個東西
是一個從Gaussian Distribution
Sample出來的東西
但是你通過Decoder以後
它看起來就是一張
比較模糊的圖
那就不會看起來像是
有Gaussian Distribution加在上面
看起來就不像是
有噪音加在上面
這就是為什麼
Meet Journey它的中間產物
其實也是
人看得出來是什麼東西的圖
它就把這些中間產生出來的
Latent Representation
丟給Decoder
然後再產生出來的結果給你看
好 那這個就是今天
StaleDR的文字生影像的framework
就是三個步驟
文字的Encoder
一個Image的Generation的Model
生個中間產物出來
Decoder再做最後的處理
你會發現最好的影像生成模型
基本上都是這樣子的套路
