to move to the next talk so the next
speaker is Dr Shan W Lee uh shangi is a
research lead and manager at maa's
fundamental AI research team Fair team
and before he joined Fair he work at
Apple Siri Amazon ala and
AWS and uh we have many cooperation
before about SSL model several year
before uh Shan Lee organized the super
Benchmark and challenge together with my
teens and recently he has lots of work
about generative Ai and today he's going
to share with us a model called voice
craft I believe many of you have heard
this model before this is a very
powerful zero resource speech editing
and TTS model so we are very excited let
shann can share uh the development
behind voice with us so let's welcome Dr
shangan Le yeah thanks H for the
introduction and also thanks Neil for
the great uh introduction of the area of
odm um it's an honor to present our work
voice craft here now um this is uh we're
going to dive deep into uh a use cases
specific to like speech editing and TDS
that's a few like powerful um popular
applications in area
um so uh this is a work uh we uh in meta
collaborate with UT Austin um and uh uh
it is based on the concept of neural
codec language model or similar to the
OD Neo just introduced and you have seen
lots of great examples along these areas
um but basically uh you can be uh
imagine as um typical LM are leverage
like a new network to generate sequence
of discrete units like deriv from audio
signals and sometimes mixed with other
modalities like um could be text or even
images um and it leveraged the recent
advancing LMS to learn a powerful models
second can modeling the sequence of
different uh discrete tokens and then
solve the different speech generation
problems specifically uh we are
exploring speech editing and PS in this
work um and the codc is actually the the
the the crucial successful pieces in the
NCL neuro um codc langage models because
uh it's important to uh have a good
codec that preserve audio and the
semantic information uh let minimize uh
losses between the code and Signal
conver uh conversion
um and uh a lot of time the development
of codc is very similar to uh
development of a ding Odom or NM because
a lot of the Matrix uh and the design
guidelines can be shared and experience
can be shared um so today we want to uh
take this opportunity of codec superb uh
challenges and we want to share our
development of voice Craft um especially
along the speech edity and use cases and
to see how uh the the recent achievement
in nclm is um and also how the codak can
play a crucial um foundations in the
nclm and what are the challenges that
guide our future um
exploration um so uh to go back I
mentioned a couple of times about speech
editing problem but what is it um it
actually um you can imagine just a text
uh that we given a speech web form um
and um sometimes we um maybe this is uh
some streaming or you are recording some
videos and you have recorded your audio
and then you want to update part of your
the sentence you have
spoken um so for example uh here is uh
uh audio um we have um and with some
transcript here
uh and uh we want to make some uh tiny
uh modification to the
transcript uh like the the Highlight
text part and ultimately we want to
through the speech editing models and
then to generate uh Speech sentences let
um reflecting all those changes you
provided in the
transcript um and U this is the typical
setup for the speech editing problem
um but there's a few challenges uh in
this uh domain U specifically how you
want to um how we can preserve the
speaker um uh entity or Matrix uh and
also the background noise sometimes the
procity and emotion and different um
speech um
behaviors um and also the speech must
flow into and out of the generated
segment naturally without um some award
audable art
artifacts um so that's the part uh the
the speech editing problems and another
uh problem we want to focus on is a zero
TTS Z TTS problem um it's a problem like
um a setting a typical setting is that
given a short reference of recording
like a few seconds of the speech from
Target is speaker and you provide some
um desired uh text transcript and then
you want your model to generate a new
speech in the same
voice um so basically uh I'm not playing
the the audio right now but you will see
a lot of demos later later on so um but
you uh you can get the impression like
the we provide uh the goal is to provide
some reference recordings and some uh
Target text you want to synthesize and
then uh at the end your model generates
uh a corresponding the speech
corresponding to the text you provided
to the models and similarly the
challenges still uh we want to preserve
the speaker um uh
uh characteristics and the background
and the PADI and so on U it's very
similar to the challenges we have in the
editing um problem um and uh we were
going to argue like in our uh the
proposed framework later on we're just
going to show we can easily uh have a um
a unified design um of the model
architecture to solve both zero shot TS
and uh and the speech editing problem
together and it's actually a very
natural settings L for into the recent
LM architecture
so the next is uh I am mention like the
different Texs and now we're entering
the the the pieces of the Real Models we
are proposing which is a voice Craft um
and the main idea is behind voice craft
is uh we want to cast a speech editing
as a problem of uh and speech editing
plus TDS as a language modeling problem
uh basically it Maps the text and audio
to tokens and units and to learn the
semantic and audio signals flow behind
uh the behind scene oh and the model uh
we uh good uh feature of model is that
it going to Auto regressively infill
some spans and with within the input and
with the new content
naturally um and a few challenges uh in
the model design the first is uh how we
can efficiently represent the speech
signals and represent um it together
with other modality like a TA prompt um
in an efficient way so the model can
capture the semantics and audio together
together uh the second is we want our
model to generate in an auto regressive
way um during the implanting um but
ideally if we can learn um the by
directional um information um in in in
your uh input that would be more helpful
because uh you are inserting uh
generated pieces into uh the two uh
chunk of a uh signals if you can
depending on both side then uh the
implanting will sound more natural
compared to just depending on one site
um and also uh every um machine learning
problems you are facing how to get your
training data so we're going to touch
these challenges later so um the first
one is how to represent the speech um uh
uh signals um we just use the neural
audio codex B specifically I develop it
in analoging in pH
meta that's called in codc um we didn't
uh do much oblations uh like the compar
in incat and Soundstream and others um
but we believe um like Neo just
mentioned lots of things uh can might
work interchangeably um but of course
there might be some tiny details
different um in in the Behavior Uh but
we just take the incoda because it's uh
available within meta um and the second
is we want to learn um both to generate
in a auto regressive way but at same
time to have the by directional
information so um you can see like in
the latest like U Transformer um
research there are actually two branches
so one is like the GPT type of the
generation uh you do your generation in
Auto regressive way basically when you
generate new tokens you only depending
on the previous token you have seen uh
so it's it's more like a wi use in
generation and another type of the uh
Transformer type of modeling is U like
bird um you can imagine like uh your
Transformer can take the attentions in
different directions and it's more
useful in the representation part uh and
we argue like both features are
important in the speech editing or the
problem we want to solve so uh we
leverage another work from my team uh
which is called cm3 or Casto masking uh
I just linked it here um the the the
concept is try to come build a new um
like language modeling type of uh losses
uh that can incorporate both audio auto
regressive uh generation and also
preserve the by Direction signals um
information so uh the here is an example
how it works so you you see like here is
typical like uh codec or token sequence
um and the typical masking language
models you just mask a few tokens and if
you do Bird Training you just try to
recover uh with some um
losses um in this work uh we want to
train the model in um in an auto
regressive way so what we want to do is
we Shuffle those tokens a little bit uh
so basically we still have the the the
the original tokens plus the masking
tokens but at the end of the sequences
we repeat the masking tokens followed by
the original token in mask uh as well as
for the second mask and its original H
and when we train this model uh we still
using the uh the Auto regressive losses
to Trend um but as you can see when we
start trending or generating the uh
tokens that we augmented to original um
sequences it start picking up the
dependencies uh not only from the uh
previous tokens um for for example the
t2 or T3 it also can depending on the
future tokens um for those masking
tokens so in this uh way we can Preserve
both uh the by directional information
and still train our model in Auto
regressive way and we're going to argue
like this is uh crucial um in uh the
settings when we w't have a clean
modeling um to do speech editing um by
leveraging the audio um L type of the uh
model
architectures um so this is a basic idea
how the model works and now we want to
provide an example
how we apply this work on to the speech
um modeling or editing
Generation Um so first we we have
a speech aios and we Leverage The incoda
to transform them into tokens um and we
Leverage The Casto masking we just um
showed previously to do the re rearrange
of the tokens uh one thing I want to
mention is here you see like there are
um for each uh for each column it
corresponding to one times uh index and
for each one time index it actually
corresponding to um here we show four
different uh code it actually
corresponding to four different code
books and each codebooks is a residual
of the uh the previous layer of the
codebooks so with this uh Progressive
codebook uh we uh also show like this o
improves fidelity of the generation
quality um and also for uh because of
this changes we have a multi-layer of
the codebooks uh when we learn our model
we also do some delay stacking um also
uh very uh like uh common in a few other
um generation tasks uh so that the model
can generate uh can learn the sequence
and when generation uh depending on the
previous tokens as well as depending on
the previous layer of the code code book
you
generated um and once we have the the
formulation and when we build our models
it actually just um for the speech
signals we just uh reformulate them into
this uh uh stacking and code codec
tokens and for the text prompt we just
uh convert them to ftiz transcript and
when the generation we just a range lamb
and uh to generate in an auto regressive
way and to uh do auto regressive
training and then um the generated
tokens can be routed to in inod and then
to generate output uh speech
waveforms um so in this uh settings you
uh we just typically uh use the the
conventional um maximum like look
likelihood to assult
losses um and uh next uh is you might
ask that how we can formulate list to a
speech editing problems you imagine like
we um for speech editing we probably
need the original waveforms and the
transcriptions and the and also the
alignment of the uh edited waveforms as
well as the corresponding um transcripts
um but the in the next examples we are
going to show we actually don't need
much uh adaptations um to um apply our
formulation to speech editing uh the
reason is uh you imagine um in the
current uh settings we actually have
it's easy for us to get uh a pair of the
speech and the transcription and it's
easy for us to masking a part of the
speech and uh so in the in the training
uh The Voice craft can directly
um in this favor and once the voice
craft is pretend with this favor um at
the inference time it actually can apply
to uh Speech editing or zero TTS um
singly listly so the example is um so
here we give or example for the original
pair of the transcript as well as the
the the original
audios and uh we want to uh do the
editing so first we uh want first we
want to do some uh Force alignment to
figure out uh for
the uh for for each token um where it
corresponding to the in the speech audio
signals um and uh so we and and then we
compare the original transcripts with
the target transcripts um and we can
easily find uh the places we want to
replace and for the
uh we want to replace we actually just
put a masking
there and when we provide the uh the
input to the model to dra the inference
we actually provide the target
transcript and the uh speech audio
signals um with the masking uh places
there and to have the to have our model
to generate and in this way you can see
like the because for the models it
actually is agnostic to uh whether it's
the original uh transcript or the target
transcript the model just follow the the
prompt you provide to them and to uh
generate the implanting naturally um and
we also have a few tricks uh to make the
implanting more natural like uh we
shifting some uh masking um hyper
parameters but I won't cover that in the
more details um and the the for the text
text to speech inference is also very
similar um like for an example we have
the yellow highlight part as the
original reference voice and the uh blue
part as the target when we provide to
our models uh we actually just provide
um both uh the original and the target
transcript along with the audio and the
uh when the voice C models U pick up it
will just uh view uh the the missing
part as a Max maxing part and it can
auto complete the max Max part for us
and so to complete uh the generation for
the full uh Target uh
transcript um here is a few like tricks
uh we make our model better like when we
do experiment without uh like remember
we have this uh four layers of codebooks
we found like uh
the the earlier codebooks uh usually it
requires um more bigger weight that's
pretty following our intuition and also
like there there's a few uh tricks like
how we tune the masking uh part to uh so
that the implanting can sound more
natural or um sometimes because this is
the model like do auto uh regression
sometimes it can um fall into some
infinity loop to continuously generating
repetitions and we have some uh mechanis
to remove them um following some like
the Moshi and a few um existing popular
Works um the next is uh the the
experiments we actually use the Giga
speech uh data sets to train our model
which uh contains about 10K hours of
speech from three multiple domains uh we
also evaluate our models against these
uh three different domains as well and
when we do evaluation uh we have uh
multiple ways U for sure um the the the
most most um popular and reliable is to
have the um human to uh compare uh the
different generated signals um and uh
when we do evaluation we also build uh a
a data set we call real edit it actually
contains
300ish topos uh for for uh the original
waveforms and the transcript and as well
as Target edited transcript and uh the
edit additions uh is we try to balance
different addition types like insertion
substitution and
addition um and there are several
examples of our data sets and this data
set is also open source so feel free to
try out um and here is a few like uh uh
empirical analysis We compare uh our
works with the the original uh
non-edited Speech uh somehow the color
coding doesn't work um but you pretty
much get a sense um we asked the the
human reviewers to compare original
Edition and voice Crab side by side uh
and they will give us a rating like
which one they prefers
um the generation results and between
voice craft and original you actually
see the the preference is uh similar but
still like prefer original a little bit
more more um when we compare the model
with the another popular um not original
this is hypo this is fluent speech
another popular uh Speech editing models
you actually see like voice craft
outperform fluent speech a
lot um we're also uh doing some main op
opinion scores on the Five Point lier
skilles um uh compared with the original
uh non-edited Speech and the full and
speech and voice Craft um you can see
like the ratings uh voice craft uh is
performs a little better than the flu
and speech but still have some Gap but
the small gaps to the
original um we also do some evaluation
similarly for the zero TTS um and
compare with a few existing uh baselines
and S models uh we have the Matrix both
for the uh the the human rating
opinion based on opinions we also have
some automatic way to uh do the
evaluation like we compare um around the
ASR results and to use word rate as appr
proxy to the generation quality and also
compute the cosign similarity of a
speaker embeding we actually found um an
interesting thing is the the trend is
not like consistent for example you will
see like fluent speech actually performs
well in the world ER at um but for from
the human perspective um voice craft is
uh usually more preferred compared to
fluent speech so um one thing is we
still feel like the voice craft is
pretty powerful based on the human uh uh
opinion um but we also feel a little bit
off because uh this kind of indicating
the automatic Matrix is something wrong
uh we look into the results and we
sometimes find like the um the the the
the the war are is uh is usually more
hackable um because you can generate
some uh more mechanic uh signals or or
audio or more s simple audios uh that's
allows a model to draun array better so
uh we also argue like in future a better
and more consistent um uh uh Matrix
especially um automatic Matrix uh will
be preferred to guide The Future model
development U lastly I want to show some
demos to uh let give you some flavor how
uh we the model performs so um let me
play some demo
I think
it's
[Music]
natural
e e
so um yeah this is a few examples uh in
the speech editing and the TTS domain so
you kind of get the flavor um the models
not only preserve um and follow the
prompt to do the addition it also kind
of preserve the speaker M
characteristics and sometimes also pick
up the background noises so it makes the
continuation sound pretty natural um so
uh I'm going to skip this demo um and
jump to uh the conclusion so um in in
this work you see we have um using the
voice craft as an example to show the
recent progress in uh neuro um codec
language model or
a um and right now the synthesized
speech actually can provide some IND
distinguishable um natural sound and for
out recordings
um and our model and the code and the
data are open sourced so um feel free to
try and let us know how do you feel um I
think your feedback is very important
for us to guide the future work um one
more thing I want to highlight is as you
can see um there is still a lots of
missing pieces about like the good
Matrix to guide the design of the model
and the reliable benchmarks to compare
uh across different uh
uh model and works so that's why we
believe um effort like code DE superb is
very important to continuously providing
a good environment to drive the uh the
research advancement so um here conclude
my talk um and I'll hand that back to
Hom okay so any question from the
audience okay we have a question there
uh thank you for the wonderful talk and
uh it seems like these papers provide a
unifying uh parag parading for the TDS
and also the uh spe EDI uh actually
myself I did uh the speech editing
problems many years ago I was the author
of The edit speech and I think the uh
the uh uh a big difficulty in the speech
editing is how to utilize the
information in the future and in the
past and here you use the decoder only
uh uh generation methods and however the
M token they are usually maybe lies in
the middle of the sentence so you need
to put it to the end of the sentence
right so uh yeah but this seem quite um
seems not so natural for me I means uh
why don't we just use like the uh MLM
lws just like you you can just Direct
use like the bird generator like like
also like the one in the M GCT you just
mask something and then you utilize the
surrounding information and then you
directly generate the token in the
original location so uh I'm am just kind
of curious so what what's the motivation
to utilize the auto regressive
generation but we but you also need to
put the M token to the end of the
sentence will be some um troublesome
yeah yeah um I think that's a very good
point and I think you're you're um uh
totally correct I believe there's a lots
of ways to do the modeling and the
different techniques have the tradeoff
for example uh we see it's possible to
use bird or even uh some kind of audio
and maybe multimodal representation to
First encode them into embeding space
and then you and then after that you
have tons of ways to do the the the the
generation even you can incorporate like
a diffusion that's pretty popular lately
um we specifically pick up this work
because we want to uh limit to uh the
usage of the the the token based and
also using the discrete tokens to do the
Generation Um so that's why we uh we
want have the model to have a cleaner um
uh architecture in terms of the losses
um and so we can just uh Trend a
a whole models in one loss um but the
tradeoff is like you say the the
reshuffling or rearrangement is kind of
a a natural so we believe this is just a
different ways to view a problem and uh
um I I feel like from the research or
academic perspective um this is just uh
there's a lot of unknowns and we want to
push our um uh uh uh uh uh Frontier to
to see how how much we can um how much
we can do in One Direction so that's why
we pick up this uh AR this architecture
design okay thank
you so any questions from the
audience okay if not due to the time
limitation uh we can stop the talk here
and let's thanks the speaker again thank
you thank you for the excellent talk
