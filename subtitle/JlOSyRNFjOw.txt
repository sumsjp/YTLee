hi everyone my name is Andy I'm with the
speech lab from National Taiwan
University I'm here to present our work
as the first author the title of our
paper is Mockingjay unsupervised speech
representation learning with deep
bi-directional transformer encoders so
let us begin birth is a very well
performing language representation model
which is unsupervised trained from large
amount of unlabeled text when input with
text tokens the model outputs
representations when used with
downstream NOP models such as QA model
summarization model through feature
extraction or find him birth achieved
many state-of-the-art results inspired
by birth in NLP we aim to build a speech
version of birth which will be
unsupervised trained on north amount of
unlabeled speech instead of taking
discrete tokens as input the speech bird
will take acoustic frames and sequence
of representation will be generated for
each utterance once trained speech
rotations can be extracted or fine tune
with downstream speech models in this
talk I will introduce how we use speech
bird to learn speech with cetaceans
first of all I would like to go through
some of the recent unsupervised speech
representation learning approaches in
2018 deepmind introduced a contrastive
predictive coding approach CPC a five
layer CN n with a sequence model of GRU
is used to encode past contacts while
the model predicts the future to solve
the contrastive binary classification
tasks in 2019 MIT introduced the
autoregressive predictive coding APC
largely inspired by language models lm4
text the APC model can be seen as a
speech version of text om both the CPC
paper and APC paper evaluated with
downstream phone and speaker
classification tasks the APC approach
devotes its effort in comparing with CPC
and claimed to outperform si
see note that the CPC and ABC approach
shares a common heuristic that is they
both encode past information and
predicts information about future frames
in contrast the proposed Mockingjay
predicts the current frame by jointly
conditioning on both past and future
contacts during training mask frames are
given and the model learns to
reconstruct and predict the original
frames
hence we gave the name Mockingjay
following the bird that mimics sound at
inference time sequence of frames are
given and the model generates the
desired representations in this work
following CPC and ABC we evaluated with
phone classification and speaker
verification as well in additional we
also set up a third downstream task
sentiment classification on spoken
content now let's go into details about
the proposed pre training tasks which we
use to pre train our transformer
encoders we will illustrate the pipeline
from input to output first we have some
real frames at the input these frames
are phoning level spectrograms
next we have a masking probabilistic
policy that will mask frames to zero at
random these mask frames will then be
the input to our model during training
so after we mask those frames we feed
them to our model we use multiple layers
of transformer encoders with multi
health self attention to achieve
bi-directional encoding the output of
the last encoder layer is does the
representation we want note that we only
asked the model to predict the mask
frames and not all of the frames we
outline them in red this representation
are fed through a prediction head and
the model then tries to predict the
missing frames with a feed-forward
Network for example the frame C is
mapped to 0 so the model predicts and
reconstruct frame C at the output hence
the model tries to reconstruct real
frames from corrupted input where we use
a one loss to minimize reconstruction
error the idea is that if the model can
predict the missing frames from
neighboring frames then it should
provide a good bi-directional
understanding of the
and future content as the representation
considers the whole utterance here I
will introduce how we mask frames at
random during training we use a
probabilistic policy to generate mask
frames dynamically we illustrate the
policy and the process in this page in
the first step we randomly select 15% of
the input frames for masking in a second
step for all the 15% selected frames we
roll the dice again for 80% of the time
we mask all the frames to zero so for
example frames b and d will be set to 0
for 10% of the time we replace the
selected frames with random frames for
example frames b and d will be replaced
with a random frame gmy within the same
utterance for the other 10% of the time
we leave them untouched so the input
frames is identical with the original
ground truth frames note that only one
case will happen at a time either a mask
or 15% replace all 15% always do nothing
to sum up the proposed pre training task
we will show some illustrations here we
show how the import features
looks like we use a t dimensional mail
spectrogram with its first derivative
the mask frames are highlighted in
yellow so the model takes mask frames as
input and generates mockingjay
representations then the model predicts
and reconstructs the original frames a1
loss is used to compute a reconstruction
error
note that the predicted frames and real
frames looks almost identical they are
real samples while the hidden
representation looks nothing like them
now I would like to address some of the
main differences between pert a NOP
model that operates on discrete text and
our model Mockingjay the operates on
acoustic features as we all know
acoustic features can be very long in
sequence length and locally smooth hence
we need to try to shorten the input
sequence and mask over a longer span the
solution to the above two problems are
down sampling and consecutive masking
which will illustrate
at the bottom left we first show how we
apply down sampling on input features to
adapt our model to long sequences to
reduce the lengths of frames by a factor
of R we use the reshape technique by
stacking our consecutive frames into one
step we illustrate the case where R is
equal to three at the bottom right will
show how we use consecutive masking
where we mask consecutive frames
c20 by masking a longer span the model
is required to infer on global structure
rather than local information we
illustrate the case where C is equal to
3 and how consecutive masking works was
down sampling I would like to go over
some details about the model at the left
side we illustrate a conventional
transformer encoder block which consists
of multi header attention feed-forward
Network and residual connections here we
also list some of the parameters the
hidden dimension is 768 with fee for
word I mention of 3072 we used 12
attention heads and pressuring for
500,000 steps on the libras speech 360
our subset when we find you in the
Mockingjay model with down 3 models we
fine-tune for 50,000 steps which is
approximately 2 a box of training data
we train two models the base model with
three layers and the large model with 12
layers there are a total of three ways
to incorporate the pre trained
Mockingjay model with downstream tasks
in this page we illustrate the first way
which is to extract features from the
Train model and use them as speech
representations at the right hand side
we prochaine the Mockingjay model like
mentioned in previous slides next we
freeze the pre train model and use it
for feature extraction here we Ellis
trait an example downstream model for
example it can be a phoneme classifier
or speaker classifier which we will
train with little pair data there is a
green box here which illustrate that the
supervised loss is applied on a
classifier only and no gradient will
flow through the frozen Mockingjay model
the second case we extract features from
all layers in other words we expose the
deep internals of mockingjay to
downstream models same as the previous
case we have a frozen Mockingjay model
here but instead of using the last
layers hidden state we use the hidden
States overall layers here we Ella
straight how to use a mixture of
representations from all layers all the
hidden representations from all layers
are integrated into a new recent ation
through weighted sum note that all the
weights are learned from data and we
denote this approach as WS in later
experiments the third case where we
fine-tune the Mockingjay model with some
zoom classifiers same as the previous
two cases we have a pre trained
Mockingjay model at the right hand side
in the fine-tuning case the downstream
model consists of the Mockingjay
following of classifier where we
initialize the Mockingjay model with the
pre trained waste from the right the
loss is applied on both the classifier
and the Mockingjay model the Mockingjay
model is no longer frozen and updates
with the classifier when we find him our
models with little data we find him for
two airports only and we denote this
kind of setting as ft 2 in later
experiments represent results of three
downstream tasks including phoneme
classification speaker recognition and
sentiment classification on spoken
content first we will talk about the
phoneme classification task the phone
classifier performs frame wise
classification with a feed-forward
network and is trained on the Libre
speech 360 our subset tested on the test
crane subset there are certainly three
possible phone classes which we are
obtained through force alignment the
other two tasks use a RN classifier
which takes an utterance as input and
output a single classification result we
illustrate this at the bottom of the
page the speaker recognition classifier
is trained under liberal speech 100 our
subset where trained and testbed is
performed randomly
with a nine to one ratio there are a
total of 63 possible speakers the
sentiment classification classifier is
trained under mo sei dataset however the
mockingjay features are pre-trained from
liebe speech we use this cert asked to
evaluate transfer ability across
different data sets in this table we'll
list results of the three downstream
tasks we see that results are consistent
over all the three tasks where the base
model outperforms the baseline mail
features while the large model
outperforms the base model here we list
the results where we extract features
from all layers and integrate them with
a learned weighted sum denoted as large
WS first we see that integrating hidden
recitations gives a performance boost
overall task next we see that
fine-tuning also gives a performance in
particular in the film classification
case in the last row we also report
results of the reproduce APC approach
the base ftq model achieved the highest
score on the phone classification tasks
and speaker recognition tasks all
performing all features whereas the
large ws model achieved the highest
score on the sentiment classification
task our performing baseline and APC
features to demonstrate how pre-training
on speech can improve supervised
training in low resource scenarios we
train the classifier with reduced amount
of label data the performance variation
of different models are plotted in the
figure where we measure over various
intervals of constraint training data to
observe performance drop we first show
mel feature performance in the blue
curve next we see how the base model
increases accuracy over male features
across various amount of transcribed
data here we have the large model of
performing the base model in some cases
order large model also outperforms male
features across various amount of
transcribed data by using the weighted
sum approach the large ws model boosts
performance over the large model which
results in an hour
5.75 percent performance boost we
discovered another interesting fact
large WS with 52.8%
accuracy even outperform male features
with 49.1% accuracy that uses all 100%
of label data we highlight these two
data points in red and blue boxes
respectively by fine-tuning the base
model the base ft2 model outperforms all
others over all amounts of label data we
plotted an additional data point where
we fine-tuned for 500,000 steps called
base ft 500 which is highlighted in a
purple box we use this to show that
fine-tuning for to air box is enough to
reveal our message potential as there is
only a small gap of 3.9% between base
ft2 and base ft 500 note that with only
0.1% of labels available base ft to
outperform male features that uses all
100% of labeled data we highlight these
two data points in purple and blue boxes
and finally the APC approach performs
well on the four resource but fails to
generalize for limited amount of label
data we conclude that Mockingjay
pre-training substantially improved
performance on supervised task that
requires human annotations
you
