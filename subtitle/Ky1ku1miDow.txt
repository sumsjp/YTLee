臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
這個是我們上一次做失敗的 code 這樣子
我們疊了一些，我們要做手寫數字辨識
那我們疊的 network 呢，它的這個
hidden layer 的 size，就是 689
用 sigmoid function
我本來要疊 10 層，疊 10 層其實也不 work
就把它註解掉了
那用的 loss function 是 MSE，然後用了 SGD
等等，那結果是差的
怎麼辦？
你自己在 train network 的時候才會遇到這個問題
很多個 network train 下去，結果是差的
這個時候你就會去問老師說，怎麼辦？
每次有人問我這個問題的時候，我第一個會問你的就是
你在 training set 上
得到多少的 performance
你可能會問說
你怎麼不是問 testing set 上得到多少的 performance
而是問 training set 上，得到多少的 performance
因為如果你有看
錄影的話，你會知道說
今天 deep learning 在 training 的時候
你非常容易 train 壞掉
它跟其他方法不一樣，它跟 SVM 不一樣
SVM 是解一個 convex optimization problem
所以，你每次找的時候
它都可以找到一個 optimal 的 solution
也就是當你用 SVM 的時候
SVM 會竭盡全力
給你它可以得到最好的結果
那 deep learning 不是這樣的方法
它雖然很 powerful
但它其實就跟噴火龍一樣，你不見得能夠叫得動它
所以，你需要看一下你的 training set
來看看說你到底有沒有把它
的能力做起來
那你可能會想說，它在 training set 上 performance 好
可能只是 overfitting 阿
沒錯，它可能只是 overfitting
但是，如果你連在 training set 上 overfitting 都做不到
你更遑論去做在 testing set 上舉一反三了
所以，我們先讓它至少
在 training set 上得到好的結果
那 training set 上得到好的結果有可能
可以舉一反三到 testing set 上
也有可能 overfitting
我們不知道，但是
如果你在 training set 上都沒有好的結果
那你其實在 testing set 上
可以得到好的結果的機會是微乎其微的
所以，如果你在 testing set 上結果不好
你應該先看看你的 training set
結果是怎麼樣，然後才看說
現在是不是 overfitting
所以，我們來看一下在 training set 上的結果吧
其實，Keras 在訓練的過程中
就已經會告訴你它在 training set 上得到的 performance
不過，我們今天特別再把 training set 的結果
再 print 出來
那我們來 print 一下 training set 的結果
只要把 x_test 改成 x_train
y 改成 y_train 就好
我們實際上來跑一下
那其實在 training 的過程中
Keras 就會告訴你說
它現在算出來的 accuracy 是多少
在每一個 η 後面
它都會告訴你說
這個 η 結束的時候，它算出來的 accuracy 是多少
那如果我們今天看這個實驗結果，你就會發現說
其實，啊！我這邊忘記把 test 改成 train
不過大家知道我的意思就好
上面這個 accuracy 是 training set 上的 accuracy
下面這個 accuracy 是 testing set 上的 accuracy
如果你只有看 testing set 上的 accuracy
你並不知道你現在是不是 overfitting
有人看到 testing set 上的 accuracy 就會說
看 testing set 上的 accuracy，它 performance 很差
就會胡亂得到一個結論說
所以，deep learning 很容易 overfitting
所以，deep learning 很不 work
那其實不是這樣
今天在這個 task 裡面
如果我們看 training set 的 accuracy 的話
你會發現 training set 的 accuracy 也是差的
這告訴我們什麼，這告訴我們 network 在 train 的時候
它就沒 train 好
它可能卡在一個 local minimum
它可能卡在一個很差的 saddle point
總之，它在 training set 上的 performance 就沒做好
所以，這個時候
你遇到的問題並不是overfitting
而是 training 沒有 train 好
你要想個辦法，先在 training set 上
得到比較好的 performance
那把這個 test 改成 train
那這邊到底少了甚麼東西呢？
其實這邊少的是 loss function
設得不對，其實我們已經有跟大家解釋過說
其實，用 mean square error 看分類的問題
你其實不會得到好的結果
我們在講 Logistic Regression 的時候
已經講過這件事了
我們現在實際來示範一下
我們就只是單純地把 mean square error 換成
這個 cross entropy
在 Keras 裡面，categorical 的 cross entropy
就是我們上課講的 cross entropy 啦
那從 mean square error 換成 cross entropy
這個你可不會覺得有甚麼特別厲害的地方
paper 也不會跟你 emphasize 這件事
但是，我們看看它有甚麼樣的差別
那我們剛才是做不起來的
我們來看一下，你看
當我們換成 cross entropy 以後
在 training set 上的 accuracy
就起飛了
現在 training set 就得到
87% 的正確率阿
這其實是個巧合，我沒有辦法特別設成這個結果
testing set 上得到 85% 的正確率，所以現在就
比較有 train 起來了
現在試一下，batch_size 會對結果造成的影響
現在，你看我們 batch_size 設 100
那我們現在呢，把 batch_size 改成 10000
把 batch_size 改成 10000，再跑跑看
剛才我們可以得到
training set 上 87%
testing set 上 85% 的正確率
你看 batch_size 設 10000
跑超快，因為你是用 GPU 平行運算
所以，在 GPU 可以平行運算的能力
它可以承受的前提之下
batch_size 越大，它其實跑得越快
但是，batch_size 一開大
performance
開太大的時候，performance 就壞掉了
那至於為什麼，我們上課有解釋過了
就會發現說，一樣的 network 架構，batch_size 一開大
結果就壞掉了
那我們試著把 batch_size 弄小一點
剛才是從 100 改到 10000
現在改回 1，現在改成 1
那改成 1 你會發現怎麼樣呢？
今天如果 batch_size 只有 1 的時候
GPU 就沒有辦法發揮它平行運算的效能
就會發現說，變得很慢這樣子
變得很慢，所以，有人他不知道說
你要能夠用 GPU 加速
前提是你在 training 的時候，batch 開大一點
GPU 才能夠真的加速
如果，你今天 batch_size 設 1
你就做 Stochastic Gradient Descent
GPU 可以對你帶來的幫助
其實就不會很大，所以你看現在
跑得非常非常慢，每一個 η 要 20 秒
我相信大家應該不會有興趣看我把它跑完
所以，我們就把它停下來
有些人可能想說
那就應該要用 deep 了吧
再加 10 層
現在改成用 10 層
我們看一下 testing 的 accuracy
先來看一下 training 的 accuracy
看一下 training 的 accuracy，就會發現說
沒做起來
卡住了
那我們在錄影裡面有解釋過為甚麼會這樣
疊 10 層的時候
會有 gradient vanishing 的問題，所以卡住了
所以，你看 testing set 的 performance
大概是 11% 的正確率
那如果你沒有 training deep learning 的概念
你可能會說，所以 10 層 overfitting
所以 performance 這麼差
但是，如果你仔細看一下 training set
它的 performance 其實也是差的
所以，這個不是 overfitting，這個是沒 train 起來
那怎麼辦？
那現在要怎麼辦呢？
我們來改一下 activation function
我們把 sigmoid 都改成 ReLU
sigmoid 現在通通改成 ReLU
再 train 一次
你會發現說，現在 training 的 accuracy 呢
它就爬起來了
現在已經跑到 98、99 這樣
你會發現說，現在 training 的 accuracy
已經將近 100% 的 testing
可以得到 95.6% 的正確率
這邊有個有趣的地方可以跟大家分享一下
現在我們的 image 阿
它是有 normalize 的
所謂有 normalize 的意思是說
每一個 pixel，我們用一個 0~1 之間的值來表示它
1 代表最黑，0 代表沒有塗黑
其實，你剛拿到一個 image 的時候
通常我們是用灰階來表示它的
也就是每一個 pixel 的值，是用 0~255 來表示它
所以我這邊特別除上 255，做 normalize 這件事
如果今天我們把 255 拿掉
會發甚麼事呢？
你會發現說，你又做不起來了
所以，這種小小的地方
只是有沒有做 normalization 的地方
對你的結果會有關鍵的影響
而這些事情，是很多人都忽略的
因為我們知道說，現在 AI 非常地潮
現在多數人的心力都集中在 AI 會不會統治世界這件事情
或講一些奇奇怪怪不符合實際的話
這個東西，像這個小小的 normalization
一點都不潮，不會統治世界的東西
但對結果其實有非常大的影響
我們把它改回去
改回去，那接下來
我想示範的一個東西是
我們把
這個時程註解起來
然後，我們再跑一次
那你會發現說，今天
在 training 的時候阿
跑得很快，我們就讓它跑完
那今天在 training 的時候
大概在第一個 epoch 的時候得到 77% 的正確率
在第二個 epoch 的時候得到 90% 的正確率
那我們現在換一下
training 的，Gradient Descent 的 strategy
把它從 SGD
改成 Adam
我上課有講過 Aden，把它改成 Adam
然後，再跑一次
你會發現說，當我們用 Adam 的時候
它可能最後收斂的地方差不多
但是，你會發現它上升的速度是變快的
我們剛才在
第一個 epoch
在還沒有用 Adam 的時候，第一個 epoch
它的正確率是 7 開頭
如果現在，有加上了
用 Adam 的話呢
在第一個 epoch，它的正確率就有 85%
第二個 epoch 就有 95%
那今天在這個 test，因為一個 epoch 跑得非常非常快
所以，你可能沒有甚麼特別的感覺
但是，如果今天一個 epoch 要跑一天
你就會覺得說有 Adam 真是好這樣子
我在 testing set 上呢
故意加上了 noise
training set 沒有 noise
testing set 的每一個 image
每一個 pixel 都故意給它加上 random 的 noise
然後我們實際來
操作一下，看看結果會掉多少
我們本來在 testing set 上已經可以得到 96% 的正確率
但現在 training 和 testing 呢
是不 match 的
所以，一做下去，結果就爛掉了
結果就爛掉了，現在 testing 只有不到 50% 的正確率
那怎麼辦呢？
我們來試一下 dropout 可以帶給我們甚麼樣的結果
我們來加一下 dropout，怎麼加 dropout 呢？
你就打 model.add
Dropout
然後，你要設一個 dropout rate
這個 dropout rate 其實就是你自己設的啦
就像是 network hidden layer 的 size 一樣
你要設多少是你自己決定的
常見的是設 0.5，不過
因為今天在這個 task 裡面
training 跟 testing 非常的 mismatch
所以，我覺得 dropout rate 可以設大一點
比如說，我設 0.7 試試看
每一個 dropout 就是加在每一個 hidden layer 後面
那這邊有一件事情，大家要注意的就是說
今天當你加了 dropout 以後
其實，training 上的 performance 是會變差的
這個很合理嘛
因為加 dropout 就是去綁住 network 的手腳
在 training 的時候，它的 performance會變差
所以，如果你今天
你的 performance 不好是來自於
你在 training set 上的 performance 不好
你不要再加 dropout，你只會越弄越差而已
你今天在 training 上已經跑得太好
它 overfitting，你才加上 dropout
我們看一下，其實剛才阿
我們的正確率都可以做到
100% 的正確率這樣
看到沒有，100% 的正確率
這個才是真正的 overfitting
所以，我們現在加了 dropout 以後
應該就跑不到 100% 的正確率了
你看剛才在 training 的時候
在最後幾個 epoch 的正確率都已經是 100%
現在加上 dropout
你就會發現說，network 就 train 不到那個 performance
在 training 的時候，就等於是綁住 network 的手腳
你就會發現說，它現在有點被卡住了
它在 training 的時候
它的正確率現在在 94, 95 中間徘徊
那今天在這個
testing training data 的時候呢
其實就不會加上 dropout 啦
所以，你會發現說
在 testing 的時候
用 testing data 的時候
它 performance 是遠比 training 的時候
所呈現的 performance 要好得多
那有加 dropout 的時候
network 的 performance 呢
network train 的時候會綁住手腳
所以它的 performance 會差一點
那你會發現說，在 testing 的時候
剛才，正確率連 50% 都不到
但加了 dropout 以後
現在就有 60% 的正確率了
那這邊就是
實際示範一下，這個
如果我們把上課教的種種 tip
真的拿來實做在 MNIST 的時候
會有甚麼樣的不同
那其實還有很多東西沒有講的
那你可以自己回去試試看
或在作業三的時候
試試看不同的 tip 對 network 會有怎麼樣的影響
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
