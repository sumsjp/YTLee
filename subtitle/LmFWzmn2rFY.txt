好 這件事怎麼做呢
我們就需要用到一個
Likelihood的概念
這個概念是這樣
我們收集到了N筆資料
我們就把它畫在這邊
接下來
我們假設我們有一個
probability density function
這probability density function
我們把它寫作fθ(x)
那θ它是這個
probability density function的參數
θ的數值決定了這一個
probability density function長什麼樣
講到目前為止
你可能還會覺得有點抽象
如果你機率不熟的話
那麼等一下有更具體的例子
我現在就假設說
有一個probability density function
它的參數是θ
而這個θ決定它的形狀
但是我們並不知道這個θ
長什麼樣
這個θ必須要從訓練資料裡面
根據訓練資料去找出θ
那我們現在的假設是說
這個probability density function
它生成了我們所看到的這些數據
我們這些我們所看到這些數據
它是由背後的一個
probability density function
所生成出來的
而我們現在要做的事情
是找出背後這個
probability density function
它究竟長什麼樣
那我們要計算一個東西
叫做likelihood
所謂likelihood的意思是說
根據我現在手上的
probability density function
這樣子的資料
被產生出來的機率有多大
那這邊嚴格說起來
你知道這個fθ(x) 它並不是機率
它的output是probability的density
那其實它output值
也不是介於0到1之間
它還是有可能大於1的
那如果你機率學得很熟
你應該會知道我在說什麼
那如果你機率都忘得差不多了
那你就把它當作是機率就好
你就把它當作是機率就好
好 那我們現在就是要計算
根據我們的probability density function
去計算這一組data
這樣子的一個data set
三個training data
它被產生來的來likelihood
或是你可以直想就是機率
產生出來的機率有多大
怎麼算這個data被產生出來的機率呢
你就算說x¹
根據這個probability density function
被產生出來的機率
乘上x²
被這一個probability density function
產生出來的機率
一直乘到xⁿ 它被產生出來的機率
這邊全部乘起來
就是你的likelihood
那這個likelihood 這可能性
顯然是由θ所控制的
如果你選擇了不同的θ
你就有不同的
probability density function
你就會算出不同的likelihood
而我們現在任務是
我們並不知道這一個θ
它的參數是什麼
那我們要把這個θ找出來
我們要找的是什麼樣的θ呢
我們要找一個θ
它算出來的likelihood是最大
我們要找一個θ
它用這個θ
產生這個probability density function
將這probability density function
去算出likelihood的時候
它的值是最大
那那樣子的θ
我們叫做θstart
這樣子的θ
可以讓likelihood最大的θ
是我們要找出來的
這樣講可能有一點抽象
所以我們就講更具體一點
那一個probability density function
長什麼樣子呢
一個常用的probability density function
就是Gaussian的Distribution
那我假設說 在機率裡面那門課
我們都學過了
multivariate的Gaussian Distribution
它的式子並沒有很簡單
它就長成這個樣子
那如果這個式子 你覺得有點複雜的話
你就想說
反正它就是一個function
這個function就是輸入一個東西
輸出一個東西
它輸入一個向量x
它輸入這個空間裡面的一個向量x
它的輸出就是這一個x
它被sample到的機率
它輸出就這個x被sample到的機率
那這個function呢
它裡面是由μ跟∑
這兩個參數所掌控的
所以這個μ跟∑
它就是我們剛才看到的θ
有這個Gaussian Distribution
它形狀長什麼樣子
由這個Gaussian Distribution的μ
跟它的covariance matrix ∑ 來決定
這兩個東西是我們要得出來的材質
那實際上它的計算也許有點複雜
它的式子 我們就把它寫在這邊
它實際上做的計算就是
1除以2π的D/2次方‎
D是x的dimension
然後再乘上1除以∑絕對值的½次方
這個絕對值是什麼呢
這個絕對值是determinant
如果你是大學生的話
你剛修過線代
你可能記得
如果是研究生的話
我猜你八成寫不出
determinant的式子了
反正它就是一個
它就是個scalar這樣
不知道哪來的
反正它就是scalar
好 然後後面要取這個
exp｛-½(x－μ)T∑－１(x－μ)｝這樣 好
x－μ它是一個vector
∑－１還是個matrix
x－μ是個vector
這個vector乘這個matrix乘這個vector
它會變成一個scalar
總之 經過你帶進一個x
根據μ跟∑
你會計算出一個數值
可能會計算出一個output
如果你不知
如果你對Gaussian Distribution
沒有非常熟悉的話
你就這樣想就好了
好 那所以現在
我們怎麼計算我們的likelihood呢
我們現在的θ
就被置換成mean μ
跟covariance matrix ∑
有不同的mean
跟不同的covariance matrix
我們就有不同的
probability的density function
所以我們可以計算x¹
產生出來的likelihood
我們可以計算x²它的機率
我們可以計算xⁿ它的機率
把這些機率通通乘起來
它就是這一個資料
這一個訓練資料被產生出來的
Likelihood
所以可以想像說
假設我們今天
有一個Gaussian Distribution
它的μ在這個地方
那我們知道Gaussian Distribution
的特性就是在μ附近
data被sample到的機率很高
因為遠離μ
被sample到的機率就越低
所以假設這一筆data背後
是由這樣子的一個
Gaussian Distribution sample出來的
那它的likelihood是比較大的
如果是另外一個case
假設我們現在的μ
是落在比較偏離這個
high density的地方
比較偏離這個高密度的地方
是落在這個地方
那我們知道說
今天照理說從這個μ
從這樣子的μ
從這樣Gaussian Distribution的
sample資料
多數資料
應該落在這個藍色圈圈的範圍內
而不應該落在這個地方
但是你不知道怎麼回事
就往這個μ做sample以後
居然data都落在這個地方
這顯然是一個很罕見的狀況
你算出來的likelihood顯然會低很多
所以我們這邊要做的事情
實際上就是
窮舉所有的μ跟∑
看看哪一個μ跟∑帶進去算likelihood
算出來的likelihood最大
這就是我們要找出來的
μ*跟∑*
找出這個μ*跟∑*以後
我們就知道說
產生這裡data背後的distribution
長的是什麼樣子
講到這邊
往往會有同學問一個問題就是
為什麼這邊要用Gaussian呢
為什麼不用別的呢
這個簡單的答案就是
我選別的distribution
你也會問我同樣的問題
那但是因為Gaussian
它真的非常常用
常用到什麼地步呢
常用到說
有時候你覺得你的feature看起來
even不是Gaussian
可這個其實有點像Gaussian
但是也沒那麼像Gaussian
它的分佈還是有限制在0到1之間的
Gaussian才沒有這樣的限制
所以這分布其實也沒那麼像Gaussian
但是我們很習慣使用
就是用Gaussian這樣子的distribution
甚至如果你看Andrew Lin
在講這個anomaly detection的時候
它甚至
明明它的data
看起來分佈就不是Gaussian
然後它就把它的所有的feature
都取一個log
然後讓它看起來變成像是Gaussian
或它去開根號
讓它看起來像是一個Gaussian
或甚至花了一整段的影片在講
它怎麼對那些feature
做總總的變化
讓它變成看起來像Gaussian
所以Gaussian是很常用的
但是這顯然是一個非常非常強的假設
因為你的data的分布
可能根本就不是Gaussian
該怎麼辦呢
你有可能可以做得更好
但是不是我們這一門課裡面
要講的範圍
舉例來說
假設你的probability density function
它是有一個network所操控的
而操控這個network的參數
有非常大量
如果你今天你的f
它是一個非常複雜的function
舉例來說
它是一個network
你就不會有那麼強的假設了
你就可以有比較多的自由
去選擇你要的function
來產生你的data
而不會限制在明明就看起來
就不像是Gaussian產生的data
你卻硬要說 它是由Gaussian產生的
這樣的問題
但是因為我們這一門課
還沒有講到其它進階的深層的模型
還沒有講過其它進階的
這個Generative model
所以我們這邊就簡單的用
Gaussian Distribution來當作我們data
是由Gaussian Distribution
所產生出來的
好 那這個其實是有公式的
就μ∑跟∑
就這個μ*跟∑*
其實是有closed-form solution
可以直接代個公式
就可以解這一個argmax notation的product
而且這件事情 我們在講binary classification
用generative model講binary classification
用來分類這個水系的神奇寶貝
還是一般系的神奇寶貝的時候
已經講過這件事情了
好 那μ*怎麼算呢
你就把所有的training data x做個平均
就是μ*
這邊算一下是0.29跟0.73
那∑*怎麼算呢
你就把(x－μ*)乘上(x－μ*)Ｔ
它們會變成一個矩陣
然後在把它平均起來
你就會算出∑*
總之要解這個argmax notation product
你可以直接套個公式
你就可以解這個argmax notation product
你就可以知道說
假設這筆資料
是由一個Gaussian Distribution產生的
那這Gaussian Distribution
最有可能它的mean
跟covariance matrix
分別是應該長的是什麼樣子
算出來大概是這樣
好 那有了這樣子的資料以後
我們根據這邊的data
找出了μ*跟∑*
接下來就可以做異常的偵測
你就可以說
如果今天把x帶進去這個
probability density function
它大於某一個你用development set
決定的threshold λ的話
那它就是正常的
如果它小於某一個threshold
它就是異常
那每一個輸入的點x
就算是從來在訓練的時候
從來沒有看過的
你都可以帶到這個
probability density function
去算出一個數值
如果我們把這個二維平面上所有的點
都拿來算一下那個數值的話
算起來大概是這個樣子
所以如果落在這個區域
顏色越深 越偏紅色
代表說算出來的
帶進這個f(x)算出來的數值越大
也就他越是一般的玩家
那顏色越淺的 越偏藍的
就代表說 這個玩家的行為越罕見
這個玩家的行為越異常
那你的λ呢
其實就是這個塗上等高線的
其中一條線
這個λ是你根據development set決定的
你可能就切在比如說
這一個等高線的位置
好 那你切出這個λ以後
如果有一個玩家
他的行為根據統計的結果
是落在這個位置
可能他很喜歡說垃圾話
但多數的時候都在無政府狀態發言
他可能就是一個正常的玩家
如果它落在 舉例來說這個地方
他很少說垃圾話
但是他都特別選在民主的時候發言
他都特別不在無政府狀態
發言起來非常犀利
那他就是一個比較異常的玩家
那這個結果其實非常的單純
因為只有二維
二維你就可以直接畫在一個平面上
所以你可能會覺得沒有什麼稀奇的
那machine learning厲害的地方
就是因為這些事情
