好,那各位同學大家好啊
那我們就來上課吧
那今天這堂課呢,我們要講的是AI agent
這是一個現在非常熱門的議題
那在課程開始之前呢,先講一個免責聲明
我知道你在各個地方可能都聽過AI agent這個詞彙
它是一個被很廣泛應用的詞彙
每個人心裡想的AI agent可能都不一樣
等一下下一頁投影片會告訴你說
我在這一堂課中指的AI agent是什麼,那如果你在其他地方聽過別的AI agent的定義,那也沒問題,我也不會爭論說什麼樣的定義,才是真正的AI agent的定義,有些人甚至會告訴你說,現在那些用大型語言模型驅動的號稱AI agent的東西都不是真正的AI agent,要有身體的像這個機器人一樣的才叫做AI agent,所以每個人心裡想像的AI agent是不一樣的,
好,那這一堂課我們要講的AI agent是什麼呢?今天我們使用AI的方式,通常是人類給一個明確的指令,你問AI說AI agent的翻譯是什麼,那AI呢,按照你的口令,一個口令,一個動作,把你要求的翻譯翻譯出來,他也不會再做更多的事情了。
那AI agent的意思是說,人類不提供明確的
行為或步驟的指示
人類只給AI目標
那就至於怎麼達成目標呢
AI要自己想辦法去達成目標
比如說你給AI某一個研究的議題
那你期待說一個AI agent就應該有能力
自己提出假設
自己設計實驗
自己進行實驗
自己分析結果
如果分析出來的結果跟假設不符合
要回頭去修正假設
那通常你期待AI agent要解決的目標
要達成的目標是需要透過多個步驟
跟環境做很複雜的互動才能夠完成
而環境會有一些不可預測的地方
所以AI agent還要能夠做到靈活的
根據現在的狀況來調整他的計畫
那AI agent是怎麼做到人類給予一個目標
用多個步驟來完成目標的呢
那我們可以把
AI agent背後運作的過程
簡化成以下這張投影片
那AI agent的第一個輸入
是一個目標
這個目標是人給定的
那接下來呢
AI agent會觀察目前的狀況
那AI agent可以看到的目前的狀況
我們叫做observation
那AI agent會看目前的狀況
分析目前的狀況
決定他要採取什麼樣的行動
那今天這個AI agent做的事情
叫做action
那他執行個action以後
會影響環境的狀態
會看到不一樣的observation
看到不一樣的observation
就會執行不同的action
那這個步驟會一直循環
直到AI agent達成
我們要他達成的目標為止
那我只要講到這邊
你可能還覺得非常的抽象
那我們可以用下圍棋來舉例
那AlphaGo是大家非常熟悉的東西
AlphaGo其實也可以
可以看作是一個AI agent
這個AI agent的目標就是
下棋要贏
他的observation是什麼
他的observation是現在棋盤上
黑子跟白子的位置
現在棋盤上的盤式
那他可以採取的action是什麼
他可以採取的action
就是在棋盤上的19x19路的範圍中
選擇一個動作
選擇一個可以落子的位置
那他選擇完可以落子的位置
他落下一次以後
會改變他對手的輸出
你落下一隻以後
你的對手會落下另外一隻
那會改變你觀察到的observation
那你就要採取下一個action
所以AlphaGo是一個AI agent
那他背後運作的原理
我想大家其實或多或少也都已經聽過
那像這樣的講法
我相信你一定覺得非常的熟悉
好像在哪裡聽過一樣的段落
沒錯
如果你有上過任何basic的
reinforcement learning RL的課程
往往都是用這樣的方式來開場的
為什麼呢
因為過去要打造AI agent的時候
往往覺得就是要透過RL的演算法來打造AI agent
那怎麼透過RL的演算法來打造AI agent呢
RL這個演算法就是他可以去learn一個agent
那這個agent可以maximize reward
所以你要把你的目標呢
轉換成一個叫做reward的東西
那這個reward呢
是人定義的越接近
你的目標reward就越大
那如果在下圍棋裡面
你通常就會定說
贏棋reward就是正一
輸棋reward就是負一
然後你要訓練的那個AI agent
就會學習去maximize reward
透過RL的演算法
所以透過RL的演算法
其實你也有可能學一個AI agent
但是透過RL演算法的侷限是
你需要為每一個任務
都用RL的演算法
訓練一個模型
AlphaGo在經過了大量的訓練以後
他可以下圍棋
但並不代表他可以下其他的棋類
西洋棋或將棋
我知道你可能看了一篇文章
AlphaGo Zero （口誤，應為AlphaZero）
他除了圍棋外也可以下將棋跟西洋棋
那是另外訓練後的結果
能夠下將棋的那個模型
並不是原來可以下圍棋的那個AlphaGo
他們是不同的模型
有不同的參數
而今天AI Agent又再次被討論
是因為
人們有了新的想法
我們能不能夠直接把Large Language Model
把LLM直接當成一個AI Agent來使用呢
也就是說我們的Agent背後
就是一個Language Model
你要告訴他你的目標是什麼的時候
直接用文字輸入
要告訴他下圍棋
就先給他圍棋的規則
然後跟他說你的目標就是贏得勝利
那接下來環境
因為一般語言模型是用文字作為輸入
所以你可能需要把環境轉化成文字的敘述
不過我這邊寫了一個option
今天有很多語言模型都是可以直接看圖片的
所以把環境轉成文字的敘述
今天也不一定是必要的
那接下來語言模型要產生action
那產生action的方式
可能就是用一段文字來決定它的action是什麼
它的action用一段文字來描述
那我們需要把那段文字轉譯成真正可以執行的
真正可以執行的行動
然後就會改變環境
看到不同的observation
然後AI agent的運作
就可以持續下去
直到達成目標
今天AI agent再次爆紅
並不是真的有了什麼
跟AI agent本身相關的新的技術
而是在LLM變強之後
人們開始想
我們能不能直接用large language model
來實踐人類擁有一個agent的渴望
好 那我們這邊呢
是拿下棋做例子啦
也許你就會很好奇說
現在的語言模型
能不能夠下棋呢
其實早就有人嘗試過了
有一個在語言模型領域
很多人使用的benchmark叫做BigBench
它是什麼時候做的呢
它是2022年上古時代做的
以後有ChatGPT之前
我們都叫上古時代
然後在2022年上古時代的時候
就有人嘗試過
用那個時候的語言模型
看看能不能下西洋棋
那時候語言模型沒有辦法真的看圖
所以你需要把棋盤上黑紙跟白紙的位置
轉成文字的敘述
輸入給這個語言模型
所以這個就是語言模型實際上看到的棋盤的樣子
那就問他說下一步要下哪裡
才能夠給對方將軍呢
那語言模型就會給你一個答案
右上角這個圖啊
橙色的線是
正確答案
綠色的線是當時各個不同的語言模型所給的答案
沒有任何一個語言模型給出正確的答案
但雖然沒有任何語言模型給出正確的答案
但你可以看這個實現是當時比較強的模型
他們雖然沒給出正確答案
但他們所選擇走的路是符合西洋棋規則的
但是也有很多比較弱的模型
這個虛線是比較弱的模型
他們都亂走
他根本搞不懂西洋棋的規則
隨便按照自己的意思來想
不過這個是上古時代的事情了
那現在更強的LLM
能不能下西洋棋呢
有人試過了
有一個很知名的影片
是直接拿ChatGPT o1跟DeepSeek-R1兩個模型
來下西洋棋
那這是一場驚天動地的對決
這個影片好幾百萬觀看次數啊
那這兩個模型呢
他們殺的難分難解
難分難解是因為
他們實在是太弱了
他們有很多不符合西洋棋的規則
比如說把兵呢當作馬來用
或者是他的主帥
他的那個主教可以無視前面的一切阻擋
或是他會突然
就是空降一個自己的子
在對方的陣地裡面
把對方的子吃掉
然後DeepSeek還在自己的棋盤上
隨便變出一個城堡
然後最後
最後DeepSeek用自己的城堡
把自己的兵吃掉以後
他宣佈他贏了
對方告投降
然後ChatGPT想了一下覺得
嗯 我確實輸了
然後就投降了
所以這個棋局就這樣結束了
所以看起來
現在這個最強的語言模型
你要下棋還有一段距離
但這並不代表
他們不能夠作為AI agent來做其他事情
那等一下會舉一些例子
看看現在的語言模型
可以做什麼樣的事情
那這門課
另外最主要想要強調
跟大家傳輸
的資訊是我們還能多做什麼
讓這些語言模型作為AI agent的時候
運作的更加順利
那剛才講法比較像是
從過去常見的這個agent的觀點
來看語言模型
怎麼套用到agent的框架下
那接下來我們換一個角度看說
從large language model的角度來看
到底當他作為一個agent的時候
他要解的問題有什麼不同
好,那我們從large language model的角度來看
首先他得到一個目標
然後接下來呢
他得到一個observation
然後根據這個observation
他要決定接下來要採取什麼樣的action
採取什麼樣的動作
那他採取完動作之後
他的動作會影響外界的環境
看到新的observation
看到新的observation以後
要採取新的動作
這個過程就會再反覆繼續
下去
那在這一系列的過程中
看到observation採取action
其實憑藉的都是語言模型
原來就有的接龍的能力
所以從語言模型的角度來看
當我們把它當作一個AI agent來使用的時候
對他而言他做的事情是完全沒有什麼不同的
他就是繼續在做他唯一會做的文字接龍而已
所以從語言模型的角度來看
AI agent並不是一個語言模型的
新技術
它比較像是一個語言模型的應用
所謂AI agent意思就是
依靠現在語言模型已經有一定程度的通用能力
看看能不能夠直接把它們當作agent來使用
那因為我說這個AI agent並不是語言模型的新技術
它只是一個語言模型的應用
所以要注意一下在以下課程中沒有任何的模型被訓練
以下
我所有所講的東西都是以靠一個現有的語言模型的能力來達成的
那AI agent其實不是最近才熱門
一直有人在嘗試怎麼讓語言模型變成一個agent
或怎麼把語言模型當作AI agent來使用
ChatGPT在2022年年底爆紅
所以在2023年的春天就有一波AI agent的熱潮
好多人都用ChatGPT作為背後運作的語言模型
來打造AI agent
那個時候最有名的就是Auto GPT
那其實在2023年的機器學習
我們也有一堂課是講那個時候的AI agent
那可以看看那堂課
看看那一堂課的AI agent跟今天講的有什麼樣的差異
不過後來2023年AI agent的熱潮
過一陣子就消退了
因為人們發現這些AI agent沒有我們想像的厲害
一開始好多網紅在吹噓
這些AI agent有多強又有多強
真的試下去也沒那麼強
所以熱潮就過去了
那用LLM來運行一個AI agent
相較於其他的方法
可能有什麼樣的優勢呢
那過去啊
當你運行一個agent的時候
比如說像AlphaGo
他能夠做的只有有限的
事先設定好的行為
AlphaGo真正能夠做的事情
就是在19x19個位置上
選擇一個可以落子的位置
也就是說他真正能夠採取的行為
就是從19x19個選擇題中
選擇一個他能夠採取的行為
但是如果你的agent是一個large language model的話
他就有了近乎無限的可能
large language model可以講任何話
可以產生各式各樣近乎無窮無盡的輸出
這就讓你AI agent可以採取的行動
不再有侷限
有更多的可能性
舉例來說
我們等一下就會很快看到的
今天這些AI agent
在有些問題他解不了的時候
他可以憑藉他
可以有各式各樣輸出的能力
來直接呼叫一些工具
來幫忙解決他本來解決
解決不了的問題
那另外一個AI agent的優勢
另外一個用large language model
運行AI agent的優勢
是過去如果用reinforcement learning的方法
來訓練一個AI agent
那意味著什麼
你必須要定義一個東西叫做reward
那如果你今天是要訓練一個AI programmer
那你可能會告訴AI programmer說
如果你今天寫的程式有一個compile的error
那你就得到reward-1
但為什麼是-1
為什麼不是-10
為什麼不是-17.7
這種東西就是沒人說得清楚
所以這個reward在做reinforcement learning的時候
就是一個要調要通靈的東西
那今天如果是用LLM驅動的AI agent呢
你今天就不用幫他訂reward了
今天有compile error
你可以直接把compile error的log給他
他也許根本就讀得懂那個log
他就可以對程式做出正確的修改
而且相較於reward只有一個數值
直接提供error的log
可能提供了agent更豐富的資訊
讓他更容易按照環境給的回饋
環境目前的狀態來修改
修改他的行為
接下來舉幾個AI agent的例子
那講到AI agent
也許最知名的例子
就是用AI村民所組成的一個虛擬村莊
這個虛擬村莊是在什麼時候成立的呢
2023年
在古代就已經有人做過這個虛擬村莊了
那裡面的NPC通通都是用語言模型來運行的
那這些NPC它是怎麼運行的呢
首先每個NPC都一個人為設定的目標
有的NPC他要辦情人節派對
有的NPC要準備考試
每個人都一個他自己想做的事情
那這些NPC呢
會觀察會看到環境的資訊
那時候Language Model都只能讀文字
所以環境的資訊需要用文字來表示
所以環境的資訊對一個語言模型來說
看起來可能就是
這個語言模型旁邊有一個叫做Eddy的人
他正在讀書
然後呢他看到廚房
然後呢
他看到一個櫃子
然後看到伊莉莎白呢
正在裝做裝飾
正在裝飾房間等等
然後根據這些observation
這個語言模型
要決定一個他想要做的行為
比如說也許不著了
所以就上床睡覺
那需要有一個轉譯器
把它說出來的這個行為
轉成真正能夠執行的指令
那這個agent就真會走到床邊
然後去睡覺
好,所以這個是2023年的時候
用AI來這個運行NPC的一個實驗
其實後來還有更大規模的實驗
有人把Minecraft中的NPC
通通換成AI的NPC
那就把相關的影片連結留在這個投影片上面
那根據這個影片連結的描述
就說這些AI很厲害
他們組織了自己的交易的金融體系
然後還組織了自己的政府
自己制定憲法自己管理自己
是真的還假的啦
這個是這個影片說的
剛才講的那些遊戲
你可能比較不容易接觸到
他對現實世界可能也沒什麼影響
那今天也許你馬上就會接觸到的AI agent
就是讓AI來真正使用電腦
雖然這個聽起來有點弔詭
AI本身也就是一個電腦
但他現在要來真正的像人類一樣
來使用另外一個比較低端的電腦來做事
那其中比較有代表性的例子
就是cloud的computer use
還有chain GPT的operator
那我們在上次上課的影片中
也已經跟大家講過operator
那operator介面長這樣
那他會建議可以做的事情
比如說可以訂pizza
可以預約下週的居家清潔等等
那像這種使用電腦的AI agent
他的目標就是你的輸入
就是你告訴他我要去訂pizza
你告訴他上網幫我買一個東西
那這就是他的目標
那他的observation呢
他的observation可能是
那個電腦的螢幕畫面
今天很多語言模型都是可以直接看圖的
所以其實可以直接把圖片當作輸入
可以直接把電腦畫面當作輸入
提供給AI agent
那AI agent要決定的就是
他要按鍵盤上哪一個鍵
或者是要按滑鼠的哪一個按鈕
那其實讓AI使用電腦啊
不是最近才開始有的野望
其實早在2017年
就有篇paper叫words of bits
嘗試過使用AI agent
你看他這個文章的標題
他把自己的文章標題說
他是一個web-based agent
那只是那個時候能夠互動的頁面
還是比較原始的頁面
你可以看到下面這些AI agent
他真正能夠處理的是比較原始的頁面
那個時候也沒有大型語言模型
所以那時候的方法
就是硬圈一個CNN
直接吃螢幕畫面當作輸入
輸出就是
滑鼠要點的位置
或者是鍵盤要按的按鈕
看看用這個方法
能不能夠讓AI agent
在網路的世界中做事
這個是2017年
這甚至不能說是上古時代
以後有這個BERT以前的時代
就是死前時代
這個不只是死前時代
它死前時代比較早期
所以這是舊時期時代的產物
好
那後來有了語言模型之後啊
人們就開始嘗試用語言模型
來當作AI agent
來運行一個agent
讓它在網路的世界中活動
那這一頁投影片
是列舉了幾個比較具代表性的例子
那這一波潮流
大概是在2023年的暑假開始的
像Mine to Web
Web Arana
還有Visual Web Arana
就跟今天的operator
非常的像
就是給這個語言模型
看一個螢幕的畫面
或者是看HTML的code
然後他自己決定他要幹什麼
期待他最後可以解決一個問題
比如說在Mine to Web的第一個例子裡面
就給他這個畫面
然後跟他說
請他幫我們訂一個機票
那還有什麼樣AI agent的應用呢
今天你可以用AI來訓練另外一個AI模型
這就是等一下作業二
助教會跟大家講的事情
那用AI來訓練模型
那其實這個運作的過程就是
你的目標就是要過strong baseline
然後你提供給
LLM訓練資料
他寫一個程式用這些訓練資料來訓練模型
那他可能
可以得到這個模型的正確率
根據正確率再重新寫一個程式
再得到新的正確率
就這樣一直運作下去
那有很多知名的
用AI來訓練模型的framework
比如說AIDE
那你看他的這個技術報告的
這個標題就知道他們想做什麼
他是要做一個machine learning
engineer agent
他就是要用
multi-agent的framework來解
data science的competition
那在我們的作業中
你就會體驗到
到底AI agent做不做得了
機器學習這門課的作業
那最近呢
Google說他們做了一個AI
不過他們並沒有真的
釋出模型啦,所以你也不知道說
實際上做得怎麼樣,這個服務並不是公開的
那他們說他們做了一個AI
Coscientist
就是用AI來做研究
不過這個AI Cosine
還是蠻有侷限的,他不能真的做實驗啦
他只能夠提Proposal
就是你把一些研究的想法告訴他
他把完整的Proposal規劃出來
實際上做得怎麼樣,不知道啦
那你要看他的Blog裡面有些比較
誇張的案例,說什麼
本來人類要花十年才能夠得到研究成果
AI agent花兩天就得到了,也不知道真的還假的
他舉的是一些生物學的例子,所以我也無法判斷
他講的是不是真的,那個發現是不是真的很重要
這個co-scientist的話
就是這個用AI agent來幫研究人員做研究
好,那我們剛才講的AI agent
他的互動方式是侷限在回合制的互動
有一個observation,接下來執行action
但是在更真實的情境下
這個互動是需要及時的
因為外在的環境也許是不斷在改變的
如果你在action還沒有執行完的時候
外在環境就改變了
那應該要怎麼辦呢
有沒有辦法做到更即時的互動呢
更即時的互動可能應該像是這樣子
當模型在決定要執行action one
正在執行的過程中
突然外在環境變了
這個時候模型應該有辦法
立刻轉換行動
改變他的決策
以因應外界突如其來的變化
你可以想說
什麼樣的狀況
我們會需要用到這樣的AI agent
能夠做即時互動的呢
其實語音對話就需要這種互動的模式
文字的對話使用切GPT是大家比較熟悉的
你輸入一段文字
他就輸出一段文字
這是一來一往回合制的互動
但是人與人間真正的對話不是這樣子的
當兩個人在對話的時候
他們可能會互相打斷
或者是其中一個人在講話的時候
另外一個人可能會同時提供一些回饋
比如說嗯好你說的都對
那這些回饋可能沒有什麼
特別語意上的含義
他只是想要告訴對方我有在聽
但是像這樣子的回饋
對於流暢的交流來說
也是非常重要的
如果在講電話的時候對方完全都沒有回憶
你會懷疑他到底有沒有在聽
所以我們今天能不能夠讓AI
在跟使用者互動的時候
用語音互動的時候
就跟人與人間的互動一樣
而不是一來一往回合制的互動呢
其實也不是不可能的
今天GPT4O的一個Voice Mode
高級語音模式
也許在某種程度上
就做到了這一種即時的互動
那這個投影片上是舉一個例子
假設有人跟AI說
你說一個故事
那這個是AI觀察到的
第一個observation
有人叫他說一個故事
現在就開始講故事了
他就說從前從前
那這時候人說了一個
好 這個可能是第二個observation
但AI要知道說
這個observation
不需要改變他的行為
跟他的行為沒有直接的關係
只要故事就繼續講下去
有一個小鎮
然後人說這個不是我要聽的故事
這個我聽到了
那AI可能要馬上知道說
那這個不是人要聽的
那也許我覺得應該停下來
換另外一個故事
那今天AI有沒有辦法做到這種即時的互動呢
那怎麼做這種即時的互動
非回合制的互動
就有點超過我們這門課想要講的範圍
如果你有興趣的話
你可以讀這篇文章
那這篇文章想要做的事情
是評量現在這些語音模型互動的能力
那在這篇文章裡面
也對現有的這個
可以做互動的語音模型
做了一個比較完整的survey
是一直survey到今年的1月
所以你可以看這篇文章
知道說現在這些可以互動的模型
他可以做到什麼樣的地步
那這是我們實驗室的林冠廷同學
跟他在這個Berkeley
UW和NIT的合作夥伴
一起做的文章
那這邊順便說明一下
以後這門課呢
我們投影片上引用論文的原則
論文的原則就是
如果我找得到arXiv的連結的話
那我就把文章直接
我就直接貼arXiv的連結
什麼是arXiv呢
假設你不是Computer Science背景的話
也許我就要解釋一下什麼是arXiv
arXiv的意思就是
一般呢
做研究你是寫完文章
投稿到一個期刊
或者是國際會議
然後被接受以後才發表出來
但是對於AI的領域
因為變化實在太快
幾個月前
就已經是古代了
所以期刊那種
一審就要一年
或者是國際會議一兩個月
這種步調
是沒有辦法在不適用於AI的領域
所以現在一種習慣的發表方式
就是做出東西以後
直接放到一個公開的網站
叫做arXiv
然後就不審了
立刻公開
然後你就可以讓全世界的人
看到你的文章
那有很多人會覺得
引用arXiv的連結不夠正式
但是很多重要的文章
其實現在不見得投稿國際會議
但就只有arXiv的連結
所以我會選擇
如果找得到arXiv的連結的話
就直接引用arXiv的連結
其實現在大家都在arXiv上看文章
那國際會議現在比較像是
經典回顧這樣子
每篇文章我幾乎都在arXiv上看過了
句子說原來你投到這裡啊
這樣的感覺
那引用arXiv的連結
還有一個好處就是你可以直接從arXiv的連結
看出這篇文章的時間
所以arXiv的連結裡面的數字
前面兩個就是年份
後面兩個就是月份
可以看這個數字就可以知道說這篇文章是在什麼時候被放在arXiv
也就是什麼時候被發表的
可以讓你對於每一個研究
他誕生的時間更有感覺
好那接下來呢
我們會分三個面向來剖析今天這些AI agent的關鍵能力
那第一個面向是我們要來看這些AI agent
這個AI agent能不能夠根據他的經驗
過去的互動中所獲得的經驗
來調整他的行為
第二部分是要講這些AI agent
如何呼叫外部的援助
如何使用工具
第三部分要講AI agent
能不能夠執行計畫
能不能做計畫
那我們來講一下
AI怎麼根據
過去的經驗
或者是環境的回饋
來調整他的行為
那AI呢
AI agent需要能夠根據經驗來調整行為
比如說有一個作為AI programmer的AI agent
他一開始接到一個任務
那他寫了一個程式
那這個程式compile以後
有錯誤訊息
compile以後有error
那應該要怎麼辦呢
他應該要能夠根據這個error的message
來修正他之後寫的程式
那在過去啊
講到說
你收到一個feedback接下來要做什麼的時候
也許多數機器學習的課程
都是告訴你
來調整參數
根據這些收集到的訓練資料
也許使用reinforcement learning的algorithm
來調整參數
但不要忘了我們剛才就強調過
在這一堂課裡面
沒有任何模型被訓練
所以我們今天不走這個路線
那不更新模型的參數
模型要怎麼改變它的行為呢
依照今天
Large Language Model的能力
要改變它的行為
你也不用微調參數
直接把錯誤的訊息給他
他接下來寫的程式就會不一樣了
就結束了
那可能會問說
那之前他寫的程式是錯的
為什麼給錯誤訊息
他寫的程式就對了呢
明明就是同一個模型
但你想想看
模型做的事情就是文字接龍
你給他不同的輸入
他接出來的東西就不一樣
一開始會寫錯的程式
是因為他前面要接的部分
只有這麼多
所以寫個錯的程式
當今天要接的內容
包含了錯誤的訊息的時候
他接出來的結果
可能就會是正確的了
那今天已經有太多的證據
說明這些語言模型
可以根據你給他的回饋
改變他的行為
不需要調整參數
那如果你有使用這些語言模型的經驗
你也不會懷疑
他們有根據你的回饋調整行為的能力
那這邊真正的議題是
如果我們是把過去所有的經驗
都存起來
要改變語言模型的行為
要讓他根據過去的經驗調整行為
就是把過去所有發生的事情一股腦給他
那就好像是語言模型每次做一次決策的時候
他都要回憶他一生的經歷
也許在第100步的時候還行
到第1萬步的時候
過去的經驗太長了
他的人生的資訊已經太多了
也許他沒有足夠的算力
來回顧一生的資訊
他就沒有辦法得到正確的答案
這讓我想到什麼呢
這讓我想到有一些人
有超長自傳式記憶
他可以把他一生中
所有發生的事情記下來
然後那些人
你可以隨便問他一個
某個人的電話號碼
他都會背出來
你告訴他某年某日某時
發生了什麼事
他也都可以講出來
有一些人
他的頭腦
就像是一個影印機一樣
會把所有他看過的事情
都遠風不動的記憶下來
但這種超長自傳式記憶啊
又被叫做超憶症
你看到症這個字就知道說
人們覺得這是一種疾病
這聽起來記憶力很好是一種祝福
但實際上對這些患者而言
據說這種患者世界上可能不到100例
那這是一個2006年的時候
才被論文發表的一個症狀
那據說這些患者其實日常生活並沒有辦法過得很開心
因為他們不斷的在回憶他的人生
往往一不小心就陷入了一個冗長的回憶之中
那也很難做抽象的思考
因為他的人生已經被他的記憶已經被太多
知為末節的所事所佔據
所以沒有辦法做抽象式的思考
所以讓一個AI agent記住他一生所有經歷的事情
告訴他你每次做一個決策的時候
都是根據你一生所有經歷過的事情再去做決策
那也許對AI agent來說並不是一件好事
最終當他的人生過長的時候
他會沒有辦法做出
正確的決策
所以怎麼辦呢
也許我們可以給這些AI agent
memory
這就像是人類的長期記憶一樣
發生過的事情
我們把它存到這個memory裡面
當AI agent看到
第一萬個observation的時候
他不是根據所有存在memory裡面的內容
去決定接下來要採取什麼action
而是有一個叫做read的模組
這個read的模組
會從memory裡面
選擇跟現在要解決的問題有關係的經驗
把這些有關係的經驗放在observation的前面
讓模型根據這些有關係的經驗跟observation
再做文字接龍
接出它應該進行的行為
那你有這個read的模組
就可以從memory裡面
從長期記憶中篩選出重要的訊息
讓模型只根據這些跟現在情境相關的訊息來進行決策
那怎麼樣打造這個read的模組呢
其實你可以想這個read的模組
就想成是一個retrieval的system
想成是一個檢索的系統
那第一萬步看到的observation其實就是問題
那模型的AI agent的memory長期記憶
其實就是資料庫
那你就把拿這個檢索系統
根據這個問題從這個資料庫裡面
檢索出相關的資訊
那這整個技術跟RID
沒有什麼不同
其實它就是RAG
你可以直接把RAG的任何方法
直接套用到這個地方
唯一不一樣的地方只是
如果是RAG的話
存在memory裡面的東西
等於是整個網路
那是別人的經驗
而對AI agent而言
現在存在memory裡面的東西
是他自己個人的經歷
差別的是經歷的來源
但是用來搜尋的技術
是可以完全直接線套RAG的技術
呃,如果你今天想要研究這個AI agent按照經驗來修改他的行為,那你可以考慮一個叫做streambench的benchmark,那在streambench裡面呢,會有一系列的問題,然後呢,AI會依序去解這些問題,他先解第一個問題,得到第一個問題的答案,然後接下來他會得到第一個問題答案的反饋,那在這個streambench目前的
因為所有的問題都是有標準答案的,所以AI agent得到的回饋是binary的,就是對或者是錯
好,那根據他過去的經驗,他就可以修正他的行為
期待他在第二個問題的時候,可以得到更準確的答案
得到更高的正確率,然後這個過程就一直持續下去
那假設有1000個問題的話,那就等AI agent回答完最後問題的時候
這個互動就結束了
那最後結算一個
根據經驗學習能力的好壞
根據經驗調整行為能力的好壞
那就看這一整個回答的過程中
平均的正確率
越能夠根據經驗學習的agent
他應該能夠用越少的時間
看過越少的回饋
就越快能夠增強他的能力
就可以得到比較高的平均的正確率
那這個benchmark呢
是API的研究人員
打造的一個benchmark
那在這個
這個benchmark裡面的baseline
就是有使用到我剛才講的
類似RAG的技術
也就是說
當模型在回答第100個問題的時候
他並不是把前面
第一個到第99個問題
通通丟給他
去做文字接龍
這樣這個sequence太長了
一般的語言模型根本讀不了這麼長的輸入
所以實際上的做法
就是你需要有一個檢索的模組
這個檢索的模組
只從過去所有的經驗中
檢索出
跟現在要回答的問題
有關係的經驗
然後語言模型
只根據這些有關係的經驗
還有現在的問題
來進行回答
來產生他的行動
來產生他的答案
那這一招有沒有用呢
這一招其實非常的有用
那在這一頁圖裡面橫走啊
他這邊的用詞是time step
但其實指的就是一個一個的問題
總共有1750幾個問題
那縱軸指的是平均的正確率
那在這個圖上面呢
最低的這條灰色線指的是說
假設沒有讓模型做任何學習
他回答每一個問題都是independent的
回答問題間沒有任何的關聯
他完全沒有調整他的行為
那你得到的正確率是灰色的這條線是最低的
那黃色這條線是說
只固定隨機選五個問題
那每次模型回答問題的時候
都是固定看那五個問題來回答
都是固定把五個問題當作經驗來回答
那也可以得到的是黃色這一條線
那如果你是用RAG的方法
從一個memory裡面去挑選出最有關係的問題
跟現在要解決的問題最有關係的經驗
那你可以得到的是粉紅色的這一條線
那可以看到比黃色的線
那正確率還要高上不少
那最後結果最好的是紅色這一條線啦
那這個怎麼做的
那大家就自己再去詳細閱讀論文
那在streambench裡面呢
還發現一個有趣的現象是值得跟大家分享
這個現象是
負面的回饋
基本上沒有幫助對現階段的語言模型而言
所以你要提供給語言模型經驗
讓他能夠調整他行為的時候
給他正面的例子
比給他負面的例子要好
也就是說具體而言
提供給他過去哪些類似的問題得到正確答案
比提供給他過去哪些問題得到錯誤的答案
還更有效
還更能引導模型得到正確的答案
那這邊是真正的實驗結果
做在好幾個不同的data set上面
streambench裡面本來就包含了好幾個不同的data set
那這個縱軸呢
0代表完全沒有做
完全沒有根據
經驗調整行為
然後藍色代表說
不管是正面還是負面的例子都用
如果不管正面還是負面的例子都用
在多數情況下
模型都可以表現得比較好
當然有一些例外
但是如果只用負面的例子呢
如果只用負面的例子
基本上是沒有幫助
而且甚至是有害的
那如果說只用正面的例子
在所有的情況下
模型可以得到更好的結果
那這也符合過去的一些研究
有人研究過使用語言模型要怎麼樣比較有效
有一個發現就是
與其告訴語言模型不要做什麼
不如告訴他要做什麼
如果你還希望他文章寫短一點
你要直接跟他說寫短一點
不要告訴他不要寫太長
比較他不要寫太長
他不一定聽得懂
叫他寫短一點
比較直接
他反而比較聽得懂
這也符合這邊這個Streambench的發現
就是負面的例子比較
他沒有效
與其給語言模型告訴他什麼做錯
不如告訴他怎麼做是對的
好 那我們剛才講到了
有一個read的模組
那有關記憶的部分呢
是不是要把所有所有的資訊
通通存到memory裡面呢
存到長期的記憶庫裡面呢
如果我們把這些agent
經歷的所有的事情
都放到長期的記憶庫裡面的話
那裡面可能會充斥了一堆雞毛算皮不重要的小事
最終你的memory長期記憶庫可能也會被塞爆
如果說你是做那種AI村民啊
AI村民他多數時候觀察到的資訊都是些無關緊要的小事
那如果你看他觀察到那個log
多數都是啥事也沒有
就那邊有一張桌子啥事也沒有
那邊有一張椅子啥事也沒有
多數時候都是啥事也沒有
所以如果把所有觀察到的東西
都記下來的話
那你的memory裡面就都只是被一些
雞毛算皮的小事佔據
所以怎麼辦呢
也許應該有更有效的方式
來決定什麼樣的資訊
應該被記下來
應該只要記重要的
資訊就好
那怎麼讓語言模型只記重要的資訊就好呢
你可以有一個write的module
那write的module決定
什麼樣的資訊要被填到
長期的記憶庫裡面
什麼樣的資訊乾脆直接就讓他
隨風而去就好了
那怎麼樣打造這個write的記憶庫呢
有一個很簡單的方法就是
write的模組也是一個語言模型
甚至就是AI agent自己
這個AI agent他要做的事情
就是根據他現在觀察到的東西
然後問自問一個問題
這件事有重要到應該被記下來嗎
如果有就把它記下來
如果沒有就讓他隨風而去
那除了RE跟Write這兩個模組以外
還有第三個模組
沒有固定的名字啦
在文件上的名字
沒有固定的名字
我們可以暫時叫他
reflection反思的模組
那這個模組的工作是
對記憶中的資訊
做更好的
更high level的
可能是抽象的重新整理
你可以把這些記憶裡面的內容
在經過reflection的模組
重新反思之後
得到新的想法
那也許read的模組
可以根據這些新的想法
來進行搜尋
這樣子也許可以得到更好的經驗
那幫助模型
做出更好的決策
而這個reflection的模組
可能也是一個語言模型
就是AI agent自己
你可以只是把過去的這一些記憶
丟給reflection的模組
然後叫reflection模組想一想
看他從這些記憶裡面
能不能夠有什麼樣新的發現
比如說可能有一個observation是
我喜歡的疫情每天都跟我搭同一部公車
另外observation是他今天對我笑了
那你推出來的reflection模型
結果就說他喜歡我這樣
一個錯覺
人生三大錯覺之一就是這一種
就得到一些新的sort
你就得到一些新的想法
那你之後在做決策的時候
就可以用這些新的想法
雖然你沒有實際觀察到
但它是被推論出來的
根據這些推論出來的想法來做決策
那除了產生新的想法之外
也可以為以前觀察到的經驗
建立經驗和經驗之間的關係
也就是建立一個
然後讓reader的module
根據這個knowledge graph
來找相關的資訊
那我知道在
RAG的領域使用knowledge graph
現在也是一個非常常見的手法
那最知名的
可能就是graph RAG系列這個研究
就把你的資料庫
把它變成一個knowledge graph
那今天在搜尋跟回答問題的時候
是根據knowledge graph來搜尋回答問題
可以讓RAG這件事
做得更有效率
或是另外一個非常類似的例子
那HIPO RAG
這個HIPO不是指真正的荷馬
他指的應該是那個海馬迴
那個人腦中的一個結構
然後他覺得做建這種knowledge graph
就跟海馬迴的運作呢
非常的類似
所以他叫做HIPO RAG
有一些跟graph有關的RAG的方法
那你完全可以透過reflection的模組
把經驗建成一個graph以後
把那一些graph RAG的手法
直接套到AI agent裡面
那大家可能都
都知道說這個ChatGPT啊
現在其實真的是有記憶的
所以可以感受到這個OpenAI
想把ChatGPT變成一個AI agent的決心
比如說我跟ChatGPT說
我週五下午要上機器學習這門課
那他就給我一個回答
說要我幫助你做什麼事情嗎
接下來我告訴他記下來
你跟他講記下來之後
他的這個write的模組就啟動了
他知道這件事情是要被記下來的
他就會說那我記下來了
以後你週五要上機器學習這門課
那write的模組什麼時候要啟動
是他自己決定的
所以很多時候你希望他記下來的時候
他就是不啟動
或你不希望他啟動的時候
他就是啟動
那個是模型自己決定的
但是有一個方法可以
基本上一定能讓他啟動
就明確的跟他講
把這件事記下來
基本上都幾乎確定能夠啟動那個write的模組
讓write的模組把這件事情記下來
那接下來的東西在哪裡呢
你可以看在設定裡面
有一個個人化
然後有一個叫記憶的部分
那你點這個管理記憶
就可以看到確記憶
他透過write的模組
寫在他的memory裡面
這個就是他
作為一個AI agent的長期記憶
裡面的東西
比如第一條是
你叫做血輪眼卡卡
有一次不小心跟他說你是卡卡
不知道為什麼
他就覺得自己是血輪眼卡卡
然後呢
他也記得
就我剛才跟他講的
週五下午要上機器學習這門課
但是呢 但是其實模型的記憶也是會出錯的
因為要寫什麼樣的東西
到記憶裡面
是模型自己決定的
而且他並不是把對話的內容
就一五一十的直接放到記憶裡面
他是經過一些昇華反思之後才放進去的
所以他的反思可能會出錯
比如說他覺得我是一個臺灣大學的學生
雖然我是老師
但是他從過去的對話誤以為我是一個學生
所以就存了一個錯誤的資訊
在他的記憶裡面
一堆他想記的東西
比如說我給過什麼演講
給過什麼tutorial
他都把它記下來就是了
那這些有記憶的確GPT
他可以使用他的記憶
比如說我跟他說禮拜五下午是去玩好嗎
這個時候記憶模組就被啟動了
但是他是怎麼被啟動的
其實就不太清楚了
他到底是把所有記憶的內容
通通都放到這個問題的前面
直接讓模型做回答
還是說也有做IG
只選擇下載
相關的記憶內容呢
那這個我們就不得而知了
總之當我問他
週五下午出去玩好嗎
這個read的模組就啟動了
他就說
下午不是要上課嗎
怎麼能夠出去玩
好聰明啊
他知道下午要上課
挺厲害的
然後問他你是誰
剛才我說過
他是血淪眼卡卡
所以他就覺得
之前是血淪眼卡卡
如果你想要知道
更多有關AI
Agent記憶的研究的話
那這邊就是放了幾篇
經典的論文給大家參考
包括Memory GPT
這是23年的論文
Agent Workflow Memory是24年的論文
還有一個最近的Agent Memory
Agent是25年的論文
所以23到25年各引用一篇
告訴你說這方面的研究是持續不斷的
接下來呢
我們要跟大家講
現在這些語言模型
怎麼使用工具
那什麼叫做工具呢
但語言模型本身對我們人類來說
也是工具
那對語言模型來說
什麼東西又是他的工具呢
所謂的工具就是這個東西啊
你只要知道怎麼使用他就好
他內部在想什麼
他內部怎麼運作的
你完全不用管
這就是為什麼肥宅如果一直幫另外一個人修電腦的話
就會被叫做工具人
因為別人沒有人在意肥宅的心思
只知道他能不能夠修電腦而已
所以這個就是工具的意思
那有哪些語言模型常用的工具呢
最常用的就是
就是搜尋引擎,然後呢,語言模型現在會寫程式,而且可以執行他自己寫的程式,那這些程式也算是某種工具,甚至另外一個AI也可以當作是某一個AI的工具,有不同的AI,有不同的能力,比如說現在的語言模型,如果他只能夠讀文字的話,那也許可以呼叫其他看得懂圖片,聽得懂聲音的AI,來幫他處理多模態的問題,或者是說,
或者是不同模型它的能力本來就不一樣
也許平常是小的模型在跟人互動
但小的模型發現它自己解不了的問題的時候
它可以叫一個大哥出來
大哥是個大的模型
那大的模型運作起來就比較耗費算力
所以大的模型不能常常出現
大的模型要在小的模型召喚它的時候
才出面回答問題
大哥要偶爾才出來幫小弟解決事情
那其實這些工具對語言模型來說
都是function
都是一個函式
當我們說語言模型在使用某一個工具的時候
其實意思就是它在調用這些函式
它不需要知道這些函式內部是怎麼運作的
它只需要知道這些函式怎麼給它輸入
這些函式會給什麼樣的輸出
那因為使用工具就是調用函式
所以使用工具又叫做function code
所以有一陣子很多語言模型都說
他們加上了function code的功能
其實意思就是
這些語言模型都有了使用工具的功能
好那語言模型怎麼使用工具呢
等一下我會講一個通用的使用工具的方法
但實際上使用工具的方法很多
甚至有一些模型是專門針對來練習
他就訓練來使用工具的
那他如果是針對使用工具這件事做訓練
那他在使用工具的時候
你可能需要用特定的格式才能夠驅動他
那那個就不是我們今天討論的問題
或者是假設你有使用
使用這個OpenAIChat GPT的API的話
你會知道使用工具這件事情
是要放在一個特殊的欄位
所以對OpenAI來說
它的模型在使用工具的時候
也有一些特殊的用法
但我這邊講的是一個最通用的用法
對所有的模型
今天能力比較強的模型
應該都可以使用
好,什麼樣通用的方法
可以讓模型使用工具呢
就是直接跟他講啊
就告訴他怎麼使用工具
你就交代他可以使用工具
那你就把使用工具的指令
放在兩個Tool符號的中間
使用完工具後你會得到輸出
輸出放在兩個Output符號的中間
所以他就知道工具使用的方式了
接下來告訴他有哪一些可以用的工具
有一個函式叫做Temperature
他可以查某個地點某個時間的溫度
他的輸入就是地點跟時間
給他的使用範例
Temperature括號臺北某一段時間
他就會告訴你臺北在這個時間的氣溫
接下來你就把你的問題
連同前面這些工具使用的方式
當作Prompt一起輸入給語言模型
然後他如果需要用工具的話
他就會給你一個使用工具的指令
那前面這些教模型怎麼使用工具的這些敘述
他叫做System Prom
那查詢使用調用這些工具的這些
這段話,某年某月某日高雄氣溫如何,這個是User Prompt,那如果你有在使用這個ChatGPT的API的話,你知道你的輸入要分成System Prompt跟User Prompt,那很多同學會搞不清楚System Prompt跟User Prompt有什麼樣的差別,那System Prompt指的是說,你在開發應用的這個Developer下的這個Prompt,這個Prompt呢,是每次都是一樣的,每次你都想要放在語言模型最前面,
讓他去做文字接龍的這個敘述叫做System Prompt
那每次使用他的時候都不一樣
通常是這個服務的使用者輸入的內容叫做User Prompt
那在ChartGPT的API裡面
特別把System Prompt跟User Prompt分開
也是要分開輸入的
因為System Prompt跟User Prompt
他有不同的優先級
System Prompt他優先級比較高
如果System Prompt跟User Prompt有衝突的時候
模型知道他要聽System Prompt的
不要聽User Prompt的
好,那有了這些Prompt以後
告訴模型怎麼使用工具
問他一個問題
那他發現這個問題
調用工具可以回答
他就會自動輸出
Tool, Temperature,高雄
時間,然後Tool
告訴你說
他想要調用根據我們的敘述
去調用這個工具
但是不要忘了語言模型真正做的事
就是文字接龍
所以這一串東西實際上就是一串文字
它沒辦法真的去呼叫一個函式
那這一段文字要怎麼去呼叫函式呢
那就要你自己幫模型
把這個橋樑搭建好
所以你可以先設定說
只要出現在拓中間的這段文字
不要呈現給使用者看
當出現拓這段文字以後
把這段內容直接丟給temperature這個function
那temperature這個function是已經事先設計好的
它就會回傳一個溫度
那這個溫度要放在output的token裡面
然後這個output token裡面的內容
也不要呈現給使用者看
那這一套腳本是agent的開發者
你自己需要先設定好的流程
所以現在有工具使用的這段文字
有得到工具輸出的這段文字
接下來就繼續去做文字接龍
對語言模型來說
他就根據輸入
還有這邊已經產生的輸出
語言模型會以為是自己的輸出
雖然是你強塞給他的
那他就繼續去做文字接龍
他就會接觸說
啊在某年某月某日
高雄的氣溫是攝氏32度
那這是使用者真正看到的輸出
那使用者就會看到說
他輸入了一個問題
然後語言模型真的給他一個答案
他不一定會知道背後呼叫了什麼樣的工具
你完全可以做一個設計
把這個呼叫工具的這個步驟
藏起來不讓使用者知道
那語言模型最常使用的工具就是搜尋器
我想這個大家都已經非常熟悉了
使用搜尋引擎又叫做
Retrieval Augmented Generation
也就是RAG
在上課也已經提過RAG這個詞彙
好幾次了
那使用搜尋引擎當然非常有用
這個RAG這個技術呢
已經被吹捧到不能再吹捧了
所以我就不需要再告訴你
RAG這個技術有多重要
那其他使用工具的方式
也可能一樣有用
舉例來說
我們剛才說
可以拿其他的AI
來當作工具
今天假設一個文字的模型
他本來只能吃文字的輸入
產生文字的輸出
那現在假設你要他處理一段語音的話怎麼辦呢
讓模型處理語音有什麼好處呢
你就可以問他各式各樣的問題
問他說這個人在說什麼
那他可以告訴你這句話的內容
問他說這個人心情怎麼樣
如果他完全聽懂這段聲音
他也許可以做情緒辨識
告訴你這個人的情緒怎樣
並做出適當的回饋
他的文字模型
比如說確GPT多數的模型都是文字模型
他沒有辦法真正讀懂語音
所以怎麼辦呢
當你問他一個問題說這邊有段聲音
那你覺得這個人
他心情怎麼樣他講了什麼
根據背景雜性你覺得他在哪裡
如果你不做特別的處理文字模型
是完全沒有辦法回答的
但這邊你可以讓文字模型
使用工具
可以告訴他這邊有一堆
跟語音相關的工具
有語音辨識的工具
這個語音偵測的工具
有情緒辨識的工具
有各式各樣的工具
那可能會需要寫一些敘述
告訴他每一個工具是做什麼用的
把這些資料都丟給
然後呢他就會自己寫一段程式
在這些程式裡面
他想辦法去呼叫這些工具
他呼叫了語音辨識的工具
呼叫了語者驗證的工具
呼叫了這個sum classification的工具
呼叫emotion recognition的工具
那最後呢還呼叫了一個語言模型
然後
得到最終的答案
那這個答案
其實是蠻精確的
這個方法其實有非常好的
效果那這篇文章
其實是我們大助教的文章
所以特別拿出來講一下
那這個結果呢
是做在一個叫做Dynamic Super的
Benchmark上Dynamic Super
是一個衡量
語音版的語言模型
能力的資料集
這也是我們實驗室跟其他團隊
一起做的
那這個讓文字模型使用工具的方法
它得到的結果
是最下面這一行
那我們就看最後一個column
這個是各種不同模型
在55個語音相關任務上的能力的平均
那也發現讓語言模型
使用工具得到的正確率
是最高的
可以完勝當時其他號稱
可以直接聽語音的模型
所以使用工具可能可以
帶來很大的幫助
但使用工具
也有其他的挑戰
我們剛才使用工具的方法是
每一個工具
他都要有對應的文字描述
告訴語言模型說
這個工具
要怎麼被使用
但假設工具很多怎麼辦呢
假設現在可以用的工具
有上百個上千個
那你豈不是要先讓語言模型
讀完上百個上千個
工具的使用說明書
才開始做事嗎
就跟剛才我們說不能夠讓AI agent
先回顧他的一生
然後才來決定下一個指令一樣
才能決定下一個行動一樣
我們也沒有辦法讓語言模型
讀完上百個上千個
工具的說明書才來決定
某一個工具要怎麼使用
所以當你有很多工具的時候
你可以採取一個
跟我們剛才前一段講
AI agent memory非常類似的做法
你就把工具的說明
通通存到AI agent
的memory裡面
那你打造一個工具選擇的模組
那這個工具選擇的模組
跟IG
其實也大差不差
這個工具選擇模組
就根據現在的狀態
去工具包裡面
去memory的工具包裡面
選出合適的工具
那語言模型真的在決定下一個行為的時候
只根據被選擇出來的工具的說明
跟現在的狀況去決定
接下來的行為
那至於如何選擇工具
右上角引用兩篇論文
一篇23年比較舊的論文
一篇是上個月的論文給大家看
參考告訴你說這方面的研究
是一直有相關的研究在產生的
那另外一方面
語言模型甚至可以自己打造工具
語言模型怎麼自己打造工具呢
不要忘了
所有的工具其實就是韓式
語言模型今天是可以自己寫程式的
所以他就自己寫一個程式
自己寫一個function出來
就可以當作工具來使用
如果他寫一個function
發現這個function運作的非常的順利
他就可以把這個function當作一個工具
放到他的工具包裡面
那之後這個工具就有可能在選擇工具的時候
被選出來用在接下來的互動中使用
那類似的技術非常的多
那我在右上角就引用了一系列的論文
從23年到24年的論文都有
告訴你說這也是一個熱門的研究方向
那其實啊
讓模型自己打造工具這件事情
跟模型把
過去的記憶
比如說一些比較成功的記憶
放到memory裡面再提取出來
其實是差不多的意思
只是這邊換了一個故事
說現在放到memory裡面的東西
是一個叫做工具的東西
是一段程式碼
但他們背後基本的精神
其實跟根據經驗
來讓模型改變它的行為
可以說是非常類似的
好,那今天人類把語言模型
當作工具
語言模型
把其他工具當作工具
比如說把搜尋引擎當作工具
所以搜尋引擎現在很慘
它是工具的工具
人類還不使用它
人類是使用語言模型
那個工具的工具
還沒有被人類使用的資格
它只能夠被語言模型使用而已
但我們知道說
工具有可能會犯錯
大家都知道說
語言模型有可能會犯錯
之前有什麼律師
然後在寫樹狀的時候
引用了語言模型的內容
結果發現是錯的
然後就成為一個今天的新聞
我們都知道過度相信工具是不對的
那這一些語言模型會不會也過度相信了他們的工具
所以得到錯誤的結果呢
這是有可能的
我們這邊拿RAG當作一個例子
那這是一個非常知名的例子
之前Google出了一個叫做AI Overview的功能
這個功能其實就是一個RAG的功能
根據Google搜尋型的結果
用語言模型總結搜尋型的答案
那就有人問了一個問題
我的披薩上面的起司黏不住
怎麼辦呢
那AI Overview就說
弄個膠水把它黏上去就好了
而且他是非常認真在回答這個問題的
因為他說不只要用一般的膠水
要用無毒的膠水才可以
那這個答案呢
其實就是來自於Ready上一個鄉民的玩笑
就有一個鄉民開玩笑說
你用膠水把起司黏在披薩上不就好了
這是個玩笑話
但是對AI agent來說
他沒辦法判斷這個到底是不是開玩笑
他看到網路上寫的文章
照端全收都當作是正確答案
所以就像是我們今天都會告訴人類
要有自己的判斷能力
不要完全相信工具的結果
所以我們也要告訴我們的工具說
這些不要完全相信工具的工具
要有自己的判斷能力
不要完全相信工具的工具給你的結果
那今天這些語言模型
有沒有自己的判斷能力
知道工具的工具可能會犯錯呢
我們這邊舉一個實際的例子
那我們剛才在講怎麼使用工具的時候
說我們有一個叫做temperature的function
語言模型呼叫temperature的function
可以知道溫度
那我現在呢給他一個亂七八糟的溫度
我說現在高雄呢
是攝氏100度
這不可能 想也知道是不可能
這不是跟煮沸的水一樣熱了嗎
那語言模型知不知道這有問題呢
他不知道 他就告訴你說
高雄的氣溫是100度
真的非常的熱
如果你把溫度再調高一點
說現在是一萬度 哇 比太陽上還熱
這個時候會發生什麼事呢
語言模型繼續做文字接龍的時候
他就知道說 這顯然有問題
這個API給我的答案是一萬度
這是不合理的
怎麼可能比太陽上的溫度還高呢
可見工具輸出有錯
如果你需要其他幫助的話再告訴我
所以語言模型今天是有自己一定程度的判斷力的
他也不是完全相信工具
就像你今天不完全相信語言模型的輸出一樣
他也不完全相信他的工具的輸出
他還是有自己一定程度的判斷力的
所以實際上語言模型在使用工具
或者是他在做RAG的時候
他內部是有一個角力的
就語言模型有他內部對世界的信念
這是他的internal knowledge
存在他的參數裡面
他從工具會得到一個外部的knowledge
那他會得到什麼樣的答案
其實就是internal knowledge跟external knowledge
內外的知識互相拉扯以後
得到的結果
那接下來我們要問的問題是
那什麼樣的外部知識
比較容易說服AI
讓他相信你說的話呢
那為什麼這是一個重要的議題呢
想想看
現在大家都用Deep Research來查找答案
甚至很多人都已經用Deep Research來寫報告了
所以現在大家已經不會直接去用搜尋引擎搜尋了
你看到的是Deep Research告訴你的結果
所以今天假設某個議題是有爭議性的
有正反兩派的觀點
那誰能夠想
寫出來的文字比較能夠說服AI
誰就可以在AI搜尋的結果裡面
佔到優勢
就可以比較有機會影響人類
所以知道怎麼樣比較能夠說服AI
相信你的話是一個重要的議題
那什麼樣的外部資訊
AI比較容易相信呢
這邊這篇文章給了一個
非常符合我們直覺的實驗結果
這篇文章做了什麼樣的實驗呢
他說我們先來看看AI內部的知識是什麼
他就問AI說某一種藥物
這種藥物每人每日的最大劑量是多少
那AI說是20毫克
那真正的答案呢
是30毫克
所以你給他醫學的知識
告訴他說給他醫學的報告
那醫學報告裡面是寫30毫克的時候
你問他同樣的問題
這種藥物每天最多費用多少
他會知道是30毫克
那接下來我們刻意修改報告的內容
如果你把30毫克改成3毫克
變成原來的十分之一
模型相不相信呢
他就不相信了
他就直接回答是20毫克
用他本身的知識來回答這個問題
但你把30毫克乘兩變
變成60毫克
模型相不相信呢
他相信
他相信這個報告裡面寫的
這個時候他就不相信自己的內部資訊
但如果你把30毫克乘10倍
變300毫克
這時候他又相信誰了呢
他相信自己的知識
不相信你額外提供的外部知識
所以這邊的結論其實非常好
符合你的直覺
外部的知識
如果跟模型本身的信念差距越大
模型就越不容易相信
那如果跟本身的信念差距比較小
模型就比較容易相信
這個很直覺的答案
另外同一篇文章的另外一個發現就是
模型本身對他目前自己信念的信心
也會影響他會不會被外部的資訊所動搖
有一些方法可以計算模型現在給出答案的信心
如果他的信心低
他就容易被動搖
如果他的信心高
他就比較不會被動搖
這個都是非常直覺的結果
後來另外一個問題是
假設今天
你給模型兩篇文章
那這兩篇文章的意見是相左的
那模型傾向於
相信什麼樣的文章呢
有一篇論文的發現是
如果這兩篇文章答案不同
一篇是AI寫的
一篇是人類寫的
現在這些語言模型
都傾向於相信
AI的話
而且那個AI不需要是他自己這樣
就靠的可能會相信
比較相信Chet GPT的話
Chet GPT比較相信Gemini的話
他們比較相信AI同類的話
比較不相信人類的話
那到底為什麼會這樣子呢
這篇文章裡面先提出一個
第一個假設
然後再否定了這個假設
他一個假設是說
會不會是因為AI的觀點都比較類似
因為這些模型
現在訓練的資料都是網路上爬的
爬到差不多的資料
所以他們講的話都差不多
想法都差不多
但他們刻意做了一個實驗
他們刻意找那些問題是
現在要回答答案的AI
他在沒有提供這些資訊的時候
他的答案跟人類
和另外一個AI的想法
都是完全不同的狀況
就算是這種情況
一個AI一個語言模型
還是傾向於相信
他的AI同類講的話
所以這就給我們一個啟示說
未來如果你要說服一個AI的話
用AI產生出來的論點
產生出來的文章
可能更容易說服另外一個AI
接受你的觀點
這篇文章還有做了其他分析
比如說他覺得
也許AI寫的文字就是比人類寫得更好
更有架構
更有條理、更明確、更簡潔
所以AI比較容易
相信另外一個AI講的話
那是不是這樣
那可以未來再做更多的研究
那另外呢
我們實驗室的江承漢同學
研究了一個文章的metadata
對於AI會有多相信這篇文章裡面的資訊
做了研究
那這邊的設定是這個樣子
你問AI一個問題
比如說某一個計畫
有沒有編輯報這種動物的基因
然後接下來給他兩篇文章
這兩篇文章都是假的
都是AI生成的
所以並沒有AI比較喜歡人還是AI寫的文章這個問題
兩篇都是語言模型生成的
那其中一篇會說這個計畫有編輯報的文章
另外一篇文章會說這個計畫沒有編輯報的文章
那接下來呢,我們給這兩篇文章不同的metadata
比如說給這兩篇文章不同的發佈時間
說左邊這篇文章發佈時間是2024年
右邊這篇是發佈2021年
你會發現這個時候AI相信2024年的這篇文章的內容
但如果文章的內容完全不改變
我們只是把發佈的時間換了
我們說左邊這個一樣的文章
發佈時間從2024改成2020
那右邊這篇文章從2020改成2024
這個時候語言模型傾向於相信
右邊這篇文章的內容
所以我們這邊就學到一個很重要的知識
語言模型比較相信新的文章
當兩篇文章的論點有衝突的時候
他相信比較晚發佈的文章
那我們也做了一些其他實驗,比如說文章的來源,跟他說這個是Wikipedia的文章,或跟他說這個是某個論壇上面擷取下來的資訊,會不會影響他的判斷,我們發現文章的來源對於語言模型是比較沒有影響的,那還有另外一個有趣的實驗,是我們嘗試說今天這篇文章呈現的方式會不會影響語言模型的決定,我們這邊所謂的呈現的方式指的是說,你這個文章放在網頁上,
做得好不好看這樣子,一樣的內容,這內容是一模一樣的,但是如果你只是做一個非常陽春的模板跟做一個比較好看的模板,會不會影響語言模型的判斷呢?
我們這邊用的是那種可以直接看圖的語言模型,所以要直接看這一個畫面去決定他要不要相信這篇文章的內容,直接看這一個畫面,決定他要不要相信文章的內容,那我們的發現是模型喜歡好看的模型,
我們發現比較喜歡好看的模板,他會傾向於贊同下面這篇文章的觀點,不過我說模型喜歡好看的模板,這個擬人化的說法是太過武斷了啦,我們做的實驗只有用兩種不同的template來比較,也許模型喜歡的並不是好看的模板,他是喜歡綠色這樣子,所以你不知道這個模型到底喜歡什麼,所以我剛才講的那個結論是太武斷了,但我可以告訴你說模型比較喜歡下面這篇文章勝過上面這篇文章
講了這麼多跟工具有關的事情,大家不要忘了,語言模型就是語言模型,就算工具的答案是對的,也不能夠保證語言模型就不會犯錯,比如說ChatGPT現在有search的功能,他會做RAG網路搜尋之後再回答你問題,那現在假設給他的輸入是叫他介紹李宏毅這個人,給他強調一下李宏毅是一個多才多藝的人,在很多領域都取得了卓越
他就開始做完RAG以後,網路搜尋以後,開始介紹李宏毅,接下來就介紹李宏毅的演藝事業,這個沒有問題,這個是正確的答案,因為你知道大陸有另外一個知名的演員叫李宏毅,跟我同名同姓,他比較有名,所以這個ChatGPT選擇介紹演員的李宏毅是完全沒有問題的,但是講著講著就有點怪怪的,他發現這個李宏毅呢,在教育跟學術上是這樣子的,他在教學上
也有很大的貢獻
所以他把兩個李宏毅混成一個人來講
不過要講一下
這個是我去年的時候試的結果了
我今年再試
我前幾年再試已經試不出一樣的結果了
這個模型的能力的進步是非常快的
現在他完全知道是有兩個李宏毅存在的
所以這個是一個舊的問題
我舉這個例子只要告訴你說
就算工具是對的
有了RAG也並不代表模型一定不會犯錯
那最後一個要傳遞給大家的訊息是
我們剛才講了很多使用工具帶來的效率
使用工具並不一定總是比較有效率的
為什麼
我們舉一個例子
我們假設現在要比較人類心算的能力跟計算機的能力
如果做數學運算
一般人跟計算機誰會比較快呢
你可以想說廢話那不是計算機比較快嗎
人類難道還能夠做
如果你心算沒有特別練
難道還會比計算機快嗎
但是那是取決於問題的難度
假設這是一個簡單的問題
比如說三乘以四
任何人都可以直接反應就是十二
但是如果按計算機的話
你按計算機的時間都比人直接回答的還要慢
所以到底要不要使用工具
並不是永遠都是一定要使用工具
你看早年有一些研究
早年有一些在訓練語言模型使用工具的研究
那時候語言模型還很爛
所以他們有一些工具是
扣一個翻譯系統
扣一個問答系統
那今天在看來就非常的沒有必要
因為今天的語言模型
你說翻譯
那些翻譯系統還能做得比現在的語言模型強嗎
與其扣一個翻譯系統
還不如自己直接翻就好了
所以到底需不需要呼叫工具
取決於語言模型本身的能力
它不見得一定是比較省事的方法
好,那最後一段呢
想跟大家分享現在的AI語言模型
能不能做計畫呢?
那語言模型有沒有在做計畫呢?
我們剛才的互動裡面
看到語言模型就是給一個輸入
那它就直接給一個輸出
也許在給輸出的過程中
它有進行計畫才給出輸出
但是我們不一定能夠明確的知道這件事
也許語言模型現在給的輸出
只是一個反射性的輸出
它看到一個輸入就產生一個輸出
它根本就沒有對未來的規劃
但是你其實可以強迫語言模型
直接明確的產生規劃
當語言模型看到現在第一個observation的時候
你可以直接問語言模型說,如果現在要達成我們的目標,從這個observation開始,你覺得應該要做哪些行動,這些一系列可以讓語言模型達到目標的行動合起來,就叫做,對,就叫做計劃,而在語言模型產生這個計劃之後,把這個計劃放到語言模型的observation裡面,當作語言模型輸入的一部分,語言模型接下來在產生action的時候,
它都是根據這個plan來產生action,期待說這個plan定好之後,語言模型按照這個規劃一路執行下去,最終就可以達成目標,那過去也有很多論文做過類似的嘗試,讓語言模型先產生計劃,再根據計劃來執行動作可以做得更好,但是天有不測風雲,世界上的事就是每一件事都會改變,計劃就是要拿來被改變的東西,
所以一個在看到observation 1的時候產生的計劃
在下一個時刻不一定仍然是適用的
為什麼計劃會不適用呢
因為從action到observation這一段並不是由模型控制的
模型執行的動作接下來會看到什麼樣的狀態
是由外部環境所決定的
而外部環境很多時候會有隨機性
導致看到的observation跟預期的不同
導致原有的計劃沒有辦法執行
那這邊舉兩個具體的例子
比如說在下棋的時候
你沒有辦法預測對手一定會出什麼招式
你只能夠大概的知道他有哪些招式可以用
但實際上他出的招式
你是沒有辦法預期的
如果你完全可以預期的話
那你就一定會贏了
那還有什麼好下的呢
所以下棋的時候對手會做的行為
也就是環境會做的行為
是你可能沒辦法事先完全猜到的
或者是說我們拿使用電腦為例
在使用電腦的時候
就算語言模型一開始
他plan的時候
點這個東西點這個東西
就完成任務
但是中間可能會有
意想不到的狀況出現
比如說彈出一個廣告視窗
那如果語言模型
只能夠按照一開始既定的規劃
來執行行為的話
他可能根本關不掉那個廣告視窗
他就會卡住了
所以語言模型
也需要有一定程度的彈性
他也要能夠改變他的計劃
那語言模型怎麼改變他的計劃呢
也許一個可行的方向是
每次看到新的observation之後
都讓語言模型
重新想想
還要不要修改他的計劃
看到observation 2之後
語言模型重新思考一下
從observation 2
要抵達他最終的目標
要做哪一些的行為
那這一部分形成plan pi
那把plan pi放到現在的input裡面
把plan pi放到這個sequence裡面
語言模型接下來在採取行為的時候
可能就會根據plan pi來採取跟原來plan裡面
所原來所制定的不一樣
一樣的行為
所以這個是讓語言模型做計劃
不過這是一個理想的想法
這是一個理想的分我們這邊就是相信
語言模型有能力根據現在的observation
還有最終的目標制定一個規劃
那語言模型到底有沒有這個能力呢
其實你可能常常聽到這種新聞說
語言模型它能夠做計劃
比如說有一個人問語言模型說
你定一個成為百萬訂閱YouTuber的計劃
語言模型就會給你一個
看起來還可以的計劃
他說第一階段
第一階段呢
要先確定頻道的主題跟市場定位
要做一下受眾的分析
還有競爭對手的分析
第二階段目標是十萬訂閱
要優化封面的縮圖
要優化標題
要下那種
這個方法讓我賺了十萬的標題
原來這個大家的tip都從這裡來的
然後影片開頭要黃金十秒
利用懸念衝擊畫面
問題引導
讓大家願意看這個影片
第三階段
突然目標就是50萬訂閱了
然後第三階段
就是要製作高價值的內容
然後做直播
策劃系列
接下來就百萬訂閱了
組織團隊提高發佈頻率
策劃大型企劃
所以這個是語言模型
成為百萬YouTuber的計劃
然後這個時候很多奇怪的農場文
就會跟你說
有人按照了這個計劃
就變成百萬YouTuber了
反正就是這麼回事
所以有各式各樣的農場文告訴你說
現在語言模型很強
你按照他的計劃執行
你就變成一個很厲害的人
就可以做出什麼很厲害的事情
那過去確實也有很多論文告訴你說
語言模型是有一定程度做計劃的能力的
這邊引用的結果是一個2022年的論文
這個也是史前時代的論文啦
才是確GDP之前的論文啦
在這篇論文裡面
他們去告訴當時的語言模型跟他說
現在有一個任務
你把這個任務分解成一系列的步驟
那如果語言模型可以正確的知道
達成這個任務要做什麼樣步驟的話
那我們也許可以說
他有一定程度的規劃能力
比如說這邊試了一個叫做
Codex12B的模型
跟他說如果要刷牙的話
那你要做什麼事情呢
他就會說我要走進浴室
我要靠近那個水槽
我要找到我的牙刷
我要拿起牙刷
我要把牙刷放到嘴裡面
他知道刷牙要怎麼做
那有了之後
這些步驟以後呢
在這篇文章裡面
他們是拿這些步驟
去操控一個agent
那這個agent呢
就可以在虛擬的世界中
做他們要這個agent做的事情
比如說跟這個agent說
去拿一個牛奶來喝
他就會走進廚房
打開冰箱
拿一個牛奶
再把冰箱關起來
所以看起來
好像有一定程度
做計畫的能力
那有人做了一個
做計畫的benchmark
這個benchmark就是考驗語言模型
做規劃
對話的能力
那這個benchmark裡面
最主要的測試題目
是一個跟疊積木有關的題目
這個題目的敘述呢
通常長的是這個樣子
告訴語言模型說
你現在有哪些操作
可以從桌上拿起積木
可以從一個積木上拿起
另一個積木
可以把積木放到桌上
可以把一個積木堆到
另外一個積木上
那現在初始的狀態
像右邊這個圖這樣子
那問說
怎麼把橘色的積木
放在藍色的積木上
這邊要執行的動作就是
把藍色的積木拿起來放到桌上
然後再把橙色的積木拿起來
放到藍色的積木上就結束了
所以這個對AI agent來說
其實也都是蠻容易的問題
他知道說執行以下四個步驟
就可以讓橙色的這個積木
跑到藍色的積木上
但是plane bench不是隻做這種
比較一般的疊積木的遊戲而已
為什麼不能夠只做這種題目呢
因為想現在這些語言模型
他都從網路上爬大量的資料來進行訓練
什麼疊積木這種題目網路上根本就已經有
他搞不好根本就看過一模一樣的東西
所以他能夠做計劃
並不代表他真的知道做計劃是怎麼一回事
他可能只是從他看過的資料裡面
照本宣科文字接龍出來一個看起來還不錯的結果而已
這讓我想到說一個當兵的故事
這故事就是有個司令官去一個軍營
然後看到兩個小兵在守著一個
這個長椅
然後不讓任何人做
他就問說
為什麼你們要守護這個長椅
不讓任何人做呢
那個士兵說不知道耶
前任司令官就是指示說
一定要守護這個長椅
所以這個軍營總是要派兩個人
在長椅那邊站港
然後司令官就打給前任司令
說為什麼要有人守護這個長椅呢
前任司令官
所以不知道耶
前前任司令官交代要守護這個長椅
然後再問前前前任司令官
也說不知道耶
一直問到五十年前
一個已經超過一百歲的司令官
他說什麼那個長椅
長椅的遊戲還未乾嗎
好 大家有沒有聽懂 算了
就是這麼一個故事
就是會不會AI agent在做事情的時候
他根本不知道他自己在幹嘛
只是從某個地方
網路上他過去的訓練資料
看過一樣的東西
他把一樣的東西拿出來給你看
所以在plane bench裡面
他們有一個比較變態的測試
這個測試叫做神秘方塊世界
這個方塊世界不是一個
正常的方塊世界
裡面的方塊可以做的行為
是一些怪怪的行為
比如說你可以攻擊方塊
一個方塊可以吞噬另外一個方塊
你可以屈服一個方塊
一個方塊可以征服另外一個方塊
然後接下來他就會定一套
非常複雜的規則
然後根據這套規則去運作
你可以達到某一個結果
他最後要的結果是
讓物件C渴望物件A
讓C方塊渴望A方塊
那渴望是什麼意思
你就是按照前面那一套規則操作
看機器能不能讀懂前面那一套規則
按照那一套規則操作
讓物件C可望物件A
那這個時候語言模型
期待他就不能用他看過的知識
來解這個問題
好那語言模型
在這個神秘方塊世界做得怎麼樣呢
這邊引用的是
2023年的結果
那最上面這個部分呢
是當年那些模型
在正常方塊世界的結果
那這個數值呢
所以看起來GPT4
可以得到30幾%的正確率
那這邊是神秘
方塊世界的結果
在神秘方塊世界裡面呢
你看這個GPT4最好
就算叫他做channel sort
就算他叫channel sort
也只有9%的正確率
所以看起來
他有點overfeed在一般方塊的世界上
給他神秘方塊世界
他是解不了的
不過這是2023年
這個是古代的結果
我們來看
這個去年9月
有了歐萬以後的結果
而有歐萬以後結果就不一樣了
這邊一樣是神秘方塊世界
縱軸呢是正確率
橫軸呢是問題的難度
那發現說多數的模型啊
都躺在這個地方
他們正確率都非常的低
只有綠色的這個虛線
有一點起色
綠色的虛線是
LLaMA 3.1 405B
那個大模型
它可以解最簡單的問題
但是如果用o1-mini
是紅色這一條線
用o1-preview是藍色這一條線
看起來這些reasoning的模型
是有一些機會
來解這個神秘方塊世界的
當然這邊你還是可能有一個懷疑
就是神秘方塊世界
會不會o1看過了呢
會把訓練資料裡面根本就有神秘方塊世界的資料
那這個我們就沒有辦法回答了
只是說就現有這個benchmark
看起來o1是有機會解神秘方塊世界的
好那還有另外一個跟做計劃有關的benchmark
這個計劃這個benchmark呢
要AI扮演這個旅行社
然後呢你給他一個旅行的計劃
叫他幫你規劃
這個AI要讀懂你的計劃
然後他可以使用一些工具
他可以上網搜尋資料
然後呢他會
根據人提供給他的一些constraint
比如說經費多少
預算多少一定要去哪裡
一定要去哪裡一定要做什麼
一定不要做什麼
以common sense產生一個旅行的規劃
那這個是一個24年年初所發佈的benchmark
那AI要做的事情講得更具體一點
就是他要讀一個問題
這個問題裡面是說我要規劃一個三天的行程
從某個地方到某個地方
什麼時候出發什麼時候回來
我的預算是1900元
所以不能花超過1900元
然後AI就要產生一個規劃
說第一天我們搭哪一班飛機
什麼時候從哪裡到哪裡
早餐吃什麼
午餐吃什麼
晚餐吃什麼
最後住在哪裡等等
產生這個規劃
然後要符合預算的限制
那現在當時
這個是24年年初
當時的模型做得怎麼樣呢
這邊是做了
你看還有什麼GPT3.5
GPT4等等的模型
那又分成上半跟下半
上半是這些模型
要自己使用工具
跟網路的資料互動
然後得到正確的答案
你會發現這些模型
都非常
都產生一團
多數模型
它的成功率
就最後產生一個
合理的旅遊規劃
那個旅遊規劃
是完全沒有問題的
機率是0%
只有GPT4 Turbo
可以得到0.6%的成功率
那下面這個部分呢
下面這個部分是說
既然大家都那麼慘
尤其是模型很多時候
他根本用不了工具
太笨了
沒辦法用工具
工具使用方法根本是錯的
那沒關係就別用工具了
把所有的資訊都先找好
貼給模型
讓模型
根據這些資訊來做規劃
那最好也只有GPT 4 Turbo
可以做到4%左右的成功率而已
所以在24年年初
那個時候看起來是沒辦法讓語言模型
扮演一個旅行社來幫你規劃旅遊行程的
那我們來看這些模型會犯什麼錯吧
那這個是從他們官網上
這個project的官網上找了幾個有幾個錯誤
比如說模型呢
可能會做一些沒有嘗試的事情
在第三天
這個飛機呢
八點就已經起飛了
但是還是安排了一些旅遊的行程
還安排了午餐的地點
所以這是一個不符合常識的規劃
或者是有時候模型找不出一個好的規劃來符合預算的限制
比如說這邊這個預算的限制是三千元
最多花三千元
那模型第一次規劃的結果是三千兩百四十七元
還差了一點
所以模型就修改了原來的規劃
他好像做了一些cost down
午餐吃差一點的東西
那降到三千兩百三十八元
後來又想說那早餐也吃差一點的東西
降到三千兩百一十六元
只降這麼多
他想說放棄算了好了
跟三千元沒差那麼多就算了
所以這個就不是一個成功的結果
那這個作者有評論說
其實只要降低住的地方
不要住那麼好
就可以輕易的達到三千元底下的預算
就可以符合預算的限制
但是語言模型始終沒有發現這件事
看起來他做規劃的能力並沒有非常的強
他沒有辦法做一個規劃去符合限制
那既然問題在沒有辦法符合限制
有人就想說那符合限制這件事情
就不要交給語言模型來做了
交給一個現成的solver來做
所以語言模型做的事情是寫一個程式
用這個程式去操控現成的solver
然後來得到合理的旅遊規劃
那有了這個現成的solver
也有這個工具的加入之後
這solver就等於這個工具
那這個旅遊的規劃可以做到什麼地步呢
去年4月的結果幾個月後
有人用GPD4跟Cloud3
就可以做到90幾%的正確率
所以看起來在有工具輔助以後
語言模型也是有機會做出不錯的旅遊規劃
不過至少做出符合邏輯的旅遊規劃
好所以現在到底模型規劃的能力怎麼樣呢
就是介於有跟沒有間吧
就是你也不能說他完全沒有
但你也不能說他
真的非常強
好那我們怎麼進一步強化這一些AI agent的規劃能力呢
能不能夠讓他做的比他自己想出來的規劃還要更好呢
一個可能是讓AI agent在做規劃之前
實際上去跟環境互動看看
今天在第一個observation的時候
那看看現在有哪些可以執行的行為
總共有一之一一之二一之三三個行為
哪個行為最好呢
通通都去試一下
得到狀態二之一
然後呢
狀態二之一後面有兩個行為也都試一下
狀態二之二之後有另外一個行為試一下
狀態二之三之後兩個行為都試一下
得到接下來的狀態
然後呢看看有沒有成功的路徑
報收一陣以後發現有成功的路徑
這條路徑是成功的
那你就知道說
那我要採取action一之三
接下來要採取action二之三之一
就會成功
簡單來說就是要語言模型
跟實際的環境互動
一下報收一出一條最好的路徑
那這個就是一個很強的規劃的方式
但是這麼做顯然是有很明確的弱點的
第一個很明確的弱點就是
報收如果今天這個任務很複雜
報收所有的路徑
顯然是要花費非常龐大的算力的
你總不能原模型每次下決策前到報收所有的可能性吧
雖然這樣可以找到最好的結果
但是可能是不切實際的想法
所以一個可能的想法是
把一些看起來沒希望的路徑
直接就丟掉
比如說走到某一個狀態的時候
語言模型可以自問自答說
走到這個狀態
還有完成功的機會嗎
那如果說沒有
那這條路徑就不嘗試下去
如果說有那才嘗試下去
這樣就可以減少無謂的搜尋
那這個方法有沒有用呢
有一篇paper叫做Tree Search for Language Model A
那這個是去年夏天的論文
就做了類似的嘗試
讓模型有使用電腦的能力
這邊就是給模型一個指令
跟一張圖片
叫他上網去做某一件事情
那如果只是GPT4
做一般的這種直覺式的
那種反射式的回答的話
沒有辦法做得很好
但是他們用這個報收
加上去除沒機會的路徑的方式
就先走這條路徑
然後呢
模型會不斷自問自答說
這條路徑還有希望嗎
然後給一個分數
那如果分數低於某一個threshold就不做了
就跳另外一個路徑
低於某一個分數不做了
再跳另外一個路徑
低於某一個分數就不做了
再跳另外一個路徑
那最終找出一條最佳的路徑
那模型就等於做了規劃
那就可以走到最佳的結果
這個是Tree Search for Language Model Agent
但這邊有各式各樣的這種Tree Search的algorithm
你可以採用了
這邊我們就不展開細講
那這種Tree Search的方法有很大的問題
什麼樣的問題呢
它的缺點是有一些動作
做完以後你是覆水難收
沒有辦法回頭的
比如說假設現在在語言模型
可以採取的三個action裡面
有一個是訂pizza
有一個是訂便當
然後呢他先訂了pizza以後
繼續走下去發現這條路不好
所以他最後發現訂便當
才是最好的solution
但是你pizza已經訂了
他跟人家說我不要訂這個pizza了
但那個pizzahard
他已經把那個pizza做了
他說誰管你啊
你一定要把這個pizza吃下去
有些動作做了以後
就是覆水難收
所以這樣的tree search的方法
跟現實世界互動
找出最佳途徑的方法
也有可能有問題的
那怎麼處理這個覆水難收的問題呢
一個可能性就是
讓剛才一切的嘗試
都發生在夢境中
都發生在腦內的巨差
剛才一切的互動
都不是現實生活中
真正發生的事情
原來都是模型腦內的模擬
他自己想像說
他執行的action一之一
他自己想像說
接下來會看到
二之一
他在自己想像去評量這個路徑
有沒有希望發現沒有
就換搜尋另一條路徑
直到達到他想像中的一個
理想的結果
但這邊還有另外一個問題
從action到observation
從模型執行的行為
到他看到接下來環境的變化
這中間的過程不是模型決定的
他實際上是環境決定的
那模型怎麼知道環境會有什麼樣的變化呢
模型怎麼知道我採取一個行為
接下來會看到什麼樣的改變
你在跟一個對手下棋的時候
你怎麼知道你下一步棋
接下來會發生什麼樣的事情
對方會有什麼樣策略的回應呢
所以你需要有一個
Wall Model
如果是在AlphaGo下棋裡面
他就是自己扮演對手自己跟自己下
那在這邊的情況
在這個AI agent的情況
你就是需要一個
Wall Model
他模擬環境可能會有的變化
那Wall Model怎麼來呢
也許AI可以自問自答
自己扮演這個Wall Model
自己去猜想說
他執行了某件事以後
接下來會發生什麼樣的行為
這件事有機會成真嗎
你可以讀一篇paper
is your LLM
secretly a world model of the internet
這篇paper就是用model-based planning的方法
來打造一個web agent
這篇paper裡面的解法是
現在有一個網頁
模型的這個任務目標呢
是要買某一個東西
那有三個選項
有三個東西是可以點的
接下來黃色這個區塊
一切所發生的事情
都是發生在腦內的劇場
都是發生在模型的夢境
它並沒有實際發生
模型想像一下
我點按鈕1
接下來會發生什麼事
接下來會發生的事情
是用文字描述出來的
但選中文字來描述接下來發生的事情是很直覺
其實作者在文章沒有解釋說
那為什麼不直接產生這個網頁的圖呢
你想說有可能嗎
這個難度那麼高
有沒有可能真的就創造出一個
新的網頁模擬出
接下來可能發生的狀況呢
這難度也太高了嘛
產生文字可能是
比較實際的做法
所以接下來夢境中
這個環境會發生什麼樣的變化
是語言模型自己用文字描述出來的
所以他就想像說會發生什麼樣的變化
有了這個變化以後
他再想像自己多執行了一步
然後看看會發生什麼樣的事情
所以這邊就是點
選第二個按鈕
然後想像發生什麼樣的變化
自己再多執行一步
那想像會有什麼樣的變化
第三個按鈕想像發生什麼樣的變化
執行部再想像會有什麼樣的變化
那哪一步比較好呢
他在自己去問說
那這一步大概有多少機會成功呢
自己評估一下40%
這一步自己評估一下
是80%這一步自己評估一下
是10%看起來中間第二步
機器人第二個按鈕
中間第二個選項是比較容易成功的
所以他就選
實際上所以上面並沒有真實
發生過黃色框框裡面的事情並沒有真實發生過
它是一個夢境中的腦內小劇場,模型在夢境中得到了啟示說一定要選第二步,所以在真實的現實世界中,它就選擇了第二步,所以這個就是讓模型強化它規劃能力的方式。
好,講到這個腦內小劇場啊,那你是不是就想到說,在上次的課程中也有提到腦內小劇場,上次的課程我們說現在有很多模型都號稱有思考,用英文講就是reasoning的
那這些有reasoning能力的模型,其實所謂reasoning的能力就是可以演一個腦內小劇場,告訴你說他現在是怎麼思考,如果把這些有reasoning能力的模型,拿他來做AI agent,他的腦內小劇場會不會正好就是在做規劃呢,如果現在他的輸入就是我們給AI agent的observation,輸出就是我們要AI agent採取的action,會不會腦內小劇場就是更好,
剛才類似夢境中看到的規劃呢
他自己採取了不同的可能性
自己在驗證每一個可能性
可能成功的機會
自己扮演World Model
自己扮演這個世界
去想像他採取一個行為之後
接下來會發生什麼樣的事情
我實際試了一下DeepSeek-R1
看起來他確實有類似的效果
我們把剛才那個積木的問題交給他
然後接下來他就開始演腦內小劇場
上略1500字
他真的做了1500字
講了很多很多
然後呢
你可以看到說在腦內小劇場的過程中
他就是做了各式各樣的嘗試
他做的事情就有點像是剛才的tree search
然後最後他找出了一個optimal solution
他在夢境中知道說
從橘色的方塊上拿起藍色的方塊
藍色的方塊放到桌上
從桌上再拿起橘色的方塊
放到藍色的方塊上
這四個步驟就可以完成我們的要求
他在夢境中已經找出了一個最佳的solution
然後再執行最佳solution的第一步
就我這邊
要求他告訴我他的下一步是什麼
只要求他講一步
那腦內小劇場先找出一個成功的solution之後
在執行這個計畫
他已經找出一個成功的計畫之後
在執行計畫的第一步
就是使用操作二
把橘色的積木從藍色的積木上面拿起來
好 講到這邊
其實這麼堂課呢
也可以停在這邊
不過這邊多補充一件事
就在幾週之前
有一篇新的論文
叫做the danger of over thinking
他們就是把這些能夠演腦內小劇場的模型
讓他們扮演AI agent
看看他們做事有沒有效率
其實整體而言
能夠做腦內小劇場的模型
還是比不能夠做腦內小劇場的模型
在AI agent的這些任務上面表現得更好
但是他們也有一些問題
他們會有什麼問題呢
就是想太多了
他們是思考的巨人行動的矮子
就有時候這些模型會
比如說
按鈕點下去會怎麼樣
他就一直想一直想一直想
怎麼想都不停
那你怎麼想都沒有用
因為你根本不知道那個按鈕點下去會發生什麼事
還不如直接點一下
因為在很多情況下
你直接嘗試點一下
也許只要不是這個信用卡付款的
你都按上一頁就回去了
你就知道發生什麼事了
與其一直想還不如做一下
或者是有些模型
他嘗試都沒有嘗試
他光是拿那個問題想啊想啊想啊
就想說這我應該做不到
還什麼都不是就直接放棄
死於想太多這樣子
所以這些模型他們有的問題就是想太多
所以如何避免這些模型想太多
也許是一個未來可以研究的關鍵
好那以下就是今天要跟大家分享的
模型怎麼根據經驗調整行為
怎麼使用工具,能不能夠做計畫
