臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw/
我們就接著進入第二節課的內容
大家聽得到聲音嗎
現在接著進入第二節課的內容
大家現在聽得到聲音嗎
我就先回答一下剛剛有人問的問題
有人問說 GNN 有沒有
有沒有 library
有一個 library 叫做 deep graph library
他有很多個已經寫好的
剛才講很多 model 裡面都有 implementation
有其他的資源，最後結束的時候我把投影片整理一下
附在投影片最後面讓各位看一下，如果有興趣的話可以看一下
有些很有爭議，待碼，我整理一下再把他放上去
我就進入第二節課的內容
第二節課的內容比較神奇一點
希望各位可以，各位要撐住
接下來我們這節課要講的是 graph signal processing
還有 spectral-based graph neural network
為了怕大家忘記我們要做什麼
再再次複習一次，我們要做的事情就是說
在 CNN 裡面會做 convolution
convolution 做的事情就是給你一個 kernel
你要去學出一個 filter
給定一個 kernel size 是 3 x 3
你要學出一個 3 x 3 的 filter
它可以在這上面做 convolution
問題是假設說今天要做的事情是要在 graph 上面做到 filtering
要怎麼達到這件事情
我們有沒有辦法放一個 3 x 3 的 kernel 在 graph 上面
期待說這個 3 x 3 的 kernel
放在這個節點上面
他就可以看到他九宮格裡面的元素
這好像是不太合理的
因為你在 graph 上面根本沒辦法定義說他的九宮格在哪
那些是他的九宮格裡面的鄰居
我們想到的一個做法就是
有人就這樣想，有人說
我們有沒有辦法
把一個 graph 上面的 signal
因為它這些 feature 你可以把它看成是 graph 上面的 signal
我們把這些 signal 透過 fourier transform
同時我們把某一個叫做 filter 的東西
也對他做 fourier transform
我們知道在 fourier domain 裡面
如果你在原本這個 domain 要做 convolution
在 fourier domain 裡面他做的事情其實就是
只要把他們兩個乘起來做 multiplication 就好了
如果我們可以直接對他做 multiplication
乘完之後的結果
再來做 inverse fourier transform
我們就可以得到我們要的這個經過 filter 的信號
問題是這些東西要怎麼做呢
為了要解釋這個我們必須要引入一些數學
雖然說講他是數學有點不太對，因為他只是信號與系統的東西
所以等一下會有大量的信號與系統出現
我以前修的是李琳山老師的信號與系統
所以我等一下講的東西大部分都是從李琳山老師
他在講信號與系統這堂課裡面
所用到的一些簡報內容
我們就來看大家很熟悉的合成與分析
我們說一個信號
我們可以把它看成是一個在 N 維空間裡面的一個向量
這個向量我們知道在向量空間裡面
它是由一組 basis 經過線性組合合成的
所以說一組信號我們可以看成是一組 basis 的合成
也就是他的線性組合
當我們想要知道這個線性組合裡面每一個不同的 component 他的 ak 大小是多少的時候
就要用分析
分析的方式就是我用整個信號
或是整個 vector
跟某一個特定的
basis 某一個特定的成分去做內積
得到的東西就會是他這個 component 的大小
所以這就叫分析
當然我們現在是假設說這組 basis 他是一組 orthonormal basis
在 time domain 上面有一個，或是說我們常用的一組 basis 就是
cos 跟 sin 的這組 basis
假設對於一個周期性的信號我們可以把他展開是一個 fourier series
選用了這組 basis 就是 e^(jkω0t) 的這一組 basis
他的不同 component 大小由這個 ak 來決定
所以我們就有些方式可以算出 ak 他的大小是多少
這就是 fourier series
我們說一組信號
在時域的一組信號
我們可以看成他是時域的 basis 經過
線性組合之後，合成產生的
在 time domain 的這組 basis 是誰
就是站在時間軸一排的 delta function
這排 delta function 就是 time domain 的這組 basis
所以他透過這一堆 delta 的線性組合
合成了整個 time domain 的某一個信號
同時我們也知道一組信號
一個信號或是一個向量
一個向量空間他可能不只是有一種 basis
它可以有很多不同的 basis
所以除了用 vj 組 basis 來表示，也可以用 uj 組 basis 來表示
就可以把同樣這個信號寫成是另外一個 basis 下面展開出來的一組信號
大家也一定很熟悉，我們用的就是
e^(jωt) 這組 basis
這個就是他的係數
前面這個 x(jω) 就是他的係數
這就是他的 basis
同一個信號可以用不同的方式來做合成
fourier transform 就是
如果我想要找說這個 x(jω) 他是多少的話會用 fourier transform
這就是一個他的 basis
這整個就是一個 inner product
就是一個分析的過程
接下來講 spectral graph theory
spectral graph theory 我們先定義一些符號
第一個給定一個 graph
他是一個由 V 跟 E 來代表的
V 的數量，也就是 vertice 的數量
節點的數量用 N 來表示
給你一個 graph
給你一個 adjacency matrix
你會拿到一個他的 adjacency matrix
一個 adjacency matrix 就是告訴你說哪些節點是有相連的
adjacency matrix 或者說他是 weight matrix
這個 weight matrix 他如果這個 entry 是 0 代表說這兩個節點並沒有連在一起
如果這兩個節點中是有 edge 連在一起的時候
這個 Ai,j 就是這兩個節點的 weight
這兩個節點之間的 edge 的 weight
現在先只考慮 undirected graph
也就是 adjacency matrix 一定是一個對稱矩陣的狀況
我們有 adjacency matrix 那我們也會有另外一個 matrix 叫做 degree matrix
degree matrix 是一個 diagonal matrix
每一個 diagonal 的項就是這個節點有幾個鄰居
另外所謂的 graph signal 就是這個 graph 上面每一個節點它可以有一個純量
或是他可以有一個向量
來代表這個節點上面的 signal
來實際看一個例子
假設有一個如圖非常簡單的 graph
這個 graph 上面
定義這是 v0 v1 v2 v3
上面可以有一些信號
寫成 f(0) f(1) f(2) 還有 f(3)
這些信號可能可以代表不同的意思
假設他是一個很簡單的一個城市路網圖
每一個節點代表不同的城市
我可以說這個信號可以代表說這個城市的氣溫或人口增長數之類的
看你這個信號要定義成什麼都可以
所以剛才講完了 graph 的
什麼是他的 adjacency matrix，什麼是他的 degree matrix
他的信號是什麼樣的東西
這個信號我們為了簡單起見講他是一個純量
他也可以是一個向量
向量就是把一排全部排在一起，他就變向量
接下來講一下非常重要的一個概念叫做 Graph Laplacian
所謂的 Graph Laplacian
它的定義就是 degree matrix - adjacency matrix
degree matrix 是一個 diagonal 他當然是對稱
我們剛才有講過，我們現在考慮的是一個 undirected graph
所以他的 adjacency matrix 也是一個對稱
所以這兩個相減以後也會是一個對稱矩陣
而且他不只是一個對稱矩陣
他還是一個半鎮定的對稱矩陣
他是一個半鎮定的矩陣
也就是說他所有的 igon vector
都會是大於等於 0 的數
對於一個這樣的矩陣
可以對他做 spectral decomposition
把他降一個矩陣
假設我是一個 4 x 4 的 Laplacian
我就可以把他分解成 U 乘以 Λ 乘以 UT
這個 U 每一個 column 就代表這個
這個 Laplacian 不同的 igon value
這個 Λ 就代表他對應的 igon vector
所以 diagonal 每一項都是 igon vector
U 每一個 column 都是
Λ 是 igon value
U 每一個 column 都是 igon vector
我們在 spectral graph theory 這個理論裡面
把不同的 Λ
這一些 Λ 叫做 frequency
graph 的 frequency
把這一組 igon vector 叫做對應的 basis
這邊先講一下我們選出來的這些 igon vector 要是 orthonormal
他一定是正交
但是我們要求他強度一定是 1
到這個地方應該都是有教過的
接下來先看一個簡單的例子
一樣是剛才這張圖
這張圖假設我告訴你說上面的信號是 f(0) f(1) f(3) f(4)
f(0) f(1) f(2) f(3) 長這個樣子
我可以把他寫成一個矩陣的形式
矩陣的這個 column vector 第一個 entry 就代表他在 0 這個節點上的信號大小
第二個 row 就代表他在 1 這個位置的信號大小
第三個 row 或是說
這是第零個 row 這是第一個 row 這是第三個 row 的話
這是第二個 row
就代表他在第二個節點上面的信號大小
第三個 row 代表第三個節點上面的信號大小
對於這個 graph 我們可以寫出他的 adjacency matrix
adjacency matrix 第一個 row 0 0 就表示說
0 跟 0 有沒有一個 edge
0 1 有沒有一個 edge
0 跟 1 有一個 edge
0 跟 2 之間有一個 edge
0 跟 3 中間沒有 edge
所以這邊就是 0
另外我們有他的 degree matrix
他的 degree matrix 長這樣
degree matrix 每一個 diagonal 項都是把他的 row 或是 column 的東西加起來
所以第零個 row 加起來是 2
第一個 row 加起來是 3
第二個 row 加起來是 2
第三個 row 加起來是 1
我們有 degree matrix 我們有 adjacency matrix
就可以算出來他的 Laplacian
Laplacian 可以對他做 igon decomposition
算出來 igon vector 跟 igon value
所以我們可以算出來他有這四個不同的 igon value
就像我們剛才講的 Laplacian 他是一個半鎮定的矩陣
所以他算出來的每一個 igon value 都會是大於等於 0 的
他最小就是 0
而且每一個 graph 不管是怎樣的 graph
他 Laplacian 的 igon value 最小一定是 0
0 對應到的這個 igon vector
就是 0.5 這一個
第一個 column
1 對應到的就是第二個
這個 column
3 對應到就是這個 column
4 對應到就是這個 column
把他畫在這個 graph 上面會長什麼樣子
會長成這樣
0 這個 frequency 對應到的 igon value 是這組 igon value
把這個 igon value 畫在這個 graph 上面
就會長這樣
你可能會有點難接受為什麼會畫在 graph 上面
我們剛才講說這個信號的第一個 column
第 0 個 row 代表的是他在 0 的大小
第 1 個 row 代表他在 1 的大小，第 2 個 row 在第 2 個節點的大小
所以同樣的這個 igon vector 也可以把他畫在 graph 上面
第 0 個 row 就代表他在第 0 個節點的大小
第 1 個 row 就代表他在第 1 個節點的大小
第 2 個 row 就代表他在第 2 個節點的大小
第 3 個 row 就代表他在第 3 個節點的大小
畫出來就會長這樣
0 是一個全部都一樣的
frequency 等於 1 的時候，他是一個長這樣的信號
frequency 等於 3 的時候，他是一個長這樣的信號
frequency 等於 4 的時候，他是一個長這樣的信號
各位可能很難接受說為什麼這個東西代表的是頻率
為什麼這個東西代表的是 basis
先忽略剛才講過的東西，先來看另外一個東西
先來看一下 discrete time fourier basis
discrete time fourier basis
大家都知道他應該是長這樣的 basis
他有 dc component，不同的頻率的 sin 函數
當你頻率越低的時候，他就是每個都一樣
頻率越來越高的時候，他就會變化越來越大
頻率大到某一個程度的時候
他相鄰兩個就會是完全相反的東西
所以我們可以看到
在 discrete time fourier basis 裡面
可以看到他頻率越大的時候
相鄰兩個信號點之間
變化量就越大
所以他在最低頻的時候
相鄰兩個信號是完全一樣的
在最高頻的時候，相鄰兩個信號正負號是完全相反的
所以頻率越大，相鄰兩點之間信號變化量就越大
各位請記得這個概念
這個概念我們可以用來理解
spectral graph theory 裡面這個
frequency 這個概念
來看一下我們要怎麼去理解
vertex frequency 是什麼樣的東西
假設把 Laplacian 看成是一個 graph 上面的 operator
我就可以用這個 operator 對 graph 上面的信號做 operation
我做的方式就是把這個 Laplacian 乘以這個信號
來看看會發生什麼事情
我們知道一個 Laplacian 的定義就是 D - A
所以我把這個 Laplacian apply 在 graph 上面
把他乘開來之後他就會變成 Df -Af
我們一樣拿剛剛那個
很簡單的 graph 來當例子
這是一個很簡單的 graph
剛才有說他的 signal 他的 adjacency matrix 還有他的 degree matrix 是長這樣
假設說我把這個 Lf，也就是把這個 Laplacian apply 在這個 signal 上面
我會得到某一個 column vector 叫做
這個樣子
他會有四個 entry，把這四個 entry 分別叫做 a b c d
我們來看一下第一個 entry，也就是來看 a
是怎麼被算出來的
a 是怎麼算，a 的算法就是
Df - Af，而且我們看的是第一個
所以他就是第一個 column，第一個 row 跟他的 column 相乘
再減掉 Af
就會是這個東西
所以我們把他寫出來他會長這樣
長這樣再把他乘起來
會得到這樣一個式子
各位可以觀察一下這個式子他代表的意義
第一個這個 2 x 4 的這個 2 是哪來的
這個 2 是這個 2
這個 2 代表 v0 他有幾個鄰居
也就是 v2 跟 v1 這兩個鄰居
這個 4 是哪來的
這個 4 是這個 4
所以這個 4 就是 v0 上面 signal 大小有多少
減掉兩個東西
他分別減掉是誰
他分別減掉的是 2 跟 4
這個 2 跟 4 是誰，2 跟 4 是他的兩個鄰居的信號大小
所以把這個式子整理一下
就可以得到 Laplacian
Laplacian apply 在一個信號上面
他的第一個 entry 就會是他們兩個的差
加上他們倆的差
也就是 v0 跟他信號的差異的和
當我們把 Laplacian apply 在一個 graph 的信號上面
他某種程度上代表了
某一個信號跟他旁邊節點的能量差異
這個是他減他
如果要真的看能量的話
要把他平方
平方的方式就是我們
舉 f^TLf
最後經過推導可以得到這個東西他代表的東西
會是這樣
這個式子是什麼意思，這個式子代表就是
power of signal variation
某一個節點 i 跟某一個節點 v
他們兩個之間的信號能量差
所以他代表了這個 graph
graph signal 他有多 smooth
就像我們剛才在這邊講的
當 signal 越 smooth 的時候
他相鄰兩個的信號
能量差距應該是越小
當他頻率越大
越不 smooth 的時候
相鄰兩個信號能量差距是越大的
所以可以用這個東西
這個東西代表相鄰兩個節點 graph signal 的能量差異
我們就可以用這個東西來量化
某一個 graph signal 的頻率大小是多少
他代表一種能量概念
所以我們可以把能量概念拿來當作是一種
頻率來使用
他跟頻率有什麼關係
剛才講說 graph signal
我們定義那些頻率
那些 igon vector 他的頻率就是他的 igon value
我們來看一下，假設說
我們把這個 signal，剛才前面的講的那些 f 換成是這些 igon vector
把他乘一乘就會發現
剛剛講的那些 power difference
其實就是他的 igon value
igon vector 對應的這個 λ
代表的就是這個信號
這個信號兩兩相鄰的信號能量差距是多少
所以說當 λ = 0 的時候
表示他兩兩相鄰的信號能量差是 0
所以這就是一個 dz component
當他的頻率越來越高的時候
表示說他兩兩相鄰的能量差會越來越大
可是其實各位看這個圖沒有什麼感覺
感覺不出來他，就是，這三個可能有點難看出來
你可以很輕易的理解說這個就是一個 dz component
可是你可能沒辦法接受他的頻率比他高
我們看一個特例
這個特例是這樣
他是一個數線上面的 20 個點
數線上的 20 個點我也可以把它看成是一個 graph
只是他這個 graph 就是一個很簡單的 graph
他的 mean 是長這樣，他的 edge 是長這樣
我們把這個東西的 Laplacian 算出來之後做 igon decomposition 可以得到
他在 λ = 0 的時候，他就是一個 dz component
他在 λ 最大的時候，他就是一個長這樣的東西
你可以看到隨著他的 λ 越來越大
隨著他的頻率越來越高
他就是從一個頻率比較低的 sin
變成頻率越來越高的 sin
然後變成頻率非常高的 sin
我希望透過剛才的解釋各位可以接受說
那個 λ 代表的是
這個 igon vector 的頻率大小
目前先講到這個地方，看一下大家有沒有問題
如果沒有問題的話我們就繼續講
提醒各位我們為什麼要講這些東西，就是因為我們要去定義一個 fourier transform
我們要定義出 fourier transform 我們才能在 graph 上面做 filter
先來回憶一下在傳統信號與系統上面 fourier transform 做的事情是什麼
假設給你一個 time domain 的信號
你想要透過一個 fourier transform
轉換出一個他在 frequency domain 上面不同頻率大小對應的這個 component 大小是多少
我們在做這件事情的時候就是做 fourier transform
我們要怎麼在 graph 上面也用同樣的方式
把一個 graph 上面的信號
轉換到他的 frequency domain
轉換的方法長這樣
式子寫出來長這樣
假設我有一個信號 x
我就把他乘以一個 U^T
這個 U^T 就是我們剛才前面講到的那個 igon vector 的矩陣
我把他乘出來以後就會得到這個信號他在 frequency domain 的一個信號
這個式子看起來有點難理解
我們直接來看一個比較好理解的東西
假設說我有一個信號
他在 vertex domain 上面長這樣
這個 x 就是剛才前面講到的 f
我們把他乘以一個 U^T
U^T 代表他每一個 row 都是剛才前面講的一個 igon vector
把他乘出來之後
合成出來就是他在 spectral domain 的一個信號
來看他這個東西實際上在做什麼事情
他做的事情其實就是分析
他要分析每一個 component 的大小是什麼
你在做 fourier transform 的時候你就是要知道他每一個 component a0 a1 a2 a3 a4 a5 的大小是多少
所以你在 spectral domain 的時候
你要做的事情也是你要知道 u0 對應的 λ 的 component
這個 frequency component 大小是多少
u1 對應的 λ1 他的 component 大小是多少
你做的方式就是你用 u0 這個 basis
跟 x 這個信號做內積
你就可以分析出這個 component 大小有多少
一樣，u1 這個 basis 跟 x 這個信號做內積
你就可以得到他在 λ1 這個頻率上有多少成分
所以一樣，λ2 跟 λ3 做一樣的事情，你就可以得到
他在 spectral domain 上的信號長什麼樣子
所以我們就可以知道，我們可以把一個 graph signal
透過 fourier transform，也就是乘以一個 U^T
我可以分析出這個信號
在每一個不同頻率成分上面他的大小是多少
這就是 u0 跟 f 的內積
f 就是前面講的信號
就是這邊的 x
u1 跟 x 的內積代表他在 λ1 上面的這個成分有多大
u2 乘以 f
代表說他在 λ2 頻率上面成分有多大
u3 乘以 f 代表說他在 λ3 頻率上面這個成分有多大
這兩個基本上可以完全對應在一起
還有另外一個問題就是
假設說我已經有這個東西了
我要把他做一個 inverse transform 回去 vertex domain
我們把這個信號叫做 vertex domain
這個信號叫做 spectral domain
我要想辦法把這個信號轉回去
要怎麼轉
我們先想一下我們要
怎麼把這個信號轉回去這個
用類比的方式來看看要怎麼樣在 graph 上面做一樣的事情
我們做的方法就是說
假設說我給你一個這樣的信號
他是 frequency domain 的信號
我想要請你還原出他在 time domain 的信號大小
我要問你說他在 t = t0 的時候
這個 x(t) 在時域裡面的信號大小是多少
我們的做法你應該會這樣做
你應該會先找出來 cos(ω0t0)
這個頻率成分
他在 t0 的時候他的大小是多少
乘以這個 frequency component
他的大小放在這裡
再加上他在兩倍平的時候，兩倍平這個 basis
他在 t = t0 的時候他的信號大小
乘以他的 a2
放在這裡
再加上他在三倍平的時候，t = t0
乘以這個 component 的大小
放在這裡，加起來，一直加到 n 倍平的地方
你就可以還原出這個 xt 他在 t = t0 的時候
在時域的信號大小是多少
這個概念可以把他拓展到 graph上面
假設說你已經有這樣一個信號
你想要把他還原到 vertex domain 上面
你做的事情很簡單，你就是看
這個 λ0 對應到的 igon value 是他
我們就看這個 igon value 假設我想要還原出
他在 vo 這個節點上面信號大小是多少
我就看這個 igon value
他在 0 的信號是這麼大
他在 0 的信號乘以這個 component 的大小
是 u0 乘以 f
再加上他在第二個 basis 他在 0 這個節點上面的信號大小
乘以這個 component 的大小
再加上第三個 basis 他在 0 這個地方的信號大小
乘以他 component 的大小
再加上第四個 basis 他在 0 這個節點上面的信號大小
乘以 component 的大小
全部加起來之後你就會得到這個東西
所以你把他寫成數學式子的時候
他其實就是 x = Ux/head
x/head 就是這個東西
你把他寫出來會長這樣
x = Ux/head
U 每一個 column 就代表每一個 basis
我們來看一下這個東西，也就是這邊圈起來的橘色東西他是怎麼被算出來的
再看一次
他算的方式就是
第一個 basis 在 0 的位置
跟這個東西乘起來加上
第二個 basis 他在 0 這個位置的大小
在這個地方
乘起來
加上第三個 basis 在 0 這個地方的大小
把他跟 component 的大小乘起來，再加上這個東西乘以這個東西
加起來就會是這個信號的大小
我們就已經得到 inverse fourier transform 他就是一個
x = Ux/head 這樣一個式子
接下來來講 filter，filter 就是說
我再提醒一下各位我們為什麼要講前面這些東西
講前面這些東西就是因為我們要
找出一個可以在 graph 上面做 filter 的方式
graph 上面做 filter 的方式我們必須要先定義出來
graph 上面要怎麼做 fourier transform 跟 inverse fourier transform
我們才能用 fourier transform 跟 inverse fourier transform 把他 transform 到 spectral domain
在 spectral domain 上直接相乘
也就是再做 filter
做完 filter 之後
再把他 inverse transform 回到 vertex domain
我們知道假設我們在 frequency domain 上面有一個信號長這樣
我們用一個 impulse response 長這樣子的一個 filter
他最後做出來的信號
經過 filter 的信號在 frequency domain 上應該是長這樣
就是把這兩個東西相乘，乘起來就是這個東西
spectral graph theory 裡面他做這個
做這個 filter 也是一樣的東西
你剛剛已經有轉出來一個 x\head
這個 x\head 就是剛才講說
一個信號 x 經過 graph theory transform 之後得到的那個信號
然後你把它乘以你的某一個 filter 的 frequency response
兩個相乘之後你就會得到經過 filter 的信號 y\head
如果要把它寫成矩陣相乘的方式要怎麼寫
就可以把它寫成 x\head
乘以這個 frequency response
把它寫成這樣
他就會等於這個東西
各位可以看看這個東西就是 θ0/x0\head 放在這裡
θ1 乘以 x1\head 放在這裡
就相當於這個乘以這個等於這個
這個乘以這個等於這於這個
這邊之所以會把他寫乘 gθ(λ) 的意思就表示說
這個頻率響應他是一個
頻率 λ 的函數
他只是想要表示這個意思
講完這個之後終於可以比較接近我們要講的東西
假設我們已經得到一個這樣的 y\head
這還不是我們最後想要做的東西
我們最後想要得到的是這個東西在
vertex domain 上的信號長什麼樣子
這邊他是一個 spectral domain 的信號
所以 spectral domain 的信號要把他還原到 vertex domain
必須要做 inverse fourier transform
inverse fourier transform 的方式就是他前面再乘以一個 U
所以在前面乘上一個 U 之後就可以得到最後要的信號
就是 y
今天想要 train 出一個 model
這個 model 要做什麼事情
這個 model 要做的事就是給你一個 input x
希望你可以學到一個 filter
叫做 gθ(λ)
這個 gθ(λ) 經過 filter 之後
可以 filter 出一個 y
也就是做 convolution 之後我要得到 y
把它寫成數學式子的話
就是 y = gθ(UλU^T)x
他會等於，因為這個把它放進去就是一樣的
這是線性代數
你可以把 U 丟到裡面
丟到裡面之後
這個東西就是 Laplacian
所以你要去學一個 Laplacian 的多項式函數
不一定是多項式函數，你要去學一個 Laplacian 的函數
經過這個 Laplacian 的函數之後
跟 x 相乘可以得到經過 filter 的信號 y
你要學的參數就是中間這個綠色框框框起來的
gθ(λ)
這個 gθ(λ) 或是 gθ(L) 可以是任何你想的到的函數
他可以是一個
假設奇怪一點，它可以是 log(I+L)
因為這邊是一個矩陣，所以這邊要用 identity
它可以是 log(I+L)，我們知道 log(I+L)
你可以直接把它用泰勒展開，展開成這樣
這樣的 model 會有什麼問題
他最大的問題，第一個問題就是
這個東西是你要學的參數
你的 model 要學一個 filter
可是這個 filter 的大小跟什麼東西有關
跟 input graph 大小是有關的
假設今天的 graph
給他一個十萬個節點的 graph
你要學的東西就是十萬這個數量
如果今天的 graph 只有十個節點
這個東西就只會是一個十維的 diagnose 矩陣
你的 learning compacity 會 depend on
input graph 的大小
這是一個不太好的事情
因為各位可以想想在做 CNN 的時候
不管 input 的影像是多大
你都可以用一個一樣大的 model 去訓練
但是如果按照我們用剛剛這個 filtering 的方法來做的話
今天只要有一個不同的 graph 我就必須要去學一個不同的參數
而且大小不一樣
參數量還會有很明顯的差距
這是一個很麻煩的事情
第二個問題，他還有另外一個問題
就是這個 gθ(L) 你選擇的時候
會讓這個 model 學到一些你可能不希望他學到的東西
這是什麼意思
來看一下剛才講說最後要學到的一個東西
要學一個 gθ(L)
gθ(L) 剛才也講說它可以是任意的函數
假設我讓他這個函數叫做
cos(L)
cos(L) 可以把他經過泰勒展開
得到一個無窮的級數和
這個級數和先來看一下
先看一下假設 gθ(L) 他是 L
先不要管這一行
假設 gθ(L) 他等於 L
y = gθ(L)x
gθ(L) = L 的時候，y = Lx
來看一下 y = Lx 在很簡單的例子裡面他會
出現什麼樣的狀況
先來關心 y 這個信號
在他的第 0 個 entry
也就是他在經過 filter 之後
這個東西會變成什麼
這個東西就是這個 row
跟這個 column 的內積
這個 row 你看一下他在哪個地方等於零
他在第三個 column 的地方等於零
為什麼他在第三個 column 會等於零
因為這個節點
他並沒有跟第三個節點有任何的
距離是 1 的路徑
所以說
他在這個地方會是零
我們來看，假設說
這個東西跟這個東西相乘的話
這上面第三個節點的信號
就沒有辦法影響到這個東西
因為零乘負三是零
但是如果今天 gθ(L) 我選的是 L^2 的話
L^2 我已經幫你算出來了，L^2 長這樣
你看一下，假設我用 L^2 這個 Laplacian
當作 filter 來乘以這個信號 x
在第零個節點得到的信號
是 L^2 的第一個 row
乘以 x 的這個 column
你看，這個 row 每一個 row 每一個 entry 都是
非零的數字
表示說這個信號
整個信號都會影響到他
為什麼，因為
這個 graph 上面
對於零這個節點來說
所有的節點都是在他
都是在有一條路徑是 2 以內的
path 可以連到他
對於這個節點來說
他可以走 1 2 這條路
對於這個節點來說，他可以走 1 2 這條路
對於這個節點來說他可以走這條路
這個上面每一個節點的信號都可以跑到 1 上面
經過 filter 之後
他的信號就會被其他的影響
如果說
我用了 n 次方的話
根據某些定理，這個定理你可以自己證證看
假設我用了 k 次方的話
就會導致說
假設我用的是 n 次方
這個 graph 裡面有 N 個節點
有 N 個節點的 graph
他除非不是 connective 的
給他乘 N 次
他 Laplacian 一定會是
每個 entry 都不是零
也就是說，假設你把它展開到 N 次方的時候
經過一次 filtering
你就會讓整個
整個 graph 信號互相影響
這件事情不是我們要做的事情
為什麼
因為他不是 localize
什麼叫做 localize
localize 就是說假設我們在做 CNN 的時候
可以透過 kernel size 的選擇
來決定說
要看到多少範圍
像我們選了 3 x 3 的 kernel
對於他來說，他就只看的到 3 x 3 的九宮格
如果選擇想看比較大的 neighborhood
我就可以選擇大一點的 kernel
對於他來說我選了一個 5 x 5 的 kernel
就可以看到這麼多東西
選擇了 5 x 5 就可以看到這麼多東西
所以他是一個 localize 的
localize 的 filter
可是剛才前面講的這個東西
假設我們選了他可以展開到 n 次方
根據剛才的觀察可以發現
假設你給他弄到 n 次方他只有 n 個節點的話
經過一次 filter 他就可以把所有的
整個 graph 上面的東西都看過
他就只有考慮到 neighborhood 裡面的東西
所以這就是我們說第二個問題，他不是 localize
接下來終於要開始講 model
如果各位前面全部都聽不懂
你可以嘗試從這邊開始聽懂
如果你這邊還是聽不懂，等一下會再教一次
我們就來講一下
第一個要講的就是 spectral convolution 的第一個 model
叫做 ChebNet
這個 ChebNet
他主打的就是他可以快而且可以 localize
先來看一下它怎麼做到 localize
而且他可以處理剛才講的第一個問題
他的 learning capacity 是 O(N) 這個問題
他做的方式很簡單
就是剛才說 gθ(L) 可以隨便亂選
他說我們不要隨便亂選，我們選東西必須要讓它是一個
Laplacian 的一個多項式函數
透過你選擇讓他是多項式函數的方式，你就可以讓他是 K-localized
根據剛才講的，如果你最多只有到 k 次方，他就只看的到他在 k 裡面的 neighborhood
透過這樣的方式他就可以是 K-localized
而且他同時還解決第一個問題
他要學的參數就變成
θk 這個參數
所以他要學的參數你可以決定你要學多少
你要學的是 O(K) 因為你現在的
這個 filter 就是 Laplacian 的一個多項式函數
你只要決定這個多項式函數的細數次多少就好
你要學的就是這些多項式函數的係數
現在又出現另外一個問題
現在出現的問題
我們知道這個 Laplacian
這兩個東西是等價，因為你把
這邊乘以一個 U^T 跟 U 就會是上一個式子
假設說我們學出來一個 filter 是
他的參數是 θk
我要用這個東西來對 x 做 filtering
對 x 做 convolution
我在做的過程中會有一個很高昂的運算成本
他的 complexity 是 N^2
因為我算出 θk 以後我要乘以 igon vector
這個 igon vector 在乘的時候會出現一個 N^2 的 complexity
另外你要算這個 U 你要算他的 igon vector
本身就是一個非常非常麻煩的事情
假設說你圖非常的大的話
你就是要對一個很大的對稱矩陣做
spectral decomposition
這件事情也是非常非常麻煩的
所以 ChebNet 他必須要解決的第三個問題
就是他必須要解決掉 time complexity 太高昂的問題
是怎麼解決這件事情的呢
他用了某一種很神奇的方式
他用了一個叫做 Chebyshev polynomial 的東西
先看一下什麼是 Chebyshev polynomial
他是一種特別的 polynomial 的一個家族
他定義的方式是用 recursive 定義的方式
T0(x) 他定義成是 1，T1(x) 他定義成是 2
Tk(x) 他是遞迴的去定義他
就是 2xTk-1(x) - Tk-2(x)
他的 x 必須要定義在 -1 到 1 之間
這是因為他有某些性質必須要滿足
所以他把它限制在這個地方
Chebyshev polynomial 重點就是
他有一個多項式函數
這個多項式函數式用遞迴的關係去定義
有了這個多項式函數
你也不知道要幹嘛
假設說把多項式函數裡面這個 x 換成 Laplacian 的某一個
把這個 Laplacian 或這個 λ 經過某些調整之後
調整成叫做 λ~
λ~ 跟 λ 關係是這樣
這件事情只是要讓前面這個範圍是被滿足的
各位可以不用去管為什麼，反正就是
接受他要是這樣
所以我們就可以把這個 x 換成 λ~
T0(λ~) 就是 I
T1(λ~) 就是 λ~
Tk(λ~) 根據 Chebyshev polynomial 的定義
就是這樣
原本的多項式函數原本是長這樣
我們說我們要學他是一個 λ 的多項式函數
λ 的多項式函數要學的東西就是他的 θk 是什麼
現在把它改成是要學的是這個東西
要學的東西是由 Chebyshev polynomial 組成的多項式
他 θk' 是多少
為什麼要做這件事情，這件事情在幹嘛
這有點難理解，我舉一個比較簡單的例子
大家可能會比較可以接受
我們把一個多項式函數改成由另一個多項式函數組合會有什麼好處
這是我昨天從我高中的學資翻出來的
這個題目大家可能距離有點遙遠
我講一下他在幹嘛
它告訴我們說有一個多項式函數長這樣
他如果直接問你說，你要求 f(1.99) 的近似值到小數第三位的時候
你就會
你就會很自然地想到你要用綜合除法
找出這個東西
你找出前面這些係數之後
你就可以直接把他代進去
每一項就會變成 0.01
就會變得很好算
所以 Chebyshev 這個 model 做的事情有點類似
你就可以把 λk 就是這邊的這個東西
原本的 θk 就是 3 -7 -2 2 18
我們想要經過某一個方式把它換成這邊這個東西
各位可以先不要管為什麼要換成這個東西
就是把它換成這個東西
換成這個東西之後
它的好處就是當我們把某一個東西代進去之後
他的這一項會非常好算
我們把它換成這個東西的目的在於說
我們希望 Tk(λ~) 是很好算的
就像在這個裡面我們把 1.01 代進去他會很好算
所以最後你要學的東西就是這邊的 θk' ，也就是 a b c d e
到這個地方大家有問題嗎
如果沒有的話我就繼續講
再整理一次
為什麼要用 Chebyshev polynomial，用 Chebyshev polynomial
意義在於說
我們希望透過這個方式可以讓這個東西變得比較好算
因為原本如果用 λk 這個東西來算的話
最後你再前後乘以一個 U 跟 U^T 的時候
他會出現一個 O(N^2) 的 complexity
但是我們用 Chebyshev polynomial 來重新 ***** 的時候
你就可以讓這件事情變得輕鬆很多
我們看一下這是怎麼做到的
他跟這個東西他是遞迴的有關係
假設我們已經接受這件事情
我們可以把 y 這個信號他 filter 就是 gθ(L)
我們剛才有說我們可以把它用 Chebyshev polynomial 來 ****
我們把它 **** 之後
先把它展開來，這個東西展開來就是 θ0'T0(L~)x + θ1'T1(L~)x + ...... ，加到第 K + 1 項
這個東西可以把它寫成是
用這個遞迴式把它改一下
剛才有說 Chebyshev polynomial 的遞迴式長這樣
所以就可以把這些東西換成是另外一個數列來表示
就把這邊標紅色的 T0(L~)x 當作是 x
把這個東西遞迴式乘以一個 x 變成這樣
我們定義這個東西
也就是這邊這個東西，定義成 xk bar
x0 bar 就是 x，x1 bar 就是 T1(L~)x
就是這個東西乘以 x
因為他遞迴的這個定義，所以可以把它寫成是
xk bar 就是 2L~xk-1 bar - xk-2 bar
寫成這樣之後
就可以把這個式子改成這樣
這個式子改成這樣之後
其實我們要做的事情很簡單
我們要找一個 filter 的參數
就是這個 θ0' 到 θk'
他乘以一個 x 之後
就會得到我們要 filter 出來的這個信號
現在最關鍵的問題就是這個東西你要怎麼算出來
這個東西可以根據遞迴的方式算出來
因為他是遞迴
所以你每一次在算的時候
你就不用成以 K 次方，你用這個遞迴式算的時候
他的 complexity 就只會有 O(E)
你算 k 個的話
你用遞迴式算只要 O(KE)
所以他就可以解決掉前面講到的我們需要用 O(N^2)
的 complexity 的問題
各位如果前面都聽不懂沒有關係
你可以現在聽最後結論
我們要學一個 filter
這個 filter 要做的事情就是要學一組參數，這組參數寫在這裡
要做 filter 的方式就是你先把你的 signal
用遞迴的方式算到 k 次方
放在這裡
乘以一個 weight matrix
之後你就會得到 filter 完的信號
到這個地方有問題嗎
如果沒有問題我就繼續講
你當然可以不只學一個 filter，因為剛才說
這是一組 filter 參數，你可以不只學一組 filter
你可以學第一組 filter 參數之後這樣做
就有點像是 CNN 裡面你可以不只有一個 channel
第一個 channel output 出來就是這樣
第二組參數你就一樣，這個乘以這個加上這個乘以這個加上這個乘以這個
加起來之後放在第二個 filter
假設你弄了 Fout 這麼多個 channel
做一樣的事情
ChebNet 就是講到這個地方
到這邊大家有問題嗎，大家如果聽不懂的話，你只要記得就是
我們可以用某種方式
遞迴的算出這組參數，這組參數可以讓 complexity 變得很低
他是 K-localize
接下來講完 Chebyshev polynomial 的話
我們就來看 GCN
GCN 是大家最喜歡用的一個 model
因為他非常非常簡單，非常非常直觀
跟他又好像有一些數學基礎在後面，所以大家就很喜歡用 GCN
GCN 的全名叫 graph convolution network
他做的方法很簡單，就是我們剛才講的 Chebyshev polynomial
ChebNet
他用的是一個 K-localized polynomial filter
如果 polynomial filter 選擇讓 k = 1 的時候
剛剛前面那頁的式子就會變成這樣
因為剛才有講說為了讓他符合特定的條件
我們有把 L 換成 L~
所以把 L~ 代回來之後
會變成這個式子
假設說今天用的 L 是 normalized Laplacian
normalized Laplacian 的定義在這裡
normalized Laplacian有一個很特別的特色，就是他的 λmax
他的最大這個 igon value
不會超過 2
所以你就可以假設他大概是 2
你就可以把這個式子化簡成這樣
化簡成這樣之後再用 normalized Laplacian 的定義
就可以把這個 L 換成這個東西代進去變成這樣
變成這樣以後
你可以要求你的 model，為了讓你 model 的參數比較少
所以不要讓 θ0 跟 θ1 是兩個完全獨立的參數
你要求他，比如 θ0 跟 θ1 差一個負號
你就可以把它變成是下面這個式子
下面這個式子再經過一個 renormalization trick 的東西
這個 trick 是作者自己取的名字
反正他就是這樣做
你不要問為什麼，他就是這樣做
他做的方法就是他把這個東西改成
這個 I + D^(-1/2)AD(1/2)
變成了另外一個有加 ~ 的
幫大家回憶一下，這個 D 就是 degree matrix
A 就是 adjacency matrix
D還是一樣是 degree matrix
他的 renormalization trick 做的方式是說
A~ 跟 A 原本的關係
他把 adjacency matrix 的 diagnose 如果不是 0 的話
如果不是 1 的話他把它變成是 1
也就是說他在原本的 graph 上面每一個節點
他都加一個 *** 上去
對於 A~ 來說，diagnose 項一定都是 1
他把這個東西叫做 renormalization
他經過 renormalization 之後
你的 filter 就從原本的個東西變成 y = 這個東西乘以 θ 再乘以 x
重新整理一下，式子就變成是
這個乘一個 H 就是上一個
再乘以某一個 weight
這個 weight 就是這邊這個 θ
你就會得到下一層 H^(l+1)
這個式子看起來也不太友善
看一個比較友善的式子
最後可以把它重新寫成這樣
大家如果前面信號與系統聽不懂沒關係，信號與系統已經全部結束了
我們現在要講的是 GCN 的最後一件事
這個式子告訴我們說你要 update 某一個 v 他的 feature map 的話
你要做的事情就是把他所有的 neighbor
包括他自己全部加起來
你把他所有的東西加起來
先把他的 filter 經過一個 transform 之後
加起來取平均再加一個 b
經過一個 nonlinear activation
你就會得到他再下一層
再講一次，這個事情代表的方式就是說
先把 xu 經過一個 transform 之後
把所有 neighbor 包含他自己加起來取平均
在加一個 b 經過一個 activation 之後就會得到
最後的 h
這就是 GCN
講完前面這些東西
回過頭來看一下這個
剛剛講過所有 model 的綜合比較
回來看一下 Benchmark task
Benchmark task
有這五個
因為剛才有講過
要做 Benchmark 的話一定要是
dataset 要夠大
切得要夠公平
他就提出這五個不同的 task 來做 Benchmark
第一個來看 graph classification
也就是你要對一整張圖做 graph level classification
他做的方式就是用 MNIST 跟 CIFAR10 做 SuperPixel 的 classification
你已經知道 MNIST 或是 CIFAR10 的圖片
這個 dataset 會先把這個東西透過某些演算法轉成 SuperPixel
這個 SuperPixel
每一個 SuperPixel 之間會有一些 link
你用這樣一張圖，你的 GNN 就要去做 calssification
要區分得出這張圖是 9 這張圖是 5
這是一隻青蛙，這是一台車子
這就是 SuperPixel 的 MNIST 的 task 要做的事情
他要做的事情就是 graph level 的 classification
第二個 dataset 他是
叫做 ZINC molecule graphs dataset
他要做的事情是 regression，他 regression 的方式
就是他會有很多不同的 graph
這些 graph 大部分都是真實世界的分子
這些真實世界的分子他的節點數量從 9 到 37 都有
regression 就是像各位在做 pm2.5 的預測
他要預測的東西不是 pm2.5，他要預測的是分子的溶解度
所以給你一個 graph 你要透過這個分子來預測他的溶解度是多少
另外一個是 Node classification task
他用 Stochastic Block Model
來生成出很多不同的圖
這個 dataset 裡面他要做兩件事情
task 有兩個，第一個你要去辨認
graph 裡面的 pattern
也就是說告訴你有一個 pattern 長這樣
要請你在很多 graph 裡面找出這個 pattern
所以你要去找出這個節點是不是屬於這個 pattern 裡面的一個點
所以這叫做 graph pattern recognition
另外一個 task 叫做 semi-supervised graph clustering
也就是說從 Stochastic Block Model 這樣一個 model 會生成出很多不同的
他會生成出很多不同的 graph
每一個 graph 會有很多不同的 community 或是 cluster
你要做的事情就是你要想辦法
去 identify 這些不同的 node 分別屬於哪些 cluster
所以你看這邊很明顯他就是屬於第零個 cluster
這邊是第一個 cluster
這邊是第二個 cluster
所以這也是一個 Node classification task
第四個 task 叫做 Edge classification
Edge classification 做的是 Traveling Salesman Problem
各位都知道 Traveling Salesman Problem 是一個
NP complex 的問題
這個問題是什麼
就是說有一個 slaesman
他要去很多不同的城市
給你一個 graph
graph 上面每一個節點代表一個城市
graph 跟 graph 中間的 edge 代表城市跟城市之間的路徑
他要問你說他有沒有辦法從某一個點開始走
應該是說到每一個點只能到一次
不能走重複的路徑
從一開始指定的點繞一圈
繞過所有的點，走到原來的點
他是一個 edge classificaiotn，他要 classify 的
這個 edge 是不是屬於你的 TSP 最後的最佳解
最佳路徑的一個 edge
他是一個 edge classification problem
你可以看到他這邊 dataset graph 都非常大
至少有好幾萬張圖
來看一下剛才講的所有 model
他的表現如何
第一個來看一下
剛才講的 GCN
有發現說 GCN 他其實
最陽春的 GCN 表現並不會特別好
MLP 就是說
MLP 就是說我完全不考慮 graph 的 structure
完全不考慮節點跟節點之間的 link
我就只有用到 node feature
沒有用到 edge 的關係
這樣子的狀況下
MLP 完全不考慮 structure 都可以打爆 GCN
因為 GCN 是最爛的
他們其實有發現說 GCN 並沒有特別強
GraphSage 就比他好很多
還有剛才講的 GIN
還有 MoNet
GAT 其實蠻強的
GatedGCN 就是 GCN 的變種
你可以把 GatedGCN 理解成
GAT 加上 GCN
在做 summery 的時候還有加一些 weight
經過這件事情之後他可以變非常強
來看 Regression 這個 dataset
他的衡量結果是看 MAE
看起來 GCN 也是最慘的
沒有 MLP 那麼慘
他是 GNN 裡面最慘的一個
GIN 是比較好的
GatedGCN 還是一樣是最強的
接下來看一下 SBM 那兩個 dataset
這兩個 dataset 一樣，GCN 的表現並沒有特別好
其他的就是都比較好
各位可以發現，假設我有做 Node weighted
我不要直接相加的，我用 weighted sum 的時候
這些 model 大部分會比這些還要好
GIN 是例外
另外還有一個 dataset，最後一個 dataset 就是
TSP 那個 dataset
TSP 那個 dataset 一樣，GCN 很慘
最好的還是一樣是 GatedGCN
他這是用 F1 score 來算，因為他要
他是要去 classify 每一個 edge 是不是屬於最佳路徑的 edge
他是算 precision 跟 report 小數加起來算 F1
另外一個問題就是層數
剛才講 GCN 都沒有講他要疊幾層
大家可能會覺得說會不會越多層越好
但其實並不會
TSP 這個 dataset 裡面
你把 GCN 疊到 32 層，他的 performance 在沒有 residual connection 的狀況下會爛到只剩 0.361
GIN 都不會像 GCN 那麼慘
所以就有人很好奇為什麼 GCN 會那麼慘
他們就有去分析 GCN 某些特性會導致說
GCN 在疊越多層的時候
他的 expressive power 或 exponentially decay
因為時間關係我沒有打算細講，但是
總而言之他的重點就告訴我們說
GCN 到最後
會有一個現象就是
你的 node feature 經過很深很深的 GCN 之後
同樣的一個 subgrade 裡面
很多東西會收斂到同樣的地方
像是今天這個例子來說
這是一個 graph 他有兩個 component
第一個 component 在這裡，第二個 component 在這裡
經過 GCN 之後他最後，node 1 feature node 2 feature node 3 feature
會收斂在同樣的一個地方
4 5 6 同樣是第二個 block
他會收斂在同樣的地方
GCN 疊越多層，沒有用 residual connection 的時候就會有這個問題
這個問題要怎麼解決，有一個很簡單的作法
你把他的 edge drop 掉就好
因為剛才說 GCN update 的方式
是把所有 neighbor 取平均
DropEdge 告訴你說不要把所有 edge 取平均
在取平均的時候隨便 drop 掉一些 neighbor
就可以避免剛剛講的問題
實驗結果也的確有顯示，在你加 DropEdge 之後
他會比你原本沒加 DropEdge 還要好一點點
事實上他還是比前一點的 model 還要爛
接下來 HyperGCN 我就不講，因為時間的關係我就都不講
Graph Generation
簡單講一下 Graph Generation 他在幹嘛
Graph Generation 就是你要生成圖片
生成圖片有很多不同的方式
像是 VAE 或是 GAN 或是 Auto-regressive
這些都是傳統上會拿來用的 generation model
VAE 會有一個 encoder 會有一個 decoder
餵到 encoder 裡面是一個 adjacency matrix
他的 edge feature 跟 node feature
你把他丟到 VAE 裡面，生成出來的一樣是一個 adjacency matrix
一樣是一個 edge 的 feature matrix
一樣是一個 node feature matrix
透過傳統 VAE 的訓練，你就可以訓練出一個可以生成
生成圖片的 model
另外一個是用 GAN 來做
就是你用傳統 GAN 的方式
今天他 GAN 丟進去的
丟到 GAN 裡面的東西就是 adjacency matrix
還有他的 feature marix
另外一個 Auto-regressive model 做的方式就是
要生成圖的話就是
先問這個 model 要不要生成節點
再問他要不要生成 edge
他就會告訴你要不要生成
你就可以產生節點
再來產生節點完之後你再問他要不要生成 edge
它就會告訴你要不要生成 edge
他如果要的話你就加一個
一直做就做完了
時間關係我就不講最後一個部分
各位要知道說 GNN 在 NLP 上面
可以有很多很多應用
沒時間講所以我就不講
簡單來回顧一下今天講的東西
今天講的東西就包括說怎麼把一個 graph
丟到一個 neural network 裡面
就是 GNN
我們也講到說有兩種主要的方式
一種是 spatial-based 一種是 spectral-based
大家最喜歡用的 spatial-based 的通常是 GAT
大家最喜歡用的 spectral-based 的通常是 GCN
如果你聽不懂 GCN 沒有關係
如果只有看今天的式子
你也其實不會知道他是一個 spectral-based model
最後也簡單介紹一些 supervised classification
semi-supervised classification 的 task
其實還有很多其他不同的 task
都是 GNN 可以用的
時間的關係就沒有講 NLP 的部分
最後 summary 的部分
像我剛才講的 GAT 跟 GCN 是大家最常用的方式
雖然說 GCN 有一套很完整理論
但是大家不太會管他的理論是什麼
GNN 或是 GCN
他有一個很嚴重的問題，就是你疊越深的時候，他的 model performance 會非常爛
這件事情大家目前還是正在找為什麼
大家還是要去看理論分析
另外一個趨勢就是大家很喜歡把其他在
deep learning 的東西拿來用在 graph network 上面
像是 Deep Graph InfoMax 或是 Graph Transformer 或是 GraphBert 都有人在做
做出來的結果怎麼樣
我知道 GraphBert 的結果並沒有特別好
他沒有大家想像那麼好
GNN 可以被用在很多不同 task 上面
只要他是 structure data，只要他有一些
data 之間有一些 relationship 很重要的時候都可以用 GNN 來解決
最後就講到這裡
大家如果有什麼問題可以現在問
如果沒有問題我們就可以準備結束了
大家有問題要問嗎
如果沒有的話就謝謝助教
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw/
