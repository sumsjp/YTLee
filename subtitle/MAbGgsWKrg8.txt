接下來我想跟大家分享一個語言模型生成的新的技術
叫做Speculative Decoding
雖然我們說語言模型往往生成的時候
不一定需要生成非常長的結果
所以用Auto Regressive也許就夠了
但是很多時候當你要生成長篇大論的時候
你也會覺得等待的時間非常的漫長
有一個方法叫做Speculative Decoding
它可以加快任何的語言模型生成的速度
這個方法跟你使用的模型沒有關係
這個方法不需要對模型做任何訓練
它是一個神奇的外掛
這個神奇的外掛可以加到任何語言模型上面
那個語言模型生成的速度就突然加快
所以非常神奇的方法值得跟大家介紹一下
上面這個圖是來自於下面這個連結啦
那這個圖就是要對比一下說
Llama2-13B的模型在沒有Speculative Decoding的時候
它生成的速度跟加上Speculative Decoding這個外掛之後
它生成的速度
那可以發現說Speculative Decoding它生成的速度
大概可以到原來的兩倍甚至到三倍左右
那今天我們看到這個GPT-4生出來的結果
GPT生出來的結果 搞不好他已經有用
Speculative Decoding這種技術
來加快原有語言模型生成的速度
也說不定
所以我們就來認識一下
Speculative Decoding這個技術吧
那Speculative這個字的意思呢
是猜測或投機的意思
那Speculative Decoding這個方法的
核心概念是什麼呢
我們說Auto Regressive Model
它的痛點就是需要按部就班,循序漸進
第一個Token沒生成之前
我們沒辦法生第二個Token
Speculative Decoding核心的想法就是
這個非常神奇
我們能不能夠找一個預言家
這個預言家他能夠做的事情就是
可以預判語言模型接下來的生成
如果今天預言家他可以根據輸入
直接預判說語言模型等一下
就是會產生黃色的這個Token
你就可以一次把輸入跟輸入加上黃色的Token
都丟給語言模型
讓他做平行的預算
一次就可以產生兩個Token
就可以跳過Auto Regressive
他一定要生第一個Token才能生第二個Token這個問題
先叫預言家預測接下來要輸出什麼
你就可以直接讓第二個Token跟第一個Token
同時被產生出來
那如果可以讓第一個第二個Token同時被產生出來
有什麼樣的好處呢
你的好處就是你的速度
馬上就變成原來的兩倍了
你本來只能一次生一個Token
一個第一個Token生完
就要等著才能夠生下一個Token
現在一次可以生兩個Token
生成的速度就變成原來的兩倍
那講得更具體一點
整個Speculative Decoding的運作方法是這個樣子的
當這個藍色的輸入進來的時候
先趕快給預言家看
然後預言家快速預測說
接下來語言模型會先生紅色的Token
再生黃色的Token
那我們這邊假設預言家的速度
奇快無比
他做預言的時間是幾乎可以忽略不計的
接下來呢
預言家已經說接下來會生紅色的Token
跟生黃色的Token了
所以呢
我們就可以同時把輸入
輸入加上紅色的Token
輸入加上紅色的Token加黃色的Token
一次全部丟給語言模型
讓他直接預測自己接下來三步
會產生什麼樣的東西
他可以預測自己下一步會產生什麼
他已經知道自己下一步會產生什麼
這是預言家說的
他知道自己下下步要產生什麼
預言家已經預測下一步跟下下步會產生什麼
所以語言模型可以直接預測下下下步會產生什麼
所以下一步、下下步跟下下下步
這三步可以同時被生成出來
那外人其實不知道裡面有沒有預言家
從外人的觀點來看就是
哇!一次吐出三個token
速度變成原來的三倍
本來一次只能吐一個token
現在一次可以吐出三個token
從外人的角度看這個語言模型
速度就變成原來的三倍
講到這邊
你一定會有一個很大的困惑
預言家如果這麼厲害
都可以預言語言模型會輸出什麼
那還要語言模型幹嘛
都叫預言家生成就好啦
直接用預言家生成就好了
幹嘛還需要語言模型生成呢
所以這邊的問題就是預言家有可能會犯錯
但是我們來看看
如果預言家犯錯的話會發生什麼樣的狀況
那我們現在假設預言家沒有辦法精準的預測語言模型的輸出
他又不是語言模型
所以他有可能會預測錯
所以第一個Token會生成紅色的是預測對的
但第二個Token他猜是灰色的
這是錯的
語言模型不會生出灰色的Token
但是沒有關係
我們還不知道語言模型會生出什麼
所以我們先相信預言家把輸入丟給預言模型
把輸入加紅色的token丟給預言模型
把輸入加紅色的token加灰色的token丟給預言模型
所以預言模型同時輸出三個token
但我們知道說預言家的第二個輸出是錯的
這個灰色的token是錯的
所以預言模型所輸出的第三個token
綠色的這個token是不可信的
因為輸入本來就已經是錯的啦
所以輸出當然也不是對的
但是就算是預言家犯了一個錯誤
就算是預言家的第二個token是錯的
語言模型只有前兩個token是對的
你還是可以一次輸出前兩個token
你還是有賺
原來只能一次輸出一個token
現在變成一次輸出兩個token
你還是有賺
速度還是變成原來的兩倍
那這邊問題
在這邊你可能會有的問題是
我們怎麼知道預言家犯了錯誤呢
我們怎麼知道預言家它輸出的是錯的
所以語言模型第三個輸出的token不可信
不應該被當作最後的輸出
只有第一個跟第二個token可以被輸出呢
怎麼知道預言家是錯的呢
你就看語言模型真正輸出的結果
再去比對預言家的輸出
你就知道預言家是不是對的
所以在這些token被輸入語言模型之前
我們不知道預言家做的怎麼樣
但是當語言模型產生它的token之後
你就可以去跟預言家做比對
語言模型下一個token是紅色的
下下個token是黃色的
你就知道預言家的下下個token是錯的
所以如果輸入有下下個token
那輸出就不要理他了
輸出就有可能是錯的
只把肯定是對的部分
拿出來給使用者來看
就算部分有錯
就算預言家部分有錯
最終速度上還是有賺到
但是假設預言家爛到不行
它輸出來的結果通通都是錯的呢
假設預言家很爛很不幸的
它的預測完全都是錯的
所以當你把輸入給語言模型
輸入加預言家預測的下一個token
還有輸入加預言家預測的下一個token
跟下下一個token都丟給語言模型的時候
只要有預言家的輸出都不可信
只有這一個結果是對的
那你至少可以輸出一個token
當你可以輸出一個token的時候
整體來講你幾乎可以說是不賺不賠
因為本來一樣的時間
你也只能夠輸出一個token
現在只是變成沒有預言家的狀態而已
那當然你還是有損失一些東西啦
你損失的是什麼
損失的就是首先你要等預言家預言
所以這是一些時間的浪費
但是我們這邊假設預言家就是要奇快無比
所以我們可以忽略預言家所耗費的時間
當然你還有另外的損失就是
運算資源無謂的耗損
你在這個地方的輸入
他的輸出
你在這個地方輸入預言家的
預言家預言的一個token
然後得到輸出
這邊輸入預言家預言的兩個token
得到的輸出
這些運算都是白費了
沒派上用場
那沒關係
假設你真正在意的是時間
這邊就是這個Speculative decoding
這個方法
就是假設你不在意運算資源
真正在意的是時間
你就是用了運算資源
來換取了你的時間
好
那這個接下來的問題就是誰可以來擔任預言家呢
那我們來想一下預言家的特性
預言家的特性就是
要超快
快到他的產生的速度
幾乎可以忽略
然後再來就是
他偶爾犯錯
沒有關係
因為最糟的狀況就是他的輸出全部都是錯的
也就是不得不失而已
所以誰可以來擔任預言家呢
也許 non-autoregressive model 可以來擔任這個預言家
記得我們講 non-autoregressive model 的時候
說它的特性就是生成快
所有的輸出都是同時生成的
所以它生成的速度很快
那壞處就是它容易犯錯
它生成的品質不好
那犯錯沒有關係
預言家是可以犯錯的
所以 non-autoregressive model
正好可以來擔任這個預言家
如果用 non-autoregressive model 來擔任預言家
Speculative decoding 也可以看成是一個 non-autoregressive model 跟 autoregressive model 的結合
那另外呢,你還可以把你的模型做壓縮以後來作為預言家
大家知道說這種大模型有很多壓縮的方式
比如說你可以對模型的參數進行量化
或你可以對模型做 knowledge distillation
總之有很多不同的技術可以把模型變小
那模型變小以後,當然跑起來就快了
但你會遇到的問題就是
模型做過壓縮以後
往往腦袋就不好使了
很容易說出奇怪的錯誤答案
但沒有關係
反正這個小模型壓縮後的模型
腦袋不好使也沒關係
它跑得快
所以它可以來擔任預言家
它犯錯也沒有關係
反正最後我們是看最終語言模型的輸出
然後再來就是預言家一定要是一個語言模型嗎
其實不一定要是語言模型才能擔任預言家
這邊有一篇paper,它是使用一個搜尋引擎來擔任預言家
假設現在的輸入是語言模型
你就去某一個資料庫裡面
這邊是用Google當例子,但也不一定要是Google啦
就用一個資料庫裡面
去搜尋看看語言模型後面都接些什麼樣的句子
我發現在資料庫裡面說語言模型後面可以接
經常使用在許多自然語言處理方面的應用
把這個句子直接貼出來
這個就是預言家的輸出
但這邊是假設說忽略
搜尋資料過的速度非常快
快到幾乎可以忽略
那你就可以拿一個搜尋引擎
來當作這個預言家
再來就是一定只能有一個預言家嗎
其實可以同時有多個預言家
每個人都產生他的預言
最後再看誰的預言最對
誰就可以最多的
幫助到語言模型的輸出
所以預言家可以不只有一個
那在這個投影片上
假設有兩個預言家
兩個預言家各自預測語言模型接下來會說什麼
然後再把他們預測的結果拿去讓語言模型生成接下來產生的Token
那因為預言家1呢
他有犯一些錯誤
他產生出來的結果跟語言模型實際上產生的結果不一樣
那我們就不要用預言家1的輸出
我們就只看語言模型採用預言家2的時候
我們就只看語言模型採用預言家2的輸出當作輸入的時候所產生的結果
所以總之你可以有多個預言家來強化這個預言正確的機會
預言越正確預測正確的token越多你節省的時間就越多
好總之這邊呢就是跟大家介紹一個新的技術叫做Speculative Decoding
那這個想法引入了一個預言家
先去預判語言模型接下來的輸出會是什麼
那可以大幅加快速度
這個方法對所有的語言模型都適用
而且不需要訓練
不需要去改動你原來的語言模型
它是一個外掛
掛上去你原來的語言模型的速度就加快了
好
那這個部分就是我今天想跟大家分享的內容
然後我們就先暫停個三分鐘
然後讓助教來換場一下
