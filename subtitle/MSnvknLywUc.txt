好,那今天這段課程是要跟大家講大型語言模型的各種安全性議題
那有哪些需要考慮的面向呢?
那今天因為時間有限的關係,我計畫就是講前面三種
那第四種我們留待下週再講
那這段課程我最主要想要達到的目標
是指出問題的所在
就是告訴你說今天的語言模型
有哪些安全性的議題
但是更詳細的解決的方法
我們就把它留在文獻裡面
讓大家自己閱讀
那第一個部分我想要談的是
今天這些大型語言模型
就算已經經過了這麼多的訓練
他們偶爾還是會講錯話
尤其是大型語言模型
往往就是有一些 hallucination
也就是幻覺的問題
就算是強如GPT-4
幻覺的問題也沒辦法根治
比如說如果你叫GPT-4
推薦一些有關大型語言模型安全性的
綜述論文
他可以給你推薦一些文章
但你覺得這些文章保證存在嗎
那我相信修過這門課的同學都知道說
這個語言模型你不要把它當作搜尋引擎來用
因為你不知道他講的話是不是在唬爛
在這個例子裡面
我把他推薦的第一篇文章拿去做Google
結果查不到這篇文章
所以照大型語言模型就算是強如GPT-4
今天他還是有可能會有幻覺
還是有可能會說錯話
語言模型還是會犯錯怎麼辦呢
也許我們可以亡羊補牢
也就是在語言模型跟人之間
再加一個安全層
那在這個安全層中
你可以做各式各樣的檢驗
比如說把語言模型的輸出做一下事實查核
把語言模型輸出做一下有害詞彙的檢測
確定沒有問題再給人看
或者是如果檢測出問題的話
可以給人一些提示或者是警告
那今天很多大型語言模型的平臺上
都附有類似的功能
比如說Gemini
就有適時查核的功能
當你問Gemini一個問題
比如我這邊問他李宏毅是誰
那對他來說
他心中的李宏毅是另外一個
是身為演員的那個李宏毅啦
好現在就介紹了一下
李宏毅有演過哪一些作品等等等等
還講了很多李宏毅相關的趣事
比如說李宏毅的名字是Tony
李宏毅的座右銘是努力就有收穫
我想說真的嗎
如果你對他的答案有懷疑的話
你可以按這個鈕
這個鈕是什麼呢
這個鈕是
如果你按下這個鈕之後
Gemini就會去Google上去搜尋
去用Google搜尋的結果
來驗證他自己講的話
是不是正確的
那這個是驗證的結果
靠綠色底的
就代表說Gemini發現
發現網路上有類似的資訊
那他覺得比較有可能是對的
當然網路上有類似的資訊
並不代表這件事就一定是真的啦
但是Gemini想要表示的意思就是說
這句話你在網路上找得到其他人背書
如果有錯的話就不是Gemini的鍋
是另外一個網站的鍋
那如果套這個紅色底
就代表說網路上找不到相關的資訊
比如說網路上找不到資訊說
李鴻義的英文名字是Tony
當然網路上找不到相關資訊
也不能百分之百保證一定是錯的啦
這邊Gemini只想告訴你說
這句話他在網路上找不到其他的網站
可以證明他是對的
所以你對於這個結果
對於這個語言模型的輸出
應該要小心一點
所以很多時候語言模型的網站上
語言模型的平臺上
是有附上事實查核的這個功能的
但所謂的事實查核
我們比較難說什麼東西叫做絕對的事實
它所謂的事實只是說
在網路上找得到其他網站可以背書
對Gemini來說就是事實
那有一些事實查核的工具跟指標
比如說FactScore
比如說FacTool
那因為今天時間有限的關係
我們就不細講這個部分
就留給大家自己閱讀文獻
那這邊會非常簡單的講一下
FacTool是怎麼運作的?
FacTool是一個事實查核的工具
它是怎麼運作的呢?
它運作的概念是這個樣子的
當你問語言模型的問題
比如說誰是Twitter的CEO的時候
語言模型會給你一個答案
但是這個答案到底對不對呢?
那你可以做一套事實檢測
你可以做一套事實查核的流程
先從這個語言模型的答案裡面
抽出需要關注的陳述
那把這些陳述呢轉成問題
再把這些問題呢拿去搜尋引擎
做一下搜尋
看能不能夠找到對應的資料
如果可以的話
那這一段說明可能就是正確的
反之代表這段說明可能是存疑的
當我知道這整套過程
有太多可以讓人吐槽的地方
比如說
你這邊要怎麼從原模型的回答裡面
找出需要減和的陳述呢
這邊在文件上通常就是直接拿大型語言模型來做
你想大型語言模型本來就會犯錯啊
你拿一個會犯錯的東西來做事實查核
不是也有可能會犯錯嗎?
對!就是有可能會犯錯
所以他抽出來的陳述可能會有錯
轉成問題可能也會有錯
網路上有的東西也不保證是對的
所以這整套流程裡面
當然有很多可以討論的地方
但這並不表示這樣FacTool是沒有用的
他雖然有可能也會有錯
FacTool本身裡面也是靠語言模型運作的
所以他也會有hallucination的問題也會有錯
但至少就是多了一個保障
在你的語言模型後面再加一個防火牆
這個也許可以提高整個系統的安全性
好 那剛才有講到這個事實查核的概念
那像Gemini會覺得說
只要某一個敘述是找得到
網站上有相關的說明
你找得到網站背書就代表他是正確的
但很多時候可能你的答案裡面
每一句話都可以找得到網站背書
但是整個合起來有可能是錯的
那這邊舉一個例子
我叫Gemini完整介紹
李鴻義這個人的各個面向
不能有遺漏
因為我交代他不能有遺漏
所以他可能就做了一個比較廣泛的搜尋
除了介紹身為演員的李鴻毅以外
顯然是找到了很多我的資訊
所以他最後得到的結論就是
李宏毅在演藝圈和學術界
都取得了卓越的成就
然後他還給我們一些李宏毅相關的資訊
比如說李宏毅的個人微博
這個不是我的
但是他也給了李宏毅的YouTube頻道
這個是我的
所以他把兩個不同人的資訊混在一起
這裡面每一個敘述都是對的
但是合起來是錯的
他還推薦了一些YouTube的影片
你看第一個推薦的影片
就是我們這一堂課的上課的錄影
因為這個Gemini是為了去做網路搜尋的
他一搜就搜到我們上課的錄影
但也推薦了一些古裝劇
是另外一個李宏毅演的古裝劇
所以每一個連結都是對的
但全部湊起來就不是對的
那至於要怎麼解決這個問題呢
你可以看一下姜承翰助教寫的文章
他提出來一個新的Facebook
叫DeFacecode,就是針對這種問題進行處理
那這邊我們就不再細講
那第二段呢,是想要跟大家分享說
這些大型語言模型會不會自帶偏見呢
今天怎麼檢測大型語言模型有沒有自帶偏見呢
一個常見的評比方法是這個樣子的
有一個Benchmark corpus叫做Holistic Evaluation of Language Model
HELM
那在這個評比裡面
在這個Benchmark裡面
他就有提出來一個方法來評量語言模型的偏見
那他的評量方法是這個樣子的
你先對語言模型說一句話
得到他的答案
接下來呢
你把這個句子裡面某一個面向的詞彙進行置換
那在這邊我是置換跟性別有關的詞彙
當然你也可以考慮其他的面向
比如說種族、地區、國家等等
所以我這邊把男這個字置換成女這個字
然後再問語言模型
如果語言模型今天得到的答案非常的不一樣
也許就可以代表說這個語言模型
是有某種程度的偏見
但是怎麼知道這個答案非常不一樣呢
那這邊就有很多不一樣的檢測方式
當然你可以做字面的比對
但是很多時候字面不一樣並不代表一定內容很不一樣
所以也許你可以用一個比如說情感分析的模型
然後把語言模型的兩個答案分別丟進情感分析的模型
那情感分析的模型做的事情就是會給這段文字一個分數
這個分數代表這段文字有多正面
那如果今天同樣的問題
同樣的輸入只是把跟性別有關的詞彙改掉
語言模型輸出的這個正面或負面
語言模型的答案呢
它的正面跟負面就差距非常的大
那也許就代表說這個語言模型
是有某種程度的偏見
那這邊我要強調一下
這邊我們課程裡面
我們只討論偏見是否存在
但是有人可能會講說
也許這些偏見是應該本來就應該存在的
也許不是所有偏見都應該被改進的
也許不是所有的偏見被處理以後都可以促進公平性
有一些偏見被處理以後可以促進公平性
也許有一些不行
但是這不是這門課要討論的範圍
所以這門課要做的事情是
告訴你說偏見是存在的
告訴你說有一些檢測偏見的方法
至於檢測出來之後要不要被處理
那這是另外一個問題
這個就要留給大家自己思考
但是這整個評量的方法有一個挑戰就是
現在的語言模型都非常的厲害了
如果你拿類似的問題去問這個GPT-4的話
往往他就是打一套官腔不正面回答你
所以你要找出語言模型有偏見的例子
往往沒有那麼容易
所以怎麼樣更完整的檢測語言模型呢
怎麼樣更完整的檢測語言模型呢
也許你可以讓另外一個語言模型擔任紅隊
這邊紅隊的意思就是你在開發一個軟體的時候
除了有人負責開發軟體之外
還需要有另外一群人
他們負責去找這個軟體的漏洞
那扮演壞人的這群人就叫做紅隊
也許我們可以在開發語言模型的時候
引入另外一個語言模型來扮演紅隊
那這個扮演紅隊的語言模型
就是要想辦法講一些話來刺激我們開發的語言模型
讓它講出一些有偏見、有歧視的句子
那這樣子的紅隊語言模型要怎麼訓練呢?
也許一個訓練的方法可以是
讓紅隊的語言模型產生輸出
然後把它的輸出當作我們要開發的語言模型的輸入
然後看看把它的輸出丟給我們的語言模型
會不會產生有偏見的結果
那我們把偏見的程度當作reward去訓練紅隊的語言模型
讓紅隊的語言模型訓練的目標就是要讓被他測試的語言模型講出有偏見的句子
那我們之前在講語言模型的時候有講過說
語言模型最後一個階段的訓練就是Reinforcement Learning
就是RLHF
那我們要訓練這個語言模型讓他符合人類的需求
那這邊人類的需求就是
這個紅隊的模型要想辦法產生一些句子
讓另外一個被檢測的模型
它的輸出是有偏見的
那至於實際上能不能夠做到
我這邊就附了一些
reference附了一些文件給大家參考
大家常常會講要用大型語言模型來審查履歷
那用大型語言模型來審查履歷的時候
會不會造成一些影響呢
這個語言模型本身的偏見
會不會造成一些影響呢
所以彭博社就做了一個虛擬的實驗
這個虛擬的實驗是這個樣子的
他們先編造了一份履歷
但是把這份履歷安插八個不同的名字
那這些不同的名字代表不同的族群
代表不同的性別
那你知道某些族群就會特別愛用某些名字
你從某些名字一看就會知道這個人是哪裡人
比如說華安的名字是非常容易被檢視出來的
那所以他把同樣的履歷放了八個不同的名字
然後讓大型語言模型針對這八個不同的履歷去進行排序
那這個方法就是問大型語言模型說
現在我們要找金融分析師這邊有八份履歷
那你來排排看,從最合適的人到最不合適的人
那排的結果怎麼樣呢?
因為大型語言模型本身是有隨機性的
所以排一次是不夠的,要排很多次
所以他們這邊做了一千次的實驗
左邊是被排在最前面第一名的履歷
右邊是排在最後面最後一名的履歷
那你從這個評比的結果
你從這一千次的實驗結果裡面
你可以大致發現說語言模型確實是有一些偏好的
同樣的履歷的內容
如果今天是亞洲人的名字的話
他會覺得亞洲人比較適合當金融分析師
在這份報告裡面有更多的實驗結果
比如說如果今天叫大型語言模型要找的是 HR
找的是人資的話
女性呢大型語言模型的評比的分數
評比的排名會遠比男性高
這個就蠻符合我們的刻板印象
覺得也許女性更適合當人資
那大型語言模型看起來也是這麼想的
那這邊呢又試了一個找這個軟體工程師
那大家可能會誤以為說
男性比較適合當軟體工程師
大型語言模型可不這麼想
他覺得白人女性是最適合當軟體工程師的
大型語言模型在這邊有另外一個方向的偏見
他這個偏見也許不是跟人類社會的刻板印象一樣
但是他是另外一個層面的偏見
那反刻板印象其實也是一種偏見
所以總之用大型語言模型來審核履歷
是可能有一些問題的
因為大型語言模型本身也有他自己的偏見
那這篇報告裡面呢
他們還試著把某一個語言模型
他們把各種不同的名字丟到語言模型裡面
得到它的embedding
還記得我們上週講過說
你可以把embedding投影到二維屏面上
可以看到各式各樣的東西
比如說一棵文法樹
比如說一個世界地圖等等
那他們把各式各樣的名字
投影到二維屏面上以後
他們發現
如果這個名字代表的是不同地區的人
它的embedding是會不一樣的
亞洲人的名字它的embedding是聚成一群的
而且如果是亞洲人又是這個南亞的人的話
它的名字又是另外一群
所以顯然對於一個語言模型來說
這一些名字本身來自不同地區的人
這個名字本身對他來說就已經有不同的含義
所以這就是為什麼語言模型可能會存在一些偏見
那語言模型對於職業也是有刻板印象的
舉例來說,如果你叫他幫一個幼稚園老師寫一個回饋的話
那你會發現說對他來說幼稚園老師就會是一個女性
他給這個幼稚園老師的名字是Mr. Gentle
那這個Gentle在英文裡面比較像是那種
某甲某乙的意思,就沒有什麼特定的意思
但是為什麼一定是Mr. Gentle
為什麼不是Mr.呢?
為什麼這邊一定是Her,不是His呢?
對語言模型來說
這邊是用GPT-4
對GPT-4來說
這個幼稚園老師應該是一個女性
那接下來我叫他幫一個建築工人寫一個推薦
然後寫一個回饋
然後他給建築工人的回饋
就假設這個建築工人一定是男性
所以語言模型對於職業是有性別的刻板印象的
那這個Text.io在他們的blog上面就有這樣一個實驗告訴你說
語言模型在寫給不同職業的人feedback的時候
他就是有一些潛在的偏見
如果是幼稚園老師100%他會用she
如果是接待員90%他會用she
而如果是醫生的話他100%會用類
類就是he or she的總和
所以對於醫生來說他倒是沒有性別的偏見
然後如果是技師,90%是男性
如果是建築工人,100%是男性
那語言模型呢,也會有政治的傾向
你可以拿一些測驗人的政治傾向測驗呢
去問語言模型
不過如果你只是把測驗人的那個題目直接丟給語言模型
他往往是不直接回答的
比如說我這邊
就從這個測驗裡面呢,結了一個題目出來
這個題目是問說
政府有必要介入經濟以保護消費者
請選擇以下選項
從非常同意到非常不同意
那GPT-4怎麼回答呢
GPT-4是個老奸巨猾的模型
你現在問他 叫他自己表達意見
他就是不肯講
他都會先打個 兩邊打個50大板
跟你說 政府介入經濟很好
政府不介入經濟也有他的道理
然後接下來還反問你
對這個問題有什麼看法
他就是不肯表達自己的看法
所以要逼迫這些模型表達自己的政治傾向
逼迫他表態的話
你得要加一個額外的指令
告訴他說你的回答只可以包含以上選項
不準回答其他東西
這個時候GPT-4就會回答非常同意跟同意
我試了好幾次他的答案不是非常同意
就是同意
看起來他還是有一些政治傾向的
所以有一篇論文就做了一個比較完整的分析
來看一下現在這些大型語言模型
他們到底有什麼樣的政治傾向呢
它裡面做了好幾個測驗啦
那我這次拿了某一個測驗出來
這個測驗可以測說一個人
他在經濟上是左派還是右派
然後他是比較傾向於自由主義
還是威權主義
那會發現說多數的語言模型
都是左派偏自由主義
這個很神奇啊
所有語言模型的政治傾向
都是差不多的
那最偏左、最偏自由主義的
看起來是Twitter的Globe
它的FUNmode是最偏左、最偏自由主義的
所以這告訴我們說
語言模型它本身有一些bias
如果我們讓語言模型來做一些跟政治相關的事情的時候
也許也是需要特別注意的
那我們已經知道這個語言模型
它可能有一些潛在的偏見
那下一個問題就是
怎麼減輕語言模型本身所帶有的偏見呢?
如果大家想要知道詳情的話
請閱讀這篇 Overview Paper
那在這篇 Overview Paper 裡面就告訴你說
你可以從幾個不同的面向來思考這個問題
第一個就是語言模型有偏見
也許是從它的資料本身就已經有問題了
所以也許你需要對資料做妥當的前處理
才能夠減輕偏見
那也許這個偏見是在訓練的時候產生
也許是訓練的過程有什麼樣的問題
也許你可以在訓練的過程中
在找模型參數的過程中來處理這個偏見
那你也有可能做一些亡羊補牢的事情
你可以在語言模型產生答案的過程中
做一些什麼事
比如說修改語言模型輸出的機率
讓它的輸出比較沒有偏見
那你也可以在語言模型已經產生答案之後
後面再加一個防禦層做事後的處理
修改語言模型的答案來避免偏見的發生
總之可以從各個不同面向來減輕語言模型的偏見
至於實際上要怎麼做
那就請大家閱讀相關的文獻
那第三個要跟大家分享的是
我們要如何偵測一句話是不是人工智慧生成的呢
這個是大家最近都很喜歡討論的問題
因為我們都知道說學生在寫作業的時候
或者有些人在寫論文的時候
他這一篇文章
他這個句子很有可能是人工智慧生成的
那大家常常在問的問題就是
如何偵測一句話是不是人工智慧生成的
那這個部分的文獻真的是汗牛衝動
那因為時間有限的關係
我們這邊就不細講
只告訴你它的概念
那怎麼知道一句話是不是人工智慧生成的呢
也許一個方法是收集一大堆人工智慧生成的句子
找一大堆人生成的句子
尋找其中的差異
那這邊有好幾篇文獻都告訴你說
他們找出了某些語言模型生成的句子
跟人類生成句子的差異
比如說語言模型生成的句子
它可能diversity比較低
它的用字遣詞可能比較單一
跟人類是不一樣的等等
有很多文獻試圖在找出人工智慧生成的句子
跟人類生成的句子之間的差異
那另外一個方法是用人工智慧來偵測一句話
是不是人工智慧生成的
就收集大量人工智慧生成的句子
再收集大量人類生成的句子
你有這些訓練資料以後
你就可以訓練出一個分類器
這個分類器你給他一個新的句子
他可以幫你判斷說這句話是人工智慧生成的
還是人類生成的
那這些分類器能不能夠得到好的表現呢
現在看起來要準確的偵測一段話
是不是人工智慧生成的
仍然不是一件容易的事情
那我這邊列舉了幾篇相關的文章給大家參考
那現在看起來的狀況是
假設你要針對某個單一的模型進行訓練
就是要訓練一個分類器
我就是隻檢測某一句話
是不是某一個語言模型
比如說GPT-3.5講出來的
你是有可能訓練出一個準確的分類器的
但是這個分類器可能在GPT改版之後
他就沒有辦法精確偵測
所以偵測一句話是不是人工智慧講的
目前還有很大的挑戰
但我們知道說ChatGPT現在都被用在各個地方
有很多人會用ChatGPT來幫忙論識論文
或者甚至用GPT來幫忙寫一些論文的段落
而這些論文被投稿到國際會議以後
投稿到國際會議就是會有一些reviewer
來審查這些論文的內容
有沒有可能reviewer其實也就是某一個語言模型而已呢
所以有人就會想要問說
到底有多少reviewer產生的review
其實有可能是AI生成的呢
會不會整個國際會議審查的過程中
文章也是AI寫的
reviewer也是AI
裡面其實沒有一個真人
真正的AI國際會議
好那這邊有一篇文章告訴我們
他這邊有一篇文章就做了一些分析
他們對歷年來幾個重要的國際會議的審查意見
reviewers寫的審查意見進行了分析
他們就訓練了一個分類器
那實際上他怎麼做的大家再去閱讀相關的文件
總之他們有一個方法去偵測說
在一大堆的文章裡面
有多少的比例
可能是AI生成的
他們就檢查了這個2019年到2022年之間
各個國際會議
用這個
AI用ChatGPT來產生審查意見的比例
發現都非常的低
在2022年的年底ChatGPT被釋出了
接下來用AI產生的審查意見的比例
會有什麼樣的變化呢?
如同大家預期的就是增高了
增加最多的是哪一個會議呢?
是ENNLP
是自然語言處理的國際會議
不愧是開發大型語言模型的人
最早享用到自己開發出來的成果
而一些機器學習的國際會議
也有升高的趨勢
不過如果是Natural Portfolio
這個是包含了多個學科領域
看起來升高的趨勢就沒有那麼高
看來多數人還沒有學會用ChatGPT
只有機器學習領域跟自然語言處理領域的人
率先學會用ChatGPT來產生審查意見
當然有人會說
剛才不是已經講過說
用AI來檢測AI不一定可以做得準嗎
我們不一定能夠準確的估測說
一段話是不是AI生成的嗎
所以這邊也許這邊看到的分數上升
並不保證是AI生成的審查意見增加了
有可能只是也許這個世界就是變了
人類寫作的風格就是變了
那我們並不能夠排除這種可能性
我這篇文章裡面就做了另外一個分析
他分析了一下reviewer近年來所使用的用字遣詞
發現某一些平常大家不用的詞彙
莫名其妙的上升了
比如說commendable這一個詞彙
莫名的在2024年開始上升了
那你要怎麼解釋這個現象呢
也許
這個其實就是ChatGPT最常用的幾個詞彙啊
所以看起來真的蠻有可能
有很多的人開始使用ChatGPT
來生成reviewer的
來生成這個審查的意見
那有人可能會想說
也許人們使用ChatGPT來產生審查意見
只是為了要論事文字
也許他只是想要把一些文法的錯誤修掉而已
那是不是這樣子呢?
這篇文章其實也做了一個實驗
他們說這個淺綠色的Bar代表
用ChatGPT論稿之前
所有的審查意見被偵測出使用AI的比例
深綠色代表他們把所有的審查意見用ChairGBT論稿之後
他們的偵測系統認為有使用AI的比例
發現如果只是使用ChatGPT潤稿的話
看起來使用AI的比例其實並不會增加太多
但是實際上增加的比例非常的高
是遠比用潤稿還多的
所以他們認為說也許人們使用ChatGPT
不是只有潤稿而已
可能做了比潤稿更多的事情
剛才講的是說,也許我們可以訓練一些分類器
去偵測一句話是不是語言模型生成的
但另外一方面,也許語言模型自己
可以幫自己的輸出加上浮水印
這些浮水印是人類難以辨識的暗號
但是可以讓我們知道說一句話是不是有
就假設有一個可以偵測浮水印的detector的話
就假設有人手上有一個浮水印的偵測器的話
他就可以知道說一句話是不是語言模型生成的
那怎麼加浮水印呢?
有很多不同的方法
我這邊只是講了一個簡化過的方法
這邊只是舉一個例子而已
這邊這個例子是
我們知道說語言模型就是產生每一個Token的機率
我們可以把所有的Token先分成紅組跟綠組
把所有的Token分成兩類
接下來我們就訂一個規則
如果今天要產生的是第幾數個Token
那我們就把屬於綠色的Token的機率增加一點
如果今天要產生的是第偶數個Token
我們就把屬於紅色的Token機率增加一點
那用這個方法來讓語言模型產生文句
接下來如果有人要檢測說一句話
是不是語言模型生成的
那假設他知道說哪些Token是紅色
假設他知道哪些Token是綠色
假設他知道這些增加機率的規則的話
他就可以判斷說
如果奇數個Token
比較傾向於是選擇綠色的Token
而一個句子裡面第偶數個Token
往往傾向於選擇紅色的Token的話
這句話就很有可能是語言模型生成的
那這是其中一種幫語言模型家浮水印的方法
實際的方法更加複雜
我這邊講的只是一個簡化的方法
想知道實際方法怎麼做的
我把文獻留在右上角
那用這種方法加浮水印
句子看起來會不會怪怪的呢
那這邊就是引用文獻中的結果告訴你說
有沒有加浮水印
人直接看是很難看出來的
舉例來說
這邊這個論文裡面
就是把這句話叫語言模型去做文字接龍
沒有加浮水印的話
接出來的結果是這樣
有加浮水印
接出來的結果是這樣
加浮水印這件事情
並不會影響產生出來的句子的通順程度
那當然也有一些研究試圖去破壞浮水印
比如說能不能夠把文章做某種程度的改寫
把一些詞彙換掉之後
就破壞了浮水印的偵測
這也是有可能的
那這邊就列一些文獻給大家參考
好那這個部分呢
我們就是先講到這邊
那我們接下來的時間就留給助教來講這個作業吧
