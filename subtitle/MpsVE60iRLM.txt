好 大家好
我今天要來跟大家介紹 SUPERB 這個計畫
這個 SUPERB 是語音的自督導式學習計畫
好 那在 SUPERB 這個 Project 裡面
我們要做些什麼呢
那我們先從這個自督導式學習
怎麼被用在語音上開始說起
那這個自督導式學習
那想必大家也不太陌生
那如果你有上
我的 Machine Learning 的課程的話
那在課程中一定已經聽過 BERT 這個東西
BERT 是訓練在文字上的
Self-Supervised Learning
自督導式學習的模型
那如果你有在做 Computer Vision
相關的東西的話
那你一定聽過 SimCLR
SimCLR 是影像上的自督導式學習模型
那在語音上的進展又是如何呢
那我今天就想跟大家分享一下
語音上最新的 Self-Supervised Learning 的進展
好 那在開始之前
我們還是講一下
Self-Supervised Learning 的 Framework
那我們這邊用語音來作為例子跟大家舉例
那我們知道說今天的學習
今天我們在做 Machine Learning 的時候
我們常常會把學習拆成兩個階段
第一個階段叫做 Pre-Train
在 Pre-Train 這個階段
我們有大量的沒有標註的資料
那我們今天是用語音為例
那你就可以想像說
機器去網路上爬了非常非常大量的聲音訊號
在 YouTube 上爬了一大堆的聲音訊號下來
這些聲音訊號是沒有文字的標註的
那這些沒有標註的資料
可以拿來做什麼事情呢
這些沒有標註的資料
會被拿來訓練一個 Upstream 的模型
那訓練這個 Upstream 的模型
是不需要任何標註的資料的
這個 Upstream 的模型
它的功能是輸入一段聲音訊號
它就會輸出一排 Representations
它就會輸出一排向量
那怎麼訓練這個 Upstream 的模型呢
那時間有限的關係
我們這邊就不細講
那有非常非常多各式各樣不同的方法
可以在不使用任何標註資料的情況下
訓練出這一個 Upstream 的模型
舉例來說
你可以把聲音訊號的一段把它蓋住
要求 Upstream 模型在訓練的時候
把被蓋住的部分還原
那我們知道在文字上
BERT 就是使用這樣子的訓練方法
那你也可以做
比如說 Contrastive Learning
大家知道說這個影像上的 SimCLR
叫做 Contrastive
Contrastive Learning
那在語音上也可以做非常類似的事情
那有很多各式各樣不同的方法
那這邊就不一一列舉
那最有趣的地方是
當我們在做這個 Pre-Train 的時候
這個第一階段的 Training
它是 Task-Agnostic
也就是說
我們在做第一階段 Training 的時候
我們並不知道這個 Upstream 的模型
接下來會被用在什麼應用上
這個訓練是 Task-Agnostic
在訓練的時候
我們只使用了大量沒有標註的資料
來訓練出這個模型
那至於這個模型
接下來要怎麼被用在其他的應用中
我們是不知道的
那在今天這個 Talk 裡面
我們把這個透過大量未標註資料
在 Phase 1 所訓練出來的模型
叫做 Upstream Model
那你也可能在其他的文獻上
看到其他的稱呼
比如說有人會叫做 Self-Supervised 的 Model
然後最近有另外一個很紅的詞
叫做 Foundation Model
Foundation Model
指的也是 Upstream 的 Model
好 在語音上
像這一種 Upstream 的 Model
已經有好幾個知名的模型了
你可以把它們想成就是語音版的 BERT
在文字上我們有 BERT
有 GPT
在語音上其實也已經有一系列
訓練好的 Upstream 的模型
是你可以拿來使用的
好 那在 Pre-Train 的階段
用大量未標註的資料
訓練出 Upstream 的模型以後
接下來就進入第二階段
在第二階段裡面
你有一個明確想要解決的問題
你有一個待解的任務
舉例來說
在語音上
那你馬上可以聯想到的任務
也許就是做語音辨識
把聲音訊號轉寫為文字
當然跟語音相關的任務不是只有語音辨識
你等一下會看到更多各式各樣
不同的 Downstream 的任務
好 那在這個 Downstream 的任務裡面
我們有少量的標註資料
如果做語音辨識
你有聲音訊號跟聲音訊號對應的文字
那你會用這些少量的標註資料
去訓練一個 Downstream 的模型
那這個 Downstream 的模型
往往相較於 Upstream 的模型還要簡單得多
我們通常就只需要訓練一個
簡單的 Downstream 的 Model
如果你的 Upstream 的 Model 做得夠好
你只要一個很簡單的 Downstream Model
就可以解你想要解的任務
那在語音辨識裡面
這個 Downstream 的 Model
它要做的事情就是輸入一段聲音訊號
輸出語音辨識的結果
那你就是用這些少量的標註資料
去訓練出 Downstream 的模型
那你在文獻上也會看到說
有些時候我們會選擇用這些有標註的資料
也去微調 Upstream 的模型
不過這邊我用虛線來表示
就是告訴你說
這個微調不見得是必要的
好 那在語音上
已經有一大堆的 Upstream 的模型
那這些 Upstream 的模型在文獻上
往往顯示在語音辨識上有不錯的表現
但我接下來就有了一個問題
這些模型它們是語音辨識的專才
還是人類語言相關任務的通才呢
這些模型在文獻上
往往已經顯示在語音辨識上有不錯的結果
但是它是只能用在語音辨識上嗎
還是它可以用在各式各樣跟語音相關的任務
這些模型是語音辨識的通才
還是只要拿出一個語音相關的任務
你都可以使用這些
用大量未標註的語音訊號
所訓練出來的 Upstream Model
或是講得更具體一點
我們剛才已經看到說
我們可以在 Upstream Model 後面
疊一個 Downstream Model
用少量的
拿來訓練語音辨識系統的標註資料
來訓練這個 Downstream Model
可以在語音辨識上有好的表現
那同一個 Upstream Model
可以用在其他任務上嗎
假設我們要做的不是語音辨識
而是語者辨識
那你有一些語者辨識可以用的標註資料
你能不能夠訓練另外一個 Downstream Model
這個 Downstream Model
也直接疊在 Upstream Model 後面
接下來在語者辨識上也有好的結果呢
那如果你問我
這些模型是專才還是通才
在幾個月之前
我可能會回答你說
我認為這些模型比較有可能是專才
為什麼我會這麼想呢
因為你知道這個語音辨識
語者辨識
它們其實是非常不一樣的任務
在語音辨識裡面
你要你的模型去無視語者間的差異
不同的人說同樣的詞彙
他的發音是不一樣的
但是你要你的語音辨識系統
去無視語者間的差異
但是今天如果你要做的是語者辨識系統
你要知道現在是誰在說話
那你要你的系統做的事情
是找出語者間的差異
所以大家可以了解說
語音辨識是要無視語者間的差異
語者辨識是要找出語者間的差異
這兩個任務
這兩個目標
根本就是互相衝突的
所以如果你把語音辨識做好
你可能在另外一個任務上
就不一定可以有好的表現
雖然我推論說這些模型可能是專才
但是沒有關係我們還是來做一下實驗
看看這些模型到底在語音辨識以外的任務上
有什麼樣的表現
於是就有了 SUPERB 這一個構想
那 SUPERB
是 Speech Processing Universal
PERformance Benchmark 的縮寫
那這個計畫的成員
不只是來自臺灣大學
還有成員來自 CMU
MIT
Johns Hopkins
Facebook AI
還有 LXT
那我們在今年的 Inter Speech
也發表了一篇論文
所以等一下的實驗
如果你有什麼不清楚的地方
可以去看我們發表在 Inter Speech
的這一篇論文
那 SUPERB 的概念是這個樣子
我們已經有這些 Upstream 的 Model
我們把它拿到同一個競技場上做公平的評比
所以你可以把 SUPERB 想成是
Self-Supervised 語音模型的奧林匹克運動會
但是這些模型它要做的不只是單一任務
這些模型它要參加的是鐵人十項
我們要把同一個模型
在多個不同的任務上進行評比
好 那這個是我們有評比的模型
它的一些基本的資料
那你可以從這個表格上看到說
這些模型它們有很大的不同的差異
舉例來說
如果看訓練的方法的話
那我們這邊列舉了每一個模型訓練的方法
那不同的訓練方法
我們都用一個簡單的符號來表示它
那這邊至少可以看到
這些模型是用 6 種不同的訓練方法
做各式各樣不同的組合訓練出來的
好 那我們來介紹一下我們的比賽項目
那首先我們有一些跟內容相關的任務
那有哪些跟內容相關的任務
我們有 Phoneme Recognition
給機器一段聲音訊號
然後辨識出聲音訊號裡面的 Phoneme
如果你不知道 Phoneme 是什麼
你可以把它簡單想成
是類似 KK 音標這樣的東西
然後做 Keyword Spotting
讓機器聽一段聲音
它要知道說這段聲音說的是哪一個 Keyword
那當然有大家最熟悉的語音辨識
機器聽一段聲音
辨識出正確的對應的文字
那我們還有 Query-By-Example
Query-By-Example 這個任務是要做什麼呢
Query-By-Example 這個任務是說
假設你有一段很長的錄音檔
比如說你有一段新聞
你有上課的錄音
那你現在想要從這段錄音裡面
去搜尋有沒有提到某一件事
那怎麼搜尋呢
你用說的
說出你的問題
舉例來說
你想要知道這一小時的新聞內
有沒有提到 COVID-19
那你就說 COVID-19
然後機器就會比對說 COVID-19
這段聲音訊號它背後所對應的文字
有沒有在這一段語音檔裡面有出現
那機器會自動把有提到這個關鍵字
有出現的地方
把它標示出來
那這個是 Query-By-Example
那我們還有跟語者相關的任務
我們有 Speaker 的 Identification
Speaker Identification 要做的事情
就是給機器看一段聲音訊號
它要決定說這段聲音訊號是誰說的
我們有 Speaker 的 Verification
Speaker Verification
是給機器聽兩段聲音
它要決定這兩段聲音是同一個人說的
還是不同人說的
這個 Speaker Identification
跟 Speaker Verification
雖然名字聽起來有點像
但是其實這是兩個不同的任務
還有 Speaker Diarization
Speaker Diarization 要做的事情
就是讓機器聽一段聲音檔
這段聲音檔可能是像是 Meeting 的錄音
那有好多個人同時講話
這些人甚至有可能會搶拍
這些人甚至可能會同時說話
然後所以你的機器要自動知道說
哪些聲音訊號是 A 講的
哪些聲音訊號是 B 講的
哪些時候是 A B 同時說話
這個是 Speaker Diarization
那我們還有跟與語義相關的任務
我們有語音的 Intent Classfication
就是讓機器聽一段聲音訊號
它要知道這段聲音訊號
背後所對應的意圖是什麼
那我們還有 Slot Filling
語音版的 Slot Filling
讓機器聽一段聲音訊號
它從這段聲音訊號裡面擷取出重要的資訊
它要知道說
舉例來說這是一個訂票系統
有人說他要一張從台北到紐約的機票
那機器要知道說這個台北是出發地
紐約是目的地
那像一般這種跟語義理解比較相關的任務
傳統的解法都是先做一下語音辨識
把聲音訊號轉寫成文字
在文字上面進行理解
但我們這邊做的是（N2N）的模型
也就是說機器沒有先做語音辨識再做理解
並不是把語音辨識跟理解拆成兩個階段
而是我們有一個 Unified 的模型
有一個 end2end 的模型
這個end2end的模型
直接是聲音訊號當中輸入
輸出直接就是理解的結果
那我們也做情緒辨認
那給機器一段聲音訊號
機器要判斷說現在這段聲音訊號講的人
他是什麼樣的情緒
所以我們總共有 10 個不同的任務
好 那我們有參加比賽的成員
也已經介紹完比賽的項目
接下來就開始第一回合的比賽
那在第一回合的比賽裡面
我們訂了比較嚴苛的限制
我們訂了什麼樣嚴苛的限制呢
首先我們要求 Upstream 的模型是固定的
我們並不會拿標註的資料
去微調 Upstream 的模型
Upstream 的模型
只用大量未標註的資料訓練
訓練好以後它就固定住了
它不會隨著任務而改變
我知道假設你有在使用
比如說文字的 BERT 的話
你可能會很習慣說像 BERT 這種模型
在不同的任務上都需要微調一下
那你可能會問說
這裡為什麼在語音上
我們不跟著一起微調 Upstream 的 Model 呢
微調 Upstream 的 Model
會給我們更好的結果嗎
那其實我們有做過相關的實驗
如果有微調 Upstream 的 Model
會給我們更好的結果
但是在這個第一輪的比賽裡面
我們想給這些模型更大的限制
我們想要在它手腳上加上鉛塊
看看大家有綁住鉛塊的情況下
會有什麼樣的表現
所以我們給 Upstream 的 Model
一個比較大的限制
Upstream 的 Model
不可以隨著不同的任務而改變
它的參數必須是固定的
好 那我們就會取 Upstream Model 的
最後一個 Layer 出來
把它丟到不同的 Downstream Model 裡面
去解不同的任務
那另外 Downstream Model 的
這個 Network Architecture 是事先定好的
對所有不同的 Upstream 模型而言
用的都是同樣的 Downstream 的 Network 架構
好 那在設計這個 Network 架構的時候
我們設計的原則
就是希望這些 Downstream Model 的
Network 的架構越簡單越好
它用的參數越少越好
所以假設一個任務
它是可以用 Linear 的 Layer 來解的
我們就用 Linear 的 Layer
但是有些任務真的是比較複雜
比如說語音辨識
比如說語音的 Slot Filling
真的用一個 Layer 呢
用 Linear 的 Layer 真的沒有辦法解
那我們就採取一層
或者是兩層的 LSTM 來解它
但總之大原則是
希望 Downstream 的模型越簡單越好
那有人可能會問說
為什麼要設這麼大的限制呢
那理由是這個樣子的
假設我們有剛才上述的限制
那我們就可以用同一個 Upstream Model
去解所有的任務
那因為今天這些 Downstream 的 Model
我們都刻意設計的非常簡單
如果在 Downstream Model
非常簡單的前提之下
這個不同的任務上
我們還是可以得到好的結果
那意味著什麼
意味著這個 Upstream 的模型非常地厲害
我們在比 Pre-Train 的階段
Upstream 的 Model 就學會了
抽一種 Universal 的 Feature
這種 Universal 的 Feature
是可以在所有的任務上
都有好的表現的
因為不要忘了 Upstream 的 Model
它的參數是固定住的
我們要用一個模型就去打天下
要用一個模型就去解 10 個不同的任務
如果固定一個模型
可以解 10 個不同的任務代表說
這個模型很厲害啊
它抽出來的 Feature
是所有的任務都可以派得上用場
而且這些 Downstream 的模型很弱
所以要把任務解好
不能憑藉 Downstream 的模型
它沒有什麼用的
我們要靠 Upstream 的模型抽出好的 Feature
所以如果今天在我們
剛才講的這個 Setting 之下
跟剛才講的這個設置之下
可以 10 個任務都得到好的結果
那就意味著我們有一種 Universal 的 Feature
那未來如果有新的任務進來
也許我們就完全不需要調 Upstream 的模型
直接用這個Upstream 的模型
就可以去解新的任務
所以有了這麼厲害的 Upstream 的模型
可以抽 Universal 的 Feature
未來有新的語音相關的任務
你就不用發愁
直接套用一模一樣的 Upstream 的模型
就可以解了
當然這是一個理想
我們還沒有看到現有的這些
語音上的 Upstream Model
它們在 10 個任務上的表現
但如果它們在 10 個任務上都可以表現好
那就意味著這些模型很厲害
未來有新的任務
都直接套用這些模型就好了
完全不需要再做什麼新的設計
真的有這麼理想嗎
那我們就來看看第一輪比賽的結果
好 那這個是第一輪比賽的結果
每一個 Column
每一個 Column 代表一個任務
那這邊總共有 10 個 Column
代表 10 個任務
4 個跟 Content 相關的任務
3 個跟語者相關的任務
2 個跟語意相關的任務
1 個跟情緒相關的任務
那每一個 Row 呢
每一個 Row 是什麼呢
每一個 Row 代表了不同的 Upstream 的模型
第一個 Row 代表的是
沒有使用 Upstream 模型的傳統方法
我就是沒有做 Self-Supervised Learning
直接抽與過去在這個語音領域
常用的這個 Fbank 的 Feature
就直接抽這個 Fbank 的 Feature
把它用在10個 Downstream 的任務上面
那這邊我們嘗試了
各式各樣不同的 Upstream 的模型
來看看它們在 10 個任務上的表現如何
好 那我知道這個圖表
其實沒有那麼容易解讀
因為上面有很多數字
而且不同的任務呢
用的那個評比的標準是不同的
有的任務是數值越大越好
有的任務是數值越小越好
所以在短時間內
要解讀這個表格是不太容易的
好 那所以在這邊呢
我們直接摘要一下比賽的結果
我們把表現比 Fbank 還要差的模型
就塗一個黑色
所以塗黑色的地方代表說
表現是比 Fbank 還要差的
代表說這個 Self-Supervised Learning
沒有派上用場
第一階段訓練出來的 Upstream 的模型
在某一個任務上沒有派上用場
那我們得到什麼樣的結論呢
我們現在得到的結論是
在這個表格上
你會看到沒有塗黑的地方
是比有塗黑的地方多的
所以代表說 Upstream 的模型
在很多時候都還是有派上用場的
那你會發現機器在某一些任務上
它真的沒有做得很好
有一些任務比如說 ASV SD Slot Filling 等等
對模型來說
對這些 Upstream Model 來說
要表現好還是比較掙扎的
那這邊你會發現說
我們這個 Wav2vec 跟 HuBERT
Wav2vec 2.0 跟 HuBERT
我們用的都是 Base Model
我們沒有用 Large Model
為什麼不用 Large Model 呢
因為神奇的事情
是我們發現這些 Large Model
在第一回合的比賽裡面
表現都非常非常地慘
比 Baseline Fbank 還要慘
慘到沒有把它放在這個表格裡面
那為什麼這些大的模型表現這麼差呢
那也許是因為在第一輪的比賽裡面
我們給它太大的限制
所以我們現在來進入第二輪的比賽
在第二輪的比賽裡面
我們稍微放寬了比賽的限制
我們放寬了什麼樣的限制呢
在第一輪的比賽裡面
我們只使用最後一個 Layer
那在第二輪的比賽裡面
我們放寬了這個限制
其他限制都還是在的
我們一樣只用簡單的 Downstream Model
我們一樣固定住 Upstream Model 參數
但現在唯一的調整是
我們在使用這個 Upstream Model 的時候
我們不是只抽 Upstream Model
最後一層的 Representation
而是讓 Downstream Model 有機會去選擇
它要抽 Upstream Model 的哪一層出來
進行使用
好 那之所以會有這樣的設計
是因為我們發現說
今天在一個 Self-Supervised 的模型裡面
它的每一層
是可以學到不同的資訊的
那最好的資訊
最豐富的資訊
不一定儲存在最後一層的輸出裡面
它可能存在中間的層
所以我們希望讓 Downstream 的模型
有機會挑選它要使用哪一層
但是 Upstream 的模型
它的參數仍然是固定住的
怎麼讓 Downstream 的模型去挑選
它要使用哪一層呢
那這邊的做法就是我們讓這個
我們把 Upstream 的模型
它每一層的輸出拿出來做 Weighted Sum
每一層的 Representation
拿出來做 Weighted Sum
得到新的 Representation
把 Weighted Sum 的結果
新的 Representation
當做 Downstream 模型的輸入
而這個 Weighted Sum 的 Weight
每一層都有一個 Weight
這個 Weighted Sum 的 Weight
是跟 Downstream 模型一起訓練的
所以你可以想成這個 Weighted Sum 的 Weight
算是 Downstream 模型的其中一個部分
Downstream 模型自己去學習
挑選它要使用 Upstream 模型哪一層的資料
好 那接下來我們就來看一下
第二回合比賽的結果
你發現在第二回合的比賽裡面
有塗黑的部分又變得更少了
顯示說當我們今天採用這種
Weighted Sum 的方法
我們今天讓 Downstream 的模型
可以選擇要用 Upstream 模型的哪一個 Layer
用這樣子的方法
確實可以得到更好的結果
那你也可以發現說
有好幾個模型
它都可以在 10 個不同的任務上面
都比 Fbank 還要有更好的表現
包括 NPC DeCcoAR 2.0
Wav2vec 2.0 系列 HuBERT 系列等等
它們都可以在 10 個任務上
有比 Baseline Fbank 還要有更好的表現
所以看起來有好多
Self-Supervised Learning 的模型
都可以做到 10 項全能
好 那這個就是今天的結論
就這個是一個我自己被光速打臉的過程
就假設你今天問我說
這些模型是專才還是通才
我會告訴你說神奇的事情是
這些透過大量未標註語音訊號
訓練出來的模型
它們居然是通才
它們至少在語音的 10 個不同任務上
都可以比過去使用這個 Fbank 的模型
有更好的表現
那下一階段
可以研究的一個問題就是
這些模型怎麼做到這件事情
這在第一階段 Pre-Train 的時候
我們並不知道
第二階段會有什麼樣 Downstream 的任務
但這一些 Upstream 的 Model
是如何學到 Universal 的呢
那這是下一階段
接下來可以研究的問題
好 那講了這麼多
如果你發現你自己 Train 的
語音的 Self-Supervised 的模型
沒有出現在這個比賽裡面
那怎麼辦呢
沒有關係
現在你可以自己上傳
你的 Self-Supervised Learning 的模型
到 SUPERB 的 Benchmark
好 那想要知道怎麼上傳的話
請參見 SUPERB 的官網
官網連結的 QR Code
我放在這個投影片的左邊
好 那現在呢
Pubic 的 Leaderboard 已經在線上了
你已經可以把結果 Submit 到
Pubic 的 Leaderboard 上
那 10 月中以後呢
會有 Hidden Data Set 的 Leaderboard 上線
10 月中以後可以 Submit 到
Hidden Data Set 的 Leaderboard 上
那這個呢
是跟大家介紹一下 SUPERB 這個計畫
好 那假設你正在研究這個
Self-Supervised Learning 的模型的話
不要錯過了 AAAI 2022 的
Self-Supervised Learning For Speech
And Audio Processing 的 Workshop
那我們投稿的截止日期是 11 月 12 號
好 那這個 Workshop 的網頁呢
QR Code 我也放在投影片的右邊
好 那如果你有在做 Self-Supervised Learning
在語音上相關的研究的話
那你不要錯過這個
IEEE JSTSP 的 Special Issue
那在 IEEE JSTSP 上面有一個 Special Issue
是 Self-Supervised Learning For Speech
And Audio Processing
Deadline 是今年的年底
今年的最後一天
那投稿的相關資訊
我也放在右下角的 QR Code
好 最後總結一下
今天想要其實跟大家業配的內容
那這個如果你正在研究語音的
Self-Supervised Learning 的話
到今年結束前
有三件事情是你不能錯過的
第一件事情
參加 SUPERB 的 Challenge
把你的結果上傳到 SUPERB 的 Leaderboard
第二個投稿到明年 AAAI 的 Workshop
Deadline 是 11 月 12 號
第三個投稿到 IEEE JSTSP Special Issue
Deadline 是今年的最後一天
好 這個就是要跟大家業配的內容
謝謝大家
謝謝
