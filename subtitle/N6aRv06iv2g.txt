好 各位同學大家好
我們來上課吧
好 那接下來啊
我們要講 Decoder
那我們上上週呢
已經講了 Encoder
那接下來呢
我們要講 Decoder
那 Decoder 呢 其實有兩種
等一下呢
會花比較多時間介紹
你比較常見的
這個叫做 Autoregressive 的 Decoder
那這個 Autoregressive 的 Decoder
是怎麼運作的呢
那等一下我們是用語音辨識
來當作例子來跟大家說明
或用在作業裡面的機器翻譯
其實是一模一樣的
你只是把輸入輸出
改成不同的東西而已
好 那語音辨識是怎麼做的呢
語音辨識你知道
語音辨識就是輸入一段聲音
輸出一串文字
那你會把一段聲音輸入給 Encoder
比如說你對機器說
機器學習
機器收到一段聲音訊號
聲音訊號呢 進入 Encoder以後
輸出會是什麼呢
輸出會變成一排 Vector
那我們上週花了很多時間講 Encoder
裡面有什麼樣的內容
它裡面非常地複雜
如果你忘了的話就算了
你就記得說 Encoder 做的事情
就是輸入一個 Vector Sequence
輸出另外一個 Vector Sequence
那接下來呢
就輪到 Decoder 運作了
Decoder 要做的事情就是產生輸出
接下來輪到 Decoder 產生語音辨識的結果
那 Decoder 怎麼產生這個語音辨識的結果呢
那 Decoder 做的事情
就是把 Encoder 的輸出先讀進去
那至於怎麼讀進去
那這個我們等一下再講 我們先
你先假設 Somehow 就是有某種方法
把 Encoder 的輸出讀到 Decoder 裡面
這步我們等一下再處理
那 Decoder 怎麼產生一段文字呢
語音辨識 機器的輸出就是一段文字
Decoder 怎麼產生一段文字呢
那首先呢
你要先給它一個特殊的符號
這個特殊的符號呢
代表開始
那在助教的投影片裡面呢
是寫 Begin Of Sentence
縮寫是 BOS
這邊會怕你不知道 BOS 是什麼啦
所以我就把它的意思明確地寫出來
就是開始
就是 Begin 的意思
那這個是一個 Special 的 Token
你就是在你的那個 Lexicon 裡面啊
你就在你可能
本來 Decoder 可能產生的文字裡面呢
多加一個特殊的符號
多加一個特殊的字
那這個字呢
就代表了 BEGIN
代表了開始這個事情
好 所以 Decoder 呢
就吃到這個特殊的符號
那在這個機器學習裡面啊
假設你要處理 NLP 的問題
每一個 Token
你都可以把它用一個 One-Hot 的 Vector 來表示
One-Hot Vector 就其中一維是 1
其他都是 0
所以 BEGIN 也是用 One-Hot Vector 來表示
其中一維是 1
其他是 0
好 那接下來呢
Decoder 會吐出一個向量
這個向量裡面有什麼呢
這個 Vector 裡面有什麼呢
這個 Vector 的長度啊
它很長
它的長度呢
跟你的 Vocabulary 的 Size 是一樣的
那 Vocabulary 則是什麼意思呢
你就先想好說
你的 Decoder 輸出的單位是什麼
假設我們今天做的是中文的語音辨識
我們 Decoder 輸出的是中文
那你這邊的 Vocabulary 的 Size 啊
可能就是中文的方塊字的數目
那中文的方塊字有多少呢
那不同的字典
給你的數字可能是不一樣的
那常用的中文的方塊字
大概兩 三千個
那一般人呢
可能認得的四 五千個
那在更多都是罕見字 冷僻的字
你可能看到也不知道它要怎麼唸
你平常寫也寫不出來
所以你就看看說
你要讓你的 Decoder
輸出哪些可能的中文的方塊字
你就把它列在這邊
那舉例來說
你覺得這個 Decoder
能夠輸出常見的 3000 個方塊字就好了
就把它列在這個地方
那不同的語言啊
它輸出的單位 不見不會不一樣
這個取決於你對那個語言的理解
比如說英文
你可以選擇輸出字母的 A 到 Z
輸出英文的字母
但你可能會覺得字母這個單位太小了
有人可能會選擇輸出英文的詞彙
英文的詞彙是用空白作為間隔的
但如果都用詞彙當作輸出
又太多了
所以你會發現
剛才在助教的投影片裡面
助教說他是用 Subword 當作英文的單位
就有一些方法
可以把英文的字首字根切出來
拿字首字根當作單位
那如果中文的話呢
我覺得就比較單純
通常今天你可能就用中文的這個方塊字
來當作單位
好 那在這個向量裡面啊
這個向量的長度
就跟中文裡面的方塊字
你希望機器可以輸出的方塊字的數目
是一樣多的
那每一個中文的字
都會對應到一個數值
那因為在產生這個向量之前啊
你通常會先跑一個 Softmax
就跟做分類一樣
做分類在得到最終的輸出前
我們不是會跑一個 Softmax 嗎
跑一個 Softmax
所以這一個向量裡面的分數啊
它是一個 Distribution
也就是呢
它這個向量裡面的值呢
它全部加起來
總和呢 會是 1
好 那得到這個向量以後
還不是最終輸出的結果
這個向量會給每一個中文的字一個分數
那分數最高的那一個中文字
它就是最終的輸出
那在這個例子裡面
機的分數最高
所以機呢
就當做是這個 Decoder 第一個輸出
然後接下來
你把機當做是 Decoder 新的 Input
原來 Decoder 的 Input
只有 BEGIN 這個特別的符號
現在它除了 BEGIN 以外
它還有機作為它的 Input
那機呢
也是表示成一個 One-Hot 的 Vector
當做 Decoder 的輸入
所以 Decoder 現在它有兩個輸入
一個是 BEGIN 這個符號
一個是機
根據這兩個輸入呢
它就要得到一個輸出
它輸出一個藍色的向量
根據這個藍色的向量裡面
給每一個中文的字的分數
那我們會決定第二個輸出是什麼
誰 哪一個字的分數最高
它就是輸出
器的 假設器的分數最高
那器就是輸出
那 Decoder 接下來會拿器當作輸入
然後現在 Decoder 看到了 BEGIN
看到了機 看到了器
那它接下來呢
還要再決定接下來要輸出什麼
那它可能呢
就輸出學
這一個過程就反覆的持續下去
機器輸入學以後
學會再被當作輸入
所以現在 Decode 呢
它看到了 BEGIN 機 器還有學
那 Encoder 這邊其實也有輸入啦
那我們等一下再講 Encoder 的輸入
Decoder 是怎麼處理的
所以 Decoder 看到 Encoder 這邊的輸入
看到機 看到器 看到學
決定接下來呢
要輸出習
它會輸出一個向量
這個向量裡面
習這個中文字的分數最高的
所以它就輸出習
然後這個 Process 呢
就反覆持續下去
那這邊有一個關鍵的地方
我們特別用紅色的虛線把它標出來
也就是說 Decoder 看到的輸入
其實是它在前一個時間點
自己的輸出
Decoder 會把自己的輸出
當做接下來的輸入
會把自己的輸出
當做接下來的輸入
所以當我們 Decoder 在產生一個句子的時候
它其實有可能看到錯誤的東西
因為它看到的是自己的輸出嘛
那如果今天 Decoder 有語音辨識的錯誤
它把機器的器
辨識錯成天氣的氣
那接下來 Decoder 就會看到錯誤的辨識結果
它還是要想辦法根據錯誤的辨識結果
產生它想要產生的
期待是正確的輸出
那你可能會覺得說
讓 Decoder 看到錯誤的輸入
讓 Decoder 看到自己產生出來的錯誤的輸入
再被 Decoder 自己吃進去
會不會造成問題呢
會不會造成 Error Propagation 的問題呢
所謂 Error Propagation 的問題就是
一步錯 步步錯這樣
就是在這個地方
如果不小心把機器的器
不小心寫成天氣的氣
會不會接下來就整個句子都壞掉了
都沒有辦法再產生正確的詞彙了
有可能
那這個等一下
我們最後會稍微講一下
這個問題要怎麼處理
我們現在呢
先無視這個問題
繼續走下去
好 那我們
那我們來看一下這個 Decoder 啊
它內部的結構長什麼樣子
那我們這邊呢
把 Encoder 的部分先暫時省略掉
那在 Transformer 裡面
Decoder 的結構呢
長得是這個樣子的
看起來有點複雜
比 Encoder 還稍微複雜一點
那我們現在先把 Encoder 跟 Decoder 放在一起
稍微比較一下它們之間的差異
那你會發現說
如果我們把 Decoder 中間這一塊
中間這一塊把它蓋起來
其實 Encoder 跟 Decoder
並沒有那麼大的差別
你看 Encoder 這邊
Multi-Head Attention
然後 Add & Norm
Feed Forward
Add & Norm
重複 N 次
Decoder 其實也是一樣
當我們把中間這一塊遮起來以後
我們等一下再講
遮起來這一塊裡面做了什麼事
但當我們把中間這塊遮起來以後
欸 那 Decoder 也是
有一個 Multi-Head Attention
Add & Norm
然後 Feed Forward
然後 Add & Norm
所以 Encoder 跟 Decoder
其實並沒有非常大的差別
除了中間這一塊不一樣的地方
被遮起來的地方以外
其實 Encoder 跟 Decoder 是一樣的
那只是最後呢
我們可能會再做一個 Softmax
使得它的輸出變成一個機率
那這邊有一個稍微不一樣的地方是
在 Decoder 這邊啊
Multi-Head Attention 這一個 Block 上面
還加了一個 Masked
這個 Masked 是什麼意思呢
這個 Masked 的意思是這樣子的
這是我們原來的 Self-Attention
Input 一排 Vector
Output 另外一排 Vector
這一排 Vector 每一個輸出
都要看過完整的 Input 以後
才做決定
所以輸出 b1 的時候
其實是根據 a1 到 a4 所有的資訊
去輸出 b1
當我們把 Self-Attention
轉成 Masked Attention 的時候
它的不同點在哪裡呢
它的不同點是
現在我們不能再看右邊的部分
也就是產生 b1 的時候
我們只能考慮 a1 的資訊
你不能夠再考慮 a2 a3 a4
產生 b2 的時候
你只能考慮 a1 a2 的資訊
不能再考慮 a3 a4 的資訊
產生 b3 的時候
你就不能考慮 a4 的資訊
產生 b4 的時候
你可以用整個 Input Sequence 的資訊
這個就是 Masked 的 Self-Attention
講得更具體一點
你做的事情是這樣
當我們要產生 b2 的時候
我們只拿 b2 的
我們只拿第二個位置的 Query
去跟第一個位置的 Key
和第二個位置的 Key
去計算 Attention
第三個位置跟第四個位置
就不管它
不去計算 Attention
我們這樣子不去管這個 a2 右邊的地方
只考慮 a1 跟 a2
只考慮 q1 q2
只考慮 k1 k2
q2 只跟 k1 跟 k2 去計算 Attention
然後最後只計算 b1 跟 b2 的 Weighted Sum
然後當我們輸出這個 b2 的時候
b2 就只考慮了 a1 跟 a2
就沒有考慮到 a3 跟 a4
那為什麼會這樣呢
為什麼需要加 Masked 呢
這件事情其實非常地直覺
怎麼說
你想想看我們一開始 Decoder 的運作方式
它是一個一個輸出
它的輸出是一個一個產生的
所以是先有 a1 再有 a2
再有 a3 再有 a4
這跟原來的 Self-Attention 不一樣
原來的 Self-Attention
a1 跟 a4 是一次整個輸進去你的 Model 裡面的
在我們講 Encoder 的時候
Encoder 是一次把 a1 跟 a4
都整個都讀進去
但是對 Decoder 而言
先有 a1 才有 a2
才有 a3 才有 a4
所以實際上
當你有 a2
你要計算 b2 的時候
你是沒有 a3 跟 a4 的
所以你根本就沒有辦法把 a3 a4 考慮進來
所以這就是為什麼
在那個 Decoder 的那個圖上面
Transformer 原始的 Paper 特別跟你強調說
那不是一個一般的 Attention
這是一個 Masked 的 Self-Attention
意思只是想要告訴你說
Decoder 它的 Tokent
它輸出的東西是一個一個產生的
所以它只能考慮它左邊的東西
它沒有辦法考慮它右邊的東西
好 但是講到這裡啊
我們講了一下
我們講了 Decoder 的運作方式
但是這邊呢
還有一個非常關鍵的問題
這個關鍵的問題是
Decoder 必須自己決定
輸出的 Sequence 的長度
可是到底輸出的 Sequence 的長度應該是多少呢
我們不知道
你沒有辦法輕易的從輸入的 Sequence 的長度
就知道輸出的 Sequence 的長度是多少
並不是說
輸入是 4 個向量
輸出一定就是 4 個向量
這邊在這個例子裡面
輸入跟輸出的長度是一樣的
但是你知道實際上在你真正的應用裡面
並不是這樣
輸入跟輸出長度的關係
是非常複雜的
我們其實是期待機器可以自己學到
今天給它一個 Input Sequence 的時候
Output 的 Sequence 應該要多長
但在我們目前的這整個 Decoder
的這個運作的機制裡面
機器不知道它什麼時候應該停下來
它產生完習以後
它還可以繼續重複一模一樣的 Process
就把習呢
當做輸入
然後也許 Decoder 呢
就會接一個慣
然後接下來呢
就一直持續下去
永遠都不會停下來
這讓我想到什麼呢
就讓我想到推文接龍啦
我不知道大家知不到這是什麼
這是一個這個古老的民俗傳統
流傳在 PTT 上面
這個民俗傳統是怎麼運作的呢
就有一個人呢
先推一個中文字
然後推一個超
然後接下來呢
就會有另外一個鄉民呢
去推另外一個字
然後可以接上去的
所以就可以產生一排的詞彙啦
一排字啦
就是超人正大中天外飛仙草
不知道在說些什麼
這個是 Process 呢
可以持續好幾個月
都不停下來
我也不知道為什麼
那怎麼讓這個 Process 停下來呢
那要怎麼讓它停下來呢
要有人冒險去推一個斷
推個斷
它就停下來了
所以我們要讓 Decoder 做的事情
也是一樣
要讓它可以輸出一個斷
所以你要特別準備一個特別的符號
這個符號呢
就叫做斷
我們這邊呢
用 END 來表示這個特殊的符號
所以除了所有中文的方塊字
還有 BEGIN 以外
你還要準備一個特殊的符號
叫做斷
那其實在助教的程式裡面啊
它是把 BEGIN 跟 END
就是開始跟這個斷
用同一個符號來表示
反正這個
這個 BEGIN 只會在輸入的時候出現
斷只會在輸出的時候出現
所以在助教的程式裡面
如果你仔細研究一下的話
會發現說 END 跟 BEGIN
用的其實是同一個符號
但你用不同的符號
也是完全可以的
也完全沒有問題
好 所以我們現在呢
這樣 Decoder 它可以輸出斷這個符號
輸出 END 這個符號
那我們期待說
當今天呢
產生完習以後
再把習當作 Decoder 的輸入以後
Decoder 就要能夠輸出斷
也就是說
當把習當作輸入以後
就 Decoder 看到 Encoder 輸出的這個 Embedding
看到了 BEGIN
然後機 器 學 習以後
看到這些資訊以後 它要知道說
這個語音辨識的結果已經結束了
不需要再產生更多的詞彙了
它產生出來的向量裡面
這個 END
就是斷的那個符號
它的機率必須要是最大的
然後你就輸出斷這個符號
那整個運作的過程
整個 Decoder 產生 Sequence 的過程
就結束了
好 那這個呢
就是 Autoregressive 的 Decoder
它運作的方式
好 那接下來呢
我們用兩頁投影片
非常簡短地講一下
Non-Autoregressive 的 Model
Non-Autoregressive 呢
通常縮寫成 NAT
所以有時候 Autoregressive 的 Model
也縮寫成 AT
Non-Autoregressive 的 Model 是怎麼運作的呢
這個 Autoregressive 的 Model 是
先輸入 BEGIN
然後出現 w1
然後再把 w1 當做輸入
再輸出 w2
直到輸出 END 為止
那 NAT 是這樣
它不是一次產生
就假設我們現在產生是中文的句子
它不是一次產生一個字
它是一次把整個句子都產生出來
怎麼一次把整個句子都產生出來呢
NAT 的 Decoder 啊
它可能吃的是一整排的 BEGIN 的 Token
你就把一堆一排 BEGIN 的 Token 都丟給它
讓它一次產生一排 Token 就結束了
舉例來說
如果你丟給它 4 個 BEGIN 的 Token
它就產生 4 個中文的字
變成一個句子
就結束了
所以它只要一個步驟
就可以完成句子的生成
這邊你可能會問一個問題
剛才不是說不知道要有多
輸出的長度應該是多少嗎
那我們這邊怎麼知道 BEGIN 要放多少個
當做 NAT Decoder 的收入呢
沒錯 這件事沒有辦法很自然的知道
沒有辦法很直接的知道
所以有幾個
所以有幾個做法
一個做法是
你另外扔一個 Classifier
這個 Classifier 呢
它吃 Encoder 的 Input
然後輸出是什麼
輸出是一個數字
這個數字代表 Decoder 應該要輸出的長度
所以你就扔一個 Classifier
這個 Classifier
可能吃 Encoder 的 Input
可能吃
把 Encoder Output 那些向量讀進去
它 Output 一個數字
比如說 Output 4
它 Output 4 以後
Decoder 就會吃到
NAT 的 Decoder
就會吃到 4 個 BEGIN 的 Token
然後它就產生 4 個中文的字
這是一種可能的做法
另一種可能做法就是
你就不管三七二十一
給它一堆 BEGIN 的 Token
你就假設說
你現在輸出的句子的長度
絕對不會超過 300 個字
你就假設一個句子長度的上限
然後 BEGIN 呢
你就給它 300 個 BEGIN
然後就會輸出 300 個字嘛
然後呢
你再看看說什麼地方輸出了斷
什麼地方呢
輸出 END
輸出 END 右邊的
就當做它沒有輸出
就結束了
這是另外一種處理 NAT 的這個 Decoder
它應該輸出的長度的方法
那 NAT 的 Decoder
它有什麼樣的好處呢
它第一個好處是
平行化
這個 AT 的 Decoder
它在輸出它的句子的時候
是一個一個一個字產生的
所以你有你的
假設要輸出長度一百個字的句子
那你就需要做一百次的 Decode
但是 NAT 的 Decoder 不是這樣
不管句子的長度如何
都是一個步驟就產生出完整的句子
所以在速度上
NAT 的 Decoder 它會跑得比
AT 的 Decoder 要快
那你可以想像說
這個 NAT Decoder 的想法顯然是在
由這個 Transformer 以後
有這種 Self-Attention 的 Decoder 以後才有的
對不對
因為以前如果你是用那個 LSTM
用 RNN 的話
那你就算給它一排 BEGIN
它也沒有辦法同時產生全部的輸出
它的輸出還是一個一個產生的
所以在沒有這個 Self-Attention 之前
只有 RNN
只有 LSTM 的時候
根本就不會有人想要做什麼 NAT 的 Decoder
不過自從有了 Self-Attention 以後
那 NAT 的 Decoder
現在就算是一個熱門的研究的主題了
那 NAT 的 Decoder 還有另外一個好處就是
你比較能夠控制它輸出的長度
怎麼說呢
舉語音合成為例好了
其實在語音合成裡面
NAT 的 Decoder 算是非常常用的
它並不是一個什麼稀罕 罕見的招數
比如說有
所以語音合成今天你都可以用
Sequence To Sequence 的模型來做
那最知名的呢
是一個叫做 Tacotron 的模型
那它是 AT 的 Decoder
那有另外一個模型叫 FastSpeech
那它是 NAT 的 Decoder
那 NAT 的 Decoder 有一個好處
就是你可以控制你輸出的長度
那我們剛才說怎麼決定
NAT 的 Decoder 輸出多長呢
你可能有一個 Classifier
決定 NAT 的 Decoder 應該輸出的長度
那如果在做語音合成的時候
假設你現在突然想要讓你的系統講快一點
加速
那你就把那個 Classifier 的 Output 除以二
它講話速度就變兩倍快
然後如果你想要這個講話放慢速度
那你就把那個 Classifier 輸出的那個長度啊
它 Predict 出來的長度乘兩倍
那你的這個 Decoder 呢
說話的速度就變兩倍慢
所以你可以如果有這種 NAT 的 Decoder
那你有 Explicit 去 Model
Output 長度應該是多少的話
你就比較有機會去控制
你的 Decoder 輸出的長度應該是多少
你就可以做種種的變化
那為什麼 NAT 的 Decoder
最近是一個熱門的研究主題呢
它之所以是一個熱門研究主題
就是它雖然表面上看起來有種種的厲害之處
尤其是平行化是它最大的優勢
但是 NAT 的 Decoder 呢
它的 Performance
往往都不如 AT 的 Decoder
所以發現有很多很多的研究試圖讓
NAT 的 Decoder 的 Performance 越來越好
試圖去逼近 AT 的 Decoder
不過今天你要讓 NAT 的 Decoder
跟 AT 的 Decoder Performance 一樣好
你必須要用非常多的 Trick 才能夠辦到
就 AT 的 Decoder 隨便 Train 一下
NAT 的 Decoder 你要花很多力氣
才有可能跟 AT 的 Performance 差不多
為什麼 NAT 的 Decoder Performance 不好呢
有一個問題我們今天就不細講了
叫做 Multi-Modality 的問題
那如果你想要這個深入了解 NAT
那就把之前上課
助教這個上課補充的內容呢
連結放在這邊給大家參考
NAT 也是一個大坑啊
助教也是講了一個半小時
才稍微給大家一個 NAT 的
大概 大致的描述
所以這也是個大坑
不是我們今天可以講的
就只是告訴大家說
世界上有 NAT 這個東西而已
那接下來我們就要講
Encoder 跟 Decoder
它們中間是怎麼傳遞資訊的了
也就是我們要講
剛才我們刻意把它遮起來的那一塊
那如果你仔細觀察這一塊的話
這塊呢叫做 Cross Attention
它是連接 Encoder 跟 Decoder 之間的橋樑
那這一塊裡面啊
你會發現有兩個輸入來自於 Encoder
Encoder 提供兩個箭頭
然後 Decoder 提供了一個箭頭
然後所以從左邊這兩個箭頭
Decoder 可以讀到 Encoder 的輸出
那這個模組實際上是怎麼運作的呢
那我們就實際把它運作的過程跟大家展示一下
好 這個是你的 Encoder
輸入一排向量
輸出一排向量
我們叫它 a1 a2 a3
接下來呢 輪到你的 Decoder
你的 Decoder 呢
會先吃 BEGIN 當做
BEGIN 這個 Special 的 Token
那 BEGIN 這個 Special 的 Token 讀進來以後
你可能會經過 Self-Attention
這個 Self-Attention 是有做 Mask 的
然後得到一個向量
就是 Self-Attention 就算是有做 Mask
還是一樣輸入多少長度的向量
輸出就是多少向量
所以輸入一個向量 輸出一個向量
然後接下來把這個向量呢
乘上一個矩陣做一個 Transform
得到一個 Query 叫做 q
然後這邊的 a1 a2 a3 呢
也都產生 Key
Key1 Key2 Key3
那把這個 q 跟 k1 k2 k3
去計算 Attention 的分數
得到 α1 α2 α3
當然你可能一樣會做 Softmax
把它稍微做一下 Normalization
所以我這邊加一個 '
代表它可能是做過 Normalization
接下來再把 α1 α2 α3
就乘上 v1 v2 v3
再把它 Weighted Sum 加起來會得到 v
那這一個 V
就是接下來會丟到 Fully-Connected 的
Network 做接下來的處理
那這個步驟就是 q 來自於 Decoder
k 跟 v 來自於 Encoder
這個步驟就叫做 Cross Attention
所以 Decoder 就是憑藉著產生一個 q
去 Encoder 這邊抽取資訊出來
當做接下來的 Decoder 的
Fully-Connected 的 Network 的 Input
好 那這個就是 Cross Attention 運作的過程
當然這個
就現在假設產生第二個
第一個這個中文的字產生一個機
接下來的運作也是一模一樣的
輸入 BEGIN 輸入機
產生一個向量
這個向量一樣乘上一個 Linear 的 Transform
得到 q'
得到一個 Query
這個 Query 一樣跟 k1 k2 k3
去計算 Attention 的分數
一樣跟 v1 v2 v3 做 Weighted Sum 做加權
然後加起來得到 v'
交給接下來 Fully-Connected Network 做處理
好 所以這就是這個
Cross Attention 的運作的過程
那這邊有一個實際的
文獻上的 Cross Attention
它所做的事情的效果展示
不過我這邊要稍微說明一下
這個圖並不是來自於 Transformer
這篇 Paper 它的 Title 叫做
Listen, Attend And Spell
是比較早使用 Sequence To Sequence Model
成功做語音辨識的一篇文章
它發表在 ICASS 2016
2016 年的 ICASS
我記得那一次是在上海辦的
我還特別有親耳聽了這篇 Paper 的報告
這個現場也是人山人海
大家都覺得 哇 這個非常的神奇
Sequence To Sequence 硬做
居然可以做語音辨識
而且跟 State Of The Art 的結果
當時也就只差了一點點而已
當時這篇 Paper 裡面的結果
它展現的 Sequence To Sequence Model
其實沒有贏過 State Of The Art
就是當時最好的語音辨識系統
但是只差一點點而已
所以讓大家覺得說
Sequence To Sequence 用在語音辨識上
似乎是有潛力的
所以它其實不是那個 Transformer
那時候的 Encoder 跟 Decoder 都是用 LSTM
不過那個時候就已經有 Cross Attention
這樣子的機制了
所以在有 Transformer 之前
其實就已經有 Cross Attention 這樣的機制
只是沒有 Self-Attention 的機制
所以是先有 Cross Attention
後來才有 Self-Attention
然後那個時候啊
如果你是用 Sequence To Sequence 的 Model
不知道為什麼 Paper 的 Title
一定要有三個動詞
對 那個時候一定要有三個動詞
才能夠代表你是在做
Sequence To Sequence 的 Model
所以像這篇是 Listen, Attend And Spell
告訴它說機器要聽聲音
然後做 Attention 就 Cross Attention
Spell 就是把它聽到的東西把它拼出來
然後我特別放這個圖是想要跟你
讓你想可以比較容易想像
這個 Cross Attention 是怎麼運作的
好 這一段是聲音訊號
是機器的輸入
那聲音訊號輸入給這個 Encoder 的時候
它是用一串向量來表示
一排向量來表示
所以這是時間
然後這邊是一排一排 一個一個的向量
然後這一排呀
是 Decoder 的輸出
Decoder 一次只吐一個英文的字母
所以它會吐 h 吐 o 吐 w
就代表 how
那如果它已經到一個詞彙的邊界
它會自動吐出空白
空白當做是一個特殊的字母來處理
所以機器 Decoder 有可能輸出空白
代表一個詞彙結束了
換一個新的詞彙
接下來輸出 much 就是 much
然後再輸出空白
再輸出 w
這句話其實是一個英文的繞口令啦
這句話完整的句子是
How much wood would a woodchuck chuck
實際上內容是什麼其實不太重要
它就是一句繞口令就是了
那很神奇
機器可以一次吐一個字母
然後而且它可以拼對正確的詞彙
好 那這邊這個值是什麼呢
這個值就是 Attention 的分數
所以當你要產生這個 H 的時候
在產生這個 H 之前
你的 Decoder 會去對 Encoder 的輸出
做 Attention
所以它就 Attend 在這個地方然後產生 h
然後 Attend 在這個地方產生 o
Attend 在這個地方產生 w
這邊顏色越深就代表說
那個 Attention 的那個分數啊
就那個 α 的值越大
所以你會發現說它產生 h 的時候
它就是聽到這個地方有 h 的聲音所以產生 h
那接下來再往右移一點產生 o
再往右移一點產生 w
然後接下來呢
Attend 在這個地方產生 Space
然後 Attend 在這個地方產生 m
那你會看到說這個 Attention 的這個 Weight
是由左上到右下移動的
那跟你想像 Attention 應該運作的機制很像
因為當我們這邊每次產生一個詞彙的時候
我們想要專注我們想要考慮的聲音訊號
應該就是由左向右
所以確實如果你看 Model Attention 的話
它可能也是從左上
它的這個分數高的地方
可能也是從左上一直排到右下
好 那講到這邊也許有同學會問說
那這個 Encoder 有很多層啊
Decoder 也有很多層啊
從剛才的講解裡面好像聽起來
這個 Decoder 不管哪一層
都是拿 Encoder 的最後一層的輸出這樣對嗎
對
在原始 Paper 裡面的實做是這樣子
那一定要這樣嗎
不一定要這樣
你永遠可以自己兜一些新的想法
所以我這邊就是引用一篇論文告訴你說
也有人嘗試不同的 Cross Attension 的方式
Encoder 這邊有很多層
Decoder 這邊有很多層
為什麼 Decoder 這邊每一層都一定要看
Encoder 的最後一層輸出呢
能不能夠有各式各樣不同的連接方式
這完全可以當做一個研究的問題來 Study
好 那最後呢
我們就要講訓練這件事了
我們已經講了 Encoder
講了 Decoder
講了 Encoder Decoder 怎麼互動的
我們已經
你已經清楚說 Input 一個 Sequence
是怎麼得到最終的輸出
那接下來就進入訓練的部分
我們剛才都還沒有講訓練的部分喔
我們剛才講的都還只是
假設你模型訓練好以後它是怎麼運作的
它是怎麼做 Testing 的
它是怎麼做 Influence 的
Influence 就 Testing 啦
所以當我說 Inference 的時候
我指的就是 Testing
我指的是同一件事情
那是怎麼做訓練的呢
接下來就要講怎麼做訓練
那如果是做語音辨識
那你要有訓練資料
需要什麼樣訓練資料呢
你要蒐集一大堆的聲音訊號
每一句聲音訊號都要有工讀生來聽打一下
打出說它的這個對應的詞彙是什麼
工讀生聽這段是機器學習
他就把機器學習四個字打出來
所以就知道說你的這個 Transformer
應該要學到 聽到這段聲音訊號
它的輸出就是機器學習這四個中文字
那怎麼讓機器學到這件事呢
我們已經知道說輸入這段聲音訊號
第一個應該要輸出的中文字是機
所以今天當我們把 BEGIN
丟給這個 Encoder 的時候
它第一個輸出應該要跟機越接近越好
什麼叫跟機越接近越好呢
機這個字會被表示成一個 One-Hot 的 Vector
在這個 Vector 裡面
只有機對應的那個維度是 1
其他都是 0
這是正確答案
那我們的 Decoder
它的輸出是一個 Distribution
是一個機率的分布
我們會希望這一個機率的分布
跟這個 One-Hot 的 Vector 越接近越好
所以你會去計算這個 Ground Truth
跟這個 Distribution 它們之間的 Cross Entropy
然後我們希望這個 Cross Entropy 的值
越小越好
就這樣
那你可能會發現說這件事情跟分類很像
沒錯 它就跟分類很像
剛才助教在講解作業的時候也有提到這件事情
你可以想成每一次我們在產生
每一次 Decoder 在產生一個中文字的時候
其實就是做了一次分類的問題
中文字假設有四千個
那就是做有四千個類別的分類的問題啦
好 所以實際上訓練的時候這個樣子
我們已經知道輸出應該是機器學習這四個字
那你就告訴機器說
你就告訴你的 Decoder 說
現在你第一次的輸出 第二次的輸出
第三次的輸出 第四次輸出
應該分別就是機 器 學跟習
這四個中文字的 One-Hot Vector
我們希望我們的輸出
跟這四個字的 One-Hot Vector 越接近越好
在訓練的時候
每一個輸出都會有一個 Cross Entropy
每一個輸出跟 One-Hot Vector
跟它對應的正確答案都有一個 Cross Entropy
都有一個 Cross Entropy
我們要希望所有的 Cross Entropy 的總和最小
越小越好
所以這邊做了四次分類的問題
我們希望這些分類的問題
它總合起來的 Cross Entropy 越小越好
那這邊不要忘了還有斷這個東西
還有 END 這個東西
所以其實今天假設這個句子
它的中文的字是四個
但是你學習的時候
你要 Decoder 輸出的不是只有這四個中文字
你還要叫它記得說這四個中文字輸出完以後
你還要記得輸出斷這個特別的符號
所以你要告訴你的 Decoder 說
你最終第五個位置輸出了這個向量
應該跟斷的 One-Hot Vector
它的 Cross Entropy 越小越好
那這個就是 Decoder 的訓練
你把 Ground Truth 給它
正確答案給它
希望 Decoder 的輸出跟正確答案越接近越好
那這邊有一件值得我們注意的事情
這件事是這樣
你看看 Decoder 的輸入是什麼
Decoder 的輸入是正確答案
我們會給 Decoder
在訓練的時候我們會給 Decoder 看正確答案
也就是我們會告訴它說
在已經有 BEGIN
在有機的情況下你就要輸出器
有 BEGIN 有機 有器的情況下輸出學
有 BEGIN 有機 有器 有學的情況下輸出習
有 BEGIN 有機 有器 有學 有習的情況下
你就要輸出斷
在 Decoder 訓練的時候
我們會在輸入的時候給它正確的答案
那這件事情叫做 Teacher Forcing
那我也不
其實我不太確定為什麼叫 Teacher Forcing
好像是老師會強迫你做什麼事情一樣
聽起來好像沒有很好
但是這個技術呢就叫做 Teacher Forcing
也就是我們把正確的答案
當作 Decoder 的輸入
那這個時候你馬上就會有一個問題了
這個訓練的時候
Decoder 有偷看到正確答案了
但是測試的時候
顯然沒有正確答案可以給 Decoder 看
剛才也有強調說在真正使用這個模型
在 Influence 的時候
Decoder 看到的是自己的輸入
這中間顯然有一個 Mismatch
對 這中間有一個 Mismatch
那等一下我們會有一頁投影片的說明
有什麼樣可能的解決方式
那接下來呢
就是要講訓練 Transformer 的一些
訓練這種
但不侷限於 Transformer 啊
訓練這種 Sequence To Sequence Model 的
一些 Tips
好 第一個 Tips 是 Copy Mechanism
對很多任務而言
在我們剛才的討論裡面
我們都要求 Decoder 自己產生輸出
但是對很多任務而言
也許 Decoder 沒有必要自己創造輸出出來
它需要做的事情
也許是從輸入的東西裡面複製一些東西出來
那我們有沒有辦法讓 Decoder 複製
從輸入複製一些東西出來呢
其實是有辦法
那像這種複製的行為在哪些任務會用得上呢
一個例子是做聊天機器人
舉例來說
人對機器說 你好 我是庫洛洛
庫洛洛就是團長啦
他是誰其實也沒那麼重要
也很久沒有見到他了
我是庫洛洛
然後機器應該回什麼呢
機器應該回答說
庫洛洛你好 很高興認識你
那對機器來說
它其實沒有必要創造庫洛洛這個詞彙
這對機器來說一定會是一個非常怪異的詞彙
所以它可能很難
在訓練資料裡面可能一次也沒有出現過
所以它不太可能正確地產生這段詞彙出來
這段句子出來
但是假設今天機器它在學的時候
它學到的並不是它要產生庫洛洛這三個中文字
它學到的是看到輸入的時候說我是某某某
就直接把某某某
不管這邊是什麼複製出來說某某某你好
那這樣子機器的訓練顯然會比較容易
它顯然比較有可能得到正確的結果
所以複製對於對話來說
可能是一個需要的技術 需要的能力
我這邊舉另外一個例子
小傑不能用念能力了
他不能用念能力了
你可能會回答說
你所謂的不能用念能力是什麼意思
對機器來說它要做的事情
去複述這一段它聽不懂的話
那它不需要從頭去創造這一段文字
它要學的也許是從使用者
人的輸入去 Copy 一些詞彙當做它的輸出
或者是在做摘要的時候
你可能更需要 Copy 這樣子的技能
所謂的摘要就是
你要訓練一個模型
然後這個模型去讀一篇文章
然後產生這篇文章的摘要
那這個任務完全是有辦法做的
你就是蒐集大量的文章
那每一篇文章都有人寫的摘要
然後你就訓練一個
Sequence-To-Sequence 的 Model
就結束了
你要做這樣的任務
只有一點點的資料是做不起來的
有的同學收集個幾萬篇文章
然後訓練一個這樣的
Sequence-To-Sequence Model
發現結果有點差
然後來問我為什麼
這時候我就告訴你說
你要訓練這種
你要叫機器說合理的句子
通常這個百萬篇文章是需要的
所以如果你有百萬篇文章
那些文章都有人標的摘要
那有時候你會把
直接把文章標題當作摘要
那這樣就不需要花太多人力來標
你是可以訓練一個
直接可以幫你讀一篇文章
就下一個標題 做個摘要的模型
但是我們知道說做摘要的時候
有時候很多的詞彙
你其實就是直接從
原來的文章裡面複製出來的
對不對
小時候老師叫我們寫
國文課的課文的那個摘要的時候
其實你也沒有自己創造詞彙對不對
你也是從這個課文裡面找一些句子出來
然後把它改寫一下
其實就變成摘要了
所以對摘要這個任務而言
其實從文章裡面直接複製一些資訊出來
可能是一個很關鍵的能力
那 Sequence-To-Sequence Model
有沒有辦法做到這件事呢
那簡單來說就是有
那我們就不會細講
最早有從輸入複製東西的能力的模型
叫做 Pointer Network
那這個過去上課是有講過的
我把錄影放在這邊給大家參考
好 那後來還有一個變形
叫做 Copy Network
那你可以看一下這一篇
Copy Mechanism
就是 Sequence-To-Sequence
有沒有問題
你看 Sequence-To-Sequence Model
是怎麼做到從輸入複製東西到輸出來的
好 那其實這個 Sequence-To-Sequence Model
因為你知道機器就是一個黑盒子
有時候它裡面學到什麼東西
你實在是搞不清楚
那有時候它會犯非常低級的錯誤
什麼樣低級的錯誤呢
這邊就舉一個真實的低級錯誤的例子
這邊舉的例子是語音合成
那今天語音合成
你完全可以就是訓練一個
Sequence-To-Sequence 的 Model
Sequence-To-Sequence Model
大家都很熟
Transformer 就是一個例子
你就拿出來
然後輸入是什麼
你就蒐集很多的聲音
文字跟聲音訊號的對應關係
蒐集很多的文字跟聲音訊號對應關係
告訴你的
然後接下來告訴你的
Sequence-To-Sequence Model 說
看到這段中文的句子
你就輸出這段聲音
然後就沒有然後
就硬 Train 一發就結束了
然後機器就可以學會做語音合成了
那像這樣的方法做出來結果怎麼樣呢
其實還不錯
舉例來說我叫機器連說 4 次發財
看看它會怎麼講
機器輸出的結果是這樣子的
發財 發財 發財 發財
就發現很神奇
我輸入的發財是明明是同樣的詞彙
只是重複 4 次
機器居然自己有一些抑揚頓挫
你說它為什麼有抑揚頓挫
你再仔細聽聽看
發財 發財 發財 發財
4 個發財的聲音不是一樣的
發財 發財 發財 發財
它是有抑揚頓挫的
它怎麼學到這件事
不知道
它自己訓練出來就是這個樣子
那你讓它講 3 次發財
發財 發財 發財
也沒問題
那它講 2 次發財
發財 發財
也沒問題
讓它講 1 次發財
財 財 財 財
發現怎麼沒有唸發
它不發 為什麼
就是不知道為什麼這樣子
就是你這個 Sequence-To-Sequence Model
有時候 Train 出來就是
會產生莫名其妙的結果
也許在訓練資料裡面
這種非常短的句子很少
所以機器它根本沒有辦法處
不知道要怎麼處理這種非常短的句子
你叫它唸發財
它把發省略掉只唸財
你居然叫它唸 4 次的發財
重複 4 次沒問題
叫它只唸一次
居然會有問題
就是這麼的奇怪
當然其實這個例子並沒有那麼常出現
就這個用 Sequence-To-Sequence
Learn 出來 TTS
也沒有你想像的那麼差
這個要找這種差的例子也是挺花時間的
要花很多時間才找得到這種差的例子
但這樣子的例子是存在的
好 所以怎麼辦呢
有一個可能的方法是
因為我們剛才發現說機器居然漏字了
輸入有一些東西它居然沒有看到
我們能不能夠強迫它
一定要把輸入的每一個東西通通看過呢
這個是有可能的
這招就叫做 Guided Attention
那像這種 Guiding 的任務都用得上
我覺得它最適合的是
像這種語音辨識 語音合成這種任務
因為像語音辨識這種任務
你其實很難接受說
你講一句話
今天辨識出來
居然有一段機器沒聽到
或語音合成你輸入一段文字
語音合出來居然有一段沒有唸到
這個人很難接受
那如果是其它應用
比如說 Chat Bar
或者是 Summary
可能就沒有那麼嚴格
因為對一個 Chat Bar 來說
輸入後一句話
它就回一句話
它到底有沒有把整句話看完
其實你 Somehow 也不在乎
你其實也搞不清楚
但是對語音辨識 語音合成
Guiding Attention
可能就是一個比較重要的技術
Guiding Attention 要做的事情就是
要求機器去
我們要去這個 Guide
去領導這個 Attention 的過程
要求機器它在做 Attention 的時候
是有固定的方式的
舉例來說
對語音合成或者是語音辨識來說
我們想像中的 Attention
應該就是由左向右
好 在這個例子裡面
我們用紅色的這個曲線
來代表 Attention 的分數
這個越高就代表 Attention 的值越大
那如果今天不管是
做語音辨識還是語音合成
我們以語音合成為例好了
那你的輸入就是一串文字
那你在合成聲音的時候
顯然是由左唸到右
所以機器應該是
先看最左邊輸入的詞彙產生聲音
再看中間的詞彙產生聲音
再看右邊的詞彙產生聲音
如果你今天在做語音合成的時候
你發現機器的 Attention
是顛三倒四的
它先看最後面
接下來再看前面
那再胡亂看整個句子
那顯然有些是做錯了
顯然有些是
Something is wrong
有些是做錯了
那顯然這樣子的 Attention 是有問題的
你在做語音合成的時候
你顯然沒有辦法合出好的結果
所以 Guiding Attention 要做的事情就是
強迫 Attention 有一個固定的樣貌
那如果你對這個問題
本身就已經有理解知道說
語音合成 TTS 這樣的問題
你的 Attention 的分數
Attention 的位置都應該由左向右
那不如就直接把這個限制
放進你的 Training 裡面
要求機器學到 Attention
就應該要由左向右
那這件事怎麼做呢
有一些關鍵詞彙我就放在這邊
讓大家自己 Google 了
比如說某某 Mnotonic Attention
或 Location-Aware 的 Attention
那這個部分也是大坑
也不細講
那就留給大家自己研究
好 還有一個東西叫做
Beam Search
Beam Search 這個東西
剛才助教在講解作業的時候
也是有提到這個詞彙的
Beam Search 是什麼呢
我們這邊舉一個例子
在這個例子裡面我們假設說
我們現在的這個 Decoder
它就只能產生兩個字
我們假設說這個世界上
就只有兩個可能的輸出
一個叫做 A 一個叫做 B
假如世界上只有兩個字 A 跟 B
好 那對 Decoder 而言
它做的事情就是
每一次在第一個 Time Step
它在 A B 裡面決定一個
然後決定了 A 以後
再把 A 當做輸入
然後再決定 A B 要選哪一個
那舉例來說
它可能選 B 當作輸入
再決定 A B 要選哪一個
那在我們剛才講的 Process 裡面
每一次 Decoder 都是選
分數最高的那一個 對不對
記不記得我們每次都是選
Max 的那一個
所以假設 A 的分數 0.6
B 的分數 0.4
Decoder 的第一次就會輸出 A
然後接下來假設 B 的分數 0.6
A 的分數 0.4
Decoder 就會輸出 B
好
然後再假設把 B 當做 Input
就現在輸入已經有 A 有 B 了
然後接下來
A 的分數 0.4
B 的分數 0.6
那 Decoder 就會選擇輸出 B
所以輸出就是 A 跟 B 跟 B
那像這樣子每次找分數最高的那個 Token
每次找分數最高的那個字
來當做輸出這件事情叫做
Greedy Decoding
但是 Greedy Decoding
一定是更好的方法嗎
有沒有可能我們在第一步的時候
先稍微捨棄一點東西
比如說第一步雖然 B 是 0.4
但我們就先選 0.4 這個 B
然後接下來我們選了 B 以後
也許接下來的 B 的可能性就大增
就變成 0.9
然後接下來第三個步驟
B 的可能性也是 0.9
如果你比較紅色的這一條路
跟綠色這條路的話
你會發現說綠色這一條路
雖然一開始第一個步驟
你選了一個比較差的輸出
但是接下來的結果是好的
這個就跟那個天龍八部的真龍棋局一樣
對不對
先堵死自己一塊
結果接下來反而贏了
所以如果你比較紅色這一條路
跟綠色這一條路
紅色這一條路第一步不好
第一步好
那接下來全部乘起來是不好 比較差的
然後綠色這一條路一開始比較差
但最終的結果其實是比較好的
那我們其實是不是應該要選綠色這一條路
這就讓我想到什麼
讓我想要簽博 你知不知道
我知道這個假設是你的人生的轉捩點
你正在決定要簽下去還是不簽下去
然後簽下去雖然短時間內比較辛苦
但也許未來是比較好的
不簽下去你就去找工作
短時間內可以賺到比較多錢
但也許未來是比較不好的
所以你到底應該是簽下去
還是不簽下去呢
我到現在都沒什麼唸博班
所以我們上課的時候就是要
冷不防的開始業配唸博班這件事情
好 那所以我
如果我們要怎麼找到
這個最好的綠色這一條路呢
也許一個可能是
爆搜所有可能的路徑
但問題是我們實際上
並沒有辦法爆搜所有可能的路徑
因為實際上每一個轉捩點可以的選擇太多了
如果是在對中文而言
我們中文有 4000 個字
所以這個樹每一個地方分叉
都是 4000 個可能的路徑
你走兩三步以後
你就無法窮舉
所以怎麼辦呢
有一個演算法叫做 Beam Search
它用比較有效的方法
找一個 Approximate
找一個估測的 Solution
找一個不是很精準的
不是完全精準的 Solution
這個技術叫做 Beam Search
那這個也留給大家自己 Google
好 那這個 Beam Search 這個技術
到底有沒有用呢
有趣的事就是
它有時候有用
有時候沒有用
你會看到有些文獻告訴你說
Beam Search 是一個很爛的東西
怎麼說呢
比如舉例來說這篇 Paper 叫做
The Curious Case Of Neural Text Degeneration
那這個任務要做的事情是
Sentence Completion
也就是機器先讀一段句子
接下來它要把這個句子的後半段
把它完成
你給它一則新聞
或者是一個故事的前半部
哇 它自己發揮它的想像創造力
把這個文章
把故事的後半部把它寫完
那你會發現說
Beam Search 在這篇文章裡面
一開頭就告訴你說
Beam Search 自己有問題
如果我用
如果你用 Beam Search 的話
會發現說機器不斷講重複的話
它不斷開始陷入鬼打牆 無窮迴圈
不斷說重複的話
那如果你今天不是用 Beam Search
有加一些隨機性
雖然結果不一定完全好
但是看起來至少是比較正常的句子
所以有趣的事情是
有時候對 Decorder 來說
沒有找出分數最高的路
反而結果是比較好的
這個時候你又覺得亂亂的 對不對
就是剛才前一頁投影片才說
要找出分數最高的路
現在又突然又找
又要講說找出分數最高的路不見得比較好
到底是怎麼回事呢
那其實這個就是要
看你的任務的本身的特性
就假設一個任務
它的答案非常地明確
舉例來說
什麼叫答案非常明確呢
比如說語音辨識
說一句話辨識的結果就只有一個可能
就那一串文字就是你唯一可能的正確答案
並沒有什麼模糊的地帶
我覺得對這種任務而言
通常 Beam Search 就會比較有幫助
那什麼樣的任務
Beam Search 比較沒有幫助呢
就是你需要機器發揮一點創造力的時候
這時候 Beam Search 就比較沒有幫助
舉例來說在這邊的 Sentence Completion
給你一個句子
給你故事的前半部
後半部有無窮多可能的發展方式
那這種需要有一些創造力的
有不是只有一個答案的任務
往往會比較需要在 Decoder 裡面
加入隨機性
還有另外一個 Decoder
也非常需要隨機性的任務
叫做語音合成
TTS 就是語音合成的縮寫
那這個我印象很深刻
因為我們實驗室一開始想要用這個
Sequence-To-Sequence 的 Model
做語音合成的時候
有很長一段時間都做不起來
都合不出聲音來
有一次有一個 Google 的人來 Visit
我們實驗室 Visit
我們就拿這個問題來跟他請教
然後他就說
你們不知道在做 TTS 的時候
Decoder 要加 Noise 嗎
這聽起來很神奇
這個完全違背就是
正常的 Machine Learning 會做的想法
你知道在 Machine Learning
有時候你在訓練的時候會加 Noise
比如說你訓練的時候會加（00：54：23）
所以我們這門課還沒有講（00：54：25）
那有些同學可能知道
那如果你不知道的話
你就想像說我們在訓練的時候
我們會加上一些雜訊
讓我們在訓練的時候
機器看過更多不同的可能性
在訓練的時候會比較強健
比較能夠對抗它在測試的時候
沒有看過的狀況
那沒有人會傻到說你在測試的時候
居然還要加一些雜訊
那你不是把測試的狀況弄得更困難
不是結果會更差嗎
但 TTS 神奇的地方是
測試的時候
模型訓練好以後
測試的時候
你要加入一些雜訊
合出來的聲音才會好這樣
非常的神奇
你用正常的 Decode 的方法
產生出來的聲音就有點像是機關槍那樣
根本聽不太出來是人聲
你要產生出比較好的聲音
居然是需要一些隨機性的
所以這也是一個非常神奇的地方
有時候我們其實期待 Decorder 有隨機性
反而會得到比較好的結果
這也許就呼應了一個英文的諺語
就是要接受沒有事情是完美的
那真正的美也許就在不完美之中
對於 TTS 或 Sentence Completion 來說
Decoder 找出最好的結果
不見得是人類覺得最好的結果
反而是奇怪的結果
那你加入一些隨機性
結果反而會是比較好的
好 那接下來還有另外一個問題
大家知道說我們今天在
我們就拿我們的作業為例
我們在我們的作業裡面
我們評估的標準用的是
BLEU Score
那 BLEU Score 要怎麼量呢
BLEU Score 是你的 Decoder
先產生一個完整的句子以後
再去跟正確的答案一整句做比較
我們是拿兩個句子之間做比較
才算出 BLEU Score
但我們在訓練的時候顯然不是這樣
訓練的時候
這邊每一個詞彙是分開考慮的
訓練的時候
我們 Minimize 的是 Cross Entropy
Minimize Cross Entropy
真的可以 Maximize BLEU Score 嗎
不一定
因為這兩個根本就是
它們可能有一點點的關聯
但它們又沒有那麼直接相關
它們根本就是兩個不同的數值
所以我們 Minimize Cross Entropy
不見得可以讓 BLEU Score 比較大
所以你發現說在助教的程式裡面
比如說在助教在做 Validation 的時候
並不是拿 Cross Entropy 來挑最好的 Model
他並不是挑 Cross Entropy 最低的那個 Model
而是挑 BLEU Score 最高的那一個 Model
所以我們訓練的時候
是看 Cross Entropy
但是我們實際上你作業真正評估的時候
看的是 BLEU Score
所以你 Validation Set
其實應該考慮用 BLEU Score 對不對
那接下來有人就會想說
那我們能不能在 Training 的時候
就考慮 BLEU Score 呢
我們能不能夠訓練的時候就說
我的 Loss 就是
BLEU Score 乘一個負號
那我們要 Minimize 那個 Loss
假設你的 Loss 是
BLEU Score乘一個負號
它也等於就是 Maximize BLEU Score
但是這件事實際上沒有那麼容易
你當然可以把 BLEU Score
當做你訓練的時候
你要最大化的一個目標
但是 BLEU Score 本身很複雜
它是不能微分的
你把它當做你的 Loss
你根本不知道你要怎麼算 （00：57：40）
這邊之所以採用 Cross Entropy
而且是每一個中文的字分開來算
就是因為這樣我們才有辦法處理
如果你是要計算
兩個句子之間的 BLEU Score
這一個 Loss
根本就沒有辦法做微分
那怎麼辦呢
這邊就教大家一個口訣
這個口訣就是
遇到你在 Optimization 無法解決的問題
用 RL 硬 Train 一發就對了這樣
遇到你無法 Optimize 的 Loss Function
把它當做是 RL 的 Reward
把你的 Decoder 當做是 Agent
它當作是 RL
Reinforcement Learning 的問題硬做
其實也是有可能可以做的
那有人真的這樣試過嗎
有人真的這樣試過
我把 Reference 列在這邊給大家參考
當然這是一個比較難的做法
那並沒有特別推薦你在作業裡面用這一招
好 那我們要講到
我們剛才反覆提到的問題了
就是訓練跟測試居然是不一致的
測試的時候
Decoder 看到的是自己的輸出
所以測試的時候
Decoder 會看到一些錯誤的東西
但是在訓練的時候
Decoder 看到的是完全正確的
那這個不一致的現象叫做
Exposure Bias
那假設 Decoder 在訓練的時候
永遠只看過正確的東西
那在測試的時候
你只要有一個錯
那就會一步錯 步步錯
因為對 Decoder 來說
它從來沒有看過錯的東西
它看到錯的東西會非常的驚奇
然後接下來它產生的結果可能都會錯掉
所以要怎麼解決這個問題呢
有一個可以的思考的方向是
給 Decoder 的輸入加一些錯誤的東西
就這麼直覺
你不要給 Decoder 都是正確的答案
偶爾給它一些錯的東西
它反而會學得更好
這一招叫做
Scheduled Sampling
它不是那個 Schedule Learning Rate
剛才助教有講 Schedule Learning Rate
那是另外一件事
不相干的事情
這個是 Scheduled Sampling
Scheduled Sampling 其實很早就有了
這個是 15 年的 Paper
很早就有 Scheduled Sampling
在還沒有 Transformer
只有 LSTM 的時候
就已經有 Scheduled Sampling
但是 Scheduled Sampling 這一招
它其實會傷害到
Transformer 的平行化的能力
那細節可以再自己去了解一下
所以對 Transformer 來說
它的 Scheduled Sampling
另有招數跟傳統的招數
跟原來最早提在
這個 LSTM上被提出來的招數
也不太一樣
那我把一些 Reference 的
列在這邊給大家參考
好 那以上我們就講完了
Transformer 和種種的訓練技巧
這個我們已經講完了 Encoder
講完了 Decoder
也講完了它們中間的關係
也講了怎麼訓練
也講了總總的 Tip
好 那這個這堂課到這邊就告一個段落
那也許我們在這邊停一下
看看有沒有同學有問題要問的
有沒有同學有問題要問的呢
現場線上有同學有問題要問嗎
好
好 如果大家沒有問題要問的話
我們就在這邊休息 10 分鐘
我們 10 分鐘後再回來
