臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
一般的、nonlinear 的 deep network
他到底有沒有 local minima
你要證沒有 local minima 很難
但你要證有 local minima 太容易了
找個 local minima 來
就證他有 local minima 了，對不對
所以這個就好像說，你要證明一個東西不存在很難
但是，要證明事情存在是相對比較容易的
怎麼做，這邊的說法是
是這個樣子的，這邊我就收集了一些資料
首先，這個都是文獻上的結果
首先，就算是非常簡單的 task
比如說 XOR 的 task
我們今天在，我們在講 machine learning 的時候
從 nondeep 的 network 跨到 deep network 的時候
我們就是舉了 XOR 的例子說
如果今天有一個 data point
他是這樣的分布
這兩個 point 是一類，這兩個 point 是一類
linear network 沒有辦法分，所以
一定要 nonlinear 的 network
如果你實際上去 learn 一下這種 XOR 的 problem
你會發現說，如果你用一個 network 他只有一個 hidden layer
他的 neuron 的數目是 2 3 4 5 6 到 7
就算是增加到 7 個 neuron 那麼多
你使用 Adam 這個 optimization
都沒有辦法保證你
一定可以得到 100 % 的正確率
都沒有辦法保證你一定可以得到一個
找到一個 global 的optimal
如果是這個 sigmoid
在 network 比較大的時候倒是可以
接下來這篇 paper 裡面，他想了一個更複雜的問題
這個複雜的問題叫做 jellyfish 的問題
他說，他現在有 4 個點
這 4 個點的分布，就分布在這 4 個位置
這兩個黑點是一類，這兩個白點是一類
如果是 sigmoid 他還可以解 XOR 的 problem
如果是 jellyfish，不管是 ReLU 還是 sigmoid
你都是沒有辦法解的
你都找不到，用一般的，用已經是
我們現在覺得最 state-of-the-art 的 optimization 的方法
用 Adam，你都找不到 global 的 optimal
這個是第一個
比較直接的證據告訴你說就算是
很簡單的 network 有時候你也 learn 不起來
再來，我們今天假設要證明說
ReLU 的 network 有 local minima，其實非常簡單
找一個來就行了
找一個來就證明 ReLU 的 network 有 local minima
所以怎麼辦呢？
在文獻上有人就找了一個
他就隨便找一個，這個其實你要找十個八個也是有的
他就說，我們現在有一個 case
我們有 5 筆 data
這 5 筆 data 分別都，這邊要畫一個點
我這邊要畫一個逗點，不知道為什麼畫成一個小數點
我畫錯
有 5 個點 (-1,3) (1,-3) (3,0) (4,1) (5,2)，有 5 個點
接下來，我們有一個全世界最簡單的 ReLU network
他只有一個 neuron
那把 x 放進去，他會乘上一個 weight
加上一個 bias，再通過 ReLU 這個 activation function
再乘上一個 weight，加上一個 bias，得到最終的 y
現在在這一排這 5 個 pair 裡面
前一個數字都代表 x
output 就是，第二個數字就是 target
今天假設你有一個 ReLU 的 network 是 (1,-3)，(1,0)
他的曲線畫出來
他 x 跟 y 的關係畫起來是這個樣子
他正好可以 fit 後面三個點
但他 fit 不了前面這兩個點
他的 loss 算出來是 18
那你可以輕易的證明
他是一個 local minima
怎麼輕易地證明呢？你可以證說
這個 (1,-3) (1,0)，如果你都幫他們加一個小小的 △
不管你加的 △ 是什麼
他的 loss 一定都會增加，所以他是一個 local minima
那接下來你只要找到另外一個 minima 他的值
你只要能夠找到另外一組參數，他的 loss
比 18 還要低
那你就知道說這一組參數是一個 local minima
就結束了，對不對
那找不找的到呢，就胡亂找就一組
(-7,-4)、(1,0)，這個 network
他畫的 input 跟 output 關係長這個樣子
他算出來 loss 是 14，比他還要低
他是一個 minima
然後，又發現說
有其他的參數 loss 更低
他就不是 global minima 了
所以就結束
所以 ReLU 的 network 是有 local minima 的
有不是 global minima 的 local minima 的
剛才舉的是一個非常 specific 的 case
也許你覺得這個 case 太過 specific
其實以 general 而言
ReLU 的 network 都可以找到 local minima
怎麼說呢？因為 ReLU 這種 network
有一個狀況叫做他的盲點
你可以為 ReLU 這個 network 製造盲點
什麼叫做 ReLU 的盲點呢？
我們知道 ReLU 這個 network
他的每一個 neuron 有兩種 operation 的 region
一種 region 是 input = output
一種是 output = 0
今天我們假設一個 case 是
所有的 neuron 他通通是 output = 0
這個就是代表
這種狀況叫做 ReLU 的 network 陷入盲點
就是今天，input 一個 x 進來
所有的 neuron
他的 output 通通都是 0
如果所有的 neuron 都
作用在 output 是 0 的 region 上
整個 network 最終的 output 當然也是 0
你會發現說在這個 case，你的 gradient 算出來
就是 0，對不對
因為你想想看，你今天假設在這個
盲點的這個區域裡面
你不管怎麼調整參數一點點
假設你不改變這些 neuron 的 operation 的 region
你的 output 永遠不會變
所以，今天所有的參數
他的 gradient 通通都是 0
所以你今天，你就找到一個 critical point
你就找到一個 critical point
這個 critical point，他其實是一個 local minima
怎麼說他是 local minima 的呢？
因為這個 critical point
他在他的
他其實是這樣子的
假設橫軸是你的參數的變化
假設橫軸是你的參數，我們寫作 θ 好了
這個盲點的意思就是說，在某一個區間內
在某一個區間內
你的 network 的 output
他的值都是 0
他 network 那個值的 output 值都是 0
今天在這個 output 是 0 的這個區間裡面選一個點
他其實都是 local minma
因為他跟旁邊的 region 比起來
都是大於等於旁邊的 region
所以他是一個 local minima
那他會不會是 global minima？
他一定不是 global minima
你只要你最後的那個 target
他不是 0 就好了
你只要你最好的 target 他不是 0
你只要有別的case
他可以找到的 loss 小於 output 是 0 的 case
他就不是 global minima 了
所以，對 ReLU 這個 network 來說
你可以非常容易的製造出
只要他進入那種盲點的狀態
所有的 neuron 通通 output 是 0 的狀態
他很有可能就是 local minima
你說怎麼製造他都是盲點的狀態呢？
其實很簡單，你給他
比如說每一個 neuron 的 bias 設很大
設負的很大，那他的 output 就很容易是 0
他就很容易陷入盲點的狀態
在實驗裡面，你可以輕易的做出這件事
這是文獻上的實驗，他這實驗是這樣做的
他 train 了很多 ReLU 的 network
然後做在 MNIST 上面，然後用 Adam
然後參數 update 一百萬次
就 update 到覺得已經不可能
再有任何變化，update 一百萬次
然後，今天他做了不同的 initialization 的 case
首先，上面這個 row 跟下面這個 row
上面這個 row 是用一般的 MNIST 的 training set train 的
下面這個 row
是用奇怪的 MNIST train，什麼叫奇怪的 MNIST
就是把每一張圖片的 label，隨機置換成別的 label
就比如說他是一張 1
我故意說他是 5，他這個 2 也故意說他是 7
今天假設你用一個正常的 intialization 的方式
你用一個 normal distribution
mean 是 0，variance 是 0.01，來初始化你的參數
或者是 mean 是 0，variance 是 1
來初始化你的參數
那今天這個圖上的每一個點，都代表了一個 network
然後這個 network 橫軸代表說他們有不同的 hidden unit
不同的顏色代表兩個 hidden layer
或者是 5 個 hidden layer
那你發現說不管是多少 hidden layer
不管是多少的 hidden unit
只要用這種正常的 initialization 方式
用這種正常的 initialization
你 accuracy 都可以 train 到 1
除了這個 case 有點例外，當你 neuron 有點少的時候
在這特別的 data set 上，在這個怪怪的 data set 上
有時候會 train 不起來
但是，今天假如另外一個 case
試圖讓 network 進入盲點的區域
也就是在 intial 的時候
就給他非常奇怪的 initial 的值
舉例來說，在 initial w 的時候
故意讓他是從 mean 是 -10 的
random variable sample 起
故意讓他是從 mean 是 -10 的
random variable sample 起
或者是故意讓 bias 的 mean 是 -10
故意讓 bias 的 mean 是 -10
或是 w 跟 b weight 跟 bias 的 mean 都是 -1
都是 -1，你就故意讓 network 在一開始的時候
在初始化的時候就掉入那個盲點的區域
就掉入那個 local minima 的區域
他就再也爬不出來了
他的 accuracy 就一直在
趨近於 0 的地方，他就再也爬不出來
所以，這個告訴我們什麼，首先告訴我們說
ReLU 的 network 是有 local minima 的
會不會撞到那個 local minima，跟你怎麼
做 initialization 是有關係的
這邊還有其他的實驗
這個實驗是說，不只是 initialization
會影響你有沒有 local minima
會不會碰到 local minima
你的 data 本身長什麼樣子
可能也會影響你的 network，你在 train 的時候
會不會容易遇到 local minima 這件事
怎麼說呢？
假設我們有一個 network
這個 network 只有一個 hidden layer
他有 k 個
他有 n 個 neuron
input 是一個 vector x
假設每一個 neuron 他的參數分別就是 w1, w2 到 wn
那每一個 neuron 的 input 就是 x 跟 w1 做 inner product
就是你把 w1 的 transpose 乘上 x
當作 activation function 的 input
w2 的 transpose 乘 x 當作 activation function 的 input
wn 的 transpose 乘 x 當作 activation function 的 input
通過 ReLU 以後再乘上 1
得到最終的 y，這是你的 network
而 w 是要被 train 出來的
我們剛才在討論的時候
我們都沒有假設我們的 data 應該長什麼樣子
現在假設 data 是從一個 generator 生出來的
這個 generator 怎麼生 data 呢？
這個 generator 是一個 network
但這個 network 的參數是給定的
假設我們知道說
這是一個 有 k 個 neuron 的 一個 hidden layer 的 network
他的參數，也就是每個 neuron 的 weight
分別是 v1 v2 到 vn
我寫錯一個地方，大家有發現嗎？
對，是 k 啊
其實是 k ，這個 k 跟 n 是不一樣的
這個是這個實驗的重點
就是 k 跟 n 是不一樣的
所以這個應該是 k，抱歉抱歉
應該是 k
v1 的 transpose 乘x，v2 的 transpose 乘 x 到 vn
這邊怎麼會是寫對的呢
vk 的 transpose 乘上 x
最後把他通通加起來，得到你的 label
所以，你今天在產生 data 的時候
怎麼產生 data
從一個 normal distribution
做 sample，sample 出 x
丟到這個 label generator，產生你的 label y\hat
今天這一個 network
他要做的事情是想盡辦法去 fit
這個 x 跟 y\hat 之間的關係
那你可以想見說
假設今天這兩個 network，他們的 neuron 的數目
上面這個 network 的 neuron 的數目比較多
或上面這個 network 他的 neuron 的數目
大於等於下面這個 network neuron 的數目
我們只要
這邊的第一個 neuron 等於這邊的第一個 neuron
這邊的第二個 neuron 等於這邊的第二個 neuron
這邊的第 k 個 neuron 等於這邊的第 k 個 neuron
也就是這邊的第 i 個 neuron 等於這邊的第 i 個 neuron
你就可以找到 global minima
你的 loss 就會是 0
但是我們發現說，今天雖然只要 n = k
只要 n 大於等於 k
就可以找到
就一定保證
我們就知道 global minima 長什麼樣子
但是
n = k 跟 n > k 得到的結果，其實是很不一樣
那這個是
實際實驗的結果
文獻上的結果，這個文獻告訴我們什麼呢？
這個文獻告訴我們
這個文獻告訴我們，他試了不同的 k
然後假設現在 n 的數目跟 k 的數目是一樣的
也就是你的，要 train 的那個 network neuron 的數目
跟你的 label generator neuron 的數目是一樣的
這個時候你發現說
除了在 n = 6 的情況下
local minima 很少
這邊就是說你真的去 train 那個 network
那看有多少的百分比
就你 train, train, train，train 到停下來
然後在檢查說，他是不是一個 local minima
然後，看說有多少的百分比是停在一個 local minima
發現說，今天 有很大的機率會停在 local minima
那另外呢
他還對他停下來的時候，就假設他找到一個 critical point
他會去算那個 hasion，他也真的算了一下 hasion
把他的 eigen value 都找出來
他發現說，eigen value 的平均的最小值都是正的
代表說他多數的時候找到的那個 critical point
他的 eigen value 都是正的
代表說他是一個
eigen value 正的代表說
你從這個 critical point 往四周走，都是增加的
代表他是一個 local minima
這個是 n 跟 k
也就是 label generator 跟你要 train 那個 network
他們是一樣的 neuron 的數目的情況下
但假設你現在增加了
你的 network 的參數
假設現在 network 的 neuron
比 label generator 硬是多一個 neuron 呢
這個時候你就會發現
你的 local minima，你就很少遇到 local minima
當然這是一個實驗過程，他並不是理論上的證明
而是實驗上做出來發現說
當 network 的參數量比較多的時候
有點那種 overparameterized 的情況
他參數比他需要的還要多的時候
這個時候，遇到 local minima 的機會
就變得很少
他甚至發現說，假設 n 是設
大於
大於等於 k + 2
k 是 8，你的 label generator
8 個 neuron 的時候，你的 network 有 9 個 neuron
如果今天，你的 label generator 有 8 個 neuron
你的 network 給他 10 個 neuron
這個時候你會發現說，你完全找不到 local minima
當然並不代表 local minima 真的不存在
但他在實驗的過程中
就一直試、一直試、一直 train、一直 train
然後，都不會卡在 local minima
都可以找到 global minima
這個實驗要告訴我們的事情是
data 對結果也是很重要
講這麼多到底想告訴大家是什麼呢？
因為剛才講法好像都是說
ReLU 的 network 或是 deep nonlinear 的
deep 的 network，他就是有 local minima
但是這些實驗的結果並不是要告訴大家說
deep learning 是不 work
deep learning 是有沒辦法 train 的
而是想要藉由找出什麼樣的狀況有 local minima
而反過來推說什麼樣的狀況沒有 local minima
就假設說我的狀況是這麼多
那你把有 local minima 的狀況都踢掉
剩下就知道說，什麼情況下會沒有 local minima
那這樣的理論還沒有出現
那可以想像說未來
假設有這種有關 ReLU 的 network
或 deep 的 network 有沒有 local minima 理論出現的話
那個理論的 statement 會是這樣
你不太可能證說 ReLU 的 network 就沒有 local minima
你不太可能做這種 statement，因為
他就是有 local minima
你不可能睜眼說，瞎話說
他就是沒有 local minima，剛才很多例子
他就是有的，所以到時候
如果真的有理論出來的話，那個理論的說明應該是
在某種 condition 下，雖然我們還不知道是什麼
那種 condition 應該要考慮 initialization
我們剛才講說，你故意 initialize 在
ReLU network 的那個盲點，你就可以擊敗他
那你當然不能故意 initialize 在盲點的地方
所以，在某種 initialize 的情況下
在某種 data 的 distribution 情況下
我們剛才有講過說
如果你現在的 data 相對於 network 是比較簡單的
你 network 給他過多的參數，你就找不到 local
在實驗上你就找不到 local minima 了
這意味著說
今天你的 data 跟你的 network 之間的關係
也應該被考慮的，在這些情況下
可能會有一個演算法告訴我們說， 我們一定能夠找到 global optimal
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
