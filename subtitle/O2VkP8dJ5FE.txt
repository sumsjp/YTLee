好 那接下來呢
我們要講有關分類怎麼做這件事情
這邊講的呢 是一個短的版本
因為時間有限的關係
所以我們就講一個
可以在20分鐘內講完的範本
既然有短的版本 就是有長的版本
如果你想要看長的版本的話
可以看一下過去上課的錄影
過去可能是花兩個小時
到三個小時的時間才講完
分類這件事情
我們這邊用一個最快的方法
直接跟你講分類是怎麼做的
分類是怎麼做的呢 我們已經講了
Regression就是輸入一個向量
然後輸出一個數值
我們希望輸出的數值跟某一個label
也就是我們要學習的目標
越接近越好
有同學會問說
為什麼目標 label 有加Hat
而不是輸出有加Hat
你可能在別的地方有看過
輸出有加Hat
那這邊的notation
它的使用並沒有一定的規定
如果你先上過我的機器學習的話
你就會去問別的老師說
為什麼你的Model的輸出有加Hat
然後你的 label 沒有加Hat
好 所以我們這門課
如果是正確的答案就有加Hat
Model的輸出沒有加Hat
有一個可能
假設你會用Regression的話
我們其實可以把Classification
當作是Regression來看
怎麼說呢 這個方法不一定是個好方法
這是一個比較奇妙的方法
你可以說 一樣輸入一個東西以後
我們的輸出仍然是一個scaler
它叫做y 然後這一個y
我們要讓它跟正確答案
那個Class越接近越好
但是y是一個數字
我們怎麼讓它跟Class越接近越好呢
我們必須把Class也變成數字
舉例來說 Class1就是編號1
Class2就是編號2
Class3就是編號3
接下來呢 我們要做的事情
就是希望y可以跟Class的編號
越接近越好
但是這會是一個好方法嗎
如果你仔細想想的話
這個方法也許在某些狀況下
是會有瑕疵的
但是因為
如果你假設說Class one就是編號1
Class two就是編號2
Class3就是編號3
意味著說你覺得Class1跟Class2
它是比較像的
然後Class1跟Class3 它是比較不像的
像這樣子的表示Class的方式
有時候可行 有時候不可行
假設你的Class one two three
真的有某種關係 舉例來說
你想要根據一個人的身高跟體重
然後預測他是幾年級的小學生
一年級 二年級 還是三年級
那可能一年級真的跟二年級比較接近
一年級真的跟三年級比較沒有關係
但是假設你的三個Class本身
並沒有什麼特定的關係的話
你說Class one是1
Class two是2 Class two是3
那就很奇怪了
因為你這樣是預設說
一二有比較近的關係
一三有比較遠的關係
所以怎麼辦呢
當你在做分類的問題的時候
比較常見的做法是把你的Class
用 One-hot vector來表示
那One-hot vector
我們在作業一的時候有看過對不對
我在作業一的時候說我們把美國的州
用One-hot vector來表示
同樣的道理
我們也可以把每一個Class
用一個One-hot vector來表示
如果有三個Class
我們的 label 這個ŷ
就是一個三維的向量
然後呢 如果是Class1就是100
如果是Class2就是010
如果是Class3就是001
所以每一個Class
你都用一個One-hot vector來表示
而且你用One-hot vector來表示的話
就沒有說Class1跟Class2比較接近
Class1跟Class3比較遠這樣子的問題
如果你把這個One-hot vector
拿來算距離的話
Class之間 兩兩它們的距離都是一樣
好 那接下來呢
如果我們今天的目標y hat
是一個向量 比如說這邊
ŷ是有三個element的向量
三個數值的向量
那我們的network
也應該要Output三個數字才行
到目前為止我們講的network
其實都只Output一個數值
因為我們過去做的都是
Regression的問題
所以只Output一個數字
怎麼把它改成Output三個數值呢
其實從一個數值改到三個數值
它是沒有什麼不同的
怎麼說呢 你可以Output一個數值
你就可以Output三個數值
所以把本來Output一個數值的方法
重複三次
你把a₁ a₂ a₃
乘上三個不同的Weight 加上bias
得到y₁
再把a₁ a₂ a₃乘上另外三個Weight
再加上另外一個bias得到y₂
再把a₁ a₂ a₃再乘上另外一組Weight
再加上另外一個bias得到y₃
你就可以產生三組數字
所以你就可以Input一個feature的Vector
然後產生y₁ y₂ y₃
然後期待y₁ y₂ y₃
跟我們的目標越接近越好
好 那所以我們現在
知道了Regression是怎麼做的
Input x Output y 要跟 label ŷ
越接近越好
如果是Classification呢
我們剛才說Input x呢
可能乘上一個W
再加上b 再通過activation function
再乘上W'再加上b' 得到y
我們現在的y 它不是一個數值
它是一個向量
但是在做Classification的時候
我們往往會把y再通過一個
叫做Soft-max的function得到y'
然後我們才去計算
y'跟y hat之間的距離
也就是說我們要讓y'跟ŷ
越接近越好
為什麼要加上Soft-max呢
一個比較簡單的解釋
這一段到底要怎麼在數十分鐘內講完
我其實想了很久
如果是在過去的課程裡面
我會花很長一段時間
告訴你說這裡為什麼要加Soft-max
背後有什麼樣的假設
我們會先從generative的Model開始講起
然後一路講到Logistic Regression
各位知道為什麼這邊放一個Soft-max
它有什麼樣的歷史淵源
我們現在已經不是從那個角度切入
所以就有點不知道要怎麼解釋說
為什麼要放Soft-max
這邊有一個騙小孩的解釋就是
這個ŷ 它裡面的值
都是0跟1對不對
它是One-hot vector
所以裡面的值只有0跟1
那y呢 y裡面有任何值
既然我們的目標只有0跟1
但是y有任何值
我們能不能夠就先把這個任何值
先把它Normalize
移到0到1之間
這樣才好跟 label 的計算相似度
這是一個比較簡單的講法
如果你真的想要知道說
為什麼要用Soft-max的話
你可以參考過去的上課錄影
如果你不想知道的話
你就記得一件事
這個Soft-max要做的事情
就是把 本來y裡面可以放任何值
這件事情 改成挪到0到1之間
那Soft-max它裡面是怎麼運作的呢
這是這個Soft-max的block
輸入y₁ y₂ y₃
它會產生y₁' y₂' y₃'
它裡面運作的模式是這個樣子的
我們會先把所有的y取一個exponential
所以本來不管y是正的還是負的
反正取完exponential以後
就都變正的對不對
就算是負數 負小於0的值
取exponential以後也變成正的
然後你再對它做Normalize
除掉所有y的exponential值的和
然後你就得到y'
或者是用圖示化的方法是這個樣子
y₁取exp y₂取exp y₃取exp
把它全部加起來
得到一個Summation
放在分母地方的Summation
接下來再把exp y₁'除掉Summation
exp y₂'除掉Summation
exp y₃'除掉Summation
就得到y₁' y₂' y₃'就這樣
有了這個式子以後
你就會發現說這個y₁'
這個yᵢ' y₁' y₂' y₃'
它們都是介於0到1之間
然後這個 y₁' y₂' y₃'
它們的和呢 會是1
如果舉一個例子的話呢
本來 y₁等於3y₂等於1
y₃等於負3
取完exponential的時候呢
就變成exp3 就是20
exp1就是2.7
exp-3就是0.05
做完Normalization以後
這邊就變成0.88 0.12 跟0
所以這個Soft-max它要做的事情
除了Normalized以外
除了讓 y₁' y₂' y₃'
變成0到1之間 還有和為1以外
它還有一個附帶的效果是
它會讓大的值跟小的值
它們的差距更大
所以本來-3 然後通過exponential
再做Normalized以後
會變成趨近於0的值
然後這個Soft-max的輸入
往往就叫它logit
它是有名字的
有時候就叫它logit
那這邊是考慮了3個class的狀況
那如果兩個class會是怎麼樣呢
如果是兩個class你當然可以直接套
soft-max這個function沒有問題
但是也許你更常聽到的是
當有兩個class的時候
我們就不套soft-max
我們直接取sigmoid
那當兩個class用sigmoid
跟soft-max兩個class
他們之間的關係是什麼呢
你其實可以自己想一想
你如果推一下的話
會發現說這兩件事情是等價的
我知道說我這邊講soft-max
那soft-max當然做在兩個class上
一定是沒有問題
那你更常看到的是
兩個class的時候就用sigmoid
你可能會問說
這兩者有什麼不同 哪一個比較好呢
那我這邊告訴你 他們是一模一樣
你自己回去推一下
會發現說這兩件事情是同一件事情
好 那所以呢我們把x
丟到一個Network裡面產生y以後
我們會通過soft-max得到y'
再去計算y'跟ŷ之間的距離
這個寫作е
那計算y'跟ŷ之間的距離
有不只一種做法
舉例來說 如果我喜歡的話
我要讓這個距離是Mean Square Error
跟作業е用的一模一樣
就是把y'裡面每一個element拿出來
把ŷ裡面每一個element拿出來
然後計算它們的平方和當作我們的error
這也是一個做法
這樣也是計算兩個向量之間的距離
你也可以說
你也可以做到說當minimize
Mean Square Error的時候
我們可以讓ŷ等於y'
但是有另外一個更常用的做法
叫做Cross-entropy
這個Cross-entropy它的式子乍看之下
會讓你覺得有點匪夷所思
怎麼是這個樣子呢
我們現在來看看
這個Cross-entropy的式子長什麼樣子
它是summation over所有的i
然後把ŷ的第i位拿出來
乘上y'的第i位取Natural log
然後再全部加起來
這個是Cross-entropy
那當ŷ跟y'一模一樣的時候
你也可以Minimize Cross-entropy的值
當ŷ跟y'一模一樣的時候
MSE會是最小的
Cross-entropy也會是最小的
但是為什麼會有Cross-entropy
這麼奇怪的式子出現呢
那如果要講得長一點的話
這整個故事我們可以把它講成
Make Minimize Cross-entropy
其實就是maximize likelihood這個東西
你很可能在很多地方
都聽過likelihood這個詞彙
所以這堂課裡面
我們even把likelihood拿掉了
所以我們現在
根本就沒有likelihood這個東西
所以我們就不能往這個方向解釋
我們就不能說Minimize Cross-entropy
為什麼是maximize likelihood
我們已經不知道likelihood是什麼
但是這兩件事情其實是等價的
所以如果有一天有人問你說
如果我們今天在做分類問題的時候
maximize likelihood
跟Minimize Cross-entropy
有什麼關係的時候
不要回答說它們其實很像
但是其實又有很微妙的不同這樣
不是這樣
它們兩個就是一模一樣的東西
只是同一件事不同的講法而已
所以假設你可以接受說
我們在訓練一個classifier的時候
應該要maximize likelihood就可以接受
應該要Minimizing Cross-entropy
但是我們沒有講這件事
所以我要怎麼說服你
我們應該要用Cross-entropy呢
下一頁投影片是從optimization的角度
來告訴你說Cross-entropy
比Mean Square Error
更加適合用在分類上
那Cross-entropy
真的相較於Mean Square Error
是更常用在classification上面的
至於它常用到什麼地步呢
它常用到在pytorch裡面
Cross-entropy跟Soft-max
他們是被綁在一起的
他們是一個Set
你只要Copy Cross-entropy
裡面就自動內建了Soft-max
所以如果你看助教作業二的程式的話
你會找不到Soft-max在哪裡
本來Soft-max應該是Network的一部分
照理說你在定義Network的時候
你應該也定義了Soft-max這一個function
但是你發現在助教程式裡面
你找不到Soft-max 為什麼
它放在Cross-entropy裡面
當你使用Cross-entropy
這個Loss function的時候
pytorch自動幫你把Soft-max
夾到你的Network的最後一層
所以如果你今天在用pytorch
你自己在Network加Soft-max的時候
你用Cross-entropy
就會變成加了兩次Soft-max
所以這個pytorch有趣的設計
所以這顯示說Soft-max
跟Cross-entropy它們往往是被綁在一起
他們是一個Set
總是會被一起使用
那接下來呢
我從optimization的角度
來告訴你說為什麼
相較於Mean Square Error
Cross-entropy是被更常用在分類上
那這個部分
你完全可以在數學上面做證明
但是我這邊
是直接用舉例的方式來跟你說明
如果你真的非常想看數學證明的話
我把連結放在這邊
你可以一下過去上課的錄影
如果你不想知道的話
那我們就是舉一個例子來告訴你說
為什麼是Cross-entropy比較好
好 那現在我們要做一個3個Class的分類
我的Network先輸出y₁ y₂ y₃
在通過soft-max以後
產生y₁' y₂'跟y₃'
那接下來假設我們的正確答案就是100
我們要去計算100這個向量
跟y₁' y₂'跟y₃'他們之間的距離
那這個距離我們用е來表示
е可以是Mean square error
也可以是Cross-entropy
好 那我們看一下 如果我們這個e
設定為Mean Square Error
跟Cross-entropy的時候
算出來的Error surface會有什麼樣
不一樣的地方
我們現在假設y₁的變化是從-10到10
y₂的變化也是從-10到10
y₃我們就固定設成-1000
因為y₃設很小 y₃設-1000
所以過soft-max以後y₃'就非常趨近於0
它跟正確答案非常接近
且它對我們的結果影響很少
總之我們y₃設一個定值
我們只看y_1跟y_2有變化的時候
對我們的e對我們的Loss
就是把很多個e加起來就變成loss嘛
對我們loss有什麼樣的影響
底下這兩個圖啊
就分別在我們e是Mean square error
跟Cross-entropy的時候
y₁ y₂的變化對loss的影響
對Error surface的影響
那如果今天y₁很大 y₂很小
那y₁很大 y₂很小
就代表y₁'會很接近1
y₂'會很接近0
所以不管是對Mean Square Error
或是Cross-entropy而言
y₁大 y₂小的時候 Loss都是小的
我們這邊是用紅色代表Loss大
藍色代表Loss小
那左上角這邊
y₁小 y₂大的時候
如果y₁小 y₂大的話
這邊y₁'就是0 y₂'就是1
所以這個時候Loss會比較大
所以這兩個圖都是左上角Loss大
右下角Loss小
所以我們就期待說
我們最後在Training的時候
我們的參數可以走到右下角的地方
那假設我們開始的地方
都是左上角的地方會有什麼問題呢
你會發現說如果我們選擇Cross-Entropy
左上角這個地方
它是有斜率的
所以你有辦法透過gradient
一路往右下的地方走
如果你選Mean square error的話
你就卡住了你知道嗎
Mean square error在這種Loss很大的地方
它是非常平坦的
它的gradient是非常小趨近於0的
如果你初始的時候在這個地方
離你的目標非常遠
那它gradient又很小
你就會沒有辦法用gradient descent
順利的走到右下角的地方去
所以你如果你今天自己在做classification
你選Mean square error的時候
你有非常大的可能性會train不起來
當然這個是在你沒有
好的optimizer的情況下
今天如果你用Adam
這個地方gradient很小
那gradient很小之後
它learning rate之後會自動幫你調大
也許你還是有機會走到右下角
不過這會讓你的training
比較困難一點
讓你training的起步呢
比較慢一點
所以這邊有一個很好的例子
是告訴我們說
就算是Loss function的定義
都可能影響Training是不是容易這件事情
剛才說要用神羅天征
直接把error surface炸平
這邊就是一個好的例子告訴我們說
你可以改Loss function
居然可以改變optimization的難度
