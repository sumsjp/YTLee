臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw/
今天的規劃是這樣，我們先講 PPO
這個是作業 4-1 用的到的東西，接下來講 Q-learning
這是作業 4-2 用到的東西， 然後最後助教會來講一下作業 4-2
那上週我們做的事情，是複習了一下 policy gradient
那我們今天要講的事情，是 policy gradient 的一個變形
就是 PPO，那上周已經講過 PPO 是現在 OpenAI default reinforcement learning 的 algorithm
那在講 PPO 之前呢
我們要講 on-policy and off-policy 這兩種 training 方法的區別
那什麼是 on-policy 什麼是 off-policy 呢？
我們知道在 reinforcement learning 裡面，我們要 learn 的就是一個 agent
那如果我們今天拿去跟環境互動的那個 agent
跟我們要 learn 的 agent 是同一個的話， 這個叫做 on-policy
如果我們今天要 learn 的 agent
跟和環境互動的 agent 不是同一個的話， 那這個叫做 off-policy
就如果比較擬人化的講法就是
如果今天要學習的那個 agent，它是一邊跟環境互動，一邊做學習
這個叫 on-policy
果它是在旁邊看別人玩， 透過看別人玩，來學習的話，這個叫做 off-policy
為什麼我們會想要考慮 off-policy 這樣的選項呢？
讓我們來想想看我們已經講過的 policy gradient
其實我們之前講的 policy gradient， 它是 on-policy 還是 off-policy 的做法呢？
它是 on-policy 的做法，為什麼？
我們之前講說，在做 policy gradient 的時候呢
我們會需要有一個 agent，我們會需要有一個 policy
我們會需要有一個 actor，這個 actor 先去跟環境互動
去蒐集資料，蒐集很多的 tao
那根據它蒐集到的資料，會按照這個 policy gradient 的式子
去 update 你 policy 的參數， 這個就是我們之前講過的 policy gradient
所以它是一個 on-policy 的 algorithm
你拿去跟環境做互動的那個 policy， 跟你要 learn 的 policy 是同一個
那今天的問題是
我們之前有講過說，因為在這個 update 的式子裡面
其中有一項，你的這個 expectation， 應該是對你現在的 policy theta
所 sample 出來的 trajectory tao，做 expectation
所以當你今天 update 參數以後，一旦你 update 了參數
從 theta 變成 theta prime，那這一個機率，就不對了
之前 sample 出來的 data，就變的不能用了
所以我們之前就有講過說
policy gradient 是一個會花很多時間來 sample data algorithm
你會發現大多數時間都在 sample data，跟你的 agent 去跟環境做互動以後
接下來，就要 update 參數
每次 update 完參數一次，你只能 update 參數一次
你只能做一次 gradient decent，你只能 update 參數一次
接下來你就要重新再去 collect data， 然後才能再次 update 參數
這顯然是非常花時間的
所以我們現在想要從 on-policy 變成 off-policy 的好處就是
我們希望說現在我們可以用另外一個 policy， 另外一個 actor theta prime 去跟環境做互動
用 theta prime collect 到的 data 去訓練 theta
假設我們可以用 theta prime collect 到的 data 去訓練 theta，意味著說
我們可以把 theta prime collect 到的 data 用非常多次
你在做 gradient decent 的時候
其實是 gradient ascent，因為我要 optimize 某個數值，所以是 gradient ascent
在做 gradient ascent 的時候， 我們可以執行那個 gradient ascent 好幾次
我們可以 update 參數好幾次， 都只要用同一筆 data 就好了
因為假設現在 theta 有能力從另外一個 actor theta prime
它所 sample 出來的 data 來學習的話， 那 theta prime 就只要 sample 一次
也許 sample 多一點的 data， 讓 theta 去 update 很多次，這樣就會比較有效率
所以怎麼做呢？
這邊就需要介紹一個 important sampling 的概念
那這個 important sampling 的概念不是只能用在 RL 上
它是一個 general 的想法，可以用在其他很多地方
我們先介紹這個 general 的想法
那假設現在你有一個 function f of x，那你要計算
從 t 這個 distribution
sample x
再把 x 帶到，f 裡面，得到 f of x
你要計算這個 f of x 的期望值
那怎麼做呢？ 假設你今天沒有辦法對 p 這個 distribution 做積分的話
那你可以從 p 這個 distribution 去 sample 一些 data x(i)
那這一個期望值，這個 f of x 的期望值
就等同是你 sample 到的 x(i)，把 x(i) 帶到 f of x 裡面
然後取它的平均值，就可以拿來近似這個期望值
假設你知道怎麼從 p 這個 distribution 做 sample 的話
你要算這個期望值
你只需要從 p 這個 distribution，做 sample 就好了
但是我們現在有另外一個問題
那等一下我們會更清楚知道說為什麼會有這樣的問題
我們現在的問題是這樣，我們沒有辦法從 p 這個 distribution 裡面
sample data
假設我們不能從 p sample data
我們只能從另外一個 distribution q 去 sample data
q 這個 distribution 可以是任何 distribution，不管它是怎麼樣的 distribution
在多數情況下，等一下討論的情況都成立
我們不能夠從 p 去 sample data
但我們可以從 q 去 sample x(i)
但我們從 q 去 sample x(i)，我們不能直接套這個式子
因為這邊是假設你的 x(i) 都是從 p sample 出來的
你才能夠套這個式子
從 q sample 出來的 x(i) 套這個式
你也不會等於左邊這項期望值
所以怎麼辦，做一個修正，這個修正是這樣子的
期望值這一項，其實就是積分 f of x 乘上 p of x, dx
然後我們現在上下都同乘 q of x
上下同乘 q of x 不會改變任何事
但是我們可以把這一個式子
寫成對 q 裡面所 sample 出來的 x，取期望值
我們從 q 裡面，sample x
然後再去計算 f of x 乘上 p of x 除以 q of x
再去取期望值，然後左邊這一項，會等於右邊這一項
要算左邊這一項，你要從 p 這個 distribution sample x
但是要算右邊這一項
你不是從 p 這個 distribution sample x
你是從 q 這個 distribution sample x
你從 q 這個 distribution sample x
sample 出來之後，再帶入 f of x 乘上 p of x, q of x
接下來你就可以計算左邊這項你想要算的期望值
所以就算是我們不能從 p 裡面去 sample data
你想要計算這一項的期望值
也是沒有問題的
你只要能夠從 q 裡面去 sample data
可以帶這個式子，你就一樣可以計算
從 p 這個 distribution sample x 帶入 f 以後所算出來的期望值
那這個兩個式子唯一不同的地方是說
這邊是從 x 做 sample
這邊是從 q 做 sample
因為他是從 q 裡做 sample
所以 sample 出來的每一筆 data
你需要乘上一個 weight
修正這兩個 distribution 的差異
而這個 weight 就是 p of x 的值除以 q of x 的值
所以 q of x 它是任何 distribution 都可以，這邊唯一的限制就是
你不能夠說 q 的機率是 0 的時候，p 的機率不為 0
不然這樣會沒有定義
假設 q 的機率是 0 的時候，p 的機率也都是 0 的話
那這樣 p 除以 q 是有定義的
所以這個時候你就可以 apply important sampling 這個技巧
所以你就可以本來是從 p 做 sample， 換成從 q 做 sample
這個跟我們剛才講得有什麼關係呢？
剛才講的從 on-policy 變成 off-policy，有什麼關係呢？
在繼續講之前，我們來看一下 important sampling 的 issue
雖然理論上你可以把 p 換成任何的 q
但是在實作上，並沒有那麼容易
實作上 p and q 還是不能夠差太多，如果差太多的話
會有一些問題
什麼樣的問題呢？
雖然我們知道說，左邊這個式子，等於右邊這個式子
但你要不要想想看，如果今天左邊這個是 f of x
它的期望值 distribution 是 p
這邊是 f of x 乘以 p 除以 q
的期望值，它的 distribution 是 q
我們現在如果不是算期望值
而是算 various 的話
這兩個 various 你覺得它會一樣嗎？
多數同學都覺得他們不會一樣，沒錯，它們確實是不一樣的
兩個 random variable 它的 mean 一樣，並不代表它的 various 一樣
對不對，所以你今天可以實際算一下
f of x 這個 random variable，跟 f of x 乘以 p of x 除以 q of x，這個 random variable
他們的這個 various 是不是一樣的， 可以計算一下，那我們知道 various of x
這是個公式啊， various of x 是 x^2 的 expectation 減掉 (E of x)^2
那你就帶一下這個式子，所以今天這一項的 various
是這個樣子，這一項的 various
就套一下公式，寫成這樣
然後今天這一項啊
你其實是，可以做一些整理的
這邊有一個 x^2
這邊有一個 p^2
這邊有一個 q^2
這邊又有一個 q
所以這邊有一個 f of x 的平方
然後有一個 p of x 的平方
有一個 q of x 的平方
但是前面呢
是對 q 取 expectation
所以 q 的 distribution 取 expectation
所以如果你要算積分的話
你就會把這個 q 呢
乘到前面去
算期望值其實是這樣算的，然後 q 就可以消掉了
然後你可以把這個 p 拆成兩項， 然後就會變成是對 p 呢，取期望值
那這個是左邊這一項
那右邊這一項
這一項，其實就寫在這邊
所以這一項其實就是這一項
然後你把它平方就變成這樣
所以如果你比較這一項跟這一項的話，他們的差別在那裡，他們的差別在第一項
是不同的
第一項這邊多乘了 p 除以 q
這邊沒有乘 p 除以 q，這邊多乘了 p 除以 q
如果 p 除以 q 差距很大的話
這個時候，這一項的 various
就會很大
所以雖然理論上它的 expectation 一樣
也就是說，你只要對 p 這個 distribution sample 夠多次
q 這個 distribution sample 夠多次
你得到的結果會是一樣的
但是假設你 sample 的次數不夠多
因為它們的 various 差距是很大的
所以你就有可能得到非常大的差別
如果這個地方你聽的不是很懂的話，那沒有關係，這邊就是舉一個具體的例子告訴你說
當 p and q 差距很大的時候，會發生什麼樣的問題
假設這個是 p 的 distribution
這個是 q 的 distribution
這個是 f of x
那如果我們要計算 f of x 的期望值
它的 distribution 是從 p 這個 distribution 做 sample
那顯然這一項是負的，對不對
因為 f of x 在這個區域
這個區域 p of x 的機率很高，所以要 sample 的話，都會 sample 到這個地方
而 f of x 在這個區域是負的， 所以理論上這一項算出來會是負的
接下來我們改成從 q 這邊做 sample
那因為 q 在右邊這邊的機率比較高
所以如果你 sample 的點不夠的話
那你可能都只 sample 到右側
如果你都只 sample 到右側的話
你會發現說，如果只 sample 到右側的話
算起來右邊這一項，搞不好還應該是正的
對不對，你這邊 sample 到這些點
然後你去計算它們的 f of x p 除以 q
都是正的
所以你 sample 到這些點
都是正的， 所以你取期望值以後，也都是正的
那為什麼會這樣，那是因為你 sample 的次數不夠多
因為假設你 sample 次數很少，你只能 sample 到右邊這邊
左邊這邊雖然機率很低，但也不是沒有可能
被 sample 到，假設你今天好不容易 sample 到左邊的點
因為左邊的點，p and q 是差很多的， 這邊 p 很小，q 很大
今天 f of x 好不容易終於 sample 到一個負的
這個負的就會被乘上一個非常巨大的 weight
就可以平衡掉剛才那邊
一直 sample 到 positive 的 value 的情況
eventually， 你就可以算出這一項的期望值，終究還是負的
但問題就是，這個前提是你要 sample 夠多次
這件事情才會發生，但有可能 sample 不夠
左式跟右式就有可能有很大的差距
所以這是 importance sampling 的問題
現在要做的事情就是，把 importance sampling 這件事
用在 off-policy 的 case
我要把 on-policy training 的 algorithm
改成 off-policy training 的 algorithm
那怎麼改呢
之前我們是看
我們是拿 theta 這個 policy
去跟環境做互動，sample 出 trajectory tao
然後計算中括號裡面這一項
現在我們不根據 theta
我們不用 theta 去跟環境做互動
我們假設有另外一個 policy
另外一個 policy 它的參數
theta prime，它就是另外一個 actor
它的工作是他要去做 demonstration
它要去示範給你看
這個 theta prime 它的工作是要去示範給 theta 看
它去跟環境做互動，告訴
theta 說，它跟環境做互動會發生什麼事
然後，藉此來訓練 theta
我們要訓練的是 theta 這個 model
theta prime 只是負責做 demo 負責跟環境做互動
我們現在的 tao
它是從 theta prime sample 出來的
不是從 theta sample 出來的，但我們本來要求的式子是這樣
但是我們實際上做的時候，是拿 theta prime 去跟環境做互動，所以 sample 出來的 tao
是從 theta prime sample 出來的
這兩個 distribution 不一樣
但沒有關係，我們之前講過說
假設你本來是從 p 做 sample
但你發現你不能夠從 p 做 sample
所以現在我們說我們不拿 theta 去跟環境做互動
所以不能跟 p 做 sample
你永遠可以把 p 換成另外一個 q
然後在後面這邊補上一個 importance weight
所以現在的狀況就是一樣
把 theta 換成 theta prime 以後
要在中括號裡面補上一個 importance weight
這個 importance weight 就是某一個 trajectory tao
它用 theta 算出來的機率
除以這個 trajectory tao
用 theta prime 算出來的機率
那這一項是很重要的，因為
今天你要 learn 的是 actor theta and theta prime 是不太一樣的
theta prime 會遇到的狀況，會見到的情形
跟 theta 見到的情形，不見得是一樣的
所以中間要做一個修正的項
所以我們做了一下修正
現在的 data 不是從 theta prime sample 出來的
是從 theta sample 出來的
那我們從 theta 換成 theta prime 有什麼好處呢？
我們剛才就講過說，因為現在跟環境做互動是 theta prime 而不是 theta
所以你今天 sample 出來的東西
跟 theta 本身是沒有關係的
所以你就可以讓 theta prime 做互動 sample 一大堆的 data 以後
theta 可以 update 參數很多次
然後一直到 theta 可能 train 到一定的程度
update 很多次以後，theta prime 再重新去做 sample
這就是 on-policy 換成 off-policy 的妙用
那我們其實上周有講過說呢
實際上我們在做 policy gradient 的時候
我們並不是給一整個 trajectory tao 都一樣的分數
而是每一個 state/action 的 pair
我們會分開來計算
所以我們上周其實有講過說
我們實際上 update 我們的 gradient 的時候
我們的式子是長這樣子的
我們用 theta 這個 actor 去 sample 出 st 跟 at
sample 出 state 跟 action 的 pair
我們會計算這個 state 跟 action pair 它的 advantage， 就是它有多好
那我們上周有講過說，這一項呢
就是那個 accumulated 的 reward 減掉 bias
那如果你忘記的話也沒有關係， 反正這一項就是估測出來的
它要估測的是，現在在 state st 採取 action at
它是好的，還是不好的
那接下來後面會乘上這個 log p(theta) at given st
也就是說如果這一項是正的，就要增加機率， 這一項是負的
就要減少機率
那我們現在用了 importance sampling 的技術把 on-policy 變成 off-policy
就從 theta 變成 theta prime，所以現在 st/at 它不是 theta 跟環境互動以後所 sample 到的 data
它是 theta prime，另外一個 actor 跟環境互動以後
所 sample 到的 data， 但是拿來訓練我們要調整參數的那個 model theta
但是我們有講過說
因為 theta prime 跟 theta 是不同的 model，所以你要做一個修正的項
那這項修正的項，就是用 importance sampling 的技術
把 st/at 用 theta sample 出來的機率
除掉 st/at 用 theta prime sample 出來的機率
那這邊其實有一件事情我們需要稍微注意一下
這邊 A 有一個上標 theta 代表說
這個是 actor theta 跟環境互動的時候
所計算出來的 A
但是實際上我們今天從 theta 換到 theta prime 的時候
這一項，你其實應該改成 theta prime
而不是 theta，為什麼？
這個 A 這一項是怎麼來的
A 這一項是想要估測說現在在某一個 state
採取某一個 action 接下來
會得到 accumulated reward 的值減掉 base line
對不對，我們上周有講過，你怎麼估這一項 advantage，你就會看
在這個 state st，採取這個 action at
接下來會得到的 reward 的總和
再減掉 baseline，就是這一項，這上周有講過
之前是 theta 在跟環境做互動
所以你觀察到的是 theta 可以得到的 reward
但現在不是 theta 跟環境做互動，現在是 theta prime 在跟環境做互動
所以你得到的這個 advantage， 其實是根據 theta prime 所 estimate 出來的 advantage
但我們現在先不要管那麼多， 我們就假設這兩項可能是差不多的
那接下來，st/at，這一件事情
你可以拆解成 st 的機率乘上 at given st 的機率
然後接下來這邊需要做一件事情是
我們假設當你的 model 是 theta 的時候
你看到 st 的機率
跟你的 model 是 theta prime 的時候，你看到 st 的機率
是差不多的
你把它刪掉，因為它們是一樣的
所以你可以把它刪掉，為什麼可以假設它是差不多的
當然你可以找一些理由，舉例來說，會看到什麼 state
往往跟你會採取什麼樣的 action 是沒有太大的關係的
比如說你玩不同的 Atari 的遊戲
那其實你看到的遊戲畫面都是差不多的
所以也許不同的 theta 對 st 是沒有影響的
但是有一個更直覺的理由就是
這一項到時候真的要你算，你會算嗎？
你不覺得這項你不太能算嗎？
因為想想看這項要怎麼算，這一項你還要說
我有一個參數 theta，然後拿 theta 去跟環境做互動
算 st 出現的機率，這個你根本很難算
尤其是你如果 input 是 image 的話， 同樣的 st 根本就不會出現第二次
所以你根本沒有辦法估這一項， 所以乾脆就無視這個問題
但是 given st，接下來產生 at 這個 機率
你是會算的，這個很好算
你手上有 theta 這個參數，它就是個 network
你就把 st 帶進去，st 就是遊戲畫面，你ˋ把遊戲畫面帶進去
它就會告訴你某一個 state 的 at 機率是多少
對不對，我們說，我們其實有個 policy 的 network
把 st 帶進去，它會告訴我們每一個 at 的機率是多少
所以這一項你只要知道 theta 的參數，知道 theta prime 的參數，這個就可以算
但這一項，其實不太好算，所以你就說服自己，其實這一項不太會有影響，我們只管前面這個部分就好了
那所以現在我們得到一個新的 objective function，我們得到一個新的 objective function
這一項是 gradient，其實我們可以從 gradient 去反推原來的 objective function
怎麼從 gradient 去反推原來的 objective function 呢？
這邊有一個公式，我們就背下來，f of x 的 gradient
等於 f of x 乘上 log f of x 的 gradient
這一項是個樣子的
前面有 p theta 對不對
然後下面有一個 p theta prime
然後這邊有一個 function A，那邊呢有一個 gradient log p theta 這樣
然後我們看一下這邊，我們說
這個 f of x 乘以 gradient log f of x
p theta 就當作 f of x，這個是 f of x 這個是 gradient log f of x
這兩項合起來，就可以變成 gradient f of x，所以變成 gradient p theta
然後接下來我們要做的事情就是
這個是 gradient 的項
我們要還原說原來沒有取 gradient 的樣子
那是什麼樣子？其實就是把這個 gradient 拿掉
就變成下面這個式子
所以實際上，當我們 apply importance sampling 的時候
我們要去 optimize 的那一個 objective function 長什麼樣子呢
我們要去 optimize 的那一個 objective function 就長這樣子，我們把它寫作
J(theta prime) follow theta，為什麼要這麼麻煩寫 J(theta prime) follow theta 呢
這個括號裡面那個 theta 代表我們要去 optimize 的那個參數
theta prime 代表什麼，theta prime 是說
我們拿 theta prime 去做 demonstration
就是現在真正在跟環境互動的是 theta prime
因為 theta 是不跟環境做互動，是 theta prime 在跟環境互動
然後你用 theta prime 去跟環境做互動，sample 出 st/at 以後
那你要去計算 st 跟 at 的 advantage
然後你再去把它乘上 p(theta) at given st，再除掉 p(theta prime) at given st
那這兩項都是好算的
這一項你是可以從 data 裡面
你可以從這個 sample 的結果裡面，去估測出來的
所以這一整項，你是可以算的
那我們實際上在 update 參數的時候
就是按照上面這個式子 update 參數
現在我們做的事情，我們可以把 on-policy 換成 off-policy
但是我們會遇到的問題是
我們在前面講 importance sampling 的時候，我們說 importance sampling 有一個 issue
這個 issue 是什麼呢？其實你的 p theta 跟 p theta prime 不能差太多
差太多的話，importance sampling 結果就會不好
如果 p theta 跟 p theta prime 差太多的話
你的這兩個 distribution 差太多的話，importance sampling 的結果就會不好
所以怎麼避免它差太多呢？這個就是 PPO 在做的事情
PPO 你雖然你看它原始的 paper 或你看 PPO 的前身
TRPO 原始的 paper 的話，它裡面寫了很多的數學式
但它實際上做的事情式怎麼樣呢？
它實際上做的事情就是這樣
我們原來在 off-policy 的方法裡面說，我們要 optimize 的是這個 objective function
但是我們又說這個 objective function 又牽涉到 importance sampling
在做 importance sampling 的時候
p theta 不能跟 p theta prime 差太多
你做 demonstration 的 model 不能夠跟真正的 model 差太多，差太多的話 importance sampling 的結果就會不好
我們在 training 的時候
多加一個 constrain
這個 constrain 是什麼？
這個 constrain 是 theta 跟 theta prime
這兩個 model 它們 output 的 action 的 KL diversions
就是簡單來說，這一項的意思就是要衡量說 theta 跟 theta prime 有多像
然後我們希望，在 training 的過程中
我們 learn 出來的 theta 跟 theta prime 越像越好
因為 theta 如果跟 theta prime 不像的話
最後你做出來的結果，就會不好
所以在 PPO 裡面呢
有兩個式子，一方面就是 optimize 你要得到的你本來要 optimize 的東西
但是再加一個 constrain
這個 constrain 就好像那個 regularization 的 term 一樣
就好像我們在做 machine learning 的時候不是有 L1/L2 的 regularization
這一項也很像 regularization，這樣 regularization 做的事情就是希望最後 learn 出來的 theta
不要跟 theta prime 太不一樣
那 PPO 有一個前身叫做 TRPO
TRPO 寫的式子是這個樣子的
這一項，跟前面這一項是一樣的
它唯一不一樣的地方是說
這一個 constrain 擺的位置不一樣
這邊是直接把 constrain 放到你要 optimize 的那個式子裡面
然後接下來你就可以用 gradient ascent 的方法去 maximize 這個式子
但是如果是在 TRPO 的話，它是把 KL diversions 當作 constrain
他希望 theta 跟 theta prime 的 KL diversions
小於一個 delta
那你知道你在做那種 optimization
如果你是用 gradient based optimization 的時候
有 constrain 是很難處理的
所以你會發現 PPO 上次助教有講很多，我相信大家應該都沒有聽懂
那個是很難處理的
就是因為它是把這一個 KL diversions constrain 當做一個額外的 constrain
沒有放 objective 裡面，所以它很難算
所以如果你不想搬石頭砸自己的腳的話， 你就用 PPO 不要用 TRPO
看文獻上的結果是，PPO 跟 TRPO 可能 performance 差不多，但是 PPO 在實作上，比 TRPO 容易的多
那這邊要注意一下，所謂的 KL diversions
到底指的是什麼？
這邊我是直接把 KL diversions 當做一個 function，它吃的 input 是 theta 跟 theta prime
但我的意思並不是說把 theta/theta prime 當做一個 distribution
算這兩個 distribution 之間的距離，我不是這個意思
今天這個所謂的 theta 跟 theta prime 的距離
並不是參數上的距離，而是他們 behavior 上的距離
我不知道大家可不可以了解這中間的差異
就是假設你現在有一個 model
有一個 actor 它是 theta
你有另外一個 actor 它的參數是 theta prime
所謂參數上的距離就是你算這兩組參數有多像
我今天所講的不是參數上的距離， 我今天所講的是它們行為上的距離
就是你先帶進去一個 state s
然後它不是會 output 一個 distribution 嗎？
它會對這個 action 的 space output 一個 distribution
假設你有 3 個 actions
3 個可能 actions 就 output 3 個值
那我們今天所指的 distance 是 behavior distance
也就是說，給同樣的 state 的時候
他們 output 的這個 action，這個 action 之間的差距
這兩個 actions 的 distribution 他們都是一個機率分布嘛
所以就可以計算這兩個機率分布的 KL diversions
把不同的 state 它們 output 的這兩個 distribution 的 KL diversions，平均起來
才是我這邊所指的這兩個 actor 間的 KL diversions
不知道大家聽不聽得懂我的意思
那你可能說那怎麼不直接算這個 theta 或 theta prime 之間的距離
甚至不要用 KL diversions 算
L1 跟 L2 的 node 也可以保證 theta 跟 theta prime 很接近啊
在做 reinforcement learning 的時候，之所以我們考慮的不是參數上的距離
而是 action 上的距離，是因為很有可能對 actor 來說
參數的變化跟 action 的變化，不一定是完全一致的
就有時候你參數小小變了一下，它可能 output 的行為就差很多
或是參數變很多，但 output 的行為可能沒什麼改變
所以我們真正在意的是這個 actor 它的行為上的差距
而不是它們參數上的差距
所以這裡要注意一下，在做 PPO 的時候
所謂的 KL diversions 並不是參數的距離
而是 action 的距離
那假設這個你沒有聽得很懂的話
我們等一下還有一個 PPO2，我們這個是 PPO1
它還是略為複雜的，我們來看一下 PPO1 的 algorithm
它就是這樣，它說，initial 一個 policy 的參數，theta(0)
然後在每一個 iteration 裡面呢，你要用
參數 theta(k)，theta(k) 怎麼來的
那 theta(k) 就是你在前一個 training 的 iteration
得到的 actor 的參數，你用 theta(k) 去跟環境做互動
sample 到一大堆 state/action 的 pair
然後你根據 theta(k) 互動的結果，你也要估測一下
st 跟 at 這個 state/action pair它的 advantage
然後接下來，你就 apply PPO 的 optimization 的 formulation
但是跟原來的 policy gradient 不一樣，原來的 policy gradient 你只能 update 一次參數，update 完以後
你就要重新 sample data
但是現在不用，你拿 theta(k) 去跟環境做互動，sample 到這組 data 以後
你就努力去測 theta，你可以讓 theta update 很多次
想辦法去 maximize 你的 objective function
你讓 theta update 很多次
這邊 theta update 很多次沒有關係
因為我們已經有做 importance sampling
所以這些 experience，這些 state/action 的 pair 是從 theta(k) sample 出來的沒有關係
theta 可以 update 很多次，它跟 theta(k) 變得不太一樣也沒有關係，你還是可以照樣訓練 theta，那其實就說完了
在 PPO 的 paper 裡面
這邊還有一個 adaptive 的 KL diversions
因為這邊會遇到一個問題就是
這個 beta 要設多少
它就跟那個 regularization 一樣
regularization 前面也要乘一個 weight
所以這個 KL diversions 前面也要乘一個 weight，但是 beta 要設多少呢？
所以有個動態調整 beta 的方法，這個調整方法也是蠻直觀的
在這個直觀的方法裡面呢
你先設一個 KL diversions，你可以接受的最大值
然後假設你發現說
你 optimize 完這個式子以後
KL diversions 的項太大
那就代表說後面這個 penalize 的 term 沒有發揮作用
那就把 beta 調大
那另外你定一個 KL diversions 的最小值
而且發現 optimize 完上面這個式子以後，ok
你得到 KL diversions 比最小值還要小
那代表後面這一項它的效果太強了
那你怕它都只弄後面這一項
那 theta 跟 theta(k) 都一樣，這不是你要的
所以你這個時候你叫要減少 beta
所以這個 beta 是可以動態調整的
這個叫做 adaptive 的 KL penalty
所以實際上你在算這個 KL 的時候
也是有點麻煩的
什麼有點麻煩呢？
因為這個 KL 理論上你應該 sample 一大堆不同的 set
以下部分省略因為收音關係
如果你覺得這個很複雜，為什麼還要算 KL diversions 很複雜
有一個 PPO2，PPO2 它的式子我們就寫在這邊
要去 maximize 的 objective function 寫成這樣
它的式子裡面就沒有什麼 KL 了
這個式子看起來有點複雜，但實際 implement 就很簡單
我們來實際看一下說這個式子到底是什麼意思
這式子很複雜喔
這邊是 summation over state/action 的 pair，那沒有問題
這個是 minimize
就是說這邊有個大括號
這邊有個括號，這邊有個括號，這個括號裡面有兩項，這是第一項
這個是第二項，min 這個 operator 做的事情是
第一項跟第二項裡面選比較小的那個
第一項比較單純，第二項比較複雜，第二項前面有個 clip function
clip 這個 function 是什麼意思呢？clip 這個function 的意思是說
在括號裡面有 3 項
如果第一項小於第二項的話
那就 output 1-epsilon
第一項如果大於第三項的話，那就 output 1+epsilon
那 epsilon 是一個 hyper parameter，你要 tune 的，比如說你就設 0.1 啊，設 0.2 啊
也就是說，假設這邊設 0.2 的話，就是說這個值如果算出來小於 0.8，那就當作 0.8
這個值如果算出來大於 1.2，那就當作 1.2
這個式子到底是什麼意思呢？
我們先來解釋一下，我們來看一下這個，第二項這個就好
我們先來看第二項這個算出來到底是什麼的東西
第二項這項算出來的意思是這樣
假設這個橫軸是 p(theta) 除以 p(theta k)
橫軸是第一項 p(theta) 除以 p(theta k)
縱軸是 clip 這個 function 它實際的輸出，那我們剛才講過說
如果 p(theta) 除以 p(theta k) 大於 1+epsilon
它輸出就是 1+epsilon
如果小於 1-epsilon 它輸出就是 1-epsilon
如果介於 1+epsilon 跟 1-epsilon 之間， 就是輸入等於輸出
p(theta) 除以 p(theta k) 跟 clip function 輸出的關係， 是這樣的一個關係
那接下來呢，我們就加入前面這一項
來看看前面這一項，到底在做什麼？
前面這一項呢，其實就是綠色的這一條線，對不對
前面這一項，其實就是綠色的這一條線， p(theta) 除以 p(theta k)
就是綠色的這一條線
但是這兩項裡面，第一項跟第二項，也就是綠色的線
跟藍色的線中間，我們要取一個最小的
假設今天前面乘上的這個 term A
它是大於 0 的話，取最小的結果，就是紅色的這一條線
反之，如果 A 小於 0 的話
那取紅色，取最小的以後，就得到紅色的這一條線
這一個結果，其實非常的直觀
這一個式子雖然看起來有點複雜，implement 起來是蠻簡單的
想法也非常的直觀，因為這個式子想要做的事情
就是希望 p(theta) 跟 p(theta k)
也就是你拿來做 demonstration 的那個 model， 跟你實際上 learn 的 model
最後在 optimize 以後
不要差距太大
那這個式子是怎麼讓它做到不要差距太大的呢？
你要怎麼讓它做到不要差距太大呢？
複習一下這橫軸的意思
就是 p(theta) 除以 p(theta k)
如果今天 A 大於 0
也就是某一個 state/action 的 pair 是好的
那我們想要做的事情，當然是希望增加這個 state/action pair 的機率
也就是說，我們想要讓 p theta 越大越好
但是我們覺得 p(theta) 越大越好
沒有問題，但是，它跟這個 theta k 的比值
不可以超過 1+ epsilon
如果超過 1+epsilon 的話，就沒有 benefit 了
紅色的線就是我們的 objective function
我們希望我們的 objective 越大越好
當 p(theta) 比 p(theta k) 的比值
我們希望 p(theta) 越大越好
但是 p(theta) 比 p(theta k) 的比值只要大過 1+epsilon，就沒有 benefit 了
所以今天在 train 的時候
p(theta) 只會被 train 到
比 p(theta k) 它們相除大 1+epsilon，它就會停止
那假設今天不幸的是，p(theta) 比 p(theta k) 還要小
那我們的目標是要讓 p(theta) 越大越好
假設這個 advantage 是正的
我們當然希望 p(theta) 越大越好
假設這個 action 是好的，我們當然希望這個 action 被採取的機率，越大越好
所以假設 p(theta) 還比 p(theta k) 小
那就盡量把它挪大
但只要大到 1+epsilon 就好
那負的時候也是一樣，如果今天
某一個 state/action pair 是不好的，我們當然希望 p(theta) 把它減小
假設今天 p(theta) 比 p(theta k) 還大那你就要趕快盡量把它壓小
那壓到什麼樣就停止呢？
壓到 p(theta) 除以 p(theta k) 是 1-epsilon 的時候
就停了，就算了
就不要再壓得更小
那這樣的好處就是， 你不會讓 p(theta) 跟 p(theta k) 差距太大
那要 implement 這個東西， 其實對你來說可能不是太困難的事情
那最後這頁投影片呢
只是想要 show 一下
在文獻上，PPO 跟其它方法的比較
今天這邊有 show 的有這個 Actor-Critic 的方法
這邊有 A2C+TrustRegion 他們都是 actor-critic based 的方法
Actor-Critic 我們之後會講到
然後這邊有 PPO，PPO 是紫色線的方法
然後還有 TRPO
PPO 就是紫色的線
那你會發現在多數的 task 裡面，這邊每張圖就是某一個 RL 的任務
你會發現說在多數的 cases 裡面
PPO 都是不錯的，不是最好的，就是第二好的
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
