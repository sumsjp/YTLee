接下來的下一段呢,我想跟大家介紹一下一些經典的影像生成的方法
那這邊會跟大家很快提到的方法,包括 Variational Autoencoder
它的所寫是 VAE Flow-Based Model
還有今天大家都耳熟能詳的 Diffusion Model
還有 Generative Adversarial Network,它的所寫是 GAN
那像 Sora 呢,他用的就是 Diffusion Model
如果你在論文或Blog裡面看到有一個東西
是從一大堆的雜訊慢慢生出來的
本來很多雜訊
雜訊越來越少
最後產生高清的圖的話
那這個通常指的就是Diffusion的Model
那我想用一點時間來介紹這些經典的影像生成的方法
雖然今天Diffusion Model可能是最常用的
但是我們可以多介紹幾個經典的方法
讓你知道說到底影像生成真正的難點
真正被克服的挑戰是什麼
那我們剛才已經講到
我們在生圖片的時候或生影像的時候
今天常常用的類神經網路的架構
就是Transformer
那當然過去比較常用CNN
不過今天Transformer真的是一個非常常用的
類神經網路的架構
而它的概念也非常的簡單
每一段文字產生一排patch
就可以產生圖片了
但是我們在訓練的時候
可能會遇到這樣子的一個問題
我們可能在訓練資料裡面
如果你找一隻奔跑的狗
你會發現一隻奔跑的狗
有很多不同的樣子
它可以是一個在草原上奔跑的哈士奇
也可以是在都市裡面奔跑的柴犬
一隻奔跑的狗
它不只一個樣子
對於transformer來說
對我們來說,他得到的教材是
當輸入的文字是一隻奔跑的狗的時候
有時候你告訴他
要產生這些patch
但有些時候你要告訴他
要產生另外一些patch
對這個transformer來說
他就無所適從
你叫他產生一個哈士奇也好
他能生得出來
你叫他產生一個柴犬也好,他可以生得出來
但是你有時候叫他產生柴犬
有時候叫他產生哈士奇
他頭腦就會很破
每一個位置 每一個patch 想要產生的東西都不一樣
你可能就會產生各式各樣奇怪的混種的圖片
這個就跟你如果去工作的話
如果老闆都只叫你往東那沒問題 都只叫你往西沒問題
最怕的就是遇到老闆有時候叫你往東 有時候叫你往西
你就會無所適從 這個時候是特別痛苦的
對Transformer來說 他在學影像的時候也有一樣的問題
因為通常影像所對應的文字
沒有辦法完整的描述整張圖片
所以你會有很多同樣文字對應到不同圖片的狀況
對Transformer來說
他會不知道要如何學習
不知道要聽哪一筆訓練資料的
所以怎麼辦呢
我們現在先把那個影像圖片生成的模型簡化一下啦
我這邊就直接用個框框呢
來代表那個Transformer
那大家知道我的意思
就這個框框裡面其實是一個Transformer
那給他一段文字
還要生一堆Patch出來
這些Patch會變成圖
好 那我們剛才說圖片生成或者是影像生成問題也是一樣的
他的難點就是這些文字往往沒辦法完整的描述影像
那怎麼辦呢
也許一個可能性是
雖然我們本來的訓練資料裡面這些文字不足以完整的描述影像
但我們自己把本來文字沒提到的東西加進去
我們直接把多餘的資訊加進去
都跟圖片生成模型講說
你現在不只要生成一隻奔跑的狗
你是要生成一隻哈士奇在草原上奔跑的樣子
你不是隻生成一隻狗
你要生成柴犬在都市裡奔跑的樣子
把額外的原來文字沒提到的資訊
把它標出來
也提供給圖片生成模型
這樣圖片生成模型在生成的時候
他就知道說
為什麼同樣的輸入
這兩隻圖片會不一樣
因為有額外的資訊
他會讀取這個額外的資訊
再去生成圖片
他就不會有同樣的輸入
有會有不同的輸出的這種問題
他就不會有那種同樣的輸入
你想要叫他產生不同輸出的這種問題
但是這些標註又要從哪裡取得呢
也許我們可以訓練一個資訊抽取的模型
這個資訊抽取的模型
他做的事情就是給他一張圖片
給他一段文字敘述
他把這個圖片裡面文字資訊
沒辦法沒有描述到的東西
額外抽取出來丟給圖片生成模型
如果我們有這樣的一個資訊抽取模型
我們就可以對訓練資料裡面所有的圖片
都把這些文字沒描述到的資訊抽取出來
丟給圖片生成模型
讓圖片生成模型的學習就會更加容易
但現在問題是
怎麼訓練這個資訊抽取的模型呢
你知道你要訓練一個模型的時候
你都要有大量的輸入跟輸出成對的資料
你要怎麼找到這樣子的輸入跟輸出成對的資料呢?
你要怎麼找到人去幫你標註?
你要怎麼輕易地標註出說
這張圖片裡面沒有被描述到的資訊就是這些
就是這些呢?
這邊有一個方法是
在不需要額外標註資料的情況下
就可以把資訊抽取的模型訓練出來
這個方法是這樣的
圖片生成的模型跟資訊抽取的模型
它們可以一起被訓練
他們有一個共同的目標
他們共同的目標是什麼呢
他們的共同目標是
資訊抽取的模型
讀訓練資料裡面的一張圖片
那訓練資料的每一張圖片
都有附一段文字的描述
把文字的描述也讀進去
他可以看看有哪一些
文字的描述沒提到的東西
把它抽取出來
那再把這些抽取到的資訊
丟給圖片生成的模型
圖片生成的模型看到這些
在讀這段文字的敘述產生一張圖片
那資訊抽取的模型
雖然你沒有告訴他要抽取哪些資訊
但是資訊抽取的模型跟圖片生成的模型
有一個共同的目標
他們共同的目標就是
輸入一張圖片
經過這一連串的處理之後
最終輸出的圖片
要跟輸入越接近越好
所以中間資訊抽取的模型
到底抽取出來什麼
不重要
我們自己也不知道
我們也沒有標準答案
但反正他抽出來的東西
可以幫助圖片生成的模型生出
跟輸入一模一樣的圖
資訊抽取的模型就算是成功了
所以資訊抽取的模型跟圖片生成的模型
是可以一起被訓練出來的
一次解決了兩個問題
有資訊抽取的模型抽出文字敘述
沒有表達到的資訊
圖片生成的模型在訓練的時候
又有這些額外的資訊輔助它生成圖片
這整個框架叫做AutoEncoder
資訊抽取的這個模型又叫做Encoder
圖片生成的這個模型又叫Decoder
這個Encoder跟Decoder一起訓練
要讓輸入跟輸出越接近越好
這個訓練的框架
這個訓練的方法叫做AutoEncoder
這個資訊抽取模型它的輸出不需要是文字
我們剛才的講法好像讓你覺得資訊抽取模型它的輸出一定是好像是文字
這個是為了讓你方便理解
對於一個圖片生成的模型來說
它又不是人
它不需要讀文字
你只要能夠傳遞一些密碼
它讀得懂知道那是什麼就好了
所以一般資訊抽取的模型它的輸出不是文字
而是一串數值,也就是一個向量
那通常這個項量裡面每一個維度
可能就會對應到某些特別的含義
那在這個例子裡面
第一個維度的數值也許代表是狗的品種
第二個維度的數值也許代表的是背景
在什麼樣的地方
當圖片生成的模型給它不同的向量的時候
它就會產生不同的載分號的狗
好,那現在有了這個Encoder
也有了這個Decoder之後
但是實際上在圖片生成的時候要怎麼做呢
想想看在圖片生成的時候
我們只有這段文字敘述
圖片生成的模型是要根據這段文字敘述
去產生一張圖出來
我們根本不知道這段文字敘述沒有描述的資訊是什麼
因為我們根本還沒有把圖生成出來啊
這不是在訓練的時候你已經知道這張圖是什麼了
所以資訊抽取的模型
可以把文字沒提到的資訊重新取出來
在真的使用這個模型的時候
你根本不知道這些文字沒描述的資訊是什麼
那這個文字沒描述的資訊
這個需要腦補的資訊從哪裡來呢
我們已經知道這些要腦補的資訊
這些文字沒有描述到的資訊
它就是用某一些數值來表示
所以當你在用這個圖片生成模型的時候
你就會先擲個骰子
把這個向量裡面的每一個數值
都擲骰子植出來
擲到多少就是多少
擲骰子擲出來
圖片生成的模型就根據這段文字敘述
跟這個擲骰子得到的腦補資訊
因為反正現在人沒有輸入嘛
所以人大概也不知道到底要腦補些什麼
我們就用機率來產生
我們就把這件事情交給上天來決定
用機率來產生這個要腦補的向量
把這個有腦補資訊的向量丟給這個圖片生成的模型
圖片生成的模型就根據人數的文字
跟這個隨機產生的腦補的資訊
去產生最終輸出的結果
那這個模型呢
其實就是VAE啊
其實就是VAE
我不知道多少同學以前也許有聽過VAE
也許你之前聽到的講法不是這樣講的
那我這邊就是用一個也許比較更容易理解的講法
來告訴你為什麼VAE的模型是長這個樣子
所以大家已經知道說VAE的模型就是有個Encoder
去抽文字沒辦法描述的資訊
有個Decoder可以把腦補的資訊還原回來
那有另外一個模型叫做Flow
這個Flow這個模型呢
跟VAE非常的像
它唯一不一樣的地方只有
VAE需要訓練一個Encoder
訓練一個Decoder
那Flow這個模型它想說
這個 encoder 跟 decoder 做的事情不是正好相反嗎
一個是從圖片裡面抽取資訊
一個是把資訊還原回圖片
他們做的事情正好相反
我們有必要訓練兩個不同的模型嗎
我們就訓練一個 decoder 就好
然後接下來我們強迫 decoder
一定要是 invertible 的
這個 decoder 一定有一個反函數
這個反函數做的事情正好就是 decoder 的相反
那這個反函數呢,直接就可以拿來當做encoder
在flow-based model裡面,它跟VAE不一樣的地方就是
它只訓練了一個decoder,但是它同時呢,就有了一個encoder
那在文件上啊,通常會把這個encoder抽取出來的這個東西
encoder抽取出來的這個東西
也就是decoder要拿來腦補的這個資訊呢,叫做noise
那這個詞彙可能會讓你覺得說
就是說這個Noise裡面的資訊都是不重要的,沒有用的資訊
但是其實不是,這個Noise裡面往往也包含了非常重要的資訊
有一個真實的實驗是這個樣子的
這個實驗是為了告訴你說
這些Noise裡面可能是有包含跟圖片的生成非常相關的資訊的
舉例來說,你找一大堆看起來臉很臭的人
把這些臉臭的人丟到Encoder裡面抽出一個向量
這些向量重複加起來就代表一個臉很臭的臭臉特徵的向量
那你把一些看起來在笑的人把他的臉丟到Encoder裡面
Encoder會輸出一些向量
這些向量平均起來就是笑臉的特徵相量
那知道這些事情以後可以做什麼呢?
我們剛才說VAE就是有一個Encoder有一個Decoder
擺張圖片就會Encoder
他抽出一個向量
Decoder可以根據這個向量還原成原來的圖片
但是我們可以對這個向量呢
直接做一些改造
因為我們已經知道臭臉的向量長什麼樣子
如果我們今天的目標是要讓這個人呢
啊這是我的臉吶
但是我看起來已經在笑了
但是我們想要讓他笑得更開心一點
所以我們就可以把臭臉的向量呢剪掉
把臭臉的特徵剪掉
再加上笑臉的特徵
所以把這個原來這個黃色的向量
合出來的圖片是長這個樣子
那你可以在這個黃色的向量上
再做一些改造
把臭臉的特徵剪掉
把笑臉的特徵加上去
透過decoder以後
這個人呢就會笑得更加的開心
這些向量或我們叫做noise
他其實裡面也是有豐富的資訊的
你甚至可以直接調整這一些向量
就打造調整輸出圖片的效果
好那接下來呢
我們就要來介紹Diffusion Model
我們剛才已經介紹兩個模型
VAE跟Flow-Based Model
接下來我們來介紹Diffusion Model
Diffusion Model
它的Decoder的輸入跟輸出
跟剛才幾個模型都是一樣的
記不記得剛才幾個模型都需要輸入一個Noise
進行腦補
輸入一個 noise 包含腦補的資訊
然後產生一張圖片
Diffusion Model 裡面的 decoder 做的事情
也是一樣的
只是 Diffusion Model 不一樣的地方
是他會反覆的使用同一個 decoder
那這 decoder 呢
每次在做事情的時候
每次在生圖的時候
他只做一件簡單的事情
這件簡單的事情
是去除雜訊 也就是 de-noise
所以一開始給他一個雜訊
然後這個雜訊被輸到Decoder裡面的一個Denoise的Module
再加上文字敘述的資訊
那這個Denoise的Module就會把這個圖片裡面的雜訊稍微去掉一點
希望他看起來比較像是文字的描述
比較像是雪地的一個貓
那第一次的Denoise也許不會非常的成功
只能夠去掉一點的雜訊
但是Denoise的這個Module會鍥而不捨
不斷的去做Denoise
所以根據第一次Denoise的結果
再丟給Denoise的Module
再Denoise一次
也許看起來就更像一隻貓
這個過程通常會反覆進行下去
通常需要反覆個500、1000次
最後你就可以產生一張清晰的圖片
通常Diffusion Model的痛點
也就在於Denoise必須要用很多很多次
最後才能夠產生出好的結果
而今天Diffusion Model相關的研究
也就集中在
如何用比較少的Denoise的次數
就可以得到清晰的圖片
那如果傳統的方法都是500、1000次
那很多人就會想說有沒有辦法
10次就好
有沒有辦法5次就好
甚至1次就好
那這個是今天研究的一個趨勢
那這個Denoise的model
他要學的是怎麼去除雜訊
怎麼教這個Denoise的模組
做去除雜訊這件事呢
你可以自己製造訓練資料
怎麼自己製造訓練資料呢?
你想想看,要做Denoise的時候
就是要教機器怎麼把雜訊去掉
所以你需要有雜訊的圖
跟沒有雜訊的圖
但是你的訓練資料裡面
都是沒有雜訊的圖跟它對應的文字
那怎麼產生有雜訊的圖呢?
你就自己製造就好啦
就自己弄一些雜訊來
用擲骰子的方式擲出一些雜訊
自己產生一些雜訊
這些雜訊呢,加到你的訓練資料裡面
就產生有雜訊的圖片
那你通常會加雜訊加很多次
這是第一次加雜訊
第二次加雜訊,愈加愈多
直到最後完全看不出來
原來圖片裡面的物件是什麼
所以你就有了
透過這個加雜訊的過程
你就有了加雜訊前
跟加雜訊以後的圖片
然後接下來你就可以訓練你的
Denoise去照的這個模型
就可以訓練你的Denoise的model
你就跟這個Denoise的model說
之前這一張照片
加了這個雜訊就變成這個樣子
現在你要學習的就是
給你這張圖跟給你這段文字
跟給你這段文字的敘述
你要還原回
他加雜訊前的樣子
或者是這張圖片
加了這個雜訊以後就變成了這個樣子
那你就要學到說
給你這張圖片跟這段文字
你要想辦法還原回
加雜訊之前的樣子
所以就自己加雜訊
然後就可以教Denoise的model
如何去去掉你自己加入的雜訊
之後就可以用這個Denoise的model
來產生圖片
那當然這個Denoise的model裡面
其實
它的類神經網路的架構
也是transformer
所以實際上
當我們在用這個
Diffusion的model來生圖的時候
那時候你其實可以結合Transformer
這也是今天非常常用的技術
然後在這個SORA的這個paper裡面也提到了
他們用的就是這樣的方式
把Diffusion跟Transformer結合在一起
他們就這個Diffusion跟Transformer結合在一起
就是Diffusion Transformer
也就是說本來的Transformer呢
是給一些輸入
把它產生一排Patch就可以還原成圖片
那現在呢
如果是用Diffusion Model的話
那每一次Transformer在做的事情就是Denoise,就是去掉一些雜訊
所以一開始輸入的Patch都是雜訊的Patch
然後第一個Transformer去掉一些雜訊
第二個Transformer
其實都是同樣的參數
第一個Transformer
第一次用Transformer去掉一些雜訊
然後第二次再用Transformer再去掉更多雜訊
第三次再用Transformer再去掉更多雜訊
把這個Transformer用個五百一千次
最後就產生乾淨的Patch
你可以把這個乾淨的 Patch 丟到 Decoder 裡面還原出圖片
這個就是 Sora 裡面有用到的 Diffusion Transformer
雖然這個例子裡面舉的是圖片
但是要把它 Extend 把它一般化到生影片
也其實就是一樣的事情
在影片的模型裡面
你就是有一堆的 Transformer
在這個影片生成的過程中
你就是會不斷的做 Denoise
那這個 Denoise 是由一個 Transformer 的模型完成的
它的輸入就是有雜訊的 patch,輸出就是比較乾淨的 patch
這個 Transformer 會做好幾百次、好幾千次的去照之後
期待最後產生一堆乾淨的 patch
那這些乾淨的 patch 就可以被還原為影像
這個就是今天用 Diffusion Model 加上 Transformer 生一片的模型
那根據 Sora 的 blog
他們很有可能也就使用類似的技術來產生一片的
好,那我們現在講了Diffusion的model
Diffusion的model跟剛才的VAE還有Flow-based model要怎麼來類比呢?
剛才的VAE跟Flow-based model都有一個Encoder跟一個Decoder
對於Diffusion model來說,它也有一個Decoder
它Decoder裡面做的事情是多次的Denoise
那至於Encoder呢,怎麼把一個圖片變成一個雜訊呢?
對Diffusion Model來說,它就沒有特別訓練一個模型做這件事情
因為雜訊是你自己加進去的
你自己把這個圖片加了大量雜訊以後
讓它看不出來原本物件的樣子
那這個加雜訊的過程對應到其他的模型
就是Encoder在做的事情
那在Diffusion Model的文件裡面
通常叫做Forward Process
那Decoder做的事情是Reverse Process
所以今天我們就跟大家很快地講過
VAE Flow-Based Model跟Diffusion的Model
那另外一個也很知名的模型叫做
Generative Adversarial Network
它的縮寫是GAM
我們這邊就沒有特別把GAM
拿出來跟其他的模型一起講
為什麼
因為GAM跟其他模型
其實有本質上的差異
它比較像是一個額外的外掛
我們來看看為什麼我會這樣說
那我們剛才有講到說圖片生成的困難
或影片生成的困難
就是同一段文字可以對應很多不同的圖片
所以在圖片生成的過程中
模型會生得暈頭轉向
它會無所適從
所以怎麼辦呢
也許我們不應該用這些真正的圖片
來教圖片生成的模型
也許應該先訓練出另外一個模型
先訓練另外一個AI
他先自己咀嚼奔跑的狗應該長什麼樣子
他先理解好奔跑的狗應該長什麼樣子
再來教我們的圖片生成模型
怎麼訓練另外一個AI
一隻奔跑的狗長什麼樣子呢
這邊用的概念跟剛才的clip
基本上是一模一樣的
記不記得我們在講
這個怎麼衡量文字生圖模型好壞的時候
我們講了一個clip的模型
他會吃一張圖片
讀一段文字
看看這個圖片跟這個文字是不是匹配的
那在GAME裡面也會訓練一個
其實就跟CLIP根本就是一模一樣的模型
他只是換了一個名字叫做Discriminator
這個Discriminator他做的事情就是
給一張圖片、給一張文字
還要給一個評價
看這個圖片跟這個文字有多匹配
但是如果今天訓練Discriminator的時候
都只有好的例子
都只有跟這個影像匹配的文字的話
那Discriminator從來沒有看過壞的例子
他會他訓練的
如果你在訓練的時候都只給他好的例子
到時候測試的時候就算看到壞的例子
他也都會輸出這是好的
因為只有教他說好
你只有教他輸出高分
你只有教他豎起大拇指
沒有教他做其他的事情
所以你同時在訓練的時候
也得給他一些壞的例子
那像在Clip裡面那個壞的例子
意思就是把圖片跟文字隨便打亂
隨機配對製造一些壞的例子
那在更裡面用的是另外一個方法
假設你現在已經有一個圖片生成的模型
那就先不要問這個圖片生成的模型是哪裡來的
假設你有一個它只是有點差而已
你先拿這個圖片生成的模型先生一些圖片
你只要畫一隻要奔跑的狗
畫一個抽象的狗
你叫他畫一個陽光下的貓
他畫一個背景是黃的很抽象的貓
但是隻有三隻腳他是個三角貓
然後呢你就跟Discriminator說
看到這個抽象的狗
你要說是不好的
看到這個三角貓呢
你也要說是不好的
所以你給Discriminator一些正面的例子
一些反面的例子
他就可以學會評價一張圖片
跟一段文字的敘述是不是匹配的
好那有了這個Discriminator以後
接下來啊
Generator就不去跟
正確的真正的圖片學
他跟誰學呢
他跟Discriminator來學習
所以對Generator來說他要做的事情是
他一開始生成一張圖片
把他生成出來的圖片跟文字敘述丟給Discriminator
那Discriminator覺得這個抽象的懂化太差了
給了一個負評
那Generator學習的對象
他調整參數的方向
就是希望去騙過Discriminator
Generator他要做的事情其實就是產生一張圖片
這張圖片給Discriminator
Discriminator會覺得他做得非常的棒
Discriminator覺得他做得好
然後Generator就成功了
那這跟某一個ground truth
跟某一個正確的圖片學習
不一樣的地方就是
跟正確圖片學習的時候
可能同樣的文字會對應到不同的答案
但是現在是用一個Discriminator來評價好壞
所以沒有單一的正確解答
只要今天Generator
可以產生一張圖片,Discriminator是覺得好的,就成功了
沒有什麼樣的答案是正確的,沒有什麼樣的答案才是標準答案
只要另外一個人工智慧覺得好,基本上就是成功了
實際上在做的時候,Generator跟Discriminator會交替訓練
你先有一個Generator,用這個Generator你可以去訓練Discriminator
然後有了Discriminator之後,你就回過頭來訓練Generator
讓他產生出Discriminator覺得好的圖片
然後Discriminator也會更新他的參數
讓自己變得更厲害一點
那兩個人呢
Generator跟Discriminator
這個圖片生成的模型
在Gan的那個框架下
往往又叫做Generator
這個Generator跟Discriminator
就會交替的訓練
那講到這邊有人可能會問說
對於一個Gan這樣的Generator來說
那還需不需要輸入一個雜訊呢
因為我們說輸入雜訊的目的
就是為了要讓模型可以學會根據這個雜訊的資訊來腦補
它才能夠知道說要產生什麼樣的東西
如果今天在有Discriminator的情況下
其實是可以不用這個雜訊的
雖然你看在文件上多數GAN的模型都還是有給這個雜訊
但我這邊實際的經驗就是
如果是圖文字生圖的話
在有Discriminator的情況下
每加這個雜訊也能夠把Generator訓練的起來
那其實我們也可以把GEM的訓練跟RLHF對比一下
大家還記得RLHF嗎?
我們在講大型語言模型訓練的最後一段的時候
第三段的時候
說大型語言模型訓練第三段就是RLHF
那你還記得說在講RLHF的時候
我們說會訓練一個Reward Model
Language Model其實是跟這個Reward Model學習的嗎?
那如果對應到game的框架的話
其實這個discriminator就是reward model
他們唯一不一樣的地方只是
reward model在學習的時候有人的標註
人告訴他說這個答案有多好
另外一個答案有多差
而這邊discriminator他在學習的時候
其實不需要人的介入
他就用一個假設
這個假設是隻要是AI所生成的圖
基本上就都是差的
原來訓練資料裡面的圖就都是好的
所以這個Discriminator其實就是ILHF裡面的reward model
只是他們學習的資料有一點點不同而已
常常有人會問說
這個Gain跟Diffusion Model到底誰比較強呢
其實我覺得這並不是一個好的問題
為什麼
因為就好像RLHF就是一個方法
它可以強化你的模型的能力
其實Gain也是一樣
Gain是一個外掛
這個外掛你可以掛在VAE上
你也可以掛在Flow上,你也可以掛在Diffusion Model上
所以剛才所介紹的VAE Flow或Diffusion Model
你其實都可以在他們後面再掛一個Discriminator
然後來強化這些Decoder
在Generator,在GAM的框架裡面叫做Generator
他們的能力
所以GAN其實是可以跟其他方法結合的一個外掛
那如果你想要知道更多有關經典的影像生成的模型的話
那這些都是我在我的YouTube頻道上有對應的影片的
那其實從這些影片
他們是在哪一門課裡面被錄的
你也可以知道這一些模型
它的歷史的變化
最早的時候有VAE
這個是2016年的時候機器學習講的
然後呢在2018年的機器學習及生存與結構化這門課裡面
花了三分之一的學習的時間講GAN
GAN可以講的東西太多了
所以有一個GAN的系列
2019年的機器學習講了Flow-Based Model
2023年的機器學習講了Diffusion Model
這個Diffusion Model背後有非常多的數學的原理
其實要講的話兩三週都是講不完的
那你可以慢慢的消化這些點
如果你覺得剛才講的東西沒有很清楚的話
更詳細的資訊
它背後的數學原理
都是在我的YouTube頻道上有的
好那最後一段呢
想跟大家分享的是
現在人工智慧可以生成影像
但是有沒有機會讓人類跟這些生成的影像
有更及時的互動呢
舉例來說
每次在講SORA的時候
大家都會播放這個女士在東京街頭走的影片
有沒有可能我們直接用上下左右
來操控這個女士走哪一條路呢
那你就可以等於是操控一個Avatar
在東京街頭漫遊
就等於是打造出了一個開放世界的3D遊戲
有沒有可能做到這件事情呢?
還真不是不可能的
有一篇paper叫做
Genie: Generated Interactive Environments
就是想做類似的事情
我這邊先打個預防針
他做的不是3D遊戲
他做的都是2D橫向捲軸的遊戲而已
那這個Genie做的事情是這樣子的
他的模型可以讀一個畫面作為輸入
不只是讀一個畫面作為輸入
它還可以讀一個你輸入的動作
所以你可以想像說你有一個搖桿
接到Genie的這個模型
你可以按上下左右
它可以把你的動作讀進去
而你的動作會影響這個模型的輸出
同樣的輸入畫面你按的按鈕不同的時候
你的輸出就會是不一樣的
然後這個遊戲就可以一直下去
你按了右以後看到第二個畫面
第二個畫面會變成Dynamic Model的輸入
你再按一個A,然後又會產生新的畫面
你就可以用這個影像生成的技術來玩一個遊戲
你等於是即時的控制了影像生成的結果
那Genie它想要做的是橫向2D卷軸遊戲
想要做的事情就是去收集了大量的橫向卷軸遊戲
但是這邊你可以想想看
我們要怎麼訓練像GENIE這樣子的模型呢
我們需要的是遊戲的畫面
加玩遊戲的時候人的輸入
當然不是每一個畫面人都會有輸入
我們可以把沒有按任何按鈕
也當作是一種特別的輸入
如果我們可以收集到人玩遊戲的畫面
加玩遊戲的時候按了哪一個鈕
你要訓練剛才前一頁投影片講的模型
就易如反掌
你只需要教你的AI說
看到這個圖片加這個動作
你就產生這個圖
生圖要怎麼做
大家已經都會了嘛
看到這兩個圖加這兩個動作
你就要生成這個圖
以此類推
所以只要有遊戲畫面加人按按鈕的這些動作
你就可以訓練一個像Gini一樣的模型
但是今天Gini訓練的過程中
真正的難點就是
他只能從網路上收集大量的遊戲影片
但是他沒有使用者輸入的動作
他這種遊戲畫面
他不知道產生這些畫面的時候
人到底按了什麼樣的按鈕
所以怎麼辦呢
Genie 用了一個神奇的方法
去想辦法反推
人類到底輸入的按的按鈕是哪一個
他用的做法
跟我們剛才講 VAE 的時候提到的那個
Auto-Encoded
概念基本上是一模一樣的
記不記得我們剛才才講到說
有一個資訊抽取的模型
可以抽出文字沒辦法描述的資訊
然後圖片生成的模型可以根據這些資訊還原回原來的圖片
今天對於Genie來說
當給一個之前的遊戲畫面要預測下一個畫面的時候
他其實也是有缺少資訊的
他缺少了什麼
他缺少了使用者的動作
在不知道使用者動作的情況下
根據之前的遊戲畫面是沒有辦法去預測未來的畫面的
你不知道使用者要按左還是按右
你就會不知道使用者操控那個人物會往左邊跑
還是往右邊跑
所以今天圖片生成的模型
需要知道使用者的動作
但問題就是我們並不知道使用者的動作
那怎麼辦
訓練一個動作抽取的模型
這個動作抽取的模型就是讀之前的遊戲畫面
再讀
下一個畫面
看看這兩個畫面間有什麼樣的差異
然後試圖歸納出人所會採取的動作
人到底按了哪一個按鈕
造成之前的遊戲畫面會變成這個樣子
但是我們沒有這樣子動作的標註啊,怎麼辦?
在訓練的時候就用AutoEncoder的概念
給動作抽取的模型之前的遊戲畫面跟下一個畫面
它猜現在應該要做哪一個動作,要按哪一個鈕
再把這個按鈕的資訊加之前的遊戲畫面
輸入給圖片生成的模型
讓它生成下一個畫面的樣子
然後我們要讓輸入跟輸出越接近越好
那如果今天動作抽取的模型
可以正確的猜出是哪一個動作的話
那圖片生成的模型可能就可以正確的生圖
所以最後如果圖片生成的模型可以正確的生圖
可能代表動作抽取的模型正確的猜出
現在人實際上按的是哪一個按鈕
不過因為這些動作它不是真正的動作
它是動作抽取模型猜出來的動作
所以這些動作前面加上Latent這個字
代表它是被猜出來的
它不是真正光明正大的收集到的資料
它是Latent的
它是這個動作抽取模型猜測出來的動作
好 那這邊在這個Genie那篇文獻裡面呢
他就假設說啊
人可以按的鈕就是八個
把它編號1到8
那你說這個編號1到8的按鈕
到底哪一個對應到上
哪一個對應到下
哪一個對應到A
哪一個對應到B
不知道也不重要
你就記得說反正有八個按鈕
可以操控圖片的生成就對了
他們發現說呢
確實你如果輸入同樣的按鈕
不同的畫面裡面的
變化會是一致的
比如說他們發現說
如果你輸入一串的指令
這個是6676765527
你看這一串按鈕
不管是哪一個畫面
他都會讓人物往右移動
然後整個畫面呢
會往左下角移動
然後這個顯然就是可能使用者按了一個
右跳的動作
所以大概可以猜一下
透過Latent Action猜一下
使用者實際上呢
所按的按鈕是哪一個
那有了這個東西以後
有了GENIE以後可以做什麼事情呢
你就可以隨便創造遊戲
你就給他一張圖
就這個隨便小孩子的一個塗鴉
然後你就可以把它當作一個遊戲的畫面
就可以開始玩了
你就可以按一個右上跳的按鈕
他就可以往右上跳
然後甚至連真實場景的照片也可以
給一個照片
這是一個雷神索爾
隨便拍了一張照片
然後接下來你就把它輸入給Gini
再按一個往右上跳的動作
它就跳起來了
所以未來我們就可以更及時的
跟影像生成的模型互動
那你可能會問說
這樣子的模型能幹什麼
不就只是可以比較快的創造遊戲嗎
但是你可以想像
未來就會有很多其他的應用
比如說
開車
現在開車的時候
你需要去駕訓班
上完駕訓班以後
你要去路考
那路考其實假設你駕訓班沒有學得很好
好像路口很危險的
那未來你就不需要去駕訓班了
你就在自己的電腦前面
你就可以學開車
來就可以看到螢幕的畫面
來就有一個假的方向盤
你轉方向盤螢幕的畫面就變了
那你可能會想說
這個不就跟一般的賽車遊戲一樣嗎
但是一般的賽車遊戲
它的場景是有限的
場景是程式自動規劃好了
但是有這樣子影像生成模型
你又可以跟它互動
那也許未來
你就可以在一個開放世界裡開車
你就可以一直往前開
然後畫面都會一直出現,永遠都不會停這樣
就以後這個世界可能就可以變成這個樣子
