那我們在前一段課程裡面已經講到說
在語言模型修煉的第一階段
他自我學習累積了很多實力
但是他空有上乘內功卻不知道使用的方法
現在在第二階段
我們要讓語言模型接受人類老師的點播
發揮他的潛力
那這個語言模型怎麼接受人類老師的教導呢
人類老師需要給語言模型準備的教材是長這個樣子的
要先想出人類可能會去問語言模型的問題
為每一個問題想一個正確的答案
那有問題有答案之後
就可以把這些資料轉成語言模型
可以拿來做訓練文字接龍的格式
舉例來說
假設問題是臺灣最高的山是哪座問號
答案是玉山
那對語言模型來說,他看到的訓練資料就是這樣
有使用者的輸入,冒號,臺灣最高的山是哪座,問號,AI冒號
如果輸入是這個句子,他接下來就要接玉
那如果輸入是使用者的問題,AI冒號預,那後面要接山
如果輸入是AI冒號玉山,後面就接結束的符號
那另外一個問題,有人問你是誰
把他答案要答我是人工智慧
那他的訓練資料就變成
輸入是使用者說你是誰
AI冒號
那正確的輸出就是我
輸入是使用者說你是誰
問號AI冒號我
那我後面就解釋
那用這樣子人類老師準備的資料
來學習的過程呢
叫做instruction的fine tuning
那這個instruction的意思是說
人類老師提供了一些指令
那機器會學習怎麼按照這些指令來做正確的回應
這些訓練資料是要耗費大量的人力才能取得的
那這種耗費大量人力產生資料的方式叫做資料標註
那透過這種資料標註的方式
透過耗費大量人力產生的資料訓練出來的模型
這個訓練過程叫做督導式的學習
它的英文是supervised learning
那這邊提一個細節
為什麼今天在這個輸入的資料裡面
要特別標上哪個部分是使用者講的
哪個部分是AI講的呢
你想想看
如果今天我們沒有標哪個部分是使用者講的
哪個部分是AI講的
那會遭遇什麼狀況呢
如果我們把user跟AI這兩個符號拿掉
那這個輸入有可能是使用者說
臺灣最高的山是哪座問號
AI已經回答出了玉山
要繼續去做文字接龍
也有可能是使用者不知道怎麼回事
就是自問自答
臺灣最高的山是哪座問號
自問自答
AI一個話一句話都還沒有說
那這個時候在這兩個case
他的答案是不一樣的
在上面第一個case
今天正確的答案應該是輸出結束的符號
在第二個case
語言模型的輸出可以是對
代表贊同人類的想法
這個輸入的這個未完整的句子
必須要標出說哪個部分是使用者講的
哪個部分是AI講的
這樣AI的答案比較容易是正確的
那你可能會問說
那今天我們在使用這個
ChatGPT的web介面的時候
當我在使用ChatGPT的時候
他做文字接龍的時候
有標上這些符號嗎
很有可能是有的
當你問他你是誰的時候
他並不是只從
你是誰你輸入的問題
去做文字接龍接出這個答案
更有可能的狀況是
你的這個
代表你的這個符號
代表ChatGPT的這個符號
他們也都在
文字接龍的過程中
所以他做文字接龍的時候
對ChatGPT來說
他看到的是
你說
你是誰
ChetGBT說
然後這是他的輸入
這是他輸入的未完整的句子
然後他再繼續去做文字接龍
好 那既然需要人類老師教
那怎麼不一開始
就叫人類老師來教學就好了呢
我們就叫人類老師產生大量的資料
直接去教語言模型
何必還要像第一階段
使用網路上爬的資料來教語言模型呢
如果你今天只靠人類老師教學的話
會發生什麼事呢
人類老師他能夠提供的標注資料
是非常有限的
假設今天在訓練資料裡面
人類老師只告訴機器說
如果有人問你臺灣最高的山是哪座
你就要回答玉山
那你找出來的參數可能會有什麼問題呢?
不要忘了我們之前在講機器學習的時候
我講過說
你不要用人類的想法來揣度語言模型學習的過程
對語言模型來說
他唯一要做的事情
就是找到一組參數
滿足訓練資料的要求
在今天這個例子裡面
有一組參數是滿足訓練資料的要求的
什麼樣的參數呢?
今天假設有一組參數
他這組參數可以做到的事情
是讓韓式看到最這個字
輸出就是玉山
那它也是一個完全符合訓練資料的參數啊
這個時候
這樣子的參數你是有可能在訓練的時候被找出來的
如果訓練的過程、最佳化的過程
找出來的是這樣子的參數
那你問臺灣最高的山是哪座
語言模型可以正確的回答你
但你問世界最深的海溝在哪裡
看到最這個字
它很有可能也是回答玉山
過去我們實驗室其實也做了很多這種
只用人類老師的資料來訓練出來的語言模型
過去在還不知道有大型語言模型技術的時候
我們往往會收集非常多人與人間的對話
比如說收集到上百萬筆來訓練模型
但是我們發現說如果收集到上百萬筆資料
頂多讓語言模型可以說人話
他講出來的話文法可能是對的看起來是通順的
但這些模型仍然顯得非常的無知
很容易答非所問
因為就算是有上百萬筆人類
人與人間的對話
可以當作標註資料來訓練語言模型
對語言模型理解這個世界來說
還是遠遠不夠的
而上百萬筆資料收集起來
成本已經非常的高了
你可能很難收集到像網路上爬到的資料
有從整個網路上爬到的資料那麼多
所以只從人類老師學是不夠的
過去有很多模型如果他只靠人類老師教的資料的話
他就很笨
過去的語音助理啊你跟他說
打開音樂
他會把音樂打開
但你跟他講說
不要打開音樂
他也會把音樂打開
因為那個模型學到的東西很有可能就是
只要聽到打開音樂這個是
四個字不管前後加什麼
他都會把音樂打開
所以
今天這些大型語言模型
可以成功的關鍵
是使用了第一階段
所得到的
參數作為
第二階段的初始參數
我們前面有講到說
如果今天模型訓練
出來的參數是不合理的參數
有兩種解法一種是
擴增你的資料但今天
在第二階段人類標註的資料是非常稀少的
所以我們沒有辦法走這個路線
那怎麼辦呢?
我們可以找一組比較好的初始參數
那在第二階段可以用的初始參數是什麼呢?
我們可以把第一階段
透過網路上大量文字所學出來的這組參數
當作是第二階段的初始參數
我們在前一階段跟大家示範過的
有點笨笨的GPT-3
或者是PaLM
當作第二階段的初始參數
那在第二階段學出來的參數就會跟第一階段不會差太多
因為我們之前講過說初始參數意味著
最佳化的過程
尋找參數的時候是從這個地方找起的
所以現在第二階段參數
就不會和第一階段差太多
因為第二階段參數不會和第一階段差太多
所以這個步驟又叫做Fine-tune
他的中文就是微調
代表說第二階段
只是第一階段參數的微調而已
如果我們把有用人類標註的資料學習當作真正的學習
那透過任何資料
網路上爬到的資料學出來的這些學習的這個過程
我們叫做pre-train
叫做預訓練
第一階段又叫做pre-train
他中文翻譯成預訓練
那有的人可能會擔心說
會不會這個最佳化的過程最後找出來的參數還是跟初始參數很不一樣啊
如果你擔心這件事的話
你可以用一個小技巧
這個小技巧叫做Adapter
那大家常常聽到的LoRA呢
其實就是Adapter的一種
Adapter的概念是什麼呢
Adapter的概念是說
假設我們已經有初始參數了
那我們在做最佳化找我們要的參數的時候
這個初始參數就完全通通不要動它
那既然初始參數完全不要動它
那最佳化的過程在做什麼呢
在最佳化的時候
我們在原有的函式後面
再多加幾項未知數
但是這個數目
會比原來的未知數初始參數的數目
還要小非常多
然後最佳化的過程
只找尋這些新加進來的未知數
那這樣可以確保說
新的參數跟初始參數
它會是非常類似的
那這個就是Adapter的概念
那當然用Adapter的理由
不完全是希望參數
跟初始參數非常接近
還有其他的理由
比如說這樣可以
減少你需要的運算量
比如說在我們作業裡面
如果不用LoRa這個概念
讓模型去找全部的參數
你是會算太久的
這就是為什麼在作業裡面
我們會需要用到Lora
因為假設你只用免費版的Colab
那你一定要使用Adapter
你才有辦法真的做最佳化
因為對你來說要找非常大量的參數是不可能的
你只能夠找少量的參數
那Adapter呢
其實有非常多種
LoRA只是眾多可能的Adapter的其中一種
隨著你插入的參數的位置不同
隨著你額外插入的參數的數目不同
有各式各樣不同的Adapter
但是我們今天在這裡
就不再深入討論這個部分
好 那我們回到我們剛才講的東西
我們剛才說第二階段會成功的關鍵
是要用第一階段的參數作為初始參數
為什麼第一階段透過大量網路資料
Pretrend的時候所獲得的參數
會是一個好的初始參數呢
因為在Pretrain的時候
語言模型看過非常大量的資料
因為語言模型看過非常大量的資料
而在這些非常大量的資料上
他都可以成功的做文字接龍
代表說他今天學到的做文字接龍的規則
不是簡單的規則
他絕對不會學到那種很簡單的規則
看到最,後面就出現玉山
如果看到最,後面就出現玉山
那他就沒有辦法正確地接出
世界最高的山是聖母峰
他今天之所以多數的資料都可以
正確地做出文字接龍
就是因為他可能學到了
非常複雜的規則
他知道臺灣最高的山後面要接玉山
世界最高的山後面要接聖母峰
他學到的是複雜的規則
好,那pretrain的參數
有非常複雜的規則
拿它來做初始化參數以後
今天如果是一個非常奇怪的參數
沒有道理的參數
輸入是最輸出就是玉山的這種參數
因為跟初始化參數距離太遠
你在最佳化的過程找到的參數
就不會是這種比較沒道理的參數
而比較有可能是有道理的參數
那另外因為在Pretrain的參數裡面
非常複雜的規則
模型學到非常複雜的事情
那你今天在做最佳化之後
可能你的語言模型會有非常好的舉一反三的能力
你只要告訴他說
臺灣最高的山是哪座
問號後面接玉山
在做完這個最佳化之後
這組參數可能有可能很自然的也學到說
世界最高的山是哪座
問號應該接聖母峰
因為本來在pretrain的時候
模型就已經知道世界最高的山是聖母峰了
所以這就是Pretrain的妙用
那這個舉一反三的能力有多誇張呢
這邊舉一個誇張的舉一反三能力的故事給你聽
在這個GPT系列爆紅之前
有另外一個語言模型叫做Bird
那這個Bird有一個多語言的版本
有一個Multilingual的Bird
他在104種語言上面做過文字接龍
實際上Bert的學習方式不是文字接龍啦
但是這邊為了簡化這個說明起見
你就當作是文字接龍好了
那這個看過多種語言的語言模型
他舉一反三的能力有多神奇呢
你只要教他某一個語言的某一個任務
他就會自動學會其他語言的同樣的任務
舉例來說,你教他英文的閱讀能力測驗
他自動就會了中文的閱讀能力測驗
那這邊的實驗結果呢
是來自一篇19年的文章
是個上古時代的文章
這邊的實驗呢
是做在一個叫DRCD的COPUS上面
它是一個中文的閱讀能力理解測驗的資料集
那第一個實驗呢
是沒有做pre-trained
只在中文的閱讀能力理解測驗上面做fine-tune
只拿中文的人類標註的資料來訓練模型
那這個正確率並沒有很高
這是F1 score啦
不過你當作正確率來看就好
正確率78%
那如果有pre-trained
pre-trained真的非常有用
在中文的資料上面學習做文字接龍
用中文閱讀能力理解測驗的資料去做fine-tuning
那這個時候測試在中文的閱讀能力理解測驗上
當然跟fine-tune的時候看到考題是不一樣的
這個正確率有89%
那如果pre-trained的時候是看過104種語言
這個時候fine-tune的時候
教他英文的閱讀能力理解測驗
但是測驗他能力的時候
他能力的時候考他中文的閱讀能力理解測驗
對他來說中文的閱讀能力理解測驗是裸考
他從來沒有看過中文的考題
但這個時候他也有78%左右的正確率
跟直接學在中文上的正確率居然是差不多的
這顯示說有一個好的pre-trained的模型
今天在instruction fine tuning的時候
就可以有非常好的舉一反三的能力
到此啊
我們已經有了instruction fine tuning的概念
而fine tuning的路線在這裡
分成了兩條
第一條是
我們就打造一堆專才
每一個專才負責一個特定的任務
舉例來說我們教pretrain的模型做翻譯你就收集一大堆翻譯的資料
告訴他如果有人問你Good morning
那你就輸出早安
給他一堆翻譯的資料
那他之後就會做翻譯
你跟他說How are you
他就回答你好嗎
但他是一個翻譯的專才
他只會做翻譯
因為你也沒教他做其他的事情
那如果我們要的不是一個翻譯的專才
而是要一個編修的專才怎麼辦呢
那沒關係
你就另外收集編修的資料
告訴模型說
有人輸入who care
你就要回要加s
那收集夠多這種編修的資料以後
期待你就可以打造一個編修的專才
當你跟他說how are you的時候
他不會認為你要做翻譯
他也不會覺得你在跟他打招呼
他會說這句話沒有文法錯誤
但他只會做編修
他也不會做其他的事情
那BERT這個模型啊
他走的就是專才路線
那因為時間有限,我們今天就不會特別跟大家講BERT是怎麼成為專才的
你可以看一下過去機器學習這門課的錄影
瞭解BERT是怎麼透過fine tuning打造一堆專才出來
那過去講到BERT的時候呢
都會放這個實驗數據給大家看啦
這也是來自上古時代的文章,19年的文章
那裡面不同形狀的點代表不同的任務
所以這邊總共有124加3
這邊總共有9個不同的任務
那在這個圖上做的事情是打造了
以BERT為Pretrend模型打造了9個不同的磚材
那這邊每一條直線代表某一個Pretrend的模型
把這些pre-trained的模型,拿這些pre-trained的模型以它們為基礎
去打造出九個專才
然後你可以得到在這九個專才
在不同任務上面的正確率
你會發現說多數的模型呢
都是以BERT為基礎的
其實GPT也赫然在列
以前最早的時候
人們使用GPT的時候
也是拿它來打造一堆專才的系統
雖然說是打造一堆專才的系統
這些專才的系統可厲害了
這一條黑色的線啊
是人類在這九個任務上面平均的能力
而藍色的虛線
是這九個專才他們平均的能力
你會發現到後來
有一些以某一些模型為基礎
打造出來的專才平均的能力
甚至可以說是超過了人類
但後來人們有了更瘋狂的想法
打造專才有點麻煩
世界上有這麼多的任務
難道我們要為每一件事情都打造一個
專門做這件事的語言模型嗎
顯然太麻煩了
能不能直接打造一個通才
一勞永逸解決所有的問題
怎麼打造一個通才呢
我們就收集一大堆的標注資料
把所有你想得到要語言模型做的事情
通通交給他
比如說你想叫他做翻譯
你就跟他說
有人輸入翻譯以下句子
Good morning
你就說早安
有人說請把goodbye翻譯為英文
你就說再見
你也希望他做編修
你就跟他說
有人跟你說who care
這句話有文法錯誤嗎
你就說要加s
你希望他做摘要
那你就跟他說
有人說請把這篇文章做摘要冒號
後面放文章的內容
那你就要輸出這篇文章的摘要
把所有你想得到的任務
都取得標註資料
集合起來
交給語言模型
看看能不能打造一個通才
這個通才
不僅能回答你交過他的任務
期待他還有一些舉一反三的能力
沒看過的任務
比如說這些任務的變形
也能回答
也許在fine tuning的時候
只教過他翻譯
只教過他翻譯 只教過他摘要
但今天有人說 請把這篇文章做摘要並且翻譯
他也可以得到正確答案
所以這是路線二 打造一個通才
真的有可能打造一個通才嗎?
其實我們實驗室在19年的時候
也做過類似的嘗試
這個19年真的是個上古時代啊
那個時候連GPT第三代都還沒有出來
所以我們的Portrait Model那個時候只能用GPT-2
這篇論文原來它的名字叫做
Language Model Is All You Need
但是投稿的時候Reviewer不太喜歡這個名字
後來我們就把Is All You Need幾個字拿掉了
當時覺得Language Model Is All You Need這個說法
實在是太瘋狂、太瘋癲了
但今天回頭過來看
這個Language Model Is All You Need這句話說起來
看起來也不算是太過分
當時用的是GPT-2
因為那個時候GPT-3還不存在
那時候的想法是
也許我們沒有辦法一次收集到
非常多不同任務的資料
但也許我們可以
一個一個任務的去教語言模型
想到什麼任務就一直教他
有一天他就會變得非常的聰明
比如說先教他翻譯
翻譯了再教他編修
期待他就既會編修也會翻譯了
但當時遇到的問題是
讓模型連續學一大堆任務
他有可能會忘記舊的任務
所以這篇文章最大的貢獻是
設計了一個很有趣的方法
讓模型可以在腦中複習他學過的知識
至於實際上一個語言模型
怎麼複習他學過的知識
那我把論文連結放在這邊
留給你自己參考
那後來很多不同的團隊
也都紛紛往打造通財邁進
一個知名的例子是Google的
FlAN這個模型 FineTune Language Net
那他用這麼多各式各樣不同的任務
來訓練他的語言模型
來對他的語言模型做Instruction FineTuning
這是2021年的事
那後來過了一個月之後
你看21年的10月
Marketplace出了另外一個叫T0的模型
跟Flame做的事情也很像
也是收集了一大堆的任務
來對模型做fine tuning
希望可以打造一個通才
那Flame的結果如何呢?
在Flame的原始論文裡面
他把Flame跟GPT-3做比較
我們之前看到說GPT-3
在真的使用的時候
是如此的不受控
所以他的結果並不會太好
這邊這個黃色的Bar呢
指的是原始的GPT-3
紅色的Bar指的是GPT-3加InContext Learning
記得我們在之前的課程中已經講過InContext Learning
就是給模型一些範例
那藍色的Bar是Flame這個模型
它在大量的任務上面做過Instruction Fine Tuning
那在三個不同的任務上
這邊要強調一下這三個不同的任務
可是Flame訓練的時候沒有看過的新任務
在這三個新任務上
SAN的表現都超過了GPT-3
後來Google又更進一步
在2022年的10月
他們用更多的任務來訓練了語言模型
來教語言模型Instruction Fine Tuning
用了多少任務呢
用了1800個任務
注意一下這是1800個任務
不是1800筆資料
每一個任務是比如說翻譯叫做一個任務
摘要叫做一個任務
有1800個這樣的任務每一個任務裡面
都可能有上萬筆
資料
那這些非常大量的任務裡面又有大量的資料
去對模型做instruction fine tuning
結果怎麼樣呢
結果也很不錯
在這篇論文裡面發現說呢縱軸
是在模型從來沒有看過的任務上面的正確率
橫軸,這個圖的橫軸是模型的大小
你會發現不管是什麼樣大小的模型
隨著他instruction fine tuning的時候
訓練過的任務越多,顏色越深
代表訓練過越多的任務
隨著訓練過的任務越多
在沒看過的任務上面,結果就越好
那右邊這個圖
它的橫軸是訓練的任務的數目
那縱軸一樣是在沒有看過的任務上的正確率
三條線分別是8個billion的參數量的模型
62個billion參數量的模型
跟540billion參數量的模型
那這個圖告訴我們說
不管是什麼大小的模型
接受過人類的指導
做完instruction fine tuning以後
它的能力都會有顯著的上升
尤其是小模型透過人類指導以後
它的進步是尤其顯著的
那我們在上一堂課看到說
PALM 540個Billion的參數
如此巨大的模型
你問它問題的時候都不好好回答
它會反問你更多的問題
但是一旦做過Instruction fine tuning以後
它就穩定下來了
它就能夠好好回答你問題了
這是Google那邊的進展
我們來看OpenAI這邊的時間線
OpenAI在2022年的3月
他們發表了InstructGPT這篇論文
InstructGPT也就是對他們的GPT-3
做Instruction Fine Tuning
那我們來看一下實驗結果
這個縱軸
縱軸是人類使用這個模型的時候的喜好程度
他請人打分數
分數越高代表人類用這個模型的時候
覺得這個模型越棒
那今天這兩個GPT
這個是
原始的GPT-3
這個是GPT-3加上
In-Context Learning也給他一些範例
然後這一個呢
是有做過Instruction Fine Tuning的結果
所以非常明顯的
做完Instruction Fine Tuning以後
人類會覺得這個模型
比原來的GPT-3
還要棒得多
那紅色這個我們今天先不討論
RioHF以後的結果
那在這個IntrustGPT這篇Paper裡面
他們的發現是
他們的模型不只比原來的GPT好
也勝過了Google的FLAN
還有HackingFace的T0
在Instruct GPT這篇論文裡面
他們也針對這一點
討論了一下他們的看法
他們的看法是這樣子的
在原本FLAN的那篇Paper裡面
那這張圖呢
原始的FLAN paper裡面直接截出來的
他們產生資料的方式是這個樣子的
他們先去找一些自然語言處理的資料集
比如說在自然語言處理裡面
有一個常見的任務叫natural language inference
natural language inference這個任務就是
給機器一個前提
給他一句假設
然後叫他回答說
那這個假設跟這個前提呢
有沒有蘊含的關係
有還是沒有
那他們想辦法把這些資料改造成一問一答的資料
所以像這邊有Premise有前提有假設有Hypothesize
那他們就把它轉成一問一答的資料
反正他們就是把那些NLP資料集裡面的問題
用一些研究人員訂好的模板把它轉成一問一答的問題
這樣子做的缺點是什麼
這樣做的缺點是
這一些問題非常的死板
他們可能跟人類真實的使用情境
是差距甚遠的
那OpenAI早在2020年就上線了GPT-3
雖然GPT-3沒有開源
也就沒有把參數給每個人用
但他們建立了一個線上的服務
所以一直有人在使用GPT-3
所以他們有真實的使用者
他們知道真實的使用者會問什麼樣的問題
所以在他們的instruction fine-tuning的資料裡面
他們的問題是真實的使用者問的問題
然後他們再去找數據標記員
來把這些問題真正的答案標出來
所以OpenAI說根本不會有人問這樣子的問題
一般會問的問題可能是
會跟大型圓模型做下一些腦力激盪
給我五個讓我生涯重燃熱情的方法
或者是幫我寫個故事
那如果做摘要也不會很死板的說幫我做摘要
做摘要也有各種變形
比如說這邊是提供了一個摘要
然後要模型去產生這個摘要裡面的大綱
所以人類的問題是千變萬化
可能不容易直接把它用幾個模板就描述它
那OpenAI覺得就是因為它們有真實使用者的數據
這就是為什麼它們做出來的結果
會比Google的這個Flame
Google的Instruction Fine Tuning的結果還要更好
那在Instruct GPT 這篇 Paper裡面也揭示了一件事情
就是Instruction Fine Tuning是畫龍點睛
有的人可能覺得這個Instruction Fine Tuning
一定需要非常大量的資料
在這篇論文裡面告訴你說
Instruction fine tuning不需要大量的資料
他們只用了上萬筆的資料而已
比一般人想像的還要少很多
不只OpenAI告訴大家這件事
後面其實很多其他人也都有類似的發現
比如說Lama2
Lama是Meta所打造的一個語言模型
在LamaTube的論文裡面
他們說他們只用了兩萬七千筆的資料
那為什麼不用更多呢
他們在論文裡面寫說
他們可以輕易的找到上百萬筆的資料
但他們發現
兵貴金不貴多
有上百萬筆的資料
其實沒啥用
用好好準備的這兩萬多筆
就非常足夠了
所以他們這個章節還有一個有趣的小標題
就是Quality is all you need
後來有另外一個模型叫做Lima
他也是Meta的模型
他們更進一步發現他們說less is more for alignment
他們只用了這個作者群自己想的1000筆資料
來對他們的模型做instruction fine tuning
他們發現他們只用1000筆的資料
所訓練出來的Lima居然在43%的情況下
可以跟GPT-4打成平手或者是超過GPT-4
不過因為只有43%跟GPT-4打成平手超過GPT-4
所以它顯然是沒有超過GPT-4的
不過這個實驗室要告訴你說
它只用1000筆資料
就打造出了一個好像還不錯的模型
既然Intructional File Tuning不需要太多資料
那是不是我們自己每個人
也都可以做Intructional File Tuning呢?
答案是不行
因為你沒有高品質的Intructional File Tuning的資料
你不是OpenAI
你沒有一個線上的系統
所以根本不知道使用者會問什麼樣的問題
你沒有高品質的Instruction fine-tuning的資料
你沒有這些酷東西
所以怎麼辦呢?
有人就想到
那我們就以ChatGPT為師
也就是對ChatGPT做逆向工程
看看可不可以知道ChatGPT
到底用了什麼Instruction fine-tuning的資料
這個逆向工程是這樣做的
你可以讀一下Self-Intruct這篇paper
你可以先問ChatGPT說
想想看大型語言模型可以做什麼事
他就可以列出說
那大型語言模型可以幫你寫郵件
可以幫你寫摘要
可以幫你寫信約時間
然後接下來呢
就把這些任務再輸給大型語言模型
比如說跟他說
我現在想要做的任務是撰寫郵件
幫我想一些使用者在做這個任務的時候
可能會有的輸入
比如說邀老師來演講
邀老師來參加審查會議
提醒老師記得要繳交報告
好那你現在知道使用者可能是會問什麼了
但你沒有答案啊怎麼辦
一樣把使用者可能會問的問題
但這些是ChatGPT幻想出來的問題
把這些問題直接問ChatGPT
讓確GBT把答案伸出來給你
所以現在你就有輸入
你就有問題也有答案
你就可以自己做Instruction Fine Tuning了
那有同學可能會懷疑說
對確GPT做逆向工程
真的能夠找到高品質的
Instruction Fine Tuning的資料嗎?
那這邊引用另外一篇論文
這篇論文是告訴你說
從ChatGPT做逆向工程得到的資料
可能是沒有那麼的優質
不過,沒魚蝦也好
反正你現在手上沒有Instruction Fine Tuning的資料
能夠對ChatGPT做逆向工程
總比完全沒有資料要好
不過OpenAI顯然不怎麼喜歡你對它做逆向工程啦
它的使用條款裡面有一條是說
你不可以拿OpenAI
你不可以拿ChatGPT生出來的資料
再打造一個跟OpenAI有競爭關係的模型
不過我看是沒什麼人在理會這件事啦
除了那個大公司的團隊以外
我聽到的小團隊幾乎都是對ChatGPT做逆向工程取得
Introduction fine tuning的資料
唯一我目前聽到完全沒這麼做的
只有國科會的臺德團隊而已
他們的資料
他們的Introduction fine tuning的資料
是自己一把鼻涕一把眼淚應聲出來的
到目前為止,我們有了人類標註的
我們有了Instruction fine tuning的資料
不是人類標註的,是從ChatGPT那裡取得的
那我們可以開始做Instruction fine tuning了嗎?
還不行,因為你手上沒有那些pre-trained的參數啊
OpenAI有GPT-3,Google有PALM
但是你沒有pre-trained的參數
你也沒有這些酷東西
那怎麼辦呢
於是
這整個世界就卡住了
沒人可以打造
自己的大型語言模型
直到有一天
Meta釋出了喇嘛
Meta可能是因為在這個
大型語言模型的競賽
眼看趕不上OpenAI跟Google
心一橫直接翻桌
打算改變整個生態系
Meta釋出了
Meta釋出了他自己的大型原模型
你可以拿拉瑪作為你的初始參數
來打造自己的模型了
拉瑪的第一代是在去年的
2023年的2月釋出
第二代是在2023年的7月釋出
拉瑪第一代在2月底釋出之後
整個世界就突然被解放了
兩週之後
Stanford打造了Alpaca
Elpaca
Elpaca是怎麼打造的呢?
就是先從ChadGBT的API取得5萬筆左右的資料
然後再以Lama為pre-trained的模型
就訓練了Elpaca
再過了兩週
美國其他的另外一群學校聯合起來打造了Vicuna
Vicuna它是從一個分享人跟ChadGBT對話的網站
取得了7萬筆的資料
拿這些資料做instruction fine tuning
對喇嘛做instruction fine tuning
打造了Vikuna
一時之間只能說是
舊時王謝堂前宴
飛入尋常百姓家堂前宴
每個人都可以訓練自己的大型語言模型
右邊這個圖是一個飛入尋常百姓家的Llama
現在喇嘛的子子孫孫
已經開師善業到滿坑滿谷
於是有了 Llama 之後
人人都可以 FineTune 大型語言模型的時代就開始了
這個就是我們在作業五跟作業六要做的事情
你們也可以自己 FineTune Llama
打造自己的語言模型
