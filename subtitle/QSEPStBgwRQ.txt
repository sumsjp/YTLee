臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中 http://aintu.tw
方法之間呢 如果你很熟悉 machine learning 原則的話
不同方法間他們其實是非常像的
其實就算你沒有辦法把所有方法都學過 其實也是一法通，萬法通
machine learning 的方法是非常多的
我覺得你並不需要很汲汲營營把所有方法都覺得你應該要學過一遍
掌握幾個大原則其實你就可以了解大部分的方法
Support Vector Machine 是甚麼呢
Support Vector Machine 有兩個特色
第一個是他用了 Hinge Loss
等一下我們會講 Hinge Loss 是甚麼
另外一個是我覺得最厲害的地方，他有個 Kernel Trick
把 Hinge Loss 加上 Kernel Trick 就是 Support Vector Machine，就是SVM
先來講一下 Hinge Loss 是甚麼
回到我們在開學的時候講過的 Binary Classification 的問題
我們講過在理想上
Binary Classification 應該要怎麼做呢
理想上我們知道一個 Machine Learning 的 solution 往往就有三個 step
在 Binary Classification 裡面
第一個 step 是
我們定一個 function g(x)
這個 g(x) 裡面有另外一個 function f(x)
當 f(x) > 0 的時候
他的 output 就是 +1 代表某個 class
當 f(x) < 0 的時候，他的 output 就是 -1
代表另外一個 class
我們現在的 training data
我們是 supervised problem
所以每一筆 data 都有 label : y hat
我們現在 y hat 就用 +1 跟 -1 來表示
分別代表兩個不同的 class
之前在講 Logistic Regression 的時候
我知道我們在講 y hat 的時候
我們是用 1 跟 0 表示
但這邊要用 +1 和 -1 來表示比較順
你知道說用 +1 和 -1 來表示跟用 0 跟 1 來表示是講同一件事情
就只是朝三暮四的差別而已
但是這邊如果用 +1 和 -1 的話
等一下寫式子是會比較順，所以這邊用 +1 和 -1
只要記得他的差別就好
Loss Function 呢，理想上
一個最理想的 Loss Function 就是寫成像下面這個樣子
當今天這個 g(x) 的 output 跟 y hat 不一樣的時候
machine 就得到一個 loss
如果一樣的時候，就沒有 loss
所以一個很理想的 Loss Function 是
summation 我們所有的 training data
然後對每筆 training data 都帶進 g(x) 裡面
看他的 output 多少，+1 也可能 -1
接下來看他的 output 跟 y hat 是一樣的還是不一樣的
如果是不一樣的你就得到一個 loss 1 反之就是 0
這邊用 delta 加一個括號的意思是
裡面這件事情如果為真的話，delta function 的 output 就是 1
如果 g 不等於 y，delta 的 output 就是 1
所以我們現在的 loss 就變成 g 在 training set 上總共犯了幾次錯誤
我們當然希望他犯的錯誤是越小越好
但是在今天這個 task 裡面
你要做 optimization
你要用第三步找一個好的 function 這件事情變得有點困難
因為你的 loss 是不可微分的
這個東西是沒有辦法微分的
所以根本沒有辦法用 Gradient Descent 來解他
怎麼辦呢
我們把這個 delta 用另外一個 approximate 的 loss 來表示
直接 minimize 這個 delta 做不到
所以我們不直接 minimize 這個 delta
我們 minimize 另外一個 loss function
這邊寫成小寫的 L
至於這個 loss function 可以長甚麼樣子
你就可以自己隨便定了
這是我們的 g，裡面有個 f(x)
這個圖上的橫軸是 y hat 乘上 f(x)
因為這個 y n hat 可以是 +1，也可能是 -1
我們希望 y n hat 是 +1 的時候
f(x) 要越 positive 越好
y n hat 是 +1 的話 f(x) 要越大越好
y n hat 如果是 -1 的時候 f(x) 應該要是越負越好
整體說來你會希望 y n hat 乘上 f(x)
兩者相乘以後他的值越大越好
他們要是同號的，他們兩個要乘起來越大越好
所以今天原則就是
如果我們把縱軸當作是 loss 的話
原則就是越往右，y n hat 乘以 f(x) 越大
loss 就應該要越小
我們剛才講的理想狀況是
假設 y n hat 和 f(x) 他們是反向的
他們相乘以後，得到的值是負數的話
你得到的 loss 就是 1
反之如果他們是同向的 你的 loss 就是 0
這是理想的狀況 但這件事情是沒辦法微分
那我們現在做一個 approximation 我們把 delta 用 L 來取代
縱軸就是 L 的值
你可以選各種、各式各樣不同的 function 來當作 L 這個 function
舉例來說，我現在 loss 定法是
我希望當 y n hat = 1 的時候
f(x) 跟 1 越接近越好
y n hat = -1 的時候
f(x) 跟 -1 越接近越好
這個是 square loss
square loss 其實可以寫成這個樣子
square loss 這個 loss 可以寫成 ( y n hat * f(x^n) -1 )^2
為甚麼呢 如果 y n hat 帶 1 的話
這個 function 就變成 (f(x^n) - 1 )^2
如果 y n hat 帶 -1 的話
這個 function 就變成 ( -f (x^n) - 1 )^2
( -f (x^n) - 1 )^2 可以寫成
因為他在平方裡面所以可以把負號拿掉
(f (x^n) + 1) ^ 2 也就是 f (x^n) 減 -1 的平方
當你寫這個式子的時候
就意味著如果 y n hat = 1 的時候
f(x^n) 要跟 1 越接近越好
y n hat = -1 的時候 f(x^n) 要跟 -1 越接近越好
square loss 的 function 你把 function 畫出來
他對 y n hat * f(x) 的變化是寫成這樣子
但是這個東西是不合理的
我們一開始在講 Binary Classification 的時候
我們就講過你用 square loss 是不合理的
從這個圖上你又可以更明確地看出他的不合理性
我們不希望說 y n hat 跟 f(x) 乘起來很大的時候
居然有一個很大的 loss
那另外一個是 sigmoid + square loss
這邊我們 sigmoid function 就用一個 sigma 來表示
我們希望 y n hat = 1 的時候
sigma(f(x^n) 趨近於 1
等於 -1 的時候
sigma(f(x^n) 趨近於 0
這個式子可以寫成這個樣子
為甚麼呢 如果你把 y n hat 帶 1 的話
那沒有問題 很直覺的就是希望這個 output 跟 1 越接近越好
帶 -1 的話呢
變成 sigma ( f ( sigma (-f(x)) )) 減 1 的平方
sigma ( -f (x)) 是甚麼呢
如果你了解 sigmoid function 特性的話
你把裡面的值取一個負號
其實就是 1 減 sigma ( f(x))
我想這個你們想一下的話應該可以體會 減 1 的平方
這個 1 可以消掉
所以就變成 sigma (f(x))^2 的平方
總之你寫這個式子的意思
就是希望 f(x) 通過 sigma function 以後
如果他的答案是 1 他就要接近 1
如果答案是 -1 他就要接近 0
這個式子寫出來是藍色這條線
那我們之前有講過說
其實你在做 Logistic Regression 的時候
你不會用 square loss 當作你的 loss
因為這樣 performance 不好 之前也有實際操作過
你不會用 square loss 其實你會用 cross entropy
那如果用 cross entropy 的話
在 Logistic Regression 其實我們真正是用 cross entropy
然後 entropy 的意思我們之前講過
就是你的 sigma(f(x))
代表了一個 distribution
你的 ground truth 是另外一個 distribution
這樣的 distribution 之間的 cross entropy
就是你要去 minimize 的 loss
如果你回頭過去把 square loss 的 function 寫出來的話
你會發現其實這個 function 可以寫成
log ( 1 + exp( - y n hat * f(x)))
這邊我們就不做多餘的推倒 這個比較麻煩
你可能自己回去推一下就知道
但這個式子是不是合理的呢
他其實是合理的
你想想看當你今天的 y n hat 乘上 f(x^n) 趨近無窮大的時候
當他趨近無窮大的時候 exponential 負的無窮大是 0
那就變成 log (1+0) 得到結果是 0
所以當這一項趨近無窮大的時候
這一個 sigmoid + cross entropy 這個 loss function
他是綠色這一條，他會趨近於 0
如果今天 y n hat * f(x) 負很大的時候
exponential 裡面會是正的很大再加上 1 他的值會很大
再取 log 以後他還是無窮大
log 只能跟 exponential 消掉，如果 exponential 這裡面是無窮大的
得到結果還是無窮大 所以你得到的值還是無窮大
所以這一項是這一條線
在這邊我特別偷偷把他除了一個 log 2
你對 loss function 除一個 constant
不會影響你最後找出來的解，對不對
這邊偷偷除了一個 log 2，為甚麼呢
因為我們要偷偷除 log 2 以後呢
可以讓他變成一個剛才 ideal loss 的 upper bound
我們雖然沒有辦法 minimize ideal 的 loss
黑色這條線
但是我們可以去 minimize 他的 upper bound
就可以順便 minimize 這個 ideal 的 loss function
我們可以比較一下這兩條曲線
你就可以比較了解為甚麼我們之前會選擇用 cross entropy
而不是用 square error 來當作我們的 loss function
在我們做 Logistic Regression 的時候
你想想看，我們今天如果把 y n hat * f(x) 從 -2 移到 -1 的時候
如果是 sigmoid + square error 他的變化很小
如果是 sigmoid + cross entropy 他的變化就非常大
那所以對 sigmoid 這個 function 來說他在這種極端的 case
在你的值非常 negative 的時候
照理說應該要有很大的 gradient
應該趕快調整你的值
但是實際上不是如此
因為在值很大的時候
當你在這一項 y n hat * f(x) 非常 negative 的時候
你調整你的值對你最後 total loss 影響不大
所以就會變成說
對這個 sigmoid + square loss 的 case 來說
就算調整了他 negative 的值
他也沒有辦法得到太多的回報
所以他就會不想調整那些非常 negative 的值
那對這個 cross entropy 來說
他的努力是可以得到回報的
所以他就會很樂意把原來很 negative 的值
把他往正的地方推
所以我們用 cross entropy 的時候 在實作上
你會比 square error 還更容易 training
所以我們在做 Logistic Regression 的時候都是用 cross entropy
那 hinge loss 他寫成一個很特別的式子
他說我們的 loss function 是 maximum
其實我們剛才在講 Zero-shot learning 的時候
其實有看到類似的 function
我們這邊寫成 max(0,1 - y n hat * f(x))
如果 y n hat 帶 1
loss function 就是 max 0 跟 1 - f(x)
什麼樣的狀況下會有 zero 的 loss 呢
因為這邊是 max 0 跟 1 - f(x)
所以這個 function 最小值是 0
甚麼時候會是達到 loss function 是 0 最完美的狀況呢
只要 1 - f(x) < 0 就行
1 - f(x) < 0 的時候這個 loss 的值就會是 0
也就是說只要 f(x) > 1 的時候
這個 loss 就會是 0
如果 y n hat = -1 的時候
這個 loss function 會寫成 0 跟 1 + f(x) 的 max
要讓 loss 等於 0
我們就是要讓 1 + f(x) < 0
也就是要讓 f(x) < 1
所以用 hinge loss 做 training 的時候
你想要讓 machine 做到的事情 甚麼時候 machine 會覺得他已經
他的 loss 是 0 他已經做到完美的 case 了呢
他如果對一個 positive 的 example 來說
f(x) > 1 的時候就是完美的 case
對一個 negative example 來說
他的 f(x) < -1 的時候他就是一個完美的 case
他的值不用太大
你把 f(x) 變成 2
loss 也不會比較小
你把 f(x) 變成 -2 loss 也不會比較小
他只要大過 1 跟小於 -1 就可以了
如果你把 hinge loss 畫出來的話呢他長得像這個樣子
他是紫色的這條線
你會發現在紫色的這條線上
在這一段只要 y n hat * f(x) > 1 的時候
就已經夠好了，你的 loss 就已經是 0
再更大都沒有幫助
但如果 y n hat, f(x^n) 是 positive 的 example
是 positive 還不夠好
如果 y n hat 跟 f(x^n) 是同向
machine 在做 classification 的時候他已經可以得到正確的答案
根據我們剛才前面定出來的 step 1、定出來的 function
但是對 hinge loss 來說這樣還不夠好
他會說你只得到正確的答案還不夠
你要比正確的答案還要好過一段距離
這個距離就是 margin
也就是說當 y n hat * f(x) 還沒有大於 1 的時候
其實還是會有 penalty
促使 machine 讓 y n hat 乘上 f(x) 大於 0
你可能會問這邊為甚麼是 1 呢
你可以想想看如果這邊是 1 的話
hinge loss 才會是 ideal loss 的一個 type 的 upper bound
如果你用其他值的話
他就不是一個那麼 tight 的 upper bound
所以 hinge loss 跟我們剛看到的 cross entropy 一樣
他也是 ideal loss function 的 upper bound
所以我們會期待說我們可以 minimize hinge loss
然後就可能可以得到 minimize ideal loss function 的效果
如果我們比較 hinge loss 跟 cross entropy 的話
你會發現他們最大的不同來自於他們對待已經做得好的 example 的態度
如果我們把 y n hat * f(x) 從1 挪到 2
對 cross entropy 來說可以得到 loss 的下降
所以 cross entropy 他會想要好，還可以更好
如果你已經可以把 y n hat * f(x) 的值做的夠大
cross entropy 他有動機讓他的值更大，因為這樣可以減少 loss
但是對 hinge loss 來說，如果採用 hinge loss function
他是一個及格就好的 loss function
所以只要你的值大過 margin 的時候就結束了
他就不會想要再做得更好
你可能會問說在實作上到底 hinge loss 跟 cross entropy 他們的 performance 有甚麼差別呢
在實作上的差別可能沒有你想像的那麼顯著
有一些時候我們可以看到 hinge loss 略勝 cross entropy
但其實也沒有贏那麼多
非常 hinge loss 的時候有一個好處是他比較不害怕 outlier
learn 出來的結果會是比較 robust 的
這個等下在講 kernel 的時候會比較明顯的看到這個結果
如果你看這 hinge loss 跟 cross entropy 的差別的話
這個 cross entropy 就好像是說現在期末考有很多科目要讀
但你就是想要拚死讀一科而已
你想要把某一科考到一百分
其他如果你念不起來的你就會放棄
最後可能就被二一了
hinge loss 會顧全所有的科目
所有的科目考到及格就好
他其實是會 all pass 的
所以如果你有 outlier 的時候
hinge loss 會給你比較好的結果 cross entropy 會給你比較差的結果
等一下我們在別的角度來看這個問題可能是會更清楚
甚麼是 linear SVM
linear SVM 是說我們現在的 function
就是 linear 的
我們現在的 f(x) 就是
x 裡面的每一個 feature，x i 乘上他對應的 weight ，w i
summation 起來再加上 b
我們可以把這件事情看作兩個 vector 的 inner product
其中一個 vector 是 model 的 parameter w 跟 b concatenate 的一個 vector
另外一個 vector 是 feature 的 vector x 跟 1 concatenate 的結果
如果你把這兩個 vector 做 inner product 的話
那你就會得到左邊這個式子
所以在等一下的說明裡面
我們把 w 跟 b 串起來的那個 vector
直接就用 w 來表示
這個東西是你的 model 的參數 是要透過 training data 把它找出來的
下面 x 跟 1 concatenate 起來的結果
我們就當作是一個新的 feature 你就想像成每一個 feature 下面都有 concatenate 1
這樣你就可以把 bias 這一項省略掉
所以一個 function 你就寫成 w 的 transpose 乘上 x
或 w 跟 x 的 inner product
那在 SVM 裡面
你的 f 長這個樣子
你就說 f > 0 的時候是屬於某一個 class
f < 0 的時候是另外一個 class
如果是 loss function 的話
在 SVM 裡面它的特色就是他採用了 hinge loss 這個 loss function
通常你還會加上 regularization 的 term
這個 loss function 他是一個 convex 的 function
為甚麼呢?因為我們知道 hinge loss 的 loss function
他長得是這個樣子，就像 RELU 那個 activation function 的形狀
他是一個 convex 的 function
而這一項，L2 的 regularization
他也是一個 convex 的 function
所以所有的 loss 都是 convex 的 function
regularization 也是 convex 的 function
當你把這些 convex 的 function 疊加起來以後會發生甚麼事情呢
其實你可以很輕易的證明，把 convex 的 function 疊加起來
仍然是 convex function
所以今天的 loss function 其實就是一個 convex 的 function
如果是 convex function 的話
做 gradient descent 就很簡單了
不管從哪個地方做 initialization
最後找出來的結果都會是一樣的
你可能會問說這個東西在某些點不可微
他有這種菱菱角角的東西
Hinge Loss 他在某些點是不可微的
你把很多不可微的 convex function 疊起來他就長這樣
他有很多菱菱角角的地方
感覺不是每一個位子都是可微
但是這個不是甚麼大問題 這個其實可以做的
我們之前在講 Deep Learning 的時候，你有 RELU 的 Activation Function
你有 Maxout Network 他們表面上看起來也是不可微的
但其實你都可以用 Gradient Descent 去做 optimization
所以今天這個 case 也一樣
我們可以用 Gradient Descent 去做 optimization
等一下我們會看看怎麼做這一件事情
其實如果我們比較 Logistic Regression
跟 Linear SVM 的差別，他唯一的差別就只是
我們怎麼定 Loss Function
你用 Hinge Loss 就是 Linear SVM
Cross Entropy 的 Loss 就是 Logistic Regression
而這個 function 他沒有一定要是 linear 的
他如果是 linear 的話有很多好的特質 但是如果他不是 linear 的也 ok
你也可以用 Gradient Descent 來 train
所以 SVM 是可以有 deep 的版本的
這邊列一個 reference 給大家參考
當你今天在做 Deep Learning 的時候
你不是用 Cross Entropy 當作你 minimization 的對象 當作你的 Loss Function
而是想要用 Hinge Loss 的話
你其實就是有一個 deep 版本的 SVM
所以其實沒有必要說
哦～我做的是 Deep Learning、我做的是 SVM
他們其實可以是一樣的東西
這背後的精神其實都是可以相通的
再來的問題就是怎麼用 Gradient Descent 來 learn SVM 呢
我知道你傳統上聽到的方法都不是用 Gradient Descent 來 learn SVM
但 SVM 確實是可以用 Gradient Descent 來做 training 的
有一個用 Gradient Descent 來 train SVM 的方法叫做 Picasso
我這邊忘了附 reference 我之後再附上去
總之 SVM 是可以用 Gradient Descent train
怎麼 train 呢 我們現在的 Loss Function 長這個樣子
他是一個 Hinge Loss
Gradient Descent 很簡單你只要能夠對他做微分就好了
我們只要能夠對 model 裡面的某一個 weight w i
對這個 summation 裡面的 loss 可以做偏微分
我們就可以做 Gradient Descent
這個偏微分的值是甚麼呢
w i 只跟 f(x) 有關
所以我們可以把這一項拆成 partial f 分之 partial log Loss Function
我們先用 f 對 Loss Function 做偏微分
再乘上用 w 對 f 做偏微分
用 w 對 f 做偏微分的結果是很簡單的
因為我們知道 f(x^n) 就是一個 linear 的 function
他是兩個 vector 的 Inner Product
如果你用 w i 對 f 做偏微分的話
你得到的其實就是 xn 的第 i 個 dimension 的 value
前面這個用 f 對 Loss Function 做偏微分他的解是怎樣呢
這個 f 就是這個 Hinge Loss 的 Loss Function
這個 Hinge Loss 的 Loss Function 他長得是 RELU 這個樣子
他有兩個 operation 的 region
它可以 operate 在 0 是 max 的 case
它可以 operate 在 1 - y * f(x) 是 max 的 case
如果它 operate 在 0 是 max 的 case 就是這個 region
它 operate 在 1 - y * f(x) 是 max 的 case 它就是這個 region
甚麼時候會 operate 在哪一個 case 呢
這個是 depend 你現在的 model w 是多少
也就是說假如你現在的 1 - y hat * f(x) > 0
這個 f(x) 的值取決於你現在的 model w 的值是多少
假設這個東西大於零
那就是 y hat * f(x) < 1 這樣
當 y hat * f(x) < 1 的時候
你的 model 作用在這個 region
把它對 f 作微分你得到的值就是 - y n hat
在另外一個 case 因為值就是 0 所以做微分以後還是 0
今天的微分值就有兩個可能
如果作用在這個 region 就得到這個微分值
所以微分的值你把這一整項
大 L 對 w 做偏微分以後
你得到的值就是這個樣子
summation over 所有的 Training Data
再看每一筆 Training Data 的 y n hat * f(x^n) 是不是小於 1
如果是的話這一項 delta 值就是 1
這個 1 就會乘上負的 y n hat
這前面還會乘上一項
我寫錯這邊應該是上標 n
這邊這一項就應該是 x 上標 n 下標 i
前面這一項 depend on 現在的參數是甚麼
所以我們把它寫成 c n(w)
所以 update 參數的時候就把 w i 減掉 Learning Rate
乘上 c n(w) 乘上第 n 個 feature 裡面的的第 i 維
就可以 update 參數 w i
就這樣子
所以 SVM 是可以用 Gradient Descent 來解
如果你只是要寫 linear 的 SVM
其實你用 Keras 就可以秒做
你可能會說這跟我平常看到的 SVM 不一樣
我知道你平常看到的 SVM 是甚麼樣子
我把我現在講的這個 Hinge Loss Function 變成你平常看到的 SVM
你平常看到的 SVM 是怎麼樣呢
我們把 Hinge Loss 換成用一個 notation epsilon n 來取代它
epsilon n 就等於
max(0,1 - y n hat * f(x n))
上標 n
max(0,1 - y n hat * f(x n))
我接下來甚麼都沒有做只是換了一下 notation 而已
我們現在的目標是 minimize Total Loss
這一項其實可以有另外一個寫法
我們要 0 跟 1 - y n hat * f(x) 裡面取大的那一個當作 epsilon n
所以 epsilon n 它會大於 0 也大於 1 - y n hat * f(x)
這件事情其實就等於 epsilon n >= 0
epsilon n >= 1 - y n hat * f(x)
假設我們不考慮 loss，無視上面這一塊
這個紅框框，上面紅框框裡面的式子
等於下面裡面紅框框的式子嗎
給大家三秒鐘的時間想一下
覺得相等的同學舉手
手放下，覺得不相等的同學舉手
手放下，好像差距有點大
我們來想想看
這個 epsilon n 它是 0 跟 1 - y n hat * f(x) 的 max
我今天說 epsilon n 可以大於 0 也可以大於它
但是它不見得是正好等於他們的 max
它可以是 max + 1、+ 2、+ 一百萬
所以如果我這樣講的話
我們再問一下 你覺得上面下面這兩個式子是一樣的嗎
你覺得它不一樣的同學舉手
謝謝手放下，其實人沒有變多
其實他們是不一樣的
如果我們無視上面這個式子的話
這兩個式子是不一樣的
這兩個 epsilon n
一個符合這上面式子的 epsilon n 跟符合下面這兩個 constrain 的 epsilon n
不同的 epsilon n 他們是不同的值
上面的式子跟下面的式子好像是不一樣的
但是這個只是表象上
假設我們不考慮這個 Loss Function 的話
上下這兩個疑似是不一樣
我們這邊整理一下式子
把 y n hat * f(x) 移到左邊
epsilon 移到右邊變成 y n hat * f(x) >= 1 - epsilon n
所以上面這個式子跟下面這兩個是不一樣的
但是今天重點就是今天加了 minimize Loss Function L 這件事情以後
上面這個式子跟下面這個式子就會變得一模一樣
為甚麼呢
因為現在我們要去 minimize L(f)
你要選擇一個最小的 epsilon n
讓你的 L 能夠最小
雖然我們下面只有下 constrain 說
epsilon n 要大於等於 0
epsilon n 要大於等於 1- y n hat * f(x)
理論上它不需要正好等於 0
它不需要正好等於 1- y n hat * f(x)
它可以是任何值
或者說我舉個最簡單的例子，epsilon n 我帶一兆
帶一個無窮大，它就符合這個 constrain
epsilon n 隨便帶一個很大的值
它大於 0，也大於它就符合這個 constrain
它不需要等於他們之間兩個比較大的那個
理論上這兩個東西是不相等的
epsilon n 帶任何一個很大的值
就符合下面這個 constrain
但問題是我們現在要做的事情
是要去 minimize L
當我們要做的事情是要 minimize L 的時候
我們就要想辦法讓 epsilon n 越小越好
當 epsilon n 它的 constrain 要 >= 0 跟大於 1 - y n hat * f(x)
要他最小的辦法就是讓 epsilon n 等於他們最大的
所以加上我們的目標要 minimize total loss 的時候
上面這個紅框框的式子會跟下面這個紅框框的式子 是一樣的
當我們把上面紅框框的式子轉成下面紅框框的式子
這個就是你所熟悉的 SVM 了
你所熟悉的 SVM 就是告訴我們 y n hat * f(x)
y n hat 和 f(x) 要是同號的
他們相乘以後要 >= 一個 margin 1
但是這個 margin 是 soft 的，因為 soft 是軟的
有時候你沒辦法滿足這個 margin
沒有辦法讓 y n hat * f(x) >= 1
那怎麼辦呢
你把你的 margin 稍微放寬，把它減掉一個 epsilon n
這 epsilon n 會放寬你的 margin
這個 epsilon n 叫做 slack variable
slack 大概是鬆弛的意思
讓你的 margin 變得比較鬆
但是這個鬆弛的 epsilon 絕對不能夠是負的
因為這負的不符合它的目的
如果 epsilon n 是負的話你就是把 margin 變大而不是把 margin 變小
所以這個 epsilon n 必須要有一個 constrain，要大於等於 0
那把這些事情合起來以後
你會有一個你要 minimize 的對象
再加上一些 constrain
這個 formulation 是一個 Quadratic Programming 的 problem
你就可以帶一個 Quadratic Programming 的 solver
然後把它解出來
你不見得要帶 Quadratic Programming 的 solver 才能解它
剛才看到說 SVM 其實可以用 Gradient Descent 來解
這個是這樣子的，Kernel Method
你隨便 google 就有一大堆東西
在這一整套東西裡面
我認為最重要的、大家最容易卡住的地方
只要先說服你這樣一件事情
要說服你說，實際上我們找出來的 weight
我們實際找出可以 minimize Loss Function 的 weight
我們寫作 w hat
它其實是我們 data 的 linear combination
w star 其實是 summation over 所有 training data 的 vector point，x^n
然後對所有的 x^n 都乘上一個 weight
alpha(上標 *, 下標 n)
也就是說你找出來的 model 其實就是你的 data point 的 Linear Combination
一般說服你這件事情的方法都是做
用 Lagrange multiplier 解一下剛才的那個式子然後說服你這件事情
我們這邊試著從另外一個角度來說服你
我們剛才說我們可以用 Gradient Descent 來 minimize SVM
所以我們算出來的 Gradient Descent 的式子就是長這個樣子
這個是對 w i update 的時候的式子
對 w 1 到 w k，假設現在 w 有 k 維
update 的式子都是一樣的
唯一不一樣的地方只是你會換最後乘上去的 value
在 update 第 1 維的時候，你乘上去的 value 就是 x n 的第 1 維
在 update 第 i 維的時候，你乘上去的 value 就是 x n 的第 i 維
在 update 第 k 維的時候，你乘上去的 value 就是 x n 的第 k 維
把他們全部合起來
把這邊 w 1 到 w k 串成一個 vector
這邊 x1(上標 n) 到 xk(上標 n) 也串成一個 vector
你得到的結果就是每次你 update w 的時候
你都是把 w 減掉 eta 乘上 summation over x n
乘上一個 weight
這意味著假設我們 initialize 的時候 w 是一個 zero vector
你每次在 update w 的時候
你都是加上 data points 的 Linear Combination
最後得到的 solution
用 Gradient Descent 解出來的 w
得到的結果就是 w 的 Linear Combination
這個 c^n(w) 是甚麼呢
這個 c^n(w) 是 f 對 Loss Function 的偏微分
如果我們用 Hinge Loss
Hinge Loss 他像 RELU 它有兩個 operation 的 region
如果他作用在 max = 0 的 region 的話
那它的這一項就會是 0
所以當你在用 Hinge Loss 的時候你的這一項往往都是 0
也就是不是所有的 x^n 都會被拿來加到 w 裡面去
所以最後解出來的 w*
它的 Linear Combination 的 weight 可能會是 sparse
Linear Combination 的 weight 是 sparse 的意思是
可能有很多的 data point 它對應的 alpha* 值等於 0
那些 alpha* 值不等於 0 的 x^n
它就是 support vector
如果 alpha = 0 就一點作用都沒有
對這個 model 完全沒有影響力
alpha 是 non-zero 的你才會決定
最後的 model、parameter 長甚麼樣子
而這些可以決定 parameter 長甚麼樣子的 data point
他們就叫做 support vector 所以叫做 Support Vector Machine
在 data point 裡面不是所有的點都會被選作 support vector
只有少數的點、data point 會被選作 support vector
所以 SVM 相較於其他方法可能是比較 robust 的
如果 Loss Function 選的是 Cross Entropy
在做 Logistic Regression 選的是 Cross Entropy，就沒有 sparse 的特性
如果你看 Cross Entropy 的 Loss Function
它在每一個地方微分都是不等於 0 的
他沒有微分等於 0 的地方，微分都是不等於 0 的
所以解出來的 alpha 就不會是 sparse
如果用 Hinge Loss，解出來的 alpha 就會是 sparse
解出來的 alpha 是 sparse 的意思是
那些不是 support vector 的 data point
把它從 data base 裡面 remove 掉，對最後的結果一點影響都沒有
如果有一個奇怪的 outlier，只要不要把它選做 support vector
對你最後 train 出來的 model 就不會有任何影響
而不像其他的方法 如果用 Logistic Regression
每一個 data 都 count
每一筆 data 對你最後的結果都會造成影響
今天我們把 w 寫成是 data point 的 Linear Combination
最大的好處就是我們可以使用等一下說的 kernel 的 trick
這個想法是這樣
我們已經知道 w 就是 data point 的 Linear Combination
本來我們的 w 可以寫成這樣
這個 w 可以把它寫成 我們把所有 data point x^1 到 x^N 排起來
排成一個 matrix X
然後 alpha 就是一個 vector 裡面的值是 alpha^1、alpha^2 到 alpha^N
我們把這個 matrix 乘上這個 vector
就會得到 X column 的 Linear Combination
也就是 matrix X 乘上 vector alpha
w 可以寫成 matrix X 乘上 vector alpha
當我們知道 w 可以這麼寫以後
可以做甚麼呢
我們可以改一下 function 的樣子
本來的 function 是寫成 (w^T) * x
現在已經知道 w = X * alpha
所以 f(x) = (alpha^T) * (X^T) * x
x 是一個 vector
X 的 transpose 是一堆 row 疊在一起
alpha 的 transpose 是一個倒下來的 vector
把這個 vector 乘上這個 matrix 再乘上這個 vector 得到的是一個 scalar
把這個 vector x 乘上這個 X 的 transpose 以後
得到的是一個 vector 得到的結果是甚麼呢
得到的結果是第一個 dimension 就是 x^1 跟 x 的 inner product
第二個 dimension 就是 x^2 跟 x 的 inner product
最後一個 dimension 就是 x^N 跟 x 的 inner product
接下來把這個 vector 跟這個 vector 在做 inner product 以後
得到的結果就是 f(x) 等於 summation over alpha N
乘上 x^N 跟 x 的 inner product
f(x) 怎麼算
把 x 帶進來，它跟 data base 裡面的每一個 x^N
都乘上 inner product 以後
再把 inner product 的結果用 alpha N 做 weighted sum 就是你的 f(x)
你可能會擔心 database 每一個 x^N 算一遍 inner product 會不會很費事
其實還好，因為假如用 Hinge Loss，alpha 是 sparse 的
所以只要考慮那些 alpha 不等於 0 的 vector 就好
在等一下會把 x^N 跟 x 做 inner product 這件事情
寫成一個 function
寫一個 function k(x^n, x)
k(x^n, x) 就是 x^N 跟 x 的 inner product
這個 function 叫做 Kernel Function
今天已經知道 step 1 就是把 x^N 跟 x 帶進 Kernel Function
再乘上 alpha^N 再 summation 以後的結果
在 step 2 跟 step 3 呢 我們要 maximize 的對象變成甚麼呢
我們的 model 寫成這個樣子
不知道的東西其實變成是 alpha^N
inner product 裡面沒有參數你本來就知道的
你不知道的東西是 alpha^N
在 step 2 跟 step 3 的問題就變成
要找一組最好的 alpha^N
它可以讓我們的 total loss 最小
這個最好的 alpha^N 可以長甚麼樣子呢
Loss Function 就寫成 summation over 每一筆 data 的
小 L 的 Loss Function
小 L 的 Loss Function 它吃兩個 input
一個是 f(x^n)，一個是 y hat of n
f(x^n) 就是 step 1 的這個 function
可以把這個 function 帶進來
他寫起來就是這個樣子
f(x^n) 就是下面這一項
因為 summation over n 前面已經用過了
所以這邊 summation over n'
但是都是 summation over 所有的 training data
觀察投影片上的所有式子你會發現
我們不再需要真的知道 x 的 vector 是多少
真正需要知道的其實只有 x 跟另外一個 vector z 他們之間的 inner product 值
或者是只要知道 Kernel Function 就可以做所有的 optimization
今天只需要知道 K(x^n', x^n) 的 value 是甚麼
只需要知道 K(x^n, x) 的 value 是甚麼
並不需要真的去知道 x^n 跟 x 他的 vector 長甚麼樣子
我只要能夠算個出這一項、這一項其實就結束了
等一下會看到這招可以給我們帶來一些好處
這招就叫 Kernel Trick
Kernel Trick 不是只能用在 SVM 裡面
如果你回過頭去看 w 等於 data 的 Linear Combination 這件事情
其實不是只有 SVM 適用，Logistic Regression 也可以用同樣的方法
所以你也可以有 Kernel based 的 Logistic Regression
Linear Regression 也可以用同樣的方法
所以你也可以有 Kernel based 的 Regression
這些都是可以的，只是這些都不限在 SVM 上面
Kernel Trick 怎麼用呢 Kernel Trick 是這樣
我們之前有說過如果是 Linear 的 model 他有很多的限制
可能要對 input 的 feature 做一個 Feature Transform
他才能用 Linear model 來處理
如果在 Neural Network 裡面就好幾個 Hidden Layer 來做 Feature Transform
假設有一筆 data
他是二維的 x1, x2 想要先對他做 Feature Transform
在 Feature Transform 上面再去 apply Linear SVM
在 Feature Transform 以後再去 apply Linear Model
假設 Feature Transform 的結果是我們把 phi(x)
變成 (x1)^2, 更號2 * x1 * x2 跟 (x2)^2
想要考慮 feature 和 feature 之間，也就是 x1 和 x2 之間的關係
如果要算 K(x, z) 的時候
想要算 x 跟 z 的 Kernel Function
也就是 x 跟 z 做完 Feature Transform 以後，做 inner product 的值
可以怎麼做呢 最簡單的方法就是我把 x 跟 z
都帶到這個 Feature Transform 的 function 裡面
把他們變成新的 feature
變成新的 feature 就可以直接做 Inner Product
算出來的結果就是這樣 國中生都會算的東西
可以把這一項做一下轉化
這一項是 (x1)^2 * (z1)^2 + 2(x1)(x2)(z1)(z2) + (x2)^2 * (z2)^2
它可以被簡化成 ((x1)(z1) + (x2)(z2))^2
((x1)(z1) + (x2)(z2))^2 是甚麼
((x1)(z1) + (x2)(z2))^2 正好就是 [x1, x2], [z1, z2] 這兩個 vector 的 Inner Product
所以說我們把 x 跟 z
做 Feature Transform 再做 Inner Product
等同於他們在原來 Feature Transform 之前的 space 上面
先做 Inner Product 以後再平方
那這招有時候可以給我們帶來好處
因為有時候直接計算這個結果
直接計算 x 跟 z 帶進 Kernel Function 以後的 output
會比先做 Feature Transform 再做 Inner Product 還要更快速
舉例來說，假設現在要做的事情是
我的 x 跟 z 都不是 2 維，是高維，是 k 維
想要把她投影到一個更高維的平面
這更高維裡面，會考慮所有 feature 兩兩之間的關係
假設原來有 k 維
在更高維的平面就至少是 (C k 取 2) 維
要考慮所有 feature 之間兩兩關係
所以 phi(x) 就是 (x1)^2 到 (xk)^2
√2(x1)(x2), √2(x1)(x3), √2(x2)(x3) 以此類推
如果用 Kernel Trick 的話可以輕易的把
phi(x) 跟 phi(z) 的 Inner Product 的結果輕易的算出來
怎麼算呢？其實 phi(x) 跟 phi(z) 的 Inner Product 就是
(x · z)^2
直接把 x 跟 z 做 Inner Product 再平方
只需要算 k 個 element 的相乘再做一次平方就好
但是如果先把他 project 到 high dimension 再做 Inner Product 的話
這個 dimension 很大，是 (C k 取 2) 維
如果 feature、dimension 越大，k 值的話 這個 feature 就越長
先做完 Feature Transform 再做 Inner Product 是會比 先做 Inner Product 再取平方運算量還要大
這個是比較快的
這個平方可以拆成 ((x1)(z1) + (x2)(z2) + ⋅⋅⋅ + (xk)(zk))^2
這個平方展開的話裡面有二次項裡面有 (x1)^2 * (z1)^2, (x2)^2 * (z2)^2 等等
會有兩兩相乘的 2(x1)(x2)(z1)(z2) + 2(x1)(x3)(z1)(z3)
2(x2)(x3)(z2)(z3), 2(x2)(x4)(z2)(z4) 等等
把 x 集中到一邊就可以得到這個 vector
把 z 集中到另外一邊就得到 z 做完 Feature Transform 的東西
所以這一項呢
就會等價於先做 Feature Transform 再做 Inner Product
還有一些更驚人的結果
做 Kernel Basis 的
做 RBF Kernel
做 RBF Kernel 的意思是
K(x, z) 就等於 x 跟 z 的距離乘上 (-1/2) 再取 exponential
這個東西就是在衡量 x 跟 z 之間的相似度
如果 x 跟 z 越像 Kernel 的值就越大
如果 x = z 的話，值就是 1
如果 x 跟 z 完全不一樣的話
它們的值就是 0
這個式子 exp((-1/2) ||x-z||2) 其實也可以化成
兩個 high dimension 的 vector 做 Inner Product 以後的結果
這兩個 vector 其實他們的 dimension 是有無窮多維
本來如果要把一個 x project 到無窮多維再做 Inner Product，你做不到
因為無窮多維是什麼樣的根本就不知道
但是如果直接算 x 跟 z 的距離
然後再乘 -1/2 再取 exponential 其實等同於在無窮多維的空間裡面去做 Inner Product
無窮多維的空間長甚麼樣子
可以把 exp((-1/2) ||x-z||2)
變成 exp((-1/2) ||x||2 - (1/2) ||z||2 + x ⋅ z)
接下來把 x 的 norm、z 的 norm 這兩項提出來 剩下 exp(x ⋅ z)
把 x 的 norm 這一項用 Cx 來表示它，它跟 x 有關
這一項用 Cz 來表示它，它跟 z 有關 剩下 exp(x ⋅ z)
exp(x ⋅ z) 用泰勒展開式、用 Taylor Expansion 就變成
它等於 Cx * Cz summation over i = 0 到無窮大
((x ⋅ z)^i)/ i!
這個有無窮大項 從 0 次方一直 summation 到無窮多次方
如果把它展開的話看起來就像這樣
CxCz + CxCz ( x ⋅ z ) + CxCz(1/2)( x ⋅ z )^2 ⋅⋅⋅
如果把每一項都拆開的話會得到甚麼
Cx 跟 Cz 可以看成是兩個 vector
他們都只有一個 dimension
一個是 Cx，一個是 Cz 的 Inner Product
這一項可以看成是把原來 x 的 vector 乘上 Cx
把 z 的 vector 乘上 Cz 再做 Inner Product 以後的結果
這一項 x 跟 z 的平方
x 跟 z 先做 Inner Product 再平方 剛剛在前面例子已經看過
其實可以拆成兩個 high dimension 的 vector 再 Inner Product
所以 x 跟 z 的平方等於兩個 high dimension 的 vector 做 Inner Product 的結果
這 high dimension 的 vector 它會考慮兩個 dimension 之間的關係
如果是三次方它就會考慮三個 dimension 之間的關係
現在把屬於 x 的 vector 都串起來
串成一個很長的 vector
屬於 z 這邊的也都串起來
因為這邊有無窮多項
所以串起來以後 x 跟 z 都有各自無窮長的 vector
他們做 Inner Product 最後得到的結果
就是這個 Kernel 所給你的結果
所以使用 RBF Kernel 的時候
就是在無窮多維的平面上去做事情
不過如果在無窮多維的平面上做事情
可以想像它其實是滿容易 Overfitting
所以如果用 RBF Kernel 有時候要小心
可能在 training data 上得到很好的 performance
但是在 testing data 上得到很糟的 performance
你也可以做 Sigmoid Kernel
Sigmoid Kernel 是 K(x, z) = tanh(x · z)
至於 tanh(x · z) 是哪兩個 high dimension vector 做 Inner Product 的結果
自己回去用 Taylor Expansion 展開來看就知道了
之前已經說過
當要把 x 拿來做 testing 的時候、帶到 f 裡面的時候
其實是計算 x 跟所有 training data 裡面的 x^n 的 Kernel 的 function output 然後再乘上 alpha n
如果用的是 Sigmoid Kernel 的時候
就是把 data 裡面所有的 x n 跟 x 做 Inner Product
再去 Hyperbolic Tangent 再乘 alpha n 再全部合起來以後的結果
如果用的是 Sigmoid Kernel
這個 f(x) 就可以想成它其實是一個只有一個 hidden layer 的 Neural Network
把 x 拿進來，它會跟所有的 x n 都做 Inner Product
再通過 Hyperbolic Tangent
對每一個 x 做 Inner Product 這件事情
就好像是有一個 neural 它的 weight 就是某一筆 data
把 x1 那一筆 data 拿出來當作這個 neural 的 weight
把 x2 那一筆 data 拿出來當作第二個 neural 的 weight
一直到把第 n 筆 data 拿出來當作第 n 個 neural 的 weight
把他們都通過 Hyperbolic Tangent 得到 output
然後再把它全部乘上 alpha
然後把他們全部加起來
突然發現我犯了一個錯 這個 alpha 應該要下標
上標下標都可以 前面是下標，後面也要是下標才合適
我們找出這些 alpha 把它 Weighted Sum 起來就得到最後的 f(x)
這就是一個 Neural Network 只是它只有一個 Hidden Layer
在這個 Neural Network 裡面它裡面的每一個 neural 的 weight
就是某一筆 data
那 neural 的數目呢
neural 的數目就是看有幾個 support vector 就有幾個 neural
既然有了 Kernel Trick，其實可以去直接設計 Kernel Function
可以根本完全不用理會 x 跟 z 的 feature 長甚麼樣子
只要有一個 Kernel Function 可以把 x 跟 z 這兩個東西帶進去
它可以給你一個 value
這個 value 代表了 x 跟 z 在某一個高維平面上的 vector 的 Inner Product 的話
你根本就不需要在意 x 跟 z 他們的 vector 長甚麼樣子
甚麼時候這招會有用呢 假設 x 是有 structure 的 data
比如說它是一個 sequence
如果是一個 sequence 的話，其實不容易把 sequence 表示成 vector
假設每一個 sequence 長度都不一樣
就不容易把這些不同長度的 sequence 都用一個 vector 來描述他
所以你根本不知道 x 長甚麼樣子 你就不知道 phi(x) 應該長甚麼樣子
但是可以直接定它的 Kernel Function
我們知道 Kernel Function 就是投影到高維以後的 Inner Product
所以 Kernel Function 往往就是一個類似 similarity 的東西
如果你可以定一個 Function 它是 evaluate x 跟 z 的 similarity
就算 x 跟 z 它是有 structure 的 object
比如說它是 tree structure
它是 sequence 也沒有關係 只要知道怎麼算兩個 sequence 之間的 similarity
只要知道怎麼算兩個 tree structure similarity 就有機會把它的 similarity 當作 Kernel 來使用
你可能會懷疑 我胡亂定一個 similarity
它背後有 feature vector 可以 support 它嗎
我們說這個 Kernel 是兩個 vector 做 Inner Product 以後的結果
你胡亂定一個 function 它可以拆成兩個 vector Inner Product 以後的結果嗎
不是所有的 function 都可以，但是有一個叫 Mercer's theory
可以告訴你那些 function 是可以的
所以你有辦法 check 定出來的 Kernel Function 它背後有沒有兩個 vector 做 Inner Product 這件事情
也是可以 check 的
所以在語音上，假設現在要做的分類對象 其實是 Audio Segment
Audio Segment，每一個 segment、每一段聲音訊號就是會用 vector sequence 來描述他
每一段聲音訊號長度都不一樣
所以 vector sequence 長度可能都不一樣
現在做的 task 可能是
給你一段聲音訊號，這是分類的問題 它要看這段聲音訊號裡面語者的情緒
它可能分成高興、生氣等等之類的
要把它做分類
你想要用SVM，但是一段聲音訊號沒有辦法直接用一個 vector 來描述他
怎麼辦 你可以直接定它的 Kernel
就不要管一段聲音訊號變成 vector 之後長甚麼樣子
直接定它的 Kernel 直接定一個 function K(x, z)
然後把 x 是一段聲音訊號帶進去
z 是另外一段聲音訊號帶進去的時候
這個 function 的 output 應該是甚麼
你定好這個你就可以直接用 Kernel Trick 在 SVM
就算你不知道一段聲音訊號描述成一個 vector 應該是甚麼樣子
怎麼定兩個 sequence 間的 Kernel
我就把 reference 留在這邊給大家參考
其實在這邊還有很多，比如說 SVM 可以做 Regression
SVM 做 regression 就是 Support Vector Regression
它的精神是這樣，用幾句話講一下它的精神
它的精神是，原來在做 Regression 的時候希望 model 的 output 跟 target 越近越好
如果做 Support Vector Regression 的時候
近到某一個距離裡面
我本來想說某一個劍圍的距離 但我想大家不知道甚麼是劍圍就算了
張遼的劍圍 算了我覺得這太沒有關係了
進入 target 某一個距離裡面
它的 loss 就是 0 這個是 Support Vector Regression
Ranking SVM
Ranking SVM 常常被用在當你要考慮的東西是一個排序是 list 的時候
比如說在 final project 裡面不是有個 recommendation 的題目
他要你的 output 是一個 list
你可以說我把它當作是一個 regression 的題目
我給每一個 element 一個分數 然後按照分數由高到低做搜索
但這樣並沒有直接 optimize 你的問題
其實可以直接考慮這個 list 的 ranking
如果直接考慮 ranking 的 SVM 叫做 Ranking SVM
或許在 final project 裡面用的到這個東西
還有另外一個東西叫 One-class SVM
它是會希望說屬於 positive 的 example 都自成一類
negative 的 example 就散佈在其他地方
這個都留一些 reference 給大家參考就好
我們可以比較一下 Deep Learning 跟 SVM 的差別
Deep Learning 的前幾個 layer 可以看作是 Feature Transformation
最後一個 layer 可以看作是 Linear Classifier
SVM 做的也是很類似的事情
它前面先 apply 一個 Kernel Function
把 feature 轉到 high dimension 上面
在 high dimension space 上面就可以 apply Linear Classifier
在 SVM 裡面一般 Linear Classifier 都會用 Hinge Loss
事實上 SVM 的 kernel 是 learnable
我列了一個 reference 給大家參考，事實上它是 learnable
但是它沒有辦法 learn 的像 Deep Learning 那麼多
你可以做的是你有好幾個不同的 kernel
然後把不同 kernel combine 起來
他們中間的 weight 是可以 learn 的
當你只有一個 kernel 的時候
SVM 就好像是只有一個 Hidden Layer 的 Neural Network
當你把 kernel 在做 Linear Combination 的時候
他就像一個有兩個 layer 的 Neural Network
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
