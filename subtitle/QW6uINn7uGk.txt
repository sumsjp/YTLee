現在我們要講的是
Optimization的部分
所以等一下我們要講的東西基本上跟
比如說Overfitting
沒有什麼太大的關聯
我們只討論Optimization的時候
怎麼把gradient descent做得更好
那為什麼Optimization會失敗呢
你常常在做Optimization的時候
你會發現說
隨著你的參數不斷的update
你的training的loss不會再下降
但是你對這個loss仍然不滿意
就像我剛才說的
你可以把deep的network
跟linear的model
或比較shallow network 比較
發現說它沒有做得更好
所以你覺得deepnetwork
沒有發揮它完整的力量
所以Optimization顯然是有問題的
但有時候你會甚至發現說
一開始你的model就撐不起來
一開始你不管怎麼update你的參數
你的loss通通都掉不下去
那這個時候到底發生了什麼事情呢
過去常見的一個猜想
是因為我們現在走到了一個地方
這個地方參數對loss的微分為零
當你的參數對loss微分為零的時候
gradient descent就沒有辦法
再update參數了
這個時候training就停下來了
你的參數不再update了
loss當然就不會再下降了
那講到gradient為零的時候
大家通常腦海中最先浮現的
可能就是local minima
所以常有人說 做deep learning
你用gradient descent
你會卡在local minima
然後所以gradient descent不work
所以deep learning不work什麼的
但是如果有一天你要寫
跟deep learning相關paper的時候
你千萬不要講什麼
卡在local minima這種事情
別人會覺得你非常沒有水準
為什麼
因為不是只有local minima
的gradient是零
還有其他可能會讓gradient是零
比如說 saddle point
所謂的saddle point
其實就是gradient是零
但是不是local minima
也不是local maxima的地方
像在這個例子裡面 紅色的這個點
它在左右這個方向是比較高的
前後這個方向是比較低的
它就像是一個馬鞍的形狀
所以叫做saddle point
那中文就翻成鞍點
那像saddle point這種地方
它也是gradient為零
但它不是local minima
那像這種gradient為零的點
統稱為critical point
所以你可以說你的loss
沒有辦法再下降
也許是因為卡在了critical point
但你不能說是卡在local minima
因為saddle point也是微分為零的點
好 但是今天如果你發現你的gradient
真的很靠近零
卡在了某個critical point
我們有沒有辦法知道
到底是local minima
還是saddle point呢
其實是有辦法的 不是完全沒有辦法
那為什麼我們會想要知道
到底是卡在local minima
還是卡在saddle point呢
因為如果是卡在local minima
那可能就沒有路可以走了
因為四周都比較高
你現在所在的位置已經是最低的點
loss最低的點了
往四周走 loss都會比較高
你會不知道怎麼走到其他的地方去
但saddle point就比較沒有這個問題
如果你今天是卡在saddle point的話
你今天是走在一個saddle point的話
saddle point旁邊還是有路可以走的
還是有路可以讓你的loss更低的
你只要逃離saddle point
你就有可能讓你的loss更低
所以鑑別今天我們走到
critical point的時候
到底是local minima
還是saddle point
是一個值得去探討的問題
那怎麼知道今天一個critical point
到底是屬於local minima
還是saddle point呢
這邊需要用到一點數學
以下這段 其實沒有很難的數學
就只是微積分跟線性代數
但如果你沒有聽懂的話
以下這段skip掉是沒有關係的
好 那怎麼知道說一個點
到底是local minima
還是saddle point呢
那你要知道我們loss function的形狀
可是我們怎麼知道
loss function的形狀呢
network本身很複雜
用複雜network算出來的loss function
顯然也很複雜
我們怎麼知道loss function
長什麼樣子
雖然我們沒有辦法完整知道
整個loss function的樣子
但是如果給定某一組參數
比如說藍色的這個θ'
在θ'附近的loss function
是有辦法被寫出來的
它寫出來就像是這個樣子
所以這個L(θ)完整的樣子寫不出來
但是它在θ'附近
你可以用這個式子來表示它
你就說 這個是什麼
這個還有好多種顏色啊
這個東西到底是什麼
這個東西是
Tayler Series Appoximation
這個假設你在微積分的時候
已經學過了
所以我就不會細講這一串是怎麼來的
但我們就只講一下它的概念
這一串裡面包含什麼東西呢
第一項是L(θ')
就告訴我們說
當θ跟θ'很近的時候
L(θ)應該跟L(θ')還蠻靠近的
那第二項是什麼呢
第二項是θ減θ'的traspose 乘上g
g是什麼
g是一個向量
這個g就是我們的gradient
我們在上週已經講過gradient
這個東西了
我們用綠色的這個g來代表gradient
它是一個向量
那這個gradient告訴我們什麼呢
這個gradient會來彌補
θ'跟θ之間的差距
我們雖然剛才說θ跟θ'
它們應該很接近
但是中間還是有一些差距的
那這個差距
第一項我們用這個gradient
來表示他們之間的差距
好 那這個g就是
有時候gradient會寫成∇L(θ')
這個地方的g
然後g是一個向量
它的第i個component
就是θ的第i個component
對L的微分
光是看g還是沒有辦法
完整的描述L(θ)
你還要看第三項
第三項是什麼 第三項跟Hessian有關
這邊有一個H 這個H叫做Hessian
它是一個矩陣
這個第三項是θ-θ'的transpose乘上H
再乘上(θ-θ')
所以第三項會再補足
再加上gradient以後
與真正的L(θ)之間的差距
那這個H裡面放的是什麼東西呢
這個H裡面放的是L的二次微分
這個它的第i個row
第j個column的值 是什麼呢
它第i個row
第j個column的值
就是把θ的第i個component
對L作微分
再把θ的第j個component
對L作微分
再把θ的第i個component
對L作微分
做兩次微分以後的結果 就是這個Hᵢⱼ
如果這邊你覺得有點聽不太懂的話
也沒有關係
反正你就記得這個L(θ)
這個loss function
這個error surface在θ'附近
可以寫成這個樣子
這個式子跟兩個東西有關係
跟gradient有關係
跟hessian有關係
gradient就是一次微分
hessian就是裡面有二次微分的項目
那如果我們今天走到了一個
critical point
那意味著什麼
意味著gradient為零
也就是綠色的這一項完全都不見了
g是一個zero vector
綠色的這一項完全都不見了
只剩下紅色的這一項
所以當在critical point的時候
這個loss function
它可以被近似為L(θ')
加上紅色的這一項
我們可以根據紅色的這一項來判斷說
在θ'附近的error surface
到底長什麼樣子
知道error surface長什麼樣子
我就可以判斷說
θ'它是一個local minima
是一個local maxima
還是一個saddle point
我們可以靠這一項來了解
這個error surface的地貌
大概長什麼樣子
知道它地貌長什麼樣子
我們就可以知道說
現在是在什麼樣的狀態
這個是Hessian
那我們就來看一下怎麼根據Hessian
怎麼根據紅色的這一項
來判斷θ'附近的地貌
我們現在為了等一下符號方便起見
我們把θ-θ'用v這個向量來表示
θ是一個向量 θ'也是一個向量
這兩個向量相減
我們用v這個向量來表示它
如果今天對任何可能的v
v的transpose乘上H乘上v
都大於零
也就是說 現在θ不管代任何值
v可以是任何的v
就代表說θ-θ'可以是任何值
也就是θ可以是任何值
不管θ代任何值
紅色框框裡面通通都大於零
那意味著什麼
意味著說 紅色框框裡面都大於零
那意味著說 L(θ)>L(θ')
L(θ)不管代多少 只要在θ'附近
L(θ)都大於L(θ')
那代表什麼
代表L(θ')是附近的一個最低點
所以它是local minima
如果今天反過來說
對所有的v而言
v transpose H乘上v都小於零
也就是紅色框框裡面永遠都小於零
也就是說θ不管代什麼值
紅色框框裡面都小於零
那意味著什麼
那意味著紅色框框裡面都小於零
意味著說L(θ')都是大於L(θ)
L(θ)小於L(θ')
代表說L(θ')是附近最高的一個點
所以它是local maxima
第三個可能是假設
v transpose Hᵥ
有時候大於零 有時候小於零
你代不同的v進去 代不同的θ進去
紅色這個框框裡面有時候大於零
有時候小於零
意味著說在θ'附近
有時候L(θ)>L(θ') 有時候L(θ)<L(θ')
在L(θ')附近
有些地方高 有些地方低
這意味著什麼
這意味著這是一個saddle point
這邊你會有一個接下來一個問題是
我們看紅色這一項
我們現在可以判斷他是local minima
還是saddle point
還是local maxima
但是你這邊是說我們要代所有的v
去看v transpose乘Hᵥ 是大於零
還是小於零
我們怎麼有可能把所有的v
都拿來試試看呢
所以有一個更簡便的方法
去確認說這一個條件或這一個條件
會不會發生
這個就直接告訴你結論
線性代數理論上是有教過這件事情的
如果今天對所有的v而言
v transpose Hᵥ 都大於零
那這種矩陣叫做positive definite
positive definite的矩陣有什麼特性呢
它所有的eigen value都是正的
所以如果你今天算出一個hessian
你不需要把它跟所有的v都乘看看
你只要去直接看這個H的eigen value
如果你發現說
所有eigen value都是正的
那就代表說這個條件成立
就v transpose乘上H乘上v
會大於零
也就代表說是一個local minima
所以你從hessian metric可以看出
它是不是local minima
你只要算出hessian metric算完以後
看它的eigen value發現都是正的
它就是local minima
就結束了
那反過來說也是一樣
如果今天在這個狀況
對所有的v而言
v transpose乘上H乘上v小於零
那H是negative definite
那就代表所有eigen value都是負的
所有的eigen value都是負的
就保證他是local maxima
那如果eigen value有正有負
那就代表是saddle point
那假設在這裡你沒有聽得很懂的話
你就可以記得結論
你只要算出一個東西
這個東西的名字叫做hessian
它是一個矩陣
這個矩陣如果它所有的eigen value
都是正的
那就代表我們現在在local minima
如果它有正有負
就代表在saddle point
像我們剛才講說
所以我們是有辦法判斷說
是local minima 還是saddle point
那如果剛才講的
你覺得你沒有聽得很懂的話
我們這邊舉一個例子
我們現在有一個史上最廢的network
它廢到什麼程度呢
輸入一個x
它只有一個new row乘上w₁
而且這個new row
還沒有vectorization function
所以x乘上w₁以後 之後就輸出
然後再乘上w₂ 然後就再輸出
就得到最終的數據就是y
總之這個function非常的簡單
y= w₁×w₂×x
這是一個史上最廢的new row network
那我們有一個史上最廢的
training data set
這個data set說
我們只有一筆data
這筆data是x是1的時候
它的level是1 所以輸入1進去
你希望最終的輸出跟1越接近越好
這個史上最廢的training
而這個史上最廢的training
它的error surface
也是有辦法直接畫出來的
因為反正只有兩個參數 w₁ w₂
連bias都沒有
假設沒有bias
只有w₁跟w₂兩個參數
這個network只有兩個參數 w₁跟w₂
那我們可以窮舉所有w₁跟w₂的數值
算出所有w₁ w₂數值所代來的loss
然後就畫出error surface 長這個樣
所以這邊是高的 這邊loss是高的
這邊loss也是高的
四個角落loss是高的
好 那這個圖上你可以看出來說
有一些critical point
哪些地方critical point呢
這個黑點點的地方(0,0)
原點的地方是critical point
然後事實上
這一條線這邊 也是一排critical point
也是一排critical point
如果你更進一步要分析
他們是saddle point
還是local minima的話
那圓心這個地方
原點這個地方 它是saddle point
為什麼它是saddle point呢
你往這個方向走 loss會變大
往這個方向走 loss會變大
往這個方向走 loss會變小
它是一個saddle point
而這兩群critical point
它們都是local minima
所以這個山溝裡面
有一排local minima
這一排山溝裏面有一排local minima
然後在原點的地方
有一個saddle point
這個是我們把error surface
爆搜以後 爆搜所有的參數
得到的loss function以後
得到的loss的值以後
畫出error surface
可以得到這樣的結論
現在假設如果不爆搜所有可能的loss
如果要直接算說一個點
是local minima
還是saddle point的話 怎麼算呢
好 我們可以把loss的function寫出來
這個loss的function 這個L是
正確答案 ŷ減掉model的輸出
也就是w₁ w₂x
這邊取square error
這邊只有一筆data
所以就不會summation over
所有的training data
因為反正只有一筆data
x代1 ŷ代1
我剛才說過只有一筆訓練資料最廢的
所以只有一筆訓練資料
所以loss function就是(1–w₁w₂)²
那你可以把這一個loss function
它的gradient求出來
那這邊細節我們就不講了
你如果微積分都還記得的話
算一下就會知道說這個是對的
反正w₁對L的微分
寫出來是這個樣子
w₂對L的微分寫出來是這個樣子
那這個東西就是所謂的g
所謂的gradient
什麼時候gradient會零呢
什麼時候會到一個critical point呢
舉例來說 如果w₁=0 w₂=0
就在圓心這個地方
如果你w₁代0 w₂代0
這個gradient算出來就都是零
如果w₁代0 w₂代0
w₁對L的微分 w₂對L的微分
算出來就都是零 就都是零
這個時候我們就知道說
原點就是一個critical point
但是它是一個critical point
但它是local maxima
它是local maxima
local minima
還是saddle point呢
那你就要看hessian才能夠知道了
就要看hessian才能夠知道
當然 我們剛才已經爆搜
所有可能的w₁ w₂了
所以你已經知道說
它顯然是一個saddle point
但是現在假設還沒有爆搜
所有可能的loss
所以我們要看看能不能夠用H
用Hessian看出它是什麼樣的
critical point
那怎麼算出這個H呢
H它是一個矩陣
這個矩陣裡面就是蒐集了
L的二次微分
所以這個矩陣裡面第一個row
第一個coloumn的位置
就是w₁對L微分兩次
第一個row 第二個coloumn的位置
就是先用w₂對L作微分
再用w₁對L作微分
然後這邊就是w₁對L作微分
w₂對L作微分
然後w₂對L微分兩次
這四個值組合起來
就是我們的hessian
那這個hessian的值是多少呢
這個hessian的式子
我都已經把它寫出來了
你只要把w₁=0 w₂=0代進去
代進去 你就得到在原點的地方
hessian是[(0,-2)(-2,0)]這樣的一個矩陣
好 這個矩陣告訴我們
這個hessian告訴我們
它是local minima
還是saddle point呢
那你就要看這個矩陣的eigen value
算一下發現
這個矩陣有兩個eigen value
2跟-2 eigen value有正有負
代表saddle point 對不對
我們剛才講eigen value有正有負
就代表的是saddle point
好 所以我們現在就是用一個例子
跟你操作一下 告訴你說
你怎麼從hessian看出一個點
它一個critical point 它是saddle point
還是local minima
如果今天你卡的地方是saddle point
也許你就不用那麼害怕了
為什麼
因為如果你今天你發現
你停下來的時候
是因為saddle point 停下來了
那其實就有機會可以放心了
為什麼
因為H它不只可以幫助我們判斷
現在是不是在一個saddle point
它還指出了我們參數
可以update的方向
就之前我們參數update的時候
都是看gradient 看g
但是我們走到某個地方以後
發現g變成0了 不能再看g了
g不見了 gradient沒有了
但如果是一個saddle point的話
還可以再看H
怎麼再看H呢
H怎麼告訴我們
怎麼update參數呢
我們這邊假設u是H的eigenvector
然後λ是u的eigen value
就H有一個eigen value叫λ
它的對應的eigen vector叫做u
如果我們把這邊的v換成u的話
會發生什麼事呢
如果我們把u乘在H的左邊
跟H的右邊
也就是u transpose乘上H乘於u
會得到什麼呢
H乘於u會得到λu
為什麼
因為u是一個eigen vector
所以H乘上eigen vector會得到
λ eigen value乘上eigen vector
所以我們在這邊得到uᵀ乘上λu
然後再整理一下
把uᵀ跟u乘起來
得到‖u‖²
所以得到λ‖u‖²
所以這一項 這一項
假設我們這邊v
代的是一個eigen vector
我們這邊θ減θ'
放的是一個eigen vector的話
會發現說我們這個紅色的項裡面
其實就是λ‖u‖²
那今天如果λ小於零
eigen value小於零的話
會發生什麼事呢
如果eigen value小於零的話
那λ‖u‖²就會小於零
因為‖u‖²一定是正的
所以eigen value是負的
那這一整項就會是負的
也就是u的transpose乘上H乘上u
它是負的
也就是紅色這個框框裡面
是小於零的 是負的
所以這意思是說假設θ減θ'等於u
那這一項就是負的
也就是L(θ)會小於L(θ')
也就是說假設θ減θ'等於u
也就是θ等於θ'加u
你把θ' 本來參數在θ'的位置加上u
沿著u的方向做update得到θ
你就可以讓loss變小
因為根據這個式子
你只要θ減θ'等於u
loss就會變小
所以你今天只要讓θ等於θ'加u
你就可以讓loss變小
你只要沿著u
也就是eigen vector的方向
去更新你的參數 去改變你的參數
你就可以讓loss變小了
所以雖然在critical point沒有gradient
如果我們今天是在一個saddle point
你也不一定要驚慌
你只要找出負的eigen value
再找出它對應的eigen vector
用這個eigen vector去加θ'
就可以找到一個新的點
這個點的loss比原來還要低
如果這樣你聽得不是很清楚的話
我就舉具體的例子
剛才我們已經發現說
原點是一個critical point
它的Hessian長這個樣
那我現在發現說
這個Hessian有一個負的eigen value
這個eigen value等於-2
那它對應的eigen vector
長什麼樣子呢
它有很多個
其實是無窮多個對應的eigen vector
我們就取一個出來
我們取[1¦1] [1¦1]
是它對應的一個eigen vector
那我們其實只要順著這個u的方向
順著[1¦1]這個vector的方向
去更新我們的參數
就可以找到一個
比saddle point的loss還要更低的點
如果以今天這個例子來看的話
你的saddle point在(0,0)這個地方
你在這個地方會沒有gradient
所以gradient不會告訴你說
你要怎麼更新參數
那Hessian告訴我們說
Hessian的eigen vector告訴我們說
只要往[1¦1]的方向更新
你就可以讓loss變得更小
只要往u這個eigen vector方向更新
[1¦1]的方向更新
你就可以讓你的loss變小
也就是說你可以逃離你的saddle point
然後讓你的loss變小
所以從這個角度來看
從這個角度來看
似乎saddle point並沒有那麼可怕
如果你今天在training的時候
你的gradient你的訓練停下來
你的gradient變成零
你的訓練停下來
是因為saddle point的話
那似乎還有解
但是當然實際上
在實際的implementation裡面
你幾乎不會真的把Hessian算出來
為什麼
這個要是二次微分
要計算這個矩陣的computation
需要的運算量非常非常的大
更遑論你還要把它的eigen value
跟 eigen vector找出來
所以在實作上
你幾乎沒有看到
有人用這一個方法來逃離saddle point
等一下我們會講其他
也有機會逃離saddle point的方法
他們的運算量都比要算這個H
還要小很多
那今天之所以我們把
這個saddle point跟 eigen vector
跟Hessian的eigen vector拿出來講
是想要告訴你說
如果是卡在saddle point
也許沒有那麼可怕
最糟的狀況下你還有這一招
可以告訴你要往哪一個方向走
好 那講到這邊你就會有一個問題了
這個問題是
那到底saddle point跟local minima
誰比較常見呢
我們說
saddle point其實並沒有很可怕
那如果我們今天
常遇到的是saddle point
比較少遇到local minima
那就太好了
那到底saddle point跟local minima
哪一個比較常見呢
這邊我們要講一個不相干的故事
先講一個故事
這個故事是什麼呢
這個故事發生在1543年
1543年發生了什麼事呢
那一年君士坦丁堡淪陷
這個是君士坦丁堡淪陷圖
君士坦丁堡本來是東羅馬帝國的領土
然後被鄂圖曼土耳其帝國佔領了
然後東羅馬帝國就滅亡了
在鄂圖曼土耳其人進攻
君士坦丁堡的時候
那時候東羅馬帝國的國王
是君士坦丁十一世
他不知道要怎麼對抗土耳其人
有人就獻上了一策
找來了一個魔法師叫做狄奧倫娜
這是真實的故事 知道嗎
出自三體的故事
這個狄奧倫娜這樣說
狄奧倫娜是誰呢
他有一個能力跟張飛一樣
張飛不是可以萬軍從中取上將首級
如探囊取物嗎
狄奧倫娜也是一樣
他可以直接取得那個蘇丹的頭
他可以從萬軍中取得蘇丹的頭
大家想說狄奧倫娜怎麼這麼厲害
他真的有這麼強大的魔法嗎
所以大家就要狄奧倫娜
先展示一下他的力量
這時候狄奧倫娜就拿出了一個聖杯
大家看到這個聖杯就大吃一驚
為什麼大家看到這個聖杯
要大吃一驚呢
因為這個聖杯
本來是放在聖索菲亞大教堂的地下室
而且它是被放在一個石棺裡面
這個石棺是密封的
沒有人可以打開它
但是狄奧倫娜他從裡面取得了聖杯
而且還放了一串葡萄進去
君士坦丁十一世為了要驗證
狄奧倫娜是不是真的有這個能力
就帶了一堆人真的去撬開了這個石棺
發現聖杯真的被拿走了
裡面真的有一串新鮮的葡萄
就知道狄奧倫娜真的有
這個萬軍從中取上將首級的能力
那為什麼迪奧倫娜可以做到這些事呢
那是因為這個石棺你覺得它是封閉的
那是因為你是從三維的空間來看
從三維的空間來看
這個石棺是封閉的
沒有任何路可以進去
但是狄奧倫娜可以進入四維的空間
從高維的空間中
這個石棺是有路可以進去的
它並不是封閉的
至於狄奧倫娜有沒有成功刺殺蘇丹呢
你可以想像一定是沒有嘛
所以君坦丁堡才淪陷
那至於為什麼沒有
大家請見於三體這樣 就不雷大家
那總之這個從三維的空間來看
是沒有路可以走的東西
在高維的空間中是有路可以走的
error surface會不會也一樣呢
所以你在一維的空間中
一維的一個參數的error surface
你會覺得好像到處都是local minima
但是會不會在二維空間來看
它就只是一個saddle point呢
常常會有人畫類似這樣的圖
告訴你說Deep Learning的訓練
是非常的複雜的
如果我們移動某兩個參數
error surface的變化非常的複雜
是這個樣子的
那顯然它有非常多的local minima
我的這邊現在有一個local minima
但是會不會這個local minima
只是在二維的空間中
看起來是一個local minima
在更高維的空間中
它看起來就是saddle point
在二維的空間中
我們沒有路可以走
那會不會在更高的維度上
因為更高的維度
我們沒辦法visualize它
我們沒辦法真的拿出來看
會不會在更高維的空間中
其實有路可以走的
那如果維度越高
是不是可以走的路就越多了呢
所以 今天我們在訓練
一個network的時候
我們的參數往往動輒百萬千萬以上
所以我們的error surface
其實是在一個非常高的維度中
對不對
我們參數有多少
就代表我們的error surface的
維度有多少
參數是一千萬 就代表error surface
它的維度是一千萬
竟然維度這麼高
會不會其實
根本就有非常多的路可以走呢
那既然有非常多的路可以走
會不會其實local minima
根本就很少呢
而經驗上
如果你自己做一些實驗的話
也支持這個假說
這邊是訓練某一個network的結果
每一個點代表
訓練那個network訓練完之後
把它的Hessian拿出來進行計算
所以這邊的每一個點
都代表一個network
就我們訓練某一個network
然後把它訓練訓練
訓練到gradient很小
卡在critical point
把那組參數出來分析
看看它比較像是saddle point
還是比較像是local minima
那這邊的縱軸跟橫軸是什麼意思呢
縱軸代表training的時候的loss
就是我們今天卡住了
那個loss沒辦法再下降了
那個loss是多少
那很多時候
你的loss在還很高的時候
訓練就不動了 就卡在critical point
那很多時候loss可以降得很低
才卡在critical point
這是縱軸的部分
橫軸的部分是什麼呢
橫軸的部分是minimum ratio
什麼叫minimum ratio呢
minimum ratio是eigen value的數目
分之正的eigen value的數目
又如果所有的eigen value都是正的
代表我們今天的critical point
是local minima
如果有正有負代表saddle point
那在實作上你會發現說
你幾乎找不到完全所有eigen value
都是正的critical point
你看這邊這個例子裡面
這個minimum ratio代表
eigen value的數目
分之正的eigen value的數目
最大也不過0.5到0.6間而已
代表說只有一半的eigen value是正的
還有一半的eigen value是負的
所以今天雖然在這個圖上
越往右代表我們的critical point
越像local minima
但是它們都沒有真的
變成local minima
就算是在最極端的狀況
我們仍然有一半的case
我們的eigen value是負的
這一半的case eigen value是正的
代表說在所有的維度裡面有一半的路
這一半的路 如果要讓loss上升
還有一半的路可以讓loss下降
所以從經驗上看起來
其實local minima並沒有那麼常見
多數的時候
你覺得你train到一個地方
你gradient真的很小
然後所以你的參數不再update了
往往是因為你卡在了一個saddle point
好 那接下來我們就是要講說
假設你train啊train啊
也許你卡在local minima
像我們剛才講說
local minima出現的狀況
也許沒有那麼多
或卡在saddle point
或甚至是在saddle point附近
也就是非常平坦的地方
那麼有什麼樣可能的解決方法
好那在這邊我們還是停一下
看看大家有沒有問題要問的
