那接下來這堂課要跟大家講的是生成式AI裡面的生成策略
那在這門課到目前為止我們一直專注在文字的生成上
那到現在這個地方我們要來跳脫文字的生成
來從更大的格局來看一下生成式人工智慧是怎麼回事
那在開學的第一堂課
我們就告訴你說
什麼是生成式人工智慧
生成式人工智慧
就是要讓機器產生複雜
而有結構的物件
舉例來說文字、影像
或者是聲音訊號
那我們在開學的時候
也講過說所謂的複雜
指的是幾乎沒有辦法窮舉
可能的文字、可能的文章
是沒辦法窮舉的
可能的圖片是沒辦法窮舉的
可能的一段聲音訊號是無法窮取的
這些無法窮取的複雜物件
背後卻是由有限的基本單位所構成的
那由有限的基本單位所構成這句話
是什麼意思呢
我們就從文字影像跟聲音三個面向
來跟大家剖析
那文字的部分是大家已經比較熟悉的
我們說其實一句話
就是由 Token 所構成的
而Token是人事先定好的
我們之前也說過說
如果你想要知道GPT-3.5、GPT-4
它內部用的是什麼樣的Token
當他看到一句話的時候
這句話是由什麼樣的Token所組成的
那OpenAI有一個網站可以告訴你
每一句話是怎麼被拆解成一串Token的
那到底一個語言模型有多少的Token呢
那每一個語言模型的Token的數目
都是不一樣的
因為Token是語言模型的開發者
在開發語言模型
在序列語言模型之前
就已經事先訂好的東西
但是通常需要多少的Token呢
我們這邊用Llama2來當做例子
Llama2有多少個Token呢
Llama2有三萬兩千個Token
也就是說每次Llama2在做這個
文字接龍的時候
他是從三萬兩千個符號
三萬兩千個Token裡面
選擇一個Token把它產生出來
而這些Token這三萬兩千個Token所成的集合
就叫做Vocabulary
那翻譯成中文應該可以說翻譯成
詞典這個字應該可以翻譯成
詞典
Token所組成的集合叫做
Vocabulary
這是你已經比較熟悉的文字
那如果是影像呢那想必大家都知道影像是由像素
所構成的
所以如果你把一張圖片放大來看
你就會看到一張圖片是由一個一個的格子所構成的
每一個格子就是一個像素一個pixel
那每一個像素呢
它有一個顏色
那每一個像素可以有多少顏色
取決於一個叫做BPP的東西
bit per pixel
那如果是8BPP就是256色
如果16BPP就是65536色
如果是24BPP就是1670萬色
那今天大家常常聽到的所謂全綵色
或者是真彩色指的是1670萬色
所以每個Pixel它有1670萬種可能的顏色
那把Pixel組合起來就變成一張圖片
而聲音呢
聲音是由一大堆的取樣點
一大堆的Sample所構成的
看這個投影片上的動畫
如果你遠看一段聲音
看起來就是一堆訊號
你把他拉近了看
拉近了看一直到1ms的scale
就會看到聲音是一個一個點所構成的
那聲音由
一秒鐘的聲音由多少的點所構成
這個就叫做取樣率Sampling Rate
那比如說16K的Sampling Rate
指的就是每一秒有一萬六千個點
那每一秒一萬六千個點
其實不算是特別多
如果你今天要讓聲音聽起來好聽的話
往往你至少要有22K或者是44K左右
也就是每一秒要有兩萬多個點
或者是四萬多個點
人聽起來才會覺得這個聲音
這個音樂
或這段聲音
這段人聲
聽起來是悅耳的
那每一個點可以有多少數值呢
這就取決於一個叫取樣解析度的東西
取樣解析度越高
那每一個sample的點
它可以有的數值就越多
所以可以瞭解說這個影像是由pixel所構成的
聲音是由取樣點所構成的
所以我們來看一下生成式人工智慧的本質
生成式人工智慧本質上要做的事情
就是給一個條件
它的英文叫做condition
然後呢
生成式人工智慧就要做的事情
就是把基本的單位用正確的排序
把它組合起來
這個就是生成式AI它要做的事情
舉例來說如果是大家最熟悉的
可以對話的AI
那他要做的事情就是讀一個句子
輸出一串token
那這個就是你看到的ChatGPT的答案
如果是繪圖的AI
他也是讀一個句子
這個句子可能是對要生成的那張圖片的描述
他就生出一堆的pixel
組合成一張看到的圖片
這是繪圖AI做的事情
那如果是聲音生成的AI
不管是語音合成
跟他講一句話
他會給你一個回應
那像前幾天禮拜一半夜大家看到的GPT 4.0
它也可以做很好的語音的生成
給它一個主題或給它歌詞
它就可以把一首歌唱出來
那實際上做的事情就是產生了一大堆的取樣點
這些取樣點合起來就是一首歌
所以你知道不同的生成式人工智慧的應用
它背後本質上要解的問題其實是同一個
就是給一個condition
你要如何把
基本的單位
這些基本的單位可能是
token,如果你要生成的是文字的話
他就是token,如果你要生成的是
影像的話,他可能就是pixel
如果你要生成的是
語音、聲音的話
他可能就是sample
把這些基本單位組合起來
就變成
看到的生成是AI的輸出
我們其實已經知道對話的AI是怎麼做的了
我們知道對話的AI做的就是文字接龍
那這個文字接龍的生成方式有一個專有名詞
叫做Auto Regressive Generation
我們也都知道這個文字接龍的方式是怎麼生成的
實際上生成是AI語言模型做的事情
就是讀一個輸入,它每次產生一個Token
再把他產生的Token當作自己的輸入
然後再產生下一個Token以此類推
最後就產生一段完整的回答
或產生一篇完整的文章
而我們知道這種Auto Regressive的方式
這種文字接龍的方式
在文字上已經運作的非常的成功
不需要我多說
你都知道今天這一些文字的生成式AI
有多麼的厲害
另外一方面,其他類別的生成式AI呢?
如果是影像呢?
影像能不能夠用文字接龍的方式來生成呢?
但這邊我們可能就不該叫它文字接龍
而應該叫它像素接龍
因為我們現在的基本單位文字不是Token而是像素
影像能不能用像素接龍的方式來生成呢?
完全是可以的
那我記得在2016年機器學習的課堂上
我就已經示範過怎麼用像素接龍的方式
來讓機器產生寶可夢
那我把連結留在投影片上給大家參考
而在2020年的時候呢
OpenAI也出了一個影像版的GPT
我們都知道文字版的GPT就是做文字接龍
影像版的GPT就是做像素接龍
從這個投影片裡面也可以很明確的看到說
這個影像版的GPT它是怎麼運作的
它就是把圖片裡面一個一個的像素產生出來
然後就排出來把那些一個一個像素產生出來
合起來就變成一張圖片
那聲音呢?
聲音也可以用這個Sample接龍的方式來產生一段聲音
那在2016年的時候呢
那個時候Google就出了一個非常知名的模型
叫做WevNet
它就是用這種接龍的方式
把語音訊號裡面的Sample
一個一個的產生出來
然後變成一段聲音
那你從右邊的這個動畫裡面
你就可以看到說這個Sample
是一個一個被產生出來的
就跟語言模型做文字接龍是一樣的
只是現在語音的生成是Sample接龍
不是文字接龍
但是他背後的道理
其實聽起來好像是
差不多的
所以到這邊好像生成的策略
就講完了
你已經知道文字接龍怎麼做
那同樣的道理
好像也可以做像素接龍
好像也可以做sample接龍
你就可以合出圖片
你就可以合成語音
但是實際上你會發現
語音的生成、影像的生成幾乎都不是用接龍的方式所完成的
接下來我們就要來看看
為什麼影像的生成不是用接龍的方式所完成的呢?
我們來看看Auto-Regressive的Generation
這種接龍的方式的生成它本質上有什麼樣的限制
這種Auto-Regressive的生成方式它本質上的限制是什麼呢?
它本質上的限制是你每次產生一個Token
或你每次產生一個基本單位的時候
你只能夠按部就班循序漸進
按照某一種順序來生成
對於語言模型來說
給它一個輸入
它得先產生出第一個Token之後
把第一個Token當作輸入
它才能產生第二個Token
產生完第二個Token之後
把第二個Token當作輸入
才能夠產生第三個Token
這是Autoregressive這種接龍的方式
在生成的時候它本質上的限制
那這樣子的限制會造成什麼樣的影響呢
我們來想想看如果要用同樣的方法來生產
來產生一張圖片
比如說解析度1024x1024的圖片
那會發生什麼樣的問題
一個解析度是1024x1024的圖片
它裡面有多少個像素呢
大約一百萬個像素
所以你要產生一張解析度
一千乘一千的圖片
你需要做一百萬次的像素接龍
才能夠產生出來
而一百萬次是什麼樣的量級呢
我告訴你這個紅樓夢啊
大概是九十幾萬字
也就是當你今天
要叫機器用文字接龍
用像素接龍的方式
產生一張解析度1024x1024的圖片的時候
每次他要做的事情都等於是要寫一部《紅樓夢》
你看你現在叫語言模型生一段文章
如果文章稍微長一點你都要等好久
讓他每次生一張圖片都要寫一部《紅樓夢》
那真的是要等到天荒地老
語音也有一模一樣的問題
如果你今天要產生取樣率22K的語音
就生個一分鐘就好
一秒鐘有22K有2萬個多個sample
一分鐘就有超過100萬
有大約132萬個sample
你要生成一分鐘的語音
要做132萬次的接龍
才能夠產生一分鐘的語音
所以這個時間所需要的時間
可能會是非常長的
所以那個WavNet剛出來的時候
人們驚嘆於 哇 今天做sample接龍
可以產生這麼高清這個品質這麼好的聲音訊號
但人們很快就發現他的問題
生一段聲音訊號往往要個一個小時以上
講一句話要一個小時
這個完全是沒有辦法接受
所以Auto Regressive這種方式
如果要拿它來生圖片 拿它來生語音
你會碰到非常大的障礙
怎麼辦呢 我們得換一個思路
Autoregressive最大的問題就是他需要按部就班
一個一個的把基本單位生出來
我們能不能不要按部就班
一次就把所有的基本單位同時生出來呢
我們就告訴語言模型說
這個是你的輸入
現在我們要生出輸出的第一個位置的東西
我們要生出輸出的第二個位置的東西
我們要生出輸出的第三個位置的東西
叫他把位置123的輸出同時一次生成出來
這樣的好處就是
因為所有的結果所有的基本單位都是同時生成出來的
如果你今天你手上的這個機器平行運算的能力夠強
就可以一次生出所有的基本單位
就可以大幅的加快生成的過程
當然如果是就真正需要做的運算量而言
上面這個auto-regressive的方法跟下面這個non-auto-regressive generation的方法
non-auto-regressive generation 他們有縮寫成NAR
上面這個AR的方法跟NAR的方法
在運算量上面可能沒有本質上的差異
但是就時間而言會有非常大的差異
在你平行運算的能力夠強的情況下
用NAR non-auto-regressive的方法可以大幅的加速
如果用影像生成來看的話
Auto-regressive的方法
就是影像裡面的像素是一個一個被產生出來的
就算我們現在只是要產生4x4的非常低解析度的圖片
你也是要等一下子的
但是如果是non-auto-regressive的model
所有的像素是同時被生成出來的
就可以大幅加快生成的速度
這就是為什麼今天影像的生成
其實都是採取Non-Auto-Regressive的方式
而不是Auto-Regressive的方式
而不是像素接龍的方式
其實啊
文字也完全可以用
Non-Auto-Regressive的方式來生成
因為雖然文字的生成
今天用Auto-Regressive的方式
可能也已經夠快了
但是很多時候
你想要GPT長篇大論的時候
也可能是要等上一段時間的
也許我們可以採取non-auto-regressive的方式來加快語言模型生成的速度
你怎麼把non-auto-regressive的方式直接套用到語言模型的生成上面呢?
這邊有不同的方法,一個可能的方法是說
先讓你的語言模型預測說它現在要回答幾個token
給一個輸入,它先預測說如果我要產生答案的話
那我要產生三個token
然後接下來呢就跟你的語言模型講說
這是輸入
我們產生位置1位置2跟位置3的token
叫他一次把三個token都生出來
這是一種可能的non-autoregressive的方式
那如果是影像的話就比較不會有這個問題
你知道影像生成的時候
那生出來的圖片大小通常是
事先就決定好的
這就不需要特別去預測說
現在生成式AI要產生多少的token
你可能在一開始就已經定好說
反正我就是都生512x512的圖片
或1024x1024的圖片
那還有另外一個把non-autoregressive
套到文字生成上的方法
就是反正就是讓生成式AI
讓你的語言模型產生固定長度的東西
你就告訴他說不管什麼input
你就是把位置1到4的輸出輸出出來
那你實際上在做的時候可能會更長啦
叫他把位置1到位置1000的結果
通通一次都輸出出來
然後再看說在生成的結果裡面
哪邊有出現END這個符號
那有出現END的符號之後的Token就都丟掉
只把END的符號之前的Token當作是給人看的答案
這是另外一種把Non-Auto-Regressive套用到文字上的方法
而如果你想要知道更多怎麼把Non-Auto-Regressive的Model套用到文字生成上的話
那我在右下角留了一篇Overview的Paper給大家參考
那Non-Auto-Regressive既然這麼好
這麼可以加快速度
其實也可以套用在文字上
那為什麼文字不採用
打 non-autoregressive 的方式呢
為什麼文字生成主流的方法
都採用 autoregressive
而不是 non-autoregressive 的方式呢
這就要回到 non-autoregressive
另外一方面它本質上的問題了
non-autoregressive 的生成
它常常會遇到的問題就是
生成出來的東西品質不好
為什麼 non-autoregressive
容易生成出不好的品質不好的結果呢
因為生成這個問題
是需要AI自行做一些腦補的
當你給定的輸入條件就算是一樣的
AI仍然有很多不同可輸出的可能性
舉例來說假設你現在的輸入是
假設你現在要做影像生成
那你跟這個繪圖的AI說
我現在要畫一隻在奔跑的狗
但是一隻在奔跑的狗
它實際上長什麼樣子是有很多不同的可能性的
那你告訴生成式AI說
要畫一隻在奔跑的狗
現在產生位置1的像素
然後產生一隻在奔跑的狗
現在產生位置2的像素
那整張圖片裡面如果是1024x1024的圖片的話
就總共有1024x1024的位置
但它們可以平行被算出來
但是產生位置1跟位置2的像素的時候
這個你的模型是獨立各自生成的
會不會在生成不同位置的像素的時候
模型的想法是不一樣的呢
也許當他畫第一個像素的時候
他心裡想的是
我要畫一隻向左奔跑的白狗
第二個位置的像素的時候突然說
他想要畫一個向右奔跑的黑狗
畫向左跑的白狗也沒問題
畫向右跑的黑狗也沒問題
但兩個同時畫
這張圖片裡面有些像素是想要畫白狗
有些像素是想要畫黑狗
全部加起來 可能就會產生一個似不像的東西
所以這種non-auto-regressive的model
過去很常遇到的問題就是
你發現它生成的結果品質沒那麼好
當然因為這個模型
不管是在位置1的時候還是位置2的時候
它其實是同一個模型
所以也許它生成出來的結果
也沒有那麼大的差異
因為畢竟是同一個模型
要腦補可能也是腦補出差不多的東西
但你沒有辦法保證
它兩次在做這個獨立的生成的時候
他想的一定是一模一樣的
這就是為什麼在早年用non-autoregressive的這種模型
用non-autoregressive generation來生圖的時候
你往往發現你產生出來的圖
就是糊成一團
因為模型他心裡想要畫很多很多不一樣的東西
然後全部加在一起
那你看到的就是一張非常模糊的影像
所以這是non-autoregressive generation
他本質上遇到的問題
這就是為什麼文字上
根本就不想採取 non-autoregressive 這樣子的策略
那這個問題呢
叫做 multi-modality 的問題
那我知道說通常講 multi-modality
人們以為通常比較常指的是
這個一個模型可以同時吃各種不同模態的東西
比如說語音、文字還有圖片
但是這個問題就是 non-autoregressive
在生成過程中
有時候突然想畫 A 東西
有時候突然想畫 B 東西
這個問題也叫做 multi-modality 的問題
那這邊詞彙是用的有一點混淆的
好那我們來更具體的看一下
假設語言模型採取
non-autoregressive的策略來做生成的時候
會遇到什麼樣的問題
我們現在想像語言模型的輸入
是李宏毅是這幾個字
那我們要它產生
我們現在做non-autoregressive的生成
所以我們要它產生位置1
也就是李宏毅是
下一個可以接的符號
跟下下個可以接的符號
那我們都知道說語言模型的訓練
是需要訓練資料的
那他可能在pre-train的階段
是從網路上爬了大量的資料
那你知道網路上你可以找得到的李宏毅資訊
他指的是好幾個不同的人
那其實資訊最多的是一個身為演員的李宏毅
所以語言模型他在訓練的時候看過的資料有
李宏毅是演員
李宏毅是演藝圈的
李宏毅演過變形劑
也有一些是教授的李宏毅
但是次數可能找到的資料是比較少的
所以對語言模型來說
李鴻義是下一個位置
到底可以接什麼樣的字呢
也許他統計出來的分佈是
演的機率是比較高的
然後叫的機率是稍微低一點的
那如果是第二個位置呢
李宏毅是這個句子
下下個符號
可以接哪一個token呢
根據他的訓練資料
可能是可以接元、可以接億、可以接過
但是也可以接受,看起來受的機率
在根據這個例子裡面
統計出來的機率可能還稍微高一點
所以李宏毅是下一個token
接演的機率是最高的
李宏毅是下下個token
接受的機率是最高的
那你就有可能在sample的時候
第一個位置sample出來是演
第二個位置sample出來是受
那這就是一個不好的答案
你不管接李宏毅是演員還是接李宏毅是教授
都能夠算是一個正確的答案
但是接出來如果是李宏毅是演獸就不知道是什麼東西
這是non-autoregressive會遇到的問題
那autoregressive的模型難道就不會有類似的問題嗎
還真就不會有
我們來看看如果是autoregressive的模型的話
它是怎麼運作的
根據我們的訓練資料
李宏毅是後面比較常出現眼這個符號
叫這個符號的機率低一點
所以做Auto Regressive的生成
先生成第一個Token
那你可能Sample出來就是眼
把眼貼到李宏毅是後面
所以語言模型現在的輸入變成李鴻義士眼
然後看看要接哪一個符號
那如果根據訓練資料的話
你就不需要考慮授的可能性了
因為我們只考慮李宏毅是演後面可以接的符號
那可以接圓、可以接異、可以接過
來統計出來可能圓跟異跟過的機率都差不多
那在做sample的時候
你就會sample出一個正確的結果
圓跟圓加起來就是一個合理的句子
所以知道說如果是autoregressive
它就比較不會有multimodality這種問題
它就不會有同時想講A
同時又想講B
合在一起變成四不像的這個問題
這就是為什麼今天文字往往採取的是
auto-regressive的生成方式
好
好但是
好吧所以我們現在知道說這個non-auto-regressive
有它本質上的問題
但是對於影像對於語音來說
我們好像不得不用
non-auto-regressive的方式來生成
因為auto-regressive的方式來生成圖片
那時間實在是太長了
所以怎麼辦
所以在影像生成的領域
就需要發明一些新的想法
來想辦法克服non-autoregressive在生成的時候
可能造成的問題
那怎麼克服non-autoregressive生成可能造成的問題呢
我們剛才想說non-autoregressive在生成的時候
他會遇到的問題就是
每個人生成是AI在不同位置
他想要腦補的東西不太一樣
那我們能不能一開始就把要腦補的東西講清楚
把要腦補的東西訂好
讓生成式AI在整個生成的過程中
不管哪一個位置
腦補出來的東西都是一樣的呢
當今天輸入只有說
要畫一隻在奔跑的狗的時候
我們能不能另外把資訊補齊
說這個狗就是
他是向左的
他是哈士奇
他的背景就是草原
把所有需要腦補的東西都訂好
輸入給生成式AI
那這樣每一個位置腦補的東西都是一樣的
也許畫出來的圖就會是一致的
所以這是一個可以解決
non-autoregressive腦補
但是出錯的問題的一個思路
好 那實際上是怎麼做的呢
實際上是怎麼把這個腦補的內容
加到生成式AI裡面的呢
我們之後講到影像生成模型的時候
會講得更清楚
那我知道有很多知名的影像生成模型
也許你是在其他地方有聽過的
比如說VAE
比如說GAN
比如說Flow-based model
比如說Diffusion model
那你會發現說這些模型
都不約而同有一個一樣的設計
他們一樣的設計是什麼呢
雖然他們這些是很不一樣的模型
但是發現他們是有共通性的
他們共通的地方就是
你每次在生成之前
就是你要先隨機的生出一個向量
你要先隨機的生出一排數字
而你的生成模型不是只看輸入去決定它的輸出
所以你發現影像生成的模型跟文字生成的模型是不一樣的
文字生成的模型你並不需要給它
你就只要給它你現在輸入的句子就好
你不用再給它別的東西
那你發現影像生成的模型
除了你輸入的這個condition
如果是文字生圖的話
就是你輸入的對圖片的敘述之外
他還需要隨機的產生一個向量
把這個向量丟給你的生成模型
你的生成模型在生成的時候
不是隻考慮人輸入的文字
同時也要考慮這一個向量的內容
那這個向量的作用
其實就是幫模型腦補
告訴他腦補的內容是這些
等一下在生成的時候
不管是哪一個位置的像素
我們一起腦補一樣的東西
整張圖片的內容才會是一致的
那你發現各式各樣不同的影像生成模型
都有非常都有這樣一樣的設計
都有採用這個先生成一個向量
再丟給生成式模型的設計
那我們這個下週會講得更清楚
好那還有什麼其他的方法
來這個還有什麼其他的方法
來補足Non-Auto-Regressive Model的短處呢
那我們剛才已經看到
Auto-Regressive Model跟Non-Auto-Regressive Model
它們各有所長 各有所短
所以也許我們可以把這兩個方法結合起來
怎麼把這兩個方法結合起來呢
Auto-Regressive Model它的壞處就是
它沒辦法生成很精細的東西
所以也許我們先用Auto-Regressive Model
先產生一個精簡的版本
那 non-autoregressive model
他的壞處就是他會腦補
他腦補的時候如果腦補不一致就慘了
但沒關係
autoregressive model
先產生一個精簡的版本
讓 non-autoregressive model
根據精簡的版本再產生精細的版本
因為精簡的版本已經把大框架都定好了
如果腦補的話沒有太多可以腦補的東西
也許 autoregressive model
加 non-autoregressive model
就可以產生更好的結果
所以你可以先用一個AutoRegressive的模型
給定條件
那我們這邊就沒有特別說是哪一種生成的
這邊可能是影像生成
給一段文字產生一張圖
它也可能是音樂生成
給一段歌詞
然後生一首歌出來
你知道這些不同的生成
它背後的這個道理都可以是共通的
所以我們現在就不用特別講
是哪一種生成的
我們先用AutoRegressive的模型
根據條件先產生一個精簡的版本
因為是精簡的版本
所以Auto-regressive可以很快的把它生成出來
接下來呢
再用一個Non-Auto-regressive的model
根據精簡的版本產生精細的版本
因為精簡的版本已經把
比如說如果是生圖的話
已經把圖片的草稿打好了
所以如果說狗要往左邊跑還是右邊跑
他是白色還是黑色
通通先定好
沒有什麼可以腦補的空間了
你只是把一些細節填上去而已
那就可以讓 non-auto-regressive 用比較快的速度把細節填上去
接下來你可能會問的問題是
那這個精簡的版本是什麼樣子的呢
這個精簡的版本其實不需要是一個人類可以看得懂的東西
它其實只需要是 non-auto-regressive 的 model
看得懂就可以了
那怎麼產生這個精簡的版本呢
有一個常見的做法是這個樣子的
你先拿一個 encoder
那這個 encoder 其實是一個類神經網路
總之它可以做的事情
就是把一張圖片進行壓縮
變成一個壓縮後的結果
那這個壓縮後的結果
其實人沒辦法看
人看不懂的
但沒關係
反正 decoder 可以看得懂
decoder 就看這個壓縮後的結果
它可以把它還原成原來的圖片
那這個壓縮的結果可以壓縮到什麼地步呢
我這邊是引用了一篇文獻
他用的這邊應該是一個比較standard的做法
那通常可以壓縮的程度
是你可以把本來16x16這樣子的範圍
壓縮成用一個點來表示就好
所以大概可以把本來16x16
就是256個像素256個位置
變成只有一個位置
但在另外一方面,它其實也對可以做的選項做了非常大的壓縮
做了非常大的取捨
我們本來說如果你的圖片是那種全綵色的話
每一個像素的位置有上千萬個顏色可以選擇
但你真的需要這麼多顏色來選擇嗎?
所以壓縮過後,每一個位置可能只有比如說一千個顏色可以選擇
所以你可以找到一個Encoder
把圖片壓縮成一個比較精簡的版本
這個精簡的版本人看不懂沒關係
Decoder看得懂
可以還原成圖片就可以了
那這個Encoder
這個Decoder是怎麼來的呢
這個Encoder跟Decoder
它也可以是類神經網路
它可以是
透過訓練得到的
它不需要是人定的規則
不需要人來設計
發明這個Encoder Decoder
讓機器自己學出這個Encoder、Decoder
它的基本概念就是收集一大堆的圖片
然後叫Encoder把這些圖片變成壓縮的版本
然後Decoder再把壓縮的版本解回來
那希望壓縮前跟壓縮後
那個圖片要越接近越好
那Encoder跟Decoder它們都是類神經網路
那你就可以透過機器學習的方式
去找出Encoder跟Decoder的參數
讓Encoder壓縮之後
可以被Decoder還原回一模一樣的圖片
那這整個概念呢
就叫做AutoEncoder
那如果AutoEncoder這個部分
你沒有聽得很懂的話
也沒有關係
反正你就是記得有一些方法
可以找到一個Encoder把圖片壓縮
然後有一些方法可以找到一個Decoder
把圖片還原成原來的樣子
好我們現在能夠把圖片壓縮以後
那接下來能幹嘛
接下來,AutoRegressive的model就只需要產生壓縮的版本就好
AutoRegressive的model它只要專注去學習怎麼產生這個壓縮的版本就好了
你給它一段文字,它產生出來的是人看不懂的這個壓縮的版本
這個壓縮的版本雖然人看不懂,不過沒關係
AutoRegressive的model生成出來之後,你把它丟給Decoder
Decoder其實也是一個non-autoregressive的model
它可以把壓縮的版本還原成人看得懂的圖片
所以這就是一個autoregressive跟non-autoregressive結合的例子
那有人可能會想說
剛才講說壓縮的幅度好像也沒有說真的非常大
我們說可以把16x16的範圍壓成只要一個pixel來表示
本來16x16的pixel變成只用一個pixel來表示
那這樣你只是把它變成原來的256分之一啊
如果今天是1024x1024的圖片
本來要升個一百萬次
但是你壓縮之後呢
你可能還是需要生個四千多次
才能夠產生圖片
生四千多個token也是蠻花時間的
等一下告訴你還有更厲害的大招
可以再更縮短Auto Regressive這個段落
這個步驟
生成的時間
這邊是要講語音的部分
語音的部分跟影像
也可以採取非常類似的策略
在語音上
你也可以找一個Encoder
把聲音訊號進行壓縮
產生壓縮的版本
然後壓縮的版本可以再解回來
至於要怎麼壓縮呢
用什麼樣的方法可以訓練出這些Encoder呢
你可以參見我的學生吳海斌同學寫的一篇overview paper
那你可以在裡面知道最近的這些語音上面的壓縮的方法
總之就是有一系列的方法
可以把本來非常複雜的每一秒鐘
都有好幾萬個點的聲音訊號
把它進行壓縮變成比較簡單的版本
那產生這種比較簡單的版本之後呢
你要做語音合成的話
你就不需要從文字直接合這個很複雜的聲音訊號了
你是用一個 auto-regressive 的 model 去產生壓縮的版本
那這個壓縮的版本呢
通過 decoder 然後再還原出複雜的聲音訊號
auto-regressive model 只要生成比較簡單的東西
decoder 是一個 non-auto-regressive 的 model
可以做快速的生成
那兩者呢
就可以結合進行截長補短
另外一個克服non-autoregressive品質問題的方法就是多次使用non-autoregressive的生成
你把整個生成的步驟拆解成很多階段
每一個階段都用non-autoregressive的model來生成
一開始輸入一個條件,先生成第一個版本
再根據第一個版本生成第二個版本
再根據第二個版本生出第三個版本
以此類推,也許弄個一兩百步,然後生出最後一個版本,就是你要的結果
那這邊每一個生出來的版本,也許跟前一個版本的差異都是比較小的
那就可以避免腦補的時候,腦補不一致的問題
那也許這樣講會讓你覺得非常的抽象,什麼叫第一個版本、第二個版本
這邊舉一個具體的例子,那這個其實在文獻上已經是非常廣泛被採用的方式
那我這邊引用的兩篇論文
一篇是NVIDIA的Progressive Network
這個是17年的時候就引用的
非常古老的技術
另外一篇22年引用的
是Google的一個影像生成的模型
所謂的不同的版本在這些paper裡面
其實就是不同大小的圖片
所以第一個版本是先生個4x4的圖片
第二個版本再生成個8x8的圖片
到最後一個版本生出1024x1024的圖片
那因為每一個版本之間
它的差距是很小的
每一個版本都是根據前一個版本所生成的
所以腦補的可能性就比較有限
所以你其實可以用多次Auto-Regressive
來克服Auto-Regressive Model可能腦補不一致的問題
那其實當你做多次Non-Auto-Regressive Model的時候
你其實就可以想成
其實這也是另外一種層級的Auto Regressive的Generation
只是拆解的方法變得不一樣
在原來最原始的Auto Regressive裡面
你每次只生出一個基本的單位
如果基本單位多
你生出來的就會很痛苦
你就會花非常長的時間來生一張圖片
那現在我們先定好每一個版本要生什麼樣的東西
那每一個版本都是基於前一個版本所生成的
這也可以算是一種Auto-Regressive的Generation
但是我們現在需要Auto-Regressive的步數變少了
從4x4一直到1024x1024
它的步數是沒有非常多的
我們把Auto-Regressive的步數變少了
加快生成的速度
所以這也是一種Auto-Regressive
只是原來的Auto-Regressive
每次只生成一個東西
只生成一個Unit
比如說只生成一個像素
但是我們可以定好生成的流程、定好生成的Pipeline
讓每一次生成的時候都是比較多的東西
這是一種做法
那這個不同版本間的分別
不一定是要從小圖到大圖
也可以有其他的方法來制定版本間的差異
比如說現在一個常見的方法
是從有雜訊到沒有雜訊
第一個版本是近乎通通都是雜訊的圖
第二個版本的雜訊比第一個版本少一點
然後一直到最後一個版本
就是乾淨的圖
從有雜訊的圖一直升到沒有雜訊的圖
這個過程就是鼎鼎大名的Diffusion Model
像很多同學可能都知道說
現在影像生成主要都是用Diffusion Model這個技術
那之後我們會講得更清楚
但是就概念上你現在就知道
什麼是Diffusion Model
它其實就是把生成的過程
拆解成要生成不同版本的東西
每一個版本都比前一個版本
每次後面的版本都比前一個版本
更加的清晰
他可以看作是一種
non-autoregressive跟autoregressive的結合
好那還有什麼其他的這個
版本的拆解方法呢
還有一個方法是
我們每次呢
都把前一個版本不好的地方塗掉
然後再生出下一個版本
這邊也是引用一篇論文
他做法是這個樣子的
這個是第一版生成的結果
先用non-autoregressive畫一張圖出來
non-autoregressive就是很
如果不好好處理沒有什麼特別技術的話
就是會畫出這種東西
他想要畫一隻
火鶴啦
non-autoregressive model感覺就是想畫很多隻火鶴
全部都拼在一起
不知道想畫些什麼
也許只有這個地方算是勉強畫好的
好像畫了一隻眼睛出來
所以就把生不好的地方塗掉
在這個圖上呢
這個淺色的地方代表被塗掉
深色的地方代表這個地方畫得還可以
就把它保留下來
所以就把這個淺色的地方都塗掉
這是第一個版本
然後再重新生成
重新生成的時候
就根據第一個版本
已經生成的地方再去生成
沒有被塗掉的地方
再去生成第二個版本
也許就稍微好一點
看出來這邊好像有一個鳥的頭
也許我們可以把鳥的頭這邊保留下來
其他地方塗掉 繼續做生成
最後就有可能生成一張完整的圖片
那這邊可能的一個難點是
那我們怎麼知道哪邊生不好呢
這個塗掉的這個過程必須要自動的
那不是人去塗的 那是自動塗掉的
怎麼知道哪一個地方生不好呢
那你自己 你可以這邊就引用一篇論文
你可以看一下那個編論的文獻
有一些方法自動的去決定說
這個地方覺得很有可能是生不好的
就可以自動的把它塗掉
所以這是另外一種版本拆解的方法
我們就講了好幾個不同的版本拆解的可能性
我們現在知道說
我們也可以用
把一連串non-auto-regressive的生成
當成auto-regressive的生成
來克服non-auto-regressive本身的侷限
好 那我們剛才講到說
我們可以把auto-regressive跟non-auto-regressive結合起來
但是我們又怕前面這段auto-regressive
它的生成還是太花時間了
叫你生個比較小的圖片
比如說256x256的圖片
其實生起來還是挺花時間的
所以怎麼辦呢
可以把前面這一段
Auto-regressive的生成
直接置換成一串
Non-Auto-regressive的生成
比如說這一段可能本來需要
256x256次的生成
也許換成Non-Auto-regressive
只要10次就夠了
然後所以現在生成
Non-Auto-regressive model
生成的每一個版本
它可以不要是圖片,可以是壓縮以後的結果
所以你可以讓non-autoregressive model先生成壓縮的第一版
然後再生成壓縮的第二版,一直到生成壓縮的第二版
再把壓縮的第二版丟給decoder,然後就生成最終的圖片
那事實上今天你會用到的那一些常見的影像生成的模型
大概就是這麼做
比如說MIdJourney、StableDiffusion、新版的DALLE
其實就是用類似這樣的做法做出來的
就是透過一堆Non-Auto-Regressive的生成
產生一個人看不懂的壓縮的版本
再把壓縮的版本丟到一個事先準備好的Decoder
生出最終的圖片
那像MeJourney它會把這個生成的過程展示給你看
MeJourney那個模型,你在用它的時候
如果你有用過MeJourney的話
你發現你輸入一段文字以後
它就會開始生圖
那它會第一次先生出一個非常模糊的東西
然後再慢慢的越來越清楚
越來越清楚
其實這個越來越清楚的過程
就是每一次使用
non-autoregressive model生成的過程
MidJourney會做的事情
是它會把每一個版本的輸出
都過decoder
然後給你看看
現在這個壓縮的東西過 decoder 以後產生出來的結果是什麼樣子的
那這邊是實際上我今天早上拿 Midjourney 生出來的結果啦
我跟他說產生一個教室
然後呢第一個版本根本不知道在畫什麼
然後再下一個版本你就可以看到這是一個教室
不過看起來不是很精細
然後到最後一個版本就可以產生一個非常精細的
裡面有各式各樣複雜東西的教室
所以你知道說現在這個影像生成的模型
通常都是用這種方式所進行影像生成的
好那這邊是
這一份投影片的小節
那下面還有下一段
這邊的小節是
我們現在跟大家介紹了
Auto-regressive的model
跟Non-auto-regressive的model
這兩個方法各有所長
各有所短
Auto-regressive的特性就是按部就班
那它的厲害的地方是它的品質比較好
但是它的速度比較慢
那non-autoregressive它的特性就是齊頭並進
生成的結果是一次到位的
所以它厲害的地方是
它生成的速度很快
但是往往會有生成品質的問題
那它比較常用在non-autoregressive生成
比較常用在影像上面
那我們最後也講說
autoregressive跟non-autoregressive這兩個方法
各有所長各有所短
所以可以把它們結合起來
而今天最好的影像生成的生成式AI
都是用AutoRegress跟NotAutoRegress兩種方法結合起來所生成的結果
