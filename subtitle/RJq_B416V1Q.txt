hi everyone my name is soon young
and the paper is co-authored by nddu
and professor hongi v the title is
understanding self-attention of
self-supervised
audio transformers here is our outline
i'm going to first give a background of
our work
and then introduce our methods and
finally the conclusion
self-supervised audio transformer sat
is about adapting an eop bird into
speech domain
bird is composed of several transformer
blocks
and originally be trend on huge amount
of text by a
mass clinking model lows speech birth
changes the input from text to speech
signal while plenty of works have been
done to study the internal function of
birth
it is still an explorer how
self-attention
work for speech data
here are some recently proposed methods
for
speech representation learning starting
from two years ago
some key differences are pointed out
which are the input data formats and
then the main architecture
excluding the feature extraction part
and the pre-training loads
as we can see there is a clear emergence
of adopting self-attention
by the grey squares and many of them are
pretend with the continuous factors
and reconstruction those so we focus on
these
settings to understand the functionality
of
self-attention on speech data
specifically we follow machine g
which is open source marking j
extracts the acoustic features from web
form
and then down sampling by some ratio
during the mask language model
pre-training
a range of features are masked
specified by the pre-training mask
length
after propagating through birds
representations
are linearly projected and compared
to the linear spectrogram with a onenose
after pre-training representations are
used on downstream tasks some tasks
are frame label where each acoustic
frame has
a label like phoneme classification
some tasks are all translatable the
entire o trans
gets a single label like speaker
recognition
and next we go on for the analysis
we start by visualizing self-attention
the core of birth architecture is the
transformer block
which is composed of multiple
self-attention hats
and blocks are cascaded
here we will focus on a single attention
head
for easier discussion
feeding the output sequence of the
previous transformer block
we get output representation after
self-tension
each single output is pretty much formed
by
the weighted sum of input features
where those words depend on the relation
between the query factor
and other key factors which is learned
by self-attention mechanism
in fact attention's weights are
normalized
as a distribution and each query factor
has its own attention distribution
we can visualize a hat by an attention
map
which summarizes the attention
distribution for all queries
in different time stamps
so there are majorly three categories
global vertical
and diagonal intuitively
diagonal tensions attend to neighbor
frames
on each query vertical tensions
attend to some fixed location
independent of query in an old trend
and global attention behaves randomly
all attention maps are visualized in our
demo page
some diagonal attentions are highly
focused
they are easily interpretable since
neighbor frames are in no job
helpful for reconstruction because
speech signal is continuously varying
interestingly they typically avoid the
main diagonal
either shift to left or right
and as we increase the pre-training mask
length
we obtain a larger shift
this suggests that attentions are aware
of prejudice masculine
and avoid the masked features which are
not helpful
for reconstruction some diagonal
tensions are
blocked diagonal we find that they
learned phoneme boundaries and we
verified this
with phoneme segmentation
using timid data set we have ground
truth phoneme boundaries
labeled by the professional boundaries
are symmetric on x and y
axis since yellow
and green frames are located at the same
phoneme interval all frames in this
interval
can be helpful for both of them
to reconstruct and their attention
distribution can therefore
be very similar resulting in the block
structure
and we extract the edges of
blocks to examine their correlation to
true funding boundaries
we follow the segmentation algorithm
proposed by
bati for both mfcc and
attention distributions then we evaluate
the segmentation
by precision recall and r value
which is a dedicated metric for phoneme
segmentation
and both results show that
the block diagonal tensions indeed learn
the phoneme boundaries
but it is a strong feature
compared to the widely used mfcc
under the same algorithm
and next we understand the vertical and
global attentions
by phoning relation map
the map summarize ahead us
given an input phone which phone
will the head attend to
so to find out the preference of
b attending to n we interact
over all oo transits and sum up
attention weights
whose query frame belongs to b
and key frame belongs to n for example
there's one attention weight belongs to
b
attending to n in o trans x
and four in o trans y
we sum them up as the preference
of b attending to end of this hat
after careful normalization and plotting
out
the preference of all possible phoneme
relations
we obtained a phoneme relation map
where the bright color represents
preference and the dark color represents
the
opposite so some operations can be
observed
as for vertical hats there's an attempt
there's a tendency for focusing on
specific phonemes
or neglecting them for all queries
the first hat attends to silence
and the second hand always neglects
silence
the third head attends to some phones
which are
articulated very similarly according to
the ipa
table particularly they are articulated
at the same place with slightly
different
manners and the last head neglects the
phone
as for global attentions
the first half represents that silence
attempts to silence and ignores the
others
and the speech part ignores silence and
attends
to speech the second hat
represents the opposite that silence
ignores silence
and attends to speech while speech
attends
to silence the third hat represents
attending to identity which are
frames belonging to the same phoneme as
the query
the last head represents the opposite
not attending to
identity as the negation all funding
relation maps are visualized
in our demo page given the
different functionality of three
categories we wish to answer
which one is more important hence
we quantify a tension head with three
metrics
globalness verticality and diagonality
since the detailed mathematical
definitions of metrics
are hard to follow we will directly
visualize the attention maps
ranked by these three metrics
and one can refer to our paper for the
definitions
here are the top 18 hats according to
the diagonality
and the top 18 heads for verticality
the top editing has for globalness
so we ranked the importance by
cumulative printing
in the beginning representations are
used to
trend a small model for downstream tasks
said phoneme classification and we get
an accuracy
of about 70 for example
if we print an attention head by setting
attention weights to all zeros
representations are corrupted and we
expect
performance drop when used on downstream
tasks
when more heads are pruned performance
drops
more severely moreover
we expect that if we prune
an important hat first the performance
drop
should be slower compared to
the pruning important hat first
so the question becomes which matrix
because the pruning curve
dropped faster we conduct
two downstream tasks to evaluate
representations
phoneme classification and speaker
recognition
both are trend on library speech trend
100 clean subset
and test on test clean subset
funding labels are obtained from
montreal's first aligner
phoneme classification is conducted at
frame label
with one hidden layer dna and speaker
recognition is conducted
at frame and old translable typically
we only need to recognize speakers given
the
entire ultras however we are also
interested
in how much of speaker information can
be encoded
in each single frame given that we
pre-trend
marking j by reconstruction laws which
prefers
encoding as many details as possible
and the self-attention mechanism can
entangle the whole sequence of
information
into a single frame the model of
o-translator uses mean pooling followed
by
one linear layer here is the pruning
results
lines with dots are conducted at frame
level
and lines with crosses are in all
translable
first diagonal tension is the most
important
pruning them
has significant impact on both tasks
second both vertical and global
attention have little contribution to
volume classification
even more pruning them can help the
performance
for vertical tension combining with
previous findings
this might due to that vertical heads
can entangle
these 10 phones into the current query
making downstream classifier hard to
distinguish the original phoneme signal
of raw inputs if we look at
speaker recognition we found vertical
attentions
have huge impact on speaker identity in
each frame
pruning them quickly hurt the
performance
in a more common old translator setting
we can even only consider diagonal
attention as essential
since speaker identity affected by
pruning can be
mostly recovered by simple mean pooling
finally we conclude that more than fifty
percent of
hats can be pruned and boost the
performance on funding classification
without
the sacrificing speaker identity in all
trans level
when we following the globalness metric
so in conclusion when adopting
self-attention on
continuous input with reconstruction
loads
self-attentions have the following
properties
funding phonetic information is handled
by diagonal attentions and they are
aware of either pretraining mask length
or phoneme boundaries vertical
attentions help speaker identity in each
frame
but harm phonetic information and there
are
two behaviors focus or neglect
specific phonemes some operations can be
observed in global attentions
but they have the least impact on
representation quality
and more than 50 percent of hats can be
pruned
following globalness metric and improved
performance
the end and thank you for listening here
are
our references
