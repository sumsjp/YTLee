baby baby oh baby oh baby baby hi
everyone my name is Jim DeYoung and
today I'm going to talk about our recent
work interrupted and cascade permutation
immersion training for speech separation
and the first question we would like to
ask is do we need P IT well first let us
take a look at the label ambiguity
problem at first we input a training
mixture x1 into the model and the motto
we are output to protection the blue and
the red rectum respectively into two
output channels then we will have two
possible corresponding label assignment
between the two model outputs and the
two ground truth which is the green one
and the brown one however we must choose
either one correspondence to update the
model parameter so here is how
permutation invariant training works we
will calculate both the cause of the
green and the brown one and choose the
minimum among men and in this case P I T
will choose the green one then we will
input another training sample x2 into
the model and this time P I T will maybe
choose the blue corresponding label
assignment and in every Airport this
process will be done for all training
sample and if we only take a look at the
chosen Krong truth for each Abu channel
like + 3 for opportunity and the
solitary for opportunity to this can be
seen us the cost ring of the ground
truth into 2 and that L P I T impressive
three groups of the ground troops into
two clusters so then here comes another
question go into the pre assignments of
the cost before training
then fix this assignment for all
training Airport so what would happen if
we can do the pre assignments of the
ground truth in the mixture and fix this
assignment during trending instead of
using P I T so what we're going to do is
to cluster of the ground truth in the
training data into two groups and the
first method we use is rather simple the
energy weight one where we assigned the
loader ground truth in the mixture to
capture one matching Abu 1001 and the
quieter two closer to matching the
opportunity and the second method is
based on the coloring of the speaker's
embedding where we extract the imbalance
of each ground truth in the mixture and
Krauser them into two groups then
compare those two methods to bi t where
we can view the assignment using p AI t
being dynamic and the two proposed here
being fixed then we can see from the
figure below that indicating the
performance of the validation set at
each training apple with p AI t in blue
and speaker embedding based in black and
the energy based in orange and from the
straight curve it can be easily seen
that either these two fixed assignment
methods cannot perform as well as p i--
t does so p i-- t seems necessary from
this viewpoint but is there any problem
using p i-- t well here is a common
phenomenon we have observed during p IT
training that is if we input a train a
mixture x the end at epoch I according
to P I T we might choose this assignment
however in the next egg park for the
exactly same train a mature X P I T
might choose the opposite assignment
then the next the choice made by P I T
my switches back again so the
assignments using P I T my switch back
and forth and this is even serious in
the early stage of the training that the
poor model output signals make the
choice of the assignment very random and
this inconsistency of the assignment may
lead to the inconsistency of the model
training so to verify this we have
recorded the label assignments which
which we defined as the difference of
the label assignments choices between
two consecutive upon and from the figure
below this black dot represent the
number of switches and we can see that
in the early stage of the training
before about forty the switches are
commonly launched and the blue dots are
the STR performance of the validation
data at each epoch and we can see from
the red circle that when the number of
switches is large the performance of the
validation data will drop accordingly
which verified that the switches could
cause damage for the model training and
here is our proposed approach data comes
up as similar to the before I mentioned
fix label assignment training Palestine
we get the fake assignment from the
training of a PID model in other words
we trend a model using p i-- t4l epoch
first and record the choices of the
label assignments for each training
mixture at the LZ path but instead of
using the trend model we initialize the
model parameters and trend the new model
with this fix label assignment for a 100
a pot then after the second stage
training we can continue to train this
model with p i-- t for another 100 a pop
then first we test a different error for
the second stage fixed assignment
training
where the L ranges from one ten twenty
to a hundred and H fixed assignment is
trend with one hundred
Epoque and the red thoughts indicates
the validation performance after
training for 100 Ewa where the motto
trend with l equals to one have the
validation SDR close to 16 DB and the
best model is trend with l equals to 80
with the SDR rate being seventeen point
six six DB and the blue dot indicates
the performance of the test data which
the trend is similar to the validation
data then we compare each models
performance with the PRT baseline
indicating us the red and the blue
horizontal line where the validation
performance is sixteen point one seven
TB and the testing performance is
fifteen point eight two TB and we can
see that only the model using the
assignment from l equals to one perform
worse than the p ip-based line even when
we get the fixed label from l equals to
10 which is the label assignment chosen
from the ten Sephirot
of p IT training the performance is a
lot better and the best performance we
have tested is from the label chosen
from our equal to 18 where the STR
performance on the test set is six
seventeen point three six DB which is a
one point five 4 DB boost compared to
the PRT baseline then we analysis the
number of different level assignments
compared to L equal to 18 and we can see
that a label Simon children from l
equals one have approximately 10%
difference from L equal to 80
which is why the performance isn't as
good as the others
and this is the overall training curve
of all-purpose methods which the curve
indicates the validation performance of
each Apple and the first stage of our
training is based on the original piti
where we only get the label assignments
from the stage for the training of the
second stage and after a training of the
second stage we can assume that this
model parameters is good enough for P IT
training since the concern for PRT is
that we don't have a good model
initialization and the label assignment
would be rendering them but here after
the fixed label assignment training the
model have already been optimized to her
a good direction then the third stage of
our methods is to continue to train the
model from the second stage OSBI t and
we can see that in this configuration
the performance on the validation theta
can be boost to seventeen point nine
ninety B at the end of the third stage
and we want to compare the label Simon
switches between the first stage and the
third stage since both stage use P I T
but with different initial model
parameters and the figure below
indicates the switches of the first HPI
t which is initialized with a random
model parameter and the figure at the
bottom right shows the switches of the
show stage P I T which is initialized
with a good model parameters and we can
see that the switches it's a lot lower
than the first stage and a lot smoother
and we can see the blue curve indicating
the validation STR is also better
so to conclude we have found a way to
obtain good fixed label assignment
through the previous P I T and this
method can be utilized on any separation
model and was fixed label training which
is our second stage the model
performance cannot perform PID and was
using further PID training which is our
third stage we can achieve a significant
lead better result with STR improvement
rate of 17.7 TB uncontested model and
thank you for your listening
[Music]
