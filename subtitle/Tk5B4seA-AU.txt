接下來我們要來講這個 Deep Auto-encoder
那什麽是 Auto-encoder 呢？
Auto-encoder 的想法是這樣
我們首先去找一個 encoder
那這個 encoder 你可以 input 一個東西
比如說我們現在這邊要做的是影像辨識的話
你要做是跟影像有關的東西的話
就 input 一張，假設我們要做 MNIST 的話
就 input 一張 digit 他是 784 維的 vector
那接下來呢，這一個 encoder 他可能就是一個 neural network
他的 output 呢，就是一個 code
這個 code 通常是遠比 784 維還要小的
所以他會有類似壓縮的效果
這個 code 代表了原來 input 這張 image
的某種 compact，某種精簡的有效的 representation
但是現在問題是這樣，我們現在做的是 unsupervised learning
你可以找到一大堆的 image
當作這個 NN encoder 的 input
但是我們沒有任何的 output
並不知道一個 image，如果把它變成一個 code
這個 code 到底應該要長什麼樣子
那你要 learn 一個 network 你要有 input output
只有 input 你沒辦法 learn 他
那沒有關係，我們先想說我們要做另外一件事情
我們想要 learn 一個 decoder
所謂 decoder 他做的事情是說
input 一個 vector 他就通過這個 decoder
他也是個 NN，他的 output 就是一張 image
input 給他一個 code，他 output
就根據這個 code 裡面的 information
output 就是一張 image
接下來呢，你也沒辦法 train NN 的 decoder
你也沒辦法 train 他，因為你只有 network 的 output
你沒有他的 input
這兩個 network，encoder 和 decoder，單獨一個人你都無法 train 他
但是我們可以把它接起來
然後一起 train
也就是說呢，我們接一個 neural network
input 一個 image，中間變成 code
再把 code 通過 decoder 變成原來的 image
你就可以把這個 encoder 和 decoder 一起學
那你就可以同時把 encoder 和 decoder 學出來了
我們剛才在 PCA 裡面其實看過非常類似的概念
我們來看一下，先從剛才講過的 PCA 開始講起
我們剛才講過說
PCA 實際上他在做的事情是這樣，input 一張 image x
那在剛才我們的例子裡面，我們會把 x 減掉
他的平均 x bar 當作 input
但這個減掉 x bar 並不總是
這邊我們把他省略掉
把他省略掉並不會太奇怪，因為通常你在做 NN 的時候
你拿到 data 起手式你就先做 normalize
先把它變成 mean 是零，variance 是一
所以你的 data 通常 mean 其實就是零了
所以你就不用再減掉 mean
你把 x 乘上一個 weight，通過 NN 的一個 layer
得到你的 component 的 weight，這邊寫成 c
那這個 component 的 weight 再乘上一個 matrix W 的 transpose
得到 x 的 hat
這個 x 的 hat 是根據這些 component 的 reconstruction
根據這些 component 的 weight
component 就放在這個 weight 裡面
根據 component 的 weight 和 component 做 reconstruction 的結果
我們說在 PCA 裡面
我們要做的事情就是 minimize input
跟 reconstruction 的結果
我們要讓 x 跟 x hat
他的 euclidean distance 越近越好
然後 x 跟 x hat 他們越接近越好
這個是 PCA 做的事，如果把它當成 neural network 來看的話
那input 的 x 就是 input layer
output 的 x hat 就是 output layer
中間 component 的 weight
就是 hidden layer，在 PCA 裡面他是 linear
那中間這個 hidden layer 我們通常又叫他
bottleneck layer
為什麼叫 bottleneck layer 呢？
因為你這個 code 的數目
因為你現在就是要做 dimension reduction
所以你這 component 的數目
通常會比你的 input 的 dimension 還要小得多
這樣才會達到 dimension reduction 的效果
如果把它當作一個 layer 來看的話
他是個特別窄的 layer
所以我們叫他 bottleneck layer，他就是一個瓶頸的意思
前面這個部分就是在做 encode
你把 input 變成一組 code
後面這個部份就是在做 decode
你如果把 component 的 weight 想成 code 的話
後面這件事就是把 code 變回原來的 image
hidden layer 的 output，就是我們要找的那些code
那 PCA 是這樣做的
你其實也可用 gradient decent 來做 PCA
PCA 只有一個 hidden layer
你就可以想說要不要把它變更多 hidden layer
當然可以把它變更多 hidden layer
你就兜 一個很深的 neural network，他有很多很多層
然後在這個很多很多層的 hidden layer 裡面呢
你 input 一個 x
他最後得到的 output 是 x hat
你會希望 training 的 target，希望這個 x 跟 x hat
越接近越好
training 的方法 完全沒有什麼特別的
就是 back propagation
就跟我們之前在 neural network 的時候講的呢
是完全一模一樣的東西
那中間你會有個特別窄的 layer
這個特別窄的 layer 他有特別少的 neural
那些 neural 的 output，這個 layer 的 output
就代表了一組 code
那從 input 到 bottleneck layer 的部分就是 encoder
那從 bottleneck layer 的 output 到最後的 x hat
到最後整個 network 的 output 就是你的 decoder
所以你把 input 做 encode 變 bottleneck layer 的output
再把 bottleneck layer 的 output，做decode 變成原來的 image
這個就是 decode
這個 deep 的 auto encoder
最早是出現在 06 年的 Hinton 的 science 的 paper
那個時候呢
其實 deep auto-encoder 沒有那麼好 train
你有可能 train 一 train 以後就壞掉了
那個時候需要用 RBM
做 layer wise 的 initialization，然後才可能
把 deep auto-encoder train 得比較好一點
如果是按照我們剛才在 PCA 裡面看到的
這個從 input 到第一個 hidden layer 的 weight w1
好像應該要跟最後一個 hidden layer 的 output
跟 output layer 中間的 weight 互為 transpose
這個 layer 是 weight w1，這個 layer 好像應該是 w1 transpose
這個是 w2，這個好像應該是 w2 transpose
你在 training 的時候，你可以做到這件事情
你可以把這邊的 weight，跟這邊的 weight tight 起來
讓他們在做 training 的時候永遠保持他們的值是一樣的
你可以做到這件事情，做這件事的好處呢
是你現在 auto-encoder 的參數
就少一半，比較不會有 overfitting 的情形
但這件事情並不是必要的，沒有什麼理由說
這邊的 weight，跟這邊的 weight，一定要互為 transpose
現在常見的作法就是兜個 neural network
然後用 back propagation 直接 train 下去
管他 train 出來的 weight 是什麼，就是你得到的結果
那這邊是 Hinton 06年的 Nature 的 paper
Science paper 的上面呢
截出一些，那時候看起來，相當驚人的結果
現在是覺得還好，因為現在誰都
順手就可以 reproduce 這些結果
那時候是覺得相當驚人
那這是這樣子的，如果我們今天，這是原來的 image
那在 MNIST 上面是長這個樣子
如果你做 PCA
把他從 784 維降到 30 維
然後再從 30 維 reconstruct 回 784 維
那你得到的 image 是這個樣子
你可以看出說，他是比較模糊的
他是有一點霧霧的感覺
那如果你今天是用 deep auto-encoder 的話
在 Hinton 的 paper 裡他是這麼做的
把 784 維，先擴展成 1000 維
再把 1000 維降到 500 維再降到 250 維再降到 30 維
你很難知道說為什麼他設計成這樣子
然後再把 30 維變成 250 維再變成 500 維 1000 維
然後再把他解回來
你會發現說，如果你今天用的是 deep auto-encoder 的話呢
他的結果就看起來非常的好
那如果你今天，不是把它降到 30 維，而是把他降到 2 維的話
把它降到 2 維再去 visualize 他的話
你會發現，如果你是做 PCA 的話
在這二維的平面上，所有的 digit 是被
混在一起，這邊不同顏色就代表了不同的數字
如果今天是用 PCA 把它變成二維
所有的數字他是混在一起
如果你是用 deep auto-encoder 的話，你會發現說
這些數字是分開的
你可以輕易地看到說，不同的數字會變一群一群
現在你其實可以很輕易的 reproduce 這樣的結果
我後來又做一個無聊的東西不過沒看到什麼特別的東西
就是把寶可夢的 data 又做了一下
用一個 hidden layer 的 auto-encoder，硬做了一下
但是沒有看到什麼特別的東西
那這個 auto-encoder 呢
也可以把它用在文字處理上
如果把它用在文字處理上像是怎樣呢
比如說我們會想要把一篇文章
壓成一個 vector，壓成一個code
那為什麼我們會想把一篇文章壓成一個 code 呢？
舉例來說假設我們現在要做文字的搜尋
假設我們現在要做文字的搜尋
那在文字搜尋裡面有一招，你可能都聽過叫 vector space model
那這個 vector space model 他非常單純，他就是說
我們現在把每一篇文章都表示成空間中的一個 vector
都是空間中的一個點
圖上每一個藍色的圈圈就是一篇文章
那接下來呢
我們也把，假設使用者輸入一個查詢的詞彙
那我們把查詢的詞彙也變成空間中的一個點
接下來就是計算這個輸入的查詢詞彙
跟每一篇 document 之間的 inner product
或是 cosine similarity 等等
cosine similarity 會有 normalize 的效果
可能會得到比較好的結果
如果距離最近 cosine similarity 的結果最大
你就會 retrieve 這些 document
比如說你輸入紅色的 query
可能會 retrieve 這篇 document 跟這篇 document
因為他跟紅色這個 query 的 cosine similarity 是比較大
這個模型要 work，depend on 你現在把一個 document
變成 vector 表示的好還是不好
假設你今天做的是
怎麼把一個 document 變成一個 vector呢
最 trivial 的方法叫做 bag-of-word
這個 bag-of-word 的想法是說
我們現在就開一個 vector
這個 vector 的 size 就是
lexicon 的 size，就是 lexicon 的 size
假設今天世界上有十萬個詞彙
這個 vector 的 size 就是十萬維
假設現在有一篇 document 他只有一個句子就是 this is an apple
那這篇 document 如果把他表示成一個 vector 的話
就是在 this 那維是一，is 那維是一
an 那維是一，apple 那維是一，其他都是 0
就是這樣子，沒什麼特別好講
那有時候你想把它做得更好，你會把它乘上 inverse document frequency
你每一維不只會用詞彙在 document 出現的次數
你會再乘上一個 weight，代表那個詞彙的重要性
那這個重要性你可以用不同的方法衡量他
比如說用 inverse document frequency 來衡量
舉例來說，is 它可能在每個 document 都有出現，所以重要性很低
那 1 就會乘上比較小的值
apple 只有在某些 document 出現，重要性比較高，所以乘上比較高的值，等等
但是用這個模型他很 weak
他沒辦法考慮任何 semantic 相關的東西
他沒辦法考慮任何語意相關的東西
比如說，他不知道台灣大學的指的就是台大
他不知道 apple 跟 orange 都是水果，他沒辦法知道這些事情
對他來說每一個詞彙都是 independent
對他來說 apple 跟 pen，apple 跟 an，apple 跟 is 就是不同詞彙
他們中間是完全沒有任何相關性
那我們可以用 auto-encoder 讓語意這件事情被考慮進來
舉例來說，你 learn 一個 auto-encoder
他的 input 就是一個 document
或是 query 就是一段文字
這個在 Hinton science paper 上面是有實驗的
他只有用比較小的 lexicon size，只有兩千個詞彙
一個 document 就把它變成一個 vector
把這個 vector 通過一個 encoder，把他壓成二維
那在 Hinton 的 paper 上你會看到結果是這個樣子
我記得他做的是 *** 的那個 corpus
在這個 corpus 裡面 ，document 會標說他是屬於哪一類
那這邊不同顏色的點呢
每一個點代表一個 document，不同顏色就代表說，這篇 document 屬於哪一類
那其實我們在作業四就是要做類似的事情
希望你可以得到一樣漂亮的圖
你會發現同一類的 document，就都集中在一起，散佈像一朵花一樣
所以如果你今天要做搜尋的時候
今天輸入一個詞彙、查詢詞
那就把 query 也通過這個 encoder，把他變成一個二維的 vector
假設那個 query 落在這邊，你就可以知道說
這個 query 是跟 energy marketing
跟 energy market 有關的
就把這邊的 document retrieve 出來
那這個 auto-encoder 在這邊看起來結果是相當驚人
如果你用 LSA 的話，你得不到類似的結果
如果你用 LSA 的話你會發現說
我們剛剛有講 LSA
就是建一個 matrix，然後你可以找每個詞彙
跟每一個 document 它背後 latent 的 vector
一樣假設我們用 LSA，然後每個 document
用二維的 latent vector 來表示他
那你看到的結果看起來像是，像是這個樣子
Auto-encoder 也可以用在 image 的搜尋上面
你可以用在以圖找圖上面
那怎麼做以圖找圖呢，最簡單的方式就是
你就拿一張，假設這是你要找的對象
假設這是你 image 的 query
你去計算這個 image 的 query，跟其他你的 database 裡面的 image
他的相似程度，比如說你可以算他們在 pixel 上面的相似程度
然後你再看說最像的幾張就是要 retrieve 的結果
如果你只是這麼做的話呢，你其實得不到太好的結果
假設這是你的 query，這個是 Michael Jackson
如果你拿這張 image
去跟其他 database 的 images 算相似度的話
你找出來最像的會是這幾張
你會發現 Michael Jackson 跟這個馬蹄鐵是很像的
這個是很像，說實在也是很像
所以如果只是這麼做，在 pixel wise 上做比較
你找不到好的結果的
那怎麼辦，你要用
你可以用 deep auto-encoder 把每一張 image 變成一個 code
然後在 code 上面再去做搜尋
而且因為今天做這件事情是 unsupervised
learn 一個 auto-encoder 是 unsupervised，所以你要 collect 多少 data 都行
你要 collect 很多很多的 data，要 collect 多少 data都行
train 這種 auto-encoder 的 data 是永遠不缺的
不像你 train supervised learning 很缺 data，做 unsupervised learning 是不缺 data
那現在怎麼把它變成一個 code 呢
這也是從 Hinton paper 上來的，input 一個 32 X 32 image
每一個 pixel 用 RGB 表示，所以 32 X 32 X 3
然後把這個 image 變成 8192 維，再變成 4096
再變 2048 維，1024 維，到256 維
也就是說這張 image 用 256 維的 vector 來描述他
你再把這個 code 通過另一個 decoder
它的形狀跟這個一樣，只是反過來的
再變回原來的 image
你得到 reconstruction 的結果會是這樣
他是可以被 construct 回來的
如果你不是在 pixel 上算相似度
而是在這種 code 上面算相似度的話
你就會得到比較好的結果
比如來說，如果是用 Michael Jackson 做 input
你找到的都是人臉
雖然這些 images 在 pixel level 上看起來是不像的
但是你透過很多 hidden layer ，把它轉成 code 的時候
在 256 維的空間上，他們是很像的
看起來是像的，原來是不像的
比如說這是黑頭髮，這是金頭髮
看起來不是很像，但通過很多轉換以後
可能，在那個 256 維裡面，有一個 dimension 就代表人臉
所以他們都知道說，這些 images 都對應到人臉那個 class
auto-encoder 在過去有一個
很好的 application 是可以用在 pre-training 上面
我們都知道說在 train 一個 neural network 的時候
你有時候在煩惱怎麼做參數的 initialization
有沒有一些方法讓你找到一組比較好的 initialization
這種找比較好的 initialization 方法，就叫做 pre-training
那你可以用 auto-encoder 來做 pre-training
怎麼做呢，假設我現在要做，比如說
MNIST 的 recognition
我可以兜個 network 他是 input 784 維
第一個 hidden layer 1000 維，第二個 hidden layer 1000 維，然後 500 到 10 維
我在做 pre-train 的時候，我就先 train 一個 auto-encoder
這個 auto-encoder 他是這樣子
他 input 784 維，然後中間有個 1000 維的 vector
然後再把它變回 784 維
那我希望 input 跟 output 越接近越好
那在做這件事，其實你需要稍微小心一點
因為我們一般在做 auto-encoder 的時候
你會希望你的 code 比 dimension 還要小
那如果比 dimension 還要大會遇到什麼問題呢
你有可能會遇到說他就不 learn 了
他要 reconstruct 他就只要把 784 維
放到這 1000 維裡面去，然後再解回來，就結束了
他會啥都沒 learn 到，learn 一個接近 identity 的 matrix
所以你要很小心，如果你今天發現你的 hidden layer 是比 input 還要大的時候
code 是比 input 還要大的時候，要加一個很強的regularization
在這 1000 維上，所謂很強的 regularization 是說
你可以對這 1000 維的 output 做
L1 的 regularization
你會希望說這 1000 維的 output 是 sparse
這 1000 維裡面可能只有某幾維是可以有值的，其他維都必須要是零
這樣你就可以避免，auto-encoder 直接把 input 硬背起來，再輸出的問題
總之如果你的 code 是比 input 還要大的，你要注意這種問題
我們現在先 learn 一個 auto-encoder，learn 好以後
我們把從 784 維到 1000 維的這個 weight w1
把它保留下來然後 fit 住
接下來你就把所有 database 裡面的 digit 通通變成 1000 維的 vector
接下來，你再 learn 另一個 auto-encoder
他把 1000 維的 vector 變成 1000 維的 code
再把 1000 維的 code 轉回 1000 維的 vector
你再 learn 一個這樣的 auto-encoder
他會讓 input 跟 output 越接近越好
然後你再把 w2 保存下來
接下來你 fix 住 w2 的值，再 learn 第三個 auto-encoder
第三個 auto-encoder input 1000 維，code 500維， output 1000 維
learn 好這個 auto-encoder，得到他的 weight w3，再把 w3 保留下來
這個 w1 w2 w3 就等於是你在 learn 你整個 neural network 的時候的 initialization
然後你最後再 random initialize 最後 500 到 100 的 weight
再用 back propagation 去調一遍，我們稱之為 fine tune
因為 w1 w2 w3 都已經是很好的 weight 了
我們只是微調他，所以把它叫做 fine tune
用 back propagation 把 w1 到 w4 的值調一下
你把他的值調一下，就可以 learn 好一個 neural network
這招 pre-training 在過去呢
如果你要 learn 一個很 deep 的 neural network 可能是很需要的
不過現在基本上 network 不用 pre-training
現在 training 技術進步以後，不用 pretraining 也 train 得起來
但 pre-training 有個妙用就是
如果你今天有很多 unlabeled data 只有少量 labeled data
你可以用大量的 unlabeled data 去把 w1 w2 w3
先 learn 好，先 initialize 好
那最後的 labeled data 就只需要稍微調整 weight 就好
所以 pre-training 這招在你有大量 unlabel data 的時候還是有用的
有個辦法可以讓 auto-encoder 做得更好，叫做 de-noising auto-encoder
把 reference 列在下面給大家參考
他的概念其實很簡單，你把原來的 input x
加上一些 noise 變成 x'
然後你把 x' encode 以後變成 code c
再把 c decode 回來變成 y
但是要注意一下，我們現在的 y
本來在做 auto-encoder 是讓 input 跟 output 越接近越好
但現在 de-noising auto-encoder 你是要讓 output
跟原來的 input，再加 noise 之前的 input 越接近越好
如果你有做這件事，learn 出來的結果會比較 robust
直覺解釋就是 encoder 現在不只 learn 到 encode 這件事
他還可以 learn 到把雜訊濾掉這件事
那還有另一招叫 constrictive auto-encoder
這邊就沒有要細講，那 constrictive auto-encoder 他做的事情是這樣
他會希望說，我們在 learn 這個 code 的時候
我們加上一個 constrain
這個 constrain 是說當 input 有變化的時候對這個 code 的影響是被 minimize 的
這件事其實很像 de-noising auto-encoder 只是從不同角度來看
de-noising auto-encoder 是說，我加了 noise 以後，還要 reconstruct 回原來沒有 noise 的結果
那 constrictive auto-encoder 是說
我們希望說當 input 變了，也就是加了 noise 以後
對這個 code 的影響是小的
他們做的事其實蠻類似的
那其實還有很多 non-linear dimension reduction 的方法
比如說 Restricted Boltzmann machine
我們這邊就沒有打算講 Restricted Boltzmann machine
Restricted Boltzmann machine 看起來很像 neural network，但其實不是，
有些人就用 neural network 來想，但文獻你怎麼看都看一頭霧水，因為他就不是 neural network
還有一個東西叫 deep belief network
deep belief network 聽起來很像一個 deep neural network
這要問一下大家意見，你覺得 deep belief network 跟 deep neural network 是一樣的東西嗎
你覺得是一樣東西的同學請舉手
你覺得是不一樣東西的請舉手
大家都知道說他是不一樣的東西，這是很正確的概念
他們只是名字像而已
看 paper 架構，乍看下你會覺得是不是一樣的東西
但實際上他們是不一樣的東西
這個 deep belief network 跟前面的 Restricted Boltzmann machine
他們是 graph co-model
他們只是看起來很像 neural network
你把它的概念直接套在 neural network上 ，你讀文獻會卡到不行
但我們還沒有打算要講這個 graph co-model
這個部分沒辦法細講，就留一些 reference 給大家參考
那接下來我們講一下 CNN 的 auto-encoder
如果我們今天要處理的對象是 image 的話，我們都知道你會用 CNN
那在 CNN 處理 image 的時候
你會有一些 convolution layers，pooling layers
用 convolution 和 pooling 交替，然後
讓 image 變得越來越小，最後去做 flatten
那今天如果是做 Auto-encoder 的話
你不只要有個 encoder，還要有個 decoder
如果 encoder 的部分是做 convolution 再做pooling，convolution 再做pooling
理論上 decoder 應該就是做跟 encode 相反的事情
decode 的時候你應該就是做
就是你做這些 process 得到的 code 再做反過來的事情
本來有 pooling 就做 unpooling，本來有 convolution 就做 deconvolution
但是這個 unpooling 跟 deconvolution 到底是什麼呢
那 training 的 criteria 就一樣，就是讓 input 和 output 越接近越好
但是這個 convolution 跟 deconvolution 他們是什麼呢
那我們現在來看一下這個 unpooling 的部分
我們知道在做 pooling 的時候你就是
現在得到一個 4X4 的 matrix
接下來把 matrix 裡面的 pixel 分組，四個一組
接下來從每一組裡面挑一個最大的
比如說在這個例子裡面你挑了這個東西
然後你的 image 就變成原來的四分之一
但是如果今天做的事情，如果你今天要做的是 unpooling
pooling 是這麼做，但如果你要等一下要做 unpooling
你要做另一件事，要先記得說我剛剛在做 pooling 是從哪裡取值
我在這個地址是從左上角，這四個方塊從左上角取值
所以這邊就是白的
這四個是從右下角取值，所以右下角就留下紀錄
從這個地方取值，這邊有個紀錄，你要記得是從哪裡取值
接下來如果你要做 unpooling 的時候
你要把原來比較小的 matrix 擴大
比如說原來做 pooling，原來比較大的 matrix 變成四分之一
現在要把比較小的 matrix 變成原來的四倍
那怎麼做呢 這個時候你之前紀錄的 pooling 的位子就可以派上用場
之前記得說你在 pooling 的時候是從左上角 pool 值
那現在你在做 unpooling 的時候就把值放到左上角，其他補零
記得在這裡取值，就把這個值放到右下角其他都補零
這裡取值，就把值放到這裡其他補零
這裡取值，就把值放到這邊其他補零
這就是 unpooling 的其中一種方式
所以做完 unpooling 以後本來一張比較小的 image 會變得比較大
原來14X14 的 image 會變成 28X28 的 image
你會發現它就是把原來 14X14 的 image 做一下擴散
有些藍色的地方就是補零
每一個原來在 14X14 image 裡面的值
擴散到 28X28 裡面，他都會加上三個零
其實這不是 unpooling 唯一的做法，他在 Keras 裡面作法是不一樣的
就我所知 Keras 裡面的做法
他是直接 repeat 那些 value
也就是說不用去記你之前 pooling 的位子
你就直接把這個值複製四份，就行了
就我所知 Keras 裡面是這麼做
接下來比較難理解的地方叫做 Deconvolution
原來 convolution 已經很難懂了，deconvolution 到底是什麼呢
大家很難搞清楚他是什麼
事實上 deconvolution 就是 convolution
這樣大家知道我的意思嗎？如果你看 Keras 的 code，作業三不是有個連結教你怎麼做 auto-encoder
不知道怎麼回事，大家都好像沒有做一樣
你會發現你看那個 code，根本就沒有什麼 deconvolution 這種東西
他就是做 convolution，他在做 decode 的地方再做 convolution
這是怎麼一回事，我們不是本來應該是做一個 convolution 的相反
叫做 deconvolution 嗎，怎麼又是在做 convolution
其實 deconvolution 就是 convolution 我們來解釋一下
所以有人說 deconvolution 名字取的不好會讓大家困惑
我們舉一維的 convolution
我們平常做 image 是做二維的，但那個圖有點累，我們取一維的當做例子
一維的 convolution 是怎樣呢，我們假設 input 有五個 dimension
然後我們的 filter size 是三
那我們就把 input 的這三個 value 分別乘上紅色藍色綠色的 weight，得到一個 output
再把這個 filter shift 一格，把這三個 value 分別乘上紅色藍色綠色的 weight 得到下一個 output
再 shift 一格，乘上紅色藍色綠色的 weight 再得到一個 output
這是 convolution，deconvolution 應該是怎樣呢
你的想像可能會是這樣子，deconvolution 就是 convolution 的相反
所以本來是三個值變成一個值
在做 deconvolution 的時候，就應該是一個值變三個值
然後發現這邊動畫有一個錯，這個圈圈不應該出現，請忽視他
所以你現在應該是一個值變成三個值，所以怎麼做呢
你的想像是這樣，一個值分別乘上紅色綠色藍色的 weight 變成三個值
這個值也乘上紅色綠色藍色的 weight 變成三個值
但是他已經有在這邊貢獻一些值了，他也要在這邊貢獻一些值怎麼辦呢
就加起來，他產生三個值，他也產生三個值，重疊的地方加起來
然後，他也產生三個值，重疊的地方加起來
但事實上這件事情等同於，你看看我這麼說對不對
等同於是在做 convolution，為什麼呢
他等同於是把，我們 input 就是三個 value
然後我們會做這個
把它做 padding ，在旁邊補零
接下來我們一樣做 convolution
做 convolution 的時候三個 input 乘上綠色藍色紅色的 weight 得到一個值
三個 input 乘上綠色藍色紅色的 weight 得到一個值，以此類推
你會發現說，這個框框裡面做的事情，跟這個框框裡面做的事情是一模一樣的
怎麼說呢，我們檢查中間這個值，他是三個 value 加起來
這三個 value 分別是他乘上綠色，他乘上藍色，他乘上紅色再加起來
那這邊這個 value 的值也是他乘上綠色，他乘上藍色，他乘上紅色再加起來
所以這件事情跟這件事情是一樣的
你檢查這邊的1 2 3 4 5，5 個值
跟這邊的 1 2 3 4 5，5 個值，他們是一樣的
如果你把 deconvolution 跟 convolution 做比較，他們不同點在哪裡？
不同點是在他們的 weight 是相反
他這邊是紅藍綠，這邊是綠藍紅，正好是相反
但他做的 operation 一樣也就是 convolution 這件事
所以你把這個 input 做 padding ，再做 convolution其實就等於是 deconvolution
你會發現在 Karas 裡面，你根本就不需要再另外寫一個 deconvolution 的 layer
你直接 call 一個 convolution 的 layer 就可以了
我本來想要講 sequence 的 auto-encoder
不過可以等到 RNN 的時候再講
我們知道說，剛才我們看到的 auto-encoder 他的 input
通通都是 fix-length 的 vector，但很多東西
有很多東西你本質上不該把他表示成 vector，比如說語音
一段聲音訊號有長有短他不是一個 vector
一段文章有長有短他不是一個 vector
你雖然可以用 bag-of-word 把它變成一個 vector
但這個方法你會失去詞彙和詞彙間的前後關係
所以我們剛才已經講了 dimension reduction
我們剛都是用 encoder 來把原來的 image 變成小的 dimension
但是我們同時也 train 一個 decoder 不是嗎
那個 decoder 其實是有妙用的，你可以拿 decoder 來產生新的 image
也就是說我們把 learn 好的 decoder 拿出來
然後給他一個 random 的 input number，他的 output 希望就是一張圖
這件事可以做到嗎，其實這件事做起來相當容易
我就胡亂拿MNIST 來秒 train 一下
然後我把每一張圖，784 維的 image 通過一個 hidden layer 然後 project 到二維
再把二維通過一個 hidden layer 解回原來的 image
那在 encoder 的部分，那個二維的 vector 畫出來長這樣
跟 Hinton 那個圖其實是有些像
那不同顏色的點代表不同數字，畫出來是這個樣子
接下來我在紅色這個框框裡面
等間隔的去 sample 一個二維的 vector 出來
然後把那個二維的 vector 丟到 NN decoder 裡面
然後叫他 output 一個 image 出來
這些二維的 vector 不見得是某個 image compress 以後的結果
他不見得原來有對應的 image，他就是某個二維的 vector
然後丟到 decoder 裡面，看看他可以產生什麼
你會發現我們在紅色框框內等距離的做 sample
得到的結果就是這樣
你就可以發現很多有趣的現象
從下到上，感覺是圓圈然後慢慢的就垮了
這邊本來是不知道是四還是九，然後變八
然後越來越細，變成 1
最後不知道為什麼變成 2，還蠻有趣的
你會發現這邊感覺比較差，為什麼呢
因為在這邊其實是沒有 image
你在 input image 的時候其實不會對到這邊
這個區域的 vector sample 出來，通過 decoder 他解回來不是 image
他看起來是一個有點怪怪的東西
有人可能會說，你怎麼知道要 sample 在這個地方
因為我必須要先觀察一下二維 vector 的分佈
才能知道哪邊是有值的
才知道從那個地方 sample 出來比較有可能是一個 image
如果我 sample 在這個地方，我相信我得到東西看起來就不像是 image 了
可是這樣你要先分析二維的 code 感覺有點麻煩
那怎麼辦，怎麼確保說，在我們希望的 region 裡面都是 image
有個很簡單的做法就是在你的 code 上面加 regularization
在你的 code 直接加上 L2 的 regularization，讓所有的 code 都比較接近零
接下來就在零附近 sample
比較有可能你 sample 出來的 vector 都可以對應到數字
所以我就做這樣的事情，我在 train 的時候加上 L2 的 regularization 在 code 上面
train 出來你的 code 都會集中在接近零的地方
他就會以零為核心，然後分佈在接近零的地方
接下來我就以零為中心，然後等距的在這個紅筐內 sample image
sample 出來就這個樣子
從這邊你就可以觀察到很多有趣的現象，你會發現說
他的這個 dimension 跟這個 dimension 是有意義的
從左到右橫軸代表的是有沒有圈圈
本來是很圓的圈圈， 然後接下來就慢慢變成一
那如果是縱的呢，我覺得本來是正的，然後慢慢就倒過來
這邊也是本來是正的，然後往下面慢慢就倒過來
所以你可以不只是做 encode，還可以用 code 來畫
這個 image 並不是從原來 image database sample 出來的，他是 machine 自己畫出來的
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
