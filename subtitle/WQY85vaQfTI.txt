好 我們現在要來講
Explainable 的 Machine Learning
到目前為止
我們已經訓練了很多很多的模型
你可能我們訓練過影像辨識的模型
給它一張圖片
它會給你答案
但我們並不滿足於此
接下來我們要機器給我們
它得到答案的理由
這個就是 Explainable 的 Machine Learning
好 那開始之前
開始介紹技術之前
我們需要講一下
為什麼 Explainable 的 Machine Learning
是一個重要的議題呢
我覺得那個本質上的原因是
就算今天機器可以得到正確的答案
也不代表它一定非常地聰明
舉一個例子
過去有一匹馬牠很聰明
所以大家叫牠神馬漢斯
那這個神馬漢斯可以做什麼事情呢
牠會做數學問題
舉例來說
你問牠根號 9 是多少
然後牠就開始計算得到答案
牠怎麼告訴你牠的答案呢
牠會用牠的馬蹄去跺地板
所以如果答案是 3
牠就敲三下
然後就停下來
代表牠得到正確的答案
然後旁邊的人就會歡呼
所以這個是神馬漢斯
然後一堆人呢
在看牠解數學問題
後來有人就很懷疑說
為什麼漢斯可以解數學問題呢
牠只是一匹馬
牠為什麼能夠理解數學問題呢
後來有人發現說
只要沒有人圍觀的時候
漢斯就會答不出數學問題
沒有人看牠的時候
你問牠一個數學的問題
牠就會不斷地敲牠的馬蹄
不知道什麼時候停下來
所以牠其實只是偵測到
旁邊人類微妙的情感變化
知道牠什麼時候要停下跺馬蹄
牠就可以有胡蘿蔔吃
牠並不是真的學會解數學的問題
而今天我們看到種種人工智慧的應用
有沒有可能跟神馬漢斯是一樣的狀況
而今天在很多真實的應用中
Explainable 的 Machine Learning
可解釋性的模型往往是必須的
舉例來說
銀行今天可能會用機器學習的模型
來判斷要不要貸款給某一個客戶
但是根據法律的規定
銀行作用機器學習模型來做自動的判斷
它必須要給出一個理由
所以這個時候
我們不是只訓練機器學習模型就好
我們還需要機器學習的模型
是具有解釋力的
或者是說機器學習未來
也會被用在醫療診斷上
但醫療診斷人命關天的事情
如果機器學習的模型只是一個黑箱
不會給出診斷的理由的話
那我們又要怎麼相信
它做出的是正確的判斷呢
今天也有人想
把機器學習的模型用在法律上
比如說幫助法官判案
幫助法官自動判案說
一個犯人能不能夠被假釋
但是我們怎麼知道機器學習的模型
它是公正的呢
我們怎麼知道它在做判斷的時候
沒有種族歧視等其他的問題呢
所以我們希望機器學習的模型
不只得到答案
它還要給我們得到答案的理由
再更進一步
今天自駕車未來可能會滿街跑
當今天自駕車突然急剎的時候
甚至急剎導致車上的乘客受傷
那這個自駕車到底有沒有問題呢
這也許取決於它急剎的理由
如果它是看到有一個老太太在過馬路
所以急剎
那也許自駕車是對的
但是假設它只是無緣無故
就突然發狂要急剎
那這個模型就有問題了
所以對自駕車
它的種種的行為 種種的決策
我們希望知道決策背後的理由
更進一步
也許機器學習的模型
如果具有解釋力的話
那未來我們可以憑藉著解釋的結果
再去修正我們的模型
今天在使用這些深度學習技術的時候
往往狀況是這個樣子
有某人說
這個就是你的機器學習的系統
是啊 我就是把資料丟進去
裡面就是有很多矩陣的相乘
接下來呢
就會跑出我的結果
如果結果不如預期的話
怎麼樣呢
現在大家都知道就爆調一下參數對不對
改個 Learning Rate 對不對
調一下 Network 的架構對不對
你根本不知道自己在做什麼對不對
就調一下 Network 的架構
我就把這一堆數學
這一堆 Linear Algebra 再重新打亂一下
看看結果會不會比較好
那如果其它沒有做過 Deep Learning 的人
就會大吃一驚
覺得哇 這樣怎麼可以呢
但實際上今天深度學習的模型
你往往要改進模型
就是需要調一些 Hyperparameter
但是我們期待也許未來
當我們知道
Deep Learning 的模型犯錯的時候
它是錯在什麼樣的地方
它為什麼犯錯
也許我們可以有更好的方法
更有效率的方法
來 Improve 我的模型
當然這個是未來的目標
今天離用 Explainable 的 Machine Learning
做到上述 Improve Model 的想法
還有很長的一段距離
好 那講到這邊呢
有人可能會想說
我們今天之所以這麼關注
Explainable Machine Learning 的議題
也許是因為 Deep 的 Network
它本身就是一個黑箱
那我們能不能夠用
其它的機器學習的模型呢
如果不要用深度學習的模型
改採用其他比較容易解釋的模型
會不會就不需要研究
Explainable Machine Learning 了呢
舉例來說
假設我們都採用 Linear 的 Model
Linear 的 Model
它的解釋的能力是比較強的
我們可以輕易地知道
根據一個 Linear Model 裡面的
每一個 Feature 的 Weight
知道 Linear 的 Model 在做什麼事
所以你訓練完一個 Linear Model 以後
你可以輕易地知道
它是怎麼得到它的結果的
但是 Linear Model 的問題就是
它沒有非常地 Powerful
我們其實在第一堂課就已經告訴你說
Linear 的 Model 有很巨大地限制
所以我們才很快地進入了 Deep 的 Model
但是 Deep 的 Model 它的壞處就是
它不容易被解釋
Deep 的 Network 大家都知道
它就是一個黑盒子
黑盒子裡面發生了什麼事情
我們很難知道
雖然它比 Linear 的 Model 更好
但是它的解釋的能力
是遠比 Linear 的 Model 要差
所以講到這邊
很多人就會得到一個結論
你可能常常聽到這樣的想法
我們就不應該用這種 Deep 的 Model
我們不該用這些比較 Powerful 的 Model
因為它們是黑盒子
但是在我看來
這樣的想法其實就是削足適履
我們因為一個模型
它非常地 Powerful
但是不容易被解釋就揚棄它嗎
我們不是應該是想辦法
讓它具有可以解釋的能力嗎
我聽過Yann LeCun講了一個故事
這個是Yann LeCun講的
那這個故事是個老梗
誰都聽過
就是有一個醉漢
他在路燈下面找鑰匙
大家問他說
你的鑰匙掉在路燈下嗎
他說不是
因為這邊有光
那所以我們堅持一定要用簡單
但是比較容易被解釋的模型
其實就好像是
我們堅持一定要在路燈下面
找鑰匙一樣
我們堅持因為一個模型
是比較 Interpretable 的
雖然它比較不好
但我們還是堅持要使用它
就好像一定要在路燈下面找鑰匙一樣
不知道說真實的模型
真實 Powerful 的模型
也許根本在路燈的範圍之外
而我們現在要做的事情
就是改變路燈的範圍
改變照明的方向
看能不能夠讓這些比較 Powerful 的模型
可以被置於路燈之下
比較 Interpretable
比較 Explainable
其實 Interpretable 跟 Explainable
這兩個詞彙
雖然在文獻上常常被互相使用
那其實它們是有一點點差別的
通常這個 Explainable 指的是說
有一個東西它本來是個黑箱
我們想辦法賦予它解釋的能力
叫做 Explainable
那 Interpretable 通常指的是
一個東西它本來就不是黑箱
我們可以跟
它本來就不是黑箱
我們本來就可以知道它的內容
這個叫 Interpretable
好 不過這兩者在文獻上也常常被混用了
所以我們這邊就不特別跟大家強調
Explainable 跟 Interpretable 的差異
好 那講到既 Interpretable 又 Powerful 的模型
也許有人會說
那 Decision Tree 會不會就是一個好的選擇呢
Decision Tree 相較於 Linear 的 Model
它是更強大的模型
而 Decision Tree 的另外一個好處
相較於 Deep Learning
它非常地 Interpretable
你看一個 Decision Tree 的 Structure
你就可以知道說
今天模型是憑藉著什麼樣的規則
來做出最終的判斷
那 Decision Tree
不是我們這門課會講的東西
但是就算是你沒有學過的 Decision Tree
你其實也不難想像
Decision Tree 它是在做什麼
它做的事情就是
你有很多的節點
那每一個節點都會問一個問題
讓你決定向左還是向右
最終當你走到節點的末尾
當你走到 Leaf Node 的時候
就可以做出最終的決定
因為在每一個節點都有一個問題
你看那些問題以及答案
你就可以知道
現在整個模型
憑藉著什麼樣的特徵
是如何做出最終的決斷
所以從這個角度看來
Decision Tree
它既強大又 Interpretable
所以這堂課我們可以就上到這邊
就是 Decision Tree is all you need?
然後就結束了這樣子
好 但是 Decision Tree
真的就是我們所需要的嗎
你再仔細想一下
Decision Tree 也有可能是很複雜的
舉例來說
我看到在網路上找到
有人問了一個問題
他說他有一個這麼複雜的 Decision Tree
他完全看不懂這個 Decision Tree 在幹嘛
有沒有人有什麼樣的
Explainable Machine Learning 的方法
可以把這個 Decision Tree 變得更簡單一點
我看三 四年過去了
都沒有人回答這個問題
有人看到的話
也許可以幫忙回答一下
但另外一方面
你再仔細想想看
你是怎麼實際使用 Decision Tree 這個技術的呢
我知道很多同學都會說
這個打 Cargo 比賽的時候
Deep Learning 不是最好用
什麼 Decision Tree
那個才是最好用的
那才是 Cargo 比賽的常勝軍
但是你想想看
當你在使用 Decision Tree 技術的時候
你是只用一棵 Decision Tree 嗎
其實不是
你真正用的技術叫做 Random Forest 對不對
你真正用的技術
其實是好多棵 Decision Tree 共同決定的結果
一棵 Decision Tree
你可以憑藉著每一個節點的問題跟答案
知道它是怎麼做出最終的判斷的
但當你有一片森林
當你有 500 棵 Decision Tree 的時候
你就很難知道說
這 500 棵 Decision Tree 合起來
是怎麼做出判斷
所以 Decision Tree 也不是最終的答案
並不是有 Decision Tree
我們就解決了
Explainable Machine Learning 的問題
好 那再繼續深入講
Explainable Machine Learning 的技術之前
這邊還有一個問題就是
Explainable Machine Learning 的目標是什麼
在我們之前的每一個作業裡面
我們都有一個 Leaderboard
也就是我們有一個明確的目標
要嘛是降低 Error Rate
要嘛是提升 Accuracy
我們總是有一個明確的目標
但是 Explainable 的目標到底是什麼呢
什麼才是最好的 Explanation 的結果呢
那 Explanation 它的目標其實非常地不明確
就是因為目標不明確
你才發現說 Explainable Machine Learning 的作業
就沒有 Leaderboard
因為出不了 Leaderboard
我們只能夠出選擇題
讓大家增加一些知識
我們只能夠做這樣子而已
那但到底 Explainable Machine Learning
它的終極目標是什麼呢
什麼才是最好的 Explanation
那以下是我個人的看法
並不代表它是對的
你可能不認同
那我也不會跟你爭辯
那這個只是我個人的看法而已
很多人對於 Explainable Machine Learning
會有一個誤解
它覺得一個好的 Explanation
就是要告訴我們
整個模型在做什麼事
我們要了解模型的一切
我們要知道它到底是
我們要完全了解
它是怎麼做出一個決斷的
但是你想想看
這件事情真的是有必要的嗎
我們今天說 Machine Learning 的 Model
Deep 的 Network 是一個黑盒子
所以我們不能相信它
但你想想看
世界上有很多很多的黑盒子
正在你的身邊
人腦不是也是黑盒子嗎
我們其實也並不完全知道
人腦的運作原理
但是我們可以相信
另外一個人做出的決斷
那人腦其實也是一個黑盒子
你可以相信人腦做出了決斷
為什麼 Deep 的 Netwok 是一個黑盒子
你沒有辦法相信
Deep 的 Netwok 做出來的決斷
為什麼你對 Deep 的 Netwok 會這麼恐懼呢
那我覺得其實對人而言
也許一個東西
能不能讓我們放心
能不能夠讓我們接受
理由是非常重要的
以下呢
是一個跟 Machine Learning
完全無關的心理學實驗
這個實驗是 1970 年代就做了
這是 Ellen Langer
一個哈佛大學教授做的
這個實驗非常地有名
這個實驗是這樣
這個實驗是一個跟印表機有關的實驗
在哈佛大學圖書館印表機呢
會大排長龍
很多人都排隊要印東西
這個時候
如果有一個人跟他前面的人說
拜託請讓我先印
我就印 5 頁而已
那一般人會不會接受呢
會不會讓他先印呢
有 60% 的人會讓這個人先印
所以感覺哈佛大學
學生人都還蠻好的
這個是接受程度是比我預期得要高
你給一個人說讓我先印
有 60% 的人會答應
但這個時候
你只要把剛才問話的方法稍微改一下
你本來只說能不能讓我先印
現在改成說
能不能讓我先印
因為我趕時間
他是不是真的趕時間
沒人知道
但是當你說你有一個理由
所以你要先印的時候
這個時候接受的程度變成 94%
而神奇的事情是
就算你的理由稍微改一下
舉例來說
有人說請讓我先印
因為我需要先印
光是這個樣子
接受的程度也變成 93%
所以神奇的事情是
人就是需要一個理由
你為什麼要先印
你只要講出一個理由
就算你的理由是因為我需要先印
大家也會接受
所以會不會 Explainable Machine Learning
也是同樣的道理
所以我們需要 Explainable 的 Machine Learning
所以我覺得對
什麼叫做好的 Explanation
好的 Explanation
就是人能接受的 Explanation
就是好的 Explanation
人就是需要一個理由讓我們覺得高興
而到底是讓誰高興呢
這個是高興的人
可能是你的客戶
因為很多人就是聽到
Deep Network 是一個黑盒子
他就不爽
你告訴他說這個是可以被解釋的
給他一個理由
他就高興了
他可能是你的老闆
老闆看了很多的農場文
他也覺得說 Deep Learning 黑盒子就是不好的
告訴他說這個是可以解釋的
他就高興了
或者是你今天要讓
你今天要說服的對象是你自己
你自己覺得有一個黑盒子
Deep Network 是一個黑盒子
你心裡過不去
今天它可以給你一個做出決斷的理由
你就高興了
所以我覺得什麼叫做好的 Explanation
就是讓人高興的 Explanation
就是好的 Explanation
其實你等一下再記
在各種研究的發展上會發現說
我們在設計這些技術的時候
確實跟我現在講的
什麼叫好的 Explanation
就是讓人高興的 Explanation
這個想法
這個技術的進展是蠻接近的
好 所以 Explainable Machine Learning
它的目標就像我剛才講的
就是要給我們一個理由
那 Explainable 的 Machine Learning 呢
又分成兩大類
第一大類叫做 Local 的 Explanation
第二大類叫做 Global 的 Explanation
Local 的 Explanation 是說
假設我們有一個 Image 的 Classify
我們給它一張圖片
它判斷說它是一隻貓
那我們要問的問題是
為什麼
或者機器要回答問題是
為什麼你覺得這張圖片是一隻貓
它根據某一張圖片來回答問題
這個叫做 Local Explanation
還有另外一類呢
叫 Global Explanation
Global examination 意思是說
現在還沒有給我們的 Classify 任何圖片
我們要問的是
對一個 Classify 而言
什麼樣的圖片叫做貓
我們並不是針對任何一張
特定的圖片來進行分析
我們是想要知道說
當我們有一個 Model
它裡面有一堆參數的時候
對這堆參數而言
什麼樣的東西叫作一隻貓
所以 Explainable 的 Machine 有兩大類
我們先來看第一大類
第一大類是為什麼
你覺得一張圖片是一隻貓
我們可以把一個圖片
這個問題問得更具體一點
給機器一張圖片
它知道它是一隻貓的時候
到底是這個圖片裡面的什麼東西
讓模型覺得牠是一隻貓
是眼睛嗎
是耳朵嗎
還是貓的腳
讓機器覺得它看到了一隻貓
或者講的更 General 一點
假設現在我們模型的輸入叫做 x
這個 x 可能是一張影像
可能是短文字
而 x 呢
可以拆成多個 Component
x1 到 xN
如果對於影像而言
可能每一個 Component
就是一個 Pixel
那對於文字而言
可能每一個 Component
就是一個詞彙
或者是一個 Token
那我們現在要問的問題就是
這些 Token 裡面
那這些 Component 裡面
那這個如果對文字來說是 Token
對 Image 來說可能就是 Pixel
這些 Component 裡面
哪一個對於機器
現在做出最終的決斷是最重要的呢
那怎麼知道一個 Component 的重要性呢
那基本的原則是這個樣子
就是我們把 Component 都拿出來
然後把每一個 Component 做改造
或者是刪除
如果我們改造或刪除某一個 Component 以後
今天 Network 的輸出有了巨大的變化
那我們就知道說
這個 Component 沒它不行
它很重要
如果某個 Component 被刪掉以後
現在 Network 的輸出有了巨大的變化
就代表這個 Component
沒它不行
那這個 Component
就是一個重要的 Component
講得更具體一點
你想要知道
今天一個影像裡面
每一個區域的重要性的時候
有一個非常簡單的方法
像是這個樣子
就給一張圖片
然後丟到 Network 裡面
它知道這是一隻博美狗
接下來在這個圖片裡面
不同的位置放上這個灰色的方塊
當這個方塊放在不同的地方的時候
今天你的 Network 會 Output 不同的結果
那這邊這個
下面這個圖 這些顏色
代表今天 Network 輸出博美狗的機率
那藍色代表博美狗的機率是高的
藍色說錯了
藍色代表博美狗的機率是低的
紅色代表博美狗的機率是高的
而這邊的每一個位置
代表了這個灰色方塊的位置
也就是說
當我們把灰色的方塊移到這邊
移到這邊
移到博美狗的臉上的時候
今天你的 Image Classify
就不覺得它看到一隻博美狗
如果你把灰色的方塊
放在博美狗的四周
這個時候機器就覺得
它看到的仍然是博美狗
所以知道說
它不是看到這個球
覺得它看到博美狗
也不是看到地板
也不是看到牆壁
覺得看到博美狗
而是真的看到這個狗的臉
所以它覺得
它看到了一隻狗
那這邊也有一樣的例子
把灰色的方框在
把灰色的方塊在這個圖片上移動
你會發現說呢
灰色的方塊移到輪胎上的時候
機器就不覺得它有看到輪胎了
所以機器知道輪胎長什麼樣子
它今天看到這個圖片
知道答案是輪胎的時候
並不是瞎蒙蒙到的
而是它知道說
輪胎出現在這個位置
或者是說這邊有一張圖片
然後這個圖片裡面有兩個人
還有一隻這個阿富汗獵犬
但是機器到底是真的看到了阿富汗獵犬
還是把人誤認為狗呢
這個時候你就可以把這個灰色的方框
在這個圖片上移動
然後你發現這個灰色的方框
放在這個人的臉上
或放在這個人的臉上的時候
機器仍然覺得
它有看到阿富汗獵犬
但是當你把灰色的方框
放到這個位置
放到這個位置的時候
機器就覺得
它沒有看到阿富汗獵犬
所以它是真的有看到阿富汗獵犬
它知道這一隻就是阿富汗獵犬
並不是把人誤認為阿富汗獵犬
所以這個是最簡單的
知道 Component 重要性的方法
好 接下來還有一個更進階的方法
是計算 Gradient
這個方法是這樣子的
假設我們有一張圖片
我們把它寫作 x1 到 xN
這邊的每一個 x
代表了一個 Pixel
接下來呢
我們去計算這張圖片的 Loss
我們這邊呢
用小 e 來表示
這個小 e 是什麼呢
這個小 e 是把這張圖片呢
丟到你的模型裡面
這個模型的輸出的結果跟正確答案的差距
跟正確答案的 Cross Entropy
這個 e 越大
就代表現在辨識的結果越差
好 那接下來
怎麼知道某一個 Pixel
對於影像辨識這個問題的重要性呢
那你就把某一個 Pixel 的值
做一個小小的變化
把它加上一個 Δx
然後你接下來看一下
你的 Loss 會有什麼樣的變化
如果今天把某一個 Pixel
做小小的變化以後
Loss 就有巨大的變化
代表說這個 Pixel
對影像辨識是重要的
反之如果加了 Δx
這個 Δe 趨近於零
這個 Loss 完全沒有反應
就代表說這個 Δx
這個位置
這個 Pixel 對於影像辨識而言
可能是不重要的
那我們可以用 Δe 跟 Δx 的比值
來代表這一個 Pixel
xN 的重要性
而事實上 Δx 分之 Δe 這一項
就是把 xN 對你的 Loss 做偏微分
如果你不知道偏微分是什麼的話也沒有關係
反正就是 Δx 跟 Δe 的比值
就代表了這個 xN 的重要性
那這個比值越大
就代表 xN 越重要
那你把每一個圖片裡面
每一個 Pixel
它的這個比值都算出來
你就得到一個圖呢
叫做 Saliency Map
它在我們的作業裡面
你會有很多機會
畫各式各樣的 Saliency Map
那下面這個圖
上面這個是原始圖片
下面這個黑色的
然後有亮白色點的是 Saliency Map
那在這個 Saliency Map 上面呢
越偏白色就代表這個比值越大
也就是這個位置的 Pixel 是越重要的
舉例來說
給機器看這個水牛的圖片
它並不是看到草地
覺得它看到牛
也不是看到竹子
覺得它看到牛
而是真的知道牛在這個位置
它覺得判斷這張圖片是什麼樣的類別
對它而言最重要的
是出現在這個位置的 Pixel
像是真的看到牛
所以知道說
所以才會 Output 牛這個答案
如果機器看到這個圖片
說它看到一個猴子
那猴子在哪裡呢
猴子在樹梢上面
它並不是把葉子判斷成猴子
它知道這個位置出現的東西
就是它判斷正確答案的準則
我給它這個圖片
它知道說狗呢
是出現在這個位置的
所以這個
這個技術呢 叫做 Saliency Map
那 Saliency Map 這個技術
我們等一下來舉一個實際的應用
這個應用是什麼呢
這個應用跟寶可夢還有數碼寶貝有關啦
不知道大家知不知道那個數碼寶貝是什麼
左邊這個是寶可夢啦
右邊這個呢 叫做數碼寶貝
知道數碼寶貝的同學可以舉手一下嗎
哦 好多
好 手放下
那我就不用解釋了
不知道的同學
反正就是另外一種動物就對了
欸 對 真的啊
真的 這個寶可夢是一種動物
數碼寶貝是另外一種動物
然後我在網路上呢
看到有人說
他訓練了一個數碼寶貝跟寶可夢的分類器
然後正確率非常地高
所以我決定自己也來做這個實驗
看看為什麼可以得到這麼高的正確率
好 那你可以在網路上呢
找到寶可夢的圖庫
也可以找到數碼寶貝的圖庫
所以你有一堆寶可夢的圖
有一堆數碼寶貝的圖
那這個對大家來說一定都不成問題
這個就是二元分類的問題而已 對不對
胡亂 Train 一個 Classifier
就結束了
就把作業三的 Code 改一改
然後把本來分成 11 類
改成分成兩類
就結束了
好 那接下來呢
訓練完以後呢
當然要用機器沒有看過的圖去測試它
所以你不能夠把所有的寶可夢
跟所有的數碼寶貝都拿去做訓練
你要特別留一些寶可夢跟數碼寶貝
是訓練的時候沒有看過的
看看機器看到新的寶可夢跟數碼寶貝
它能不能夠得到正確的結果
那這邊呢
我們來看一下人類
能不能夠正確的判斷寶可夢跟數碼寶貝好了
我來問一下大家
你覺得這一隻是寶可夢 還是數碼寶貝呢
覺得它是寶可夢的同學舉手一下
手放下
覺得它是數碼寶貝的同學舉手一下
好 也有一些
好 手放下
好 老實說我答案忘記了這樣子
所以
所以你可見這個寶可夢跟數碼寶貝
是很難分辨的
今天你就算是人類
你也很難夠
很難判斷說一隻動物
到底是寶可夢還是數碼寶貝
機器的表現如何呢
好 這邊就是隨便兜了一個模型
也沒有幾層
Train 下去
哇 Training Accuracy
98.9% 這個非常地高
但是不要高興地太早
這個也許 Cverfitting 而已
也許 Machine 只是把 Training 的 Data
記下來而已
因為畢竟訓練資料沒幾張啊
數碼寶貝 寶可夢才各幾千張而已
所以也許 Cverfitting
所以測試資料上沒看過的圖怎麼樣呢
正確率 98.4 啊
這個不可思議
這個偉哉機器學習
這個人類都沒有辦法判斷
寶可夢跟數碼寶貝的差異
但機器可以
還有 98.4% 的正確率
好 那接下來我就很好奇
就想要知道說
機器到底是憑藉著什麼樣的規則
判斷寶可夢和數碼寶貝的差異呢
所以我決定來畫一下 Saliency Map
這邊有幾隻動物
它們是什麼呢
它們是數碼寶貝
這些是數碼寶貝
接下來呢
我就在這些圖片上畫 Saliency Map
讓機器來告訴我說
為什麼它覺得這幾張是數碼寶貝
機器給我的答案是這個樣子
這邊亮亮的點
代表它覺得比較重要
這有點怪怪的
好像亮亮的點都分布在四個角落
不知道發生了什麼事
接下來我來分析寶可夢
這個情況更明顯
你發現說機器覺得重要的點
基本上都是避開寶可夢的本體啦
都是在影像的背景上啊
為什麼呢
因為我後來發現
寶可夢都是 PNG 檔啦
數碼寶貝都是 JPEG 檔
PNG 檔讀進來以後
背景都是黑的啦
所以機器只要看背景
就知道一張圖片是寶可夢還是數碼寶貝啦
就結束了這樣子
好
所以這個例子就是告訴我們說
Explainable AI 是一個很重要的技術
那我剛才舉的例子可能你覺得有點荒謬
也許在正常的應用中不會發生這種事情
但真的不會發生嗎
有一個真實的例子
有一個 Benchmark Corpus
叫做 PASCAL VOC 2007
裡面有各式各樣的物件
機器要學習做影像的分類
機器看到這張圖片
它知道是馬的圖片
但如果你畫 Saliency Map 的話
你發現結果是這個樣子的
只是覺得左下角對馬是最重要
為什麼
因為左下角有一串英文啊
這個圖庫裡面馬的圖片
很多都是來自於某一個網站啊
左下角都有一樣的英文啊
所以機器看到左下角這一行英文
就知道是馬
它根本不需要學習馬是長什麼樣子
所以今天在這個真實的應用中
在 Benchmark Corpus 上
類似的狀況也是會出現的
所以這告訴我們
這種 Explainable Machine Learning
這個技術是很重要的
好 那有沒有什麼方法
把 Explainable 的 Machine Learning
Saliency Map 畫得更好呢
第一個方法啊
就是助教剛才有提到的這個 SmoothGrad
什麼意思呢
這張圖片是指瞪羚
那你期待說
你今天去做 Saliency Map 的時候
機器會把它主要的精力
集中在瞪羚身上
那如果你用剛才我們講的方法
直接畫 Saliency Map 的話
你得到的結果可能是這個樣子
確實今天在
確實在瞪羚附近有比較多亮的點
但是在其他地方也有一些雜訊
讓人看起來有點不舒服
所以就有了 SmoothGrad 這個方法
SmoothGrad 會讓你的這個 Saliency Map
上面的雜訊比較少
如果在這個例子上
你就會發現多數的亮點
真的都集中在瞪羚身上
那 SmoothGrad 這個方法是怎麼做呢
非常簡單
說穿了也不值錢
就是你在你的圖片上面啊
加上各種不同的雜訊
那加不同的雜訊就是不同的圖片了嘛
每一張圖片上面
都去計算 Saliency Map
那你有加 100 種雜訊
就有 100 張 Saliency Map
平均起來
就得到 SmoothGrad 的結果
就結束了
當然有人會問說
欸 那你怎麼知道說這個 SmoothGrad
這樣子的結果一定就是比原來的結果好呢
也許對機器來說
它真的覺得這些草很重要啊
它真的覺得這個天空很重要啊
它真的覺得這個背景很重要啊
也許是
那就像我一開始說的
Explainable Machine Learning 最重要的目標
就是要讓人看了覺得爽啊
你畫了這個圖
你的老闆就會覺得不爽啊
他就會覺得
哇 這個 Model 有點爛哦
這個 Model 解釋性好像很低哦
所以你就會把它畫成 SmoothGrad 這個樣子
然後告訴別人說
你看這個 Model
它果然知道瞪羚是最重要的啊
所以這個
嗯 這個 Model 不錯
然後這個
Explainable Machine Learning 的方法也不錯
好 但是呢
其實光看 Gradient
並不完全能夠反映一個 Component 的重要性
怎麼說呢
這邊就舉一個例子給大家參考
好 這個橫軸代表的是大象鼻子的
某一個生物鼻子的長度
那縱軸代表說這個生物是大象的可能性
我們都知道說大象的特徵
就是長鼻子
所以一個生物
牠的鼻子越來越長
牠就越有可能是大象
但是當牠的鼻子長到一個極限的時候
再變更長一點
牠也不會變得更像大象
鼻子很長的大象
就只是鼻子特別長的大象
所以當一個大象的鼻子變長的時候
長到超出一個範圍的時候
你也不會覺得它變得更像大象
所以 鼻子的
生物鼻子的長度跟牠是大象的可能性
它的關係
也許一開始在長度比較短的時候
隨著長度越來越長
今天這個生物是大象的可能性越來越大
但是當鼻子的長度長到一個程度以後
就算是更長
也不會變得更像大象
這個時候
如果你計算鼻子長度
對是大象可能性的偏微分的話
那你在這個地方得到的偏微分
可能會趨近於 0
所以如果你光看 Gradient
光看 Saliency Map
你可能會得到一個結論是
鼻子的長度
對是不是大象這件事情是不重要的
鼻子的長度不是判斷是否為大象的一個指標
因為鼻子的長度的變化
對是大象的可能性的變化
是趨近於 0 的
所以鼻子根本不是
判斷是不是大象的重要的指標
那事實上是這樣嗎
事實上你知道不是這個樣子
所以光看 Gradient
光看偏微分的結果
可能沒有辦法完全告訴我們
一個 Component 的重要性
所以有其他的方法
有一個方法叫做 Interated 的 Gradient
它的縮寫叫做 IG
那這邊
我就不打算詳細講說 IG 是怎麼運作的
我們就把文件留在這邊
那助教的程式裡面也有實作的 IG
那如果你有興趣
你可以自己研究看看 IG 是怎麼運作的
如果你沒有興趣
反正你按個 Enter
就跑出那個 IG 的分析的結果了
好 那剛才我們是看 Network 它是
我們剛才是看一個輸入
它的哪些部分是比較重要的
那接下來我們要問的下一個問題是
當我們給 Network 看一個輸入的時候
它到底是怎麼去處理這個輸入的呢
它到底是怎麼對輸入做處理
然後得到最終的答案的呢
好 那有不同的方法
第一個方法最直覺的
就是人眼去看
今天 Network 裡面到底發生了什麼事情
那在作業裡面
是要你去看 BERT 裡面發生了什麼事情
是跟文字有關的
那上課舉的例子
我們就舉語音的例子
在作業二裡面
你已經訓練了一個 Network
這個 Network 就是吃一小段聲音當中輸入
判斷說這段聲音
是屬於哪一個 Phoneme
屬於哪一個 KK 音標
然後呢
假設你第一個 Layer 有 100 個 Neurons
第二個 Layer 也有 100 個 Neurons
那第一個 Layer 的輸出
就可以看作是 100 維的向量
第二個 Layer 的輸出
也可以看作是 100 維的向量
通過這些分析這些向量
也許我們就可以知道一個 Network 裡面
發生了什麼事
但是 100 維的向量
不容易看 不容易觀察 不容易分析
所以怎麼辦呢
你有很多方法
可以把 100 維的向量
把它降到二維
那至於是什麼方法
我們這邊就不細講
總之這些方法有一籮筐可以使用
把 100 維降到二維以後
你就可以畫在圖上
那你就可以細心地觀察它
看看你可以觀察到什麼現象
以下呢
舉的是語音的例子
那這個例子來自於一篇 2012 年的 Paper
你會發現 Hinton
這個深度學習之父
也是這篇文章的作者
這篇文章做的事情是什麼呢
這篇文章
其實老實說
這篇文章做的就是你的作業二 知道嗎
它跟你作業二用的 Data 是一模一樣的
所以假設你穿越時空到十年前
拿你作業二的結果給 Hinton 看
他就嚇一跳這樣子
你可以順便告訴他說
這個只是我們 15 個作業的其中一個而已
他就嚇一跳了
你還可以告訴他說
哦 這個
我只 Train 了一個小時就 Train 完了
那個時候要 Train 類似的東西
大概要 Train 個一週以上吧
然後 Hinton 就會嚇一跳這樣子
所以看到這些過去的文章啊
我真的只能說
哇 這個大人 時代變了啊
以前 Train Network 多麼麻煩
現在真的是時代變了
而且其實那個時候用的是 Deep Belief 的 Network
Deep Belief Network
是 Deep Neural Network嗎
不是這樣子
那這個是什麼東西呢
這個我們就不會講它
因為現在已經沒有人在用這個東西了
好 那但是它得到的結果啊
還是蠻值得我們今天拿來看的
其實你在作業二
應該也可以觀察到類似的結果
好 這邊做的事情是什麼呢
這邊做的事情是
首先我們把模型的 Input
就是 Acoustic Feature
也就是 MFCC 拿出來
把它降到二維
畫在二維的平面上
在這個圖上啊
每一個點代表一小段聲音訊號
那每一個顏色
代表了某一個 Speaker
某一個講話的人
那其實我們丟給這個 Network 的資料
有很多句子是重複的
就是有人說
A 說了 How are you
B 也說了 How are you
C 也說了 How are you
很多人說了一樣的句子
但從這個圖上你看不出來
從 Acoustic Feature上你會發現說
就算是不同的人唸同樣的句子
內容一樣
但是我們從 Acoustic Feature 上看不出來
同一個人他說的話就是比較相近
就算是不同的人說同樣的句子
你也沒有辦法看到
他們被align在一起
所以從這個結果
人們就會覺得 哇 這個
這個語音辨識太難啦
語音辨識不能做啊
這個同樣的人說不
這個不同的人說同樣的話
看起來這個 Feature 差這麼多啊
這個語音辨識怎麼是有可能
有辦法做的問題呢
但是當我們把 Network 拿出來看的時候
結果就不一樣了
這個是第 8 層 Network 的輸出
你會發現什麼呢
你會發現這邊變成一條一條的
每一條沒有特定的顏色
這邊每一條代表什麼呢
每一條就代表了同樣內容的某一個句子
所以你會發現說
不同人說同樣的內容
在 MFCC 上看不出來
它通過了 8 層的 Network 之後
機器知道說這些話是同樣的內容
雖然聲音訊號看起來不一樣
是不同人講的
但他們是同樣的內容
它可以把同樣的內容
不同人說的句子
把它align在一起
所以最後就可以得到精確的分類結果
好 那剛才講的是直接拿 Neuron 的輸出
來進行分析
你也可以分析這個 Attention 的 Layer
現在 Self-Attention 用得很廣
你也可以看 Attention 的結果
來決定今天 Network 學到什麼事
那我們在作業裡面
也要求大家看了 BERT 的 Attention
但是當你使用 Attention 的時候
還是有一些要注意的地方
我們直覺覺得 Attention 應該非常具有解釋力
從某一個詞彙 Attent 到另外一個詞彙
當然就代表說這兩個的詞彙有關係啊等等
那在作業裡面
我們也挑了比較明顯可以看出關聯性的例子
給大家來實作
給大家來回答問題
但是實際上
你在文獻上會找到這樣子的文獻
Attention is not Explanation
Attention 並不是總是可以被解釋的
當然也有人發文獻說
Attention is not not Explanation 這樣子
所以你知道這個
這個研究 進展得非常快
很快又搞不好又會有人做
Attention is not not not Explanation
所以這個
到底 Attention 能不能被解釋
什麼狀況可以被解釋
什麼狀況不能夠被解釋
這個還是尚待研究的問題
那除了用人眼觀察以外
還有另外一個技術叫做 Probing
Probing 就是用探針的意思
就是你用探針去插入這個 Network
然後看看說發生了什麼事
舉例來說
假設你想要知道 BERT 的某一個 Layer
到底學到了什麼東西
除了用肉眼觀察以外
不過肉眼觀察比較有極限嘛
可能有很多你沒有觀察到的現象
而且你也沒有辦法一次看過大批的資料
所以怎麼辦呢
你可以訓練一個探針
你的探針其實就是分類器
舉例來說
你訓練一個分類器
這個分類器是要根據一個 Feature
根據一個向量決定說現在這個詞彙
它的 POS Tag
也就是它的詞性是什麼
你就把 BERT 的 Embedding
丟到 POS 的 Classifier 裡面去
你就訓練一個 POS 的 Classifier
它要試圖根據這些 Embedding
決定說現在這些 Embedding
是來自於哪一個詞性的詞彙
如果這個 POS 的 Classifier 它的正確率高
就代表說這些 Embedding 裡面
有很多詞性的資訊
如果它正確率第一
就代表這些 Embedding 裡面
沒有詞性的資訊
或者是說你 Learn 一個 NER
Name Entity Recognition 的 Classifier
然後呢
它看這些 Feature
決定說現在看到的詞彙屬於哪
屬於人名還是地名
還是不是任何專有名詞
那你透過這個 NER Classifier 的正確率
就可以知道這些 Feature 裡面
有沒有名字
有沒有這個地址
有沒有人名的資訊等等
但是使用這個技術的時候
有一點你要小心
什麼你要小心呢
小心你使用的 Classifier 它的強度
為什麼
因為假設你今天發現你的 Classifier 正確率很低
真的一定保證它的輸入的這些 Feature
也就是 BERT 的 Embedding
沒有我們要分類的資訊嗎
不一定
為什麼
因為有可能就是你的 Classifier Train 爛啦
對不對
我們大家都有很多 Train Network 的經驗嘛
你沒有辦法 100% 保證
你 Classifier Train 出來一定是好的啊
那搞不好你訓練完一個 Classifier
它的正確率很低
不是因為這些 Feature 裡面
沒有我們需要的資訊
單純就是你 Learning Rate 沒有調好
你什麼東西沒有調好
所以 Train 不起來
有沒有可能是這樣呢
有可能是這個樣子
所以用 Probing Model 的時候
你要小心不要太快下結論
有時候你會得到一些結論
只是因為你 Classifier 沒有 Train好
或 Train 得太好
導致你 Classifier 的正確率
沒有辦法當做評斷的依據
好 那我知道說很快要下課了
時間真的是過得很快
我們講完一個語音的特殊的 Probing 的例子
我們就下課
Probing 不一定要是 Classifier
我這邊特別舉一個例子告訴你說
Probing 有種種的可能性
舉例來說
我們實驗室有做一個嘗試是
訓練一個語音合成的模型
一般語音合成的模型是吃一段文字
產生對應的聲音訊號
我們這邊語音合成的模型不是吃一段文字
它是吃 Network Output 的 Embedding
它吃 Network Output 的 Embedding 作為輸入
然後試圖去輸出一段聲音訊號
也就是說你在作業二
你訓練了一個 Classifier
那接下來訓練了一個 Phoneme 的 Classifier
接下來我們就把你的 Network 拿出來
然後呢 把某一個 Layer 的輸出呢
丟到 TTS 的模型裡面
然後我們訓練這個 TTS 的模型
我們訓練的目標
是希望 TTS 的模型
可以去復現 Network 輸入
就 Network 的輸入是這段聲音訊號
希望通過這些 Layer 以後
產生了 Embedding
丟到 TTS 以後
可以回復原來的聲音訊號
那這樣子的分析有什麼用呢
你可能會想說
我們訓練這個 TTS 產生原來的聲音訊號
那不就產生原來的聲音訊號
一模一樣的聲音訊號
就是輸入的那一個
那有什麼樣的可看性呢
那這邊有趣的地方是
假設這個 Network 做的事情
就是把 比如說語者的資訊去掉
那對於這個 TTS 的模型而言
這邊 Layer 2 的輸出
沒有任何語者的資訊
那它無論怎麼努力
都無法還原語者的特徵
那雖然內容說的是 Hi
然後是一個男生的聲音
你會發現說
可能通過幾個 Layer 以後
丟到 TTS 的模型 它
它這個產生出來的聲音
會變成也是 Hi 的內容
但是你聽不出來是誰講的
那這樣你就可以知道說
欸 這個 Network 啊
一個語音辨識的模型啊
它在訓練的過程中
真的學到去抹去語者的特徵
只保留內容的部分
只保留聲音訊號的內容
好 以下是真實的例子
講完這個例子我們就下課
好 這個例子是這樣子的
這邊有一個 5 層的 Bi-directional 的 Lstm
它吃聲音訊號做輸入
那輸出就是文字
它是語音辨識的模型
好 那現在我們給它一段聲音訊號做輸入
是女生的聲音
接下來再給它聽另外一個
男生講不一樣的內容
聽起來像是這樣的
接下來我們把這些聲音訊號
丟到這個 Network 裡面
然後再把這個 Network 的 Embedding
用 TTS 的模型去還原回原來的聲音訊號
看看我們聽到什麼
以下是過第一層 Lstm 的結果
你會發現聲音訊號有一點失真
但基本上跟原來是差不多的
男生的聲音是這樣的
跟原來都是差不多的
但通過了 5 層的 Lstm 以後發生什麼事呢
聲音訊號變成這個樣子
所以本來一個句子是男生講的
一個句子是女生講的
通過 5 層的 Lstm 以後
就聽不出來是誰講的
它把兩個人的聲音
都變成是一樣的
那你可能說
欸 這個不是 10 年前
Hinton 就已經知道了嗎
透過 Visualization
然後這個研究有什麼厲害的地方呢
他厲害地方就是他潮 知道嗎
就是我們可以把聲音
我們可以聽 Network 聽到的聲音
這邊再舉最後一個例子
輸入的聲音訊號是有雜訊的
有鋼琴的聲音
好 我們的 Network 現在前面有幾層 CNN
後面有幾層 Bi-directional 的 Lstm
通過地一層 CNN 以後
聲音訊號變成這樣
好 你還是聽得到鋼琴的聲音
好 最後
到最後一層進入 Lstm 之前
聲音訊號是這樣的
你還是聽到鋼琴的聲音
但是通過第一層 Lstm 以後
就不一樣了
它聽起來像是這個樣子
你會發現說
那個鋼琴的聲音就突然小很多
所以知道說
這個鋼琴的聲音 這個雜訊
聲音訊號之外
雜訊是在哪一層被濾掉的呢
在第一層 Lstm
開始被濾掉
前面 CNN
似乎沒有起到濾掉雜訊的作用
那這個就是這個分析可以告訴我們的事情
好 那我們講到這邊正好告一個段落
那我們就趕快下課吧
那大家有問題還可以留下來討論
謝謝大家 謝謝
