上次我們還沒有把
self-supervised learning
的部分講完
我們講完了BERT
那除了BERT以外
還有下一個
也是鼎鼎有名的模型
就是GPT系列的模型
那GPT系列的模型做的是什麼呢
上次我們有說
BERT做的是什麼
BERT做的是填空題
GPT做的事情是什麼呢
GPT就是改一下我們現在在
self-supervised learning的時候
要模型做的任務
GPT要做的任務是
預測接下來
會出現的token是什麼
舉例來說
假設你的訓練資料裡面
有一個句子是台灣大學
那GPT拿到這一筆訓練資料的時候
它會做什麼樣的訓練呢
它做的事情是這樣
你給它BOS這個token
然後呢
GPT output一個embedded
然後接下來呢
你用這個embedded去預測下一個
應該出現的token是什麼呢
那在這個句子裡面
根據這筆訓練資料
下一個應該出現的token是什麼呢
下一個應該出現的token是台
所以你要訓練你的模型
根據第一個token
根據BOS給你的embedded
那它要輸出台這個token
而如果這個部分
詳細來看就是這樣
你有一個embedded
這邊用h來表示
然後通過一個Linear Transform
再通過一個softmax
得到一個distribution
跟一般你做分類的問題是一樣的
接下來
你希望你output的distribution
跟正確答案的Cross entropy
越小越好
也就是你要去預測
下一個出現的token是什麼
好那接下來要做的事情
就是以此類推了
你給你的GPT
BOS跟台
它產生embedded
接下來它會預測
下一個出現的token是什麼
那你告訴它說
下一個應該出現的token
是灣
好 再反覆繼續下去
你給它BOS 台跟灣
然後預測下一個應該出現的token
它應該要預測大
你給它台跟灣跟大
接下來
下一個應該出現的token是學
它應該要預測出
下一個應該出現的token是學
那這邊呢
是指拿一筆資料 一個句子
來給GPT訓練
當然實際上你不會只用一筆句子
你會用成千上萬個句子
來訓練這個模型
然後呢 就沒有然後了這樣
就這樣子說完了
它厲害的地方就是
用了很多資料
訓了一個異常巨大的模型
然後看看它會有什麼厲害的地方
那這邊有一個小小的
應該要跟大家說的地方
是說這個GPT的模型
它像是一個transformer的decoder
我們知道transformer有encoder
有decoder 對不對
這我們上課的時候都有講過
模型呢
它的架構像是transformer的decoder
不過拿掉BOS的tension這個部分
也就是說
你會做那個mask的tension
就是你現在在預測給BOS
預測台的時候
你不會看到接下來出現的詞彙
給它台要預測灣的時候
你不會看到接下來要輸入的詞彙
以此類推 這個就是GPT
那這個GPT最知名的就是
因為GPT可以預測下一個token
那所以它有生成的能力
你可以讓它不斷地預測下一個token
產生完整的文章
所以我每次提到GPT的時候
它的形象都是一隻獨角獸
為什麼它的形象是一隻獨角獸呢
因為GPT系列最知名的一個例子
就是用GPT寫了一篇
跟獨角獸有關的新聞
因為他放一個假新聞
然後那個假新聞裡面說
在安地斯山脈發現獨角獸等等
一個活靈活現的假新聞
所以大家提到GPT的時候
都會想到那一則假新聞
好 為了讓你更清楚了解
GPT運作起來是什麼樣子
那這個線上有一個demo的網頁
叫做talk to transformer
就是有人把一個比較小的
不是那個最大的GPT的模型
最大的GPT模型
不是public available的
有人把比較小的GPT模型放在線上
讓你可以輸入一個句子
讓它會把接下來的其餘的內容
把它補完
它這個運作起來
看起來像是
好 這個就是GPT系列
它可以把一句話補完
那把一句話補完
怎麼把它用在downstream 的任務上呢
舉例來說
怎麼把它用在question answering
或者是其他的
跟人類語言處理有關的任務上呢
GPT用的想法跟BERT不一樣
其實我要強調一下
GPT也可以跟BERT用一樣的做法
大家還記得BERT是怎麼做的嗎
把BERT model 拿出來
後面接一個簡單的linear的classifier
那你就可以做很多事情
你也可以把GPT拿出來
接一個簡單的classifier
我相信也是會有效
但是在GPT的論文中
它沒有這樣做
它有一個更狂的想法
為什麼會有更狂的想法呢
因為首先就是
BERT那一招BERT用過了嘛
所以總不能再用一樣的東西
這樣寫paper就沒有人覺得厲害了
然後再來就是
GPT這個模型
也許真的太大了
大到連fine tune可能都有困難
你想想看我們在用BERT的時候
你要把BERT模型
後面接一個linear classifier
然後BERT也是你的
要train的model的一部分
所以它的參數也是要調的
所以在剛才助教公告的
BERT相關的作業裡面
你還是需要花一點時間來training
雖然助教說你大概20分鐘
就可以train完了
因為你並不是要train一個
完整的BERT的模型
BERT的模型在之前
在做這個填空題的時候
已經訓練得差不多了
你只需要微調它就好了
但是微調還是要花時間的
也許GPT實在是太過巨大
巨大到要微調它
要train一個η
可能都有困難
所以GPT系列
有一個更狂的使用方式
這個更狂的使用方式
和人類更接近
你想想看假設你去考
譬如說托福的聽力測驗
你是怎麼去考
托福的聽力測驗的呢
這個托福聽力測驗的敘述
是長什麼樣子的呢
首先你會看到一個題目的說明
告訴你說現在要考選擇題
請從ABCD四個選項裡面
選出正確的答案等等
然後給你一個範例
告訴你說這是題目
然後正確的答案是多少
然後你看到新的問題
期待你就可以舉一反三開始作答
GPT系列要做的事情就是
這個模型能不能夠
做一樣的事情呢
舉例來說假設要GPT這個模型做翻譯
你就先打Translate English to French
就先給它這個句子
這個句子代表問題的描述
然後給它幾個範例跟它說
sea otter然後=>
後面就應該長這個樣子
或者是這個什麼plush girafe
plush girafe後面
就應該長這個樣子等等
然後接下來
你問它說cheese=>
叫它把後面的補完
希望它就可以產生翻譯的結果
不知道大家能不能夠了解
這一個想法是多麼地狂
在training的時候
GPT並沒有教它做翻譯這件事
它唯一學到的就是
給一段文字的前半段
把後半段補完
就像我們剛才給大家示範的例子一樣
現在我們直接給它前半段的文字
就長這個樣子
告訴它說你要做翻譯了
給你幾個例子
告訴你說翻譯是怎麼回事
接下來給它cheese這個英文單字
後面能不能就直接接出
法文的翻譯結果呢
這個在GPT的文獻裡面
叫做Few-shot Learning
但是它跟一般的Few-shot Learning
又不一樣
所謂Few Shot的意思是說
確實只給了它一點例子
所以叫做Few Shot
但是它不是一般的learning
這裡面完全沒有什麼
gradient descent
training的時候
就是要跑gradient descent嘛
這邊完全沒有gradient descent
完全沒有要去調
GPT那個模型參數的意思
所以在GPT的文獻裡面
把這種訓練給了一個特殊的名字
它們叫做In-context Learning
代表說它不是一種
一般的learning
它連gradient descent都沒有做
當然你也可以給GPT更大的挑戰
我們在考托福聽力測驗的時候
都只給一個例子而已
那GPT可不可以只看一個例子
就知道它要做翻譯這件事
這個叫One-shot Learning
還有更狂的
是Zero-shot Learning
直接給它一個敘述
說我們現在要做翻譯了
GPT能不能夠自己就看得懂
就自動知道說要來做翻譯這件事情呢
那如果能夠做到的話
那真的就非常地驚人了
那GPT系列
到底有沒有達成這個目標呢
這個是一個見仁見智的問題啦
它不是完全不可能答對
但是正確率有點低
相較於你可以微調模型
正確率是有點低的
那細節你就再看看GPT那篇文章
第三代的GPT
它測試了42個任務
這個縱軸是正確率
這些實線 這三條實線
是42個任務的平均正確率
那這邊包括了Few Shot
One Shot跟Zero Shot
三條線分別代表Few Shot
One Shot跟Zero Shot
橫軸代表模型的大小
它們測試了一系列不同大小的模型
從只有0.1個billion的參數
到175個billion的參數
那從只有0.1個billion的參數
到175個billion的參數
我們看Few Shot的部分
從20幾%的正確率 平均正確率
一直做到50幾%的平均正確率
那至於50幾％的平均正確率
算是有做起來 還是沒有做起來
那這個就是見仁見智的問題啦
目前看起來狀況是
有些任務它還真的學會了
舉例來說2這個加減法
你給它一個數字加另外一個數字
它真的可以得到
正確的兩個數字加起來的結果
但是有些任務
它可能怎麼學都學不會
譬如說一些跟邏輯推理有關的任務
它的結果就非常非常地慘
好 那有關GPT3的細節
這個就留給大家再自己研究
然後這邊有一個過去上課的錄影
我把連結放在這邊給大家參考
好 剛才舉的例子
到目前為止我們舉的例子
都是只有跟文字有關
但是你不要誤會說
這種self-supervised learning的概念
只能用在文字上
在語音 在CV
CV就是computer vision
也就是影像
在語音跟影像的應用上也都可以用
self-supervised learning的技術
那其實今天
self-supervised learning的技術
非常非常地多
我們講的BERT跟GPT系列
它只是三個類型的
這個self-supervised learning的方法
的其中一種
它們是屬於prediction那一類
那其實還有其他的類型
那就不是我們這一堂課要講的
那接下來的課程
你可能會覺得有點流水帳
就是我們每一個主題呢
就是告訴你說這個主題裡面
有什麼 但是細節這個更多的知識
就留給大家自己來做更進一步的研究
所以這些投影片
只是要告訴你說
在self-supervised learning這個部分
我們講的只是整個領域的其中一小塊
那還有更多的內容
是等待大家去探索的
好那有關影像的部分呢
我們就真的不會細講
我這邊就是放兩頁投影片帶過去
告訴你說有一招非常有名的
叫做SimCLR
它的概念也不難
我相信你自己讀論文
應該也有辦法看懂它
那還有很奇怪的
叫做BYOL
BYOL這個東西呢
我們是不太可能在上課講它
為什麼呢
因為根本不知道它為什麼會work
不是 這個是很新的論文
這個是去年夏天的論文
那這個論文是
假設它不是已經發表的文章
然後學生來跟我提這個想法
我一定就是
我一定不會讓他做
這不可能會work的
這是個不可能會實現的想法
不可能會成功的
這個想法感覺有一個巨大的瑕疵
但不知道為什麼它是work的
而且還曾經一度得到
state of the art的結果
deep learning就是這麼神奇
好 所以這個呢
我們也就不細講就跳過去
好 那在語音的部分
你也完全可以使用
self-supervised learning的概念
你完全可以試著訓練
語音版的BERT
那怎麼訓練語音版的BERT呢
你就看看文字版的BERT
是怎麼訓練的
譬如說做填空題
語音也可以做填空題
就把一段聲音訊號蓋起來
叫機器去猜蓋起來的部分是什麼嘛
語音也可以預測接下來會出現的內容
講GPT就是預測
接下來要出現的token嘛
那語音你也可以叫它預測
叫模型預測接下來會出現的聲音去套
所以你也可以做語音版的GPT
不管是語音版的BERT
語音版的GPT
其實都已經有很多相關的研究成果了
不過其實在語音上
相較於文字處理的領域
還是有一些比較缺乏的東西
那我認為現在很缺乏的一個東西
就是像GLUE這樣子的
benchmark corpus
在自然語言處理的領域
在文字上有GLUE這個corpus
我們在這門課的剛開頭
這個投影片的剛開頭
就告訴你說有一個
這個基準的資料庫叫做GLUE
它裡面有九個NLP的任務
今天你要知道BERT做得好不好
就讓它去跑那九個任務在去平均
那代表這個self-supervised learning
模型的好壞
但在語音上 到目前為止
還沒有類似的基準的資料庫
所以我們實驗室就跟其他的研究團隊
共同開發了一個語音版的GLUE
我們叫做SUPERB
它是Speech processing Universal
PERformance Benchmark的縮寫
你知道今天你做什麼模型
都一定要硬湊梗才行啦
所以這邊也是要硬湊一個梗
把它叫做SUPERB
那其實我們已經準備了差不多了
其實網站都已經做好了
只等其他團隊的人看過以後
就可以上線了
所以現在雖然還沒有上線
但是再過一陣子
你應該就可以找得到相關的連結
在這個基準語料庫裡面
包含了十個不同的任務
那語音其實有非常多不同的面向
很多人講到語音相關的技術
都只知道語音辨識把聲音轉成文字
但這並不是語音技術的全貌
語音其實包含了非常豐富的資訊
它除了有內容的資訊
就是你說了什麼
還有其他的資訊
舉例來說這句話是誰說的
舉例這個人說這句話的時候
他的語氣是什麼樣
還有這句話背後
它到底有什麼樣的語意
所以我們準備了十個不同的任務
這個任務包含了語音不同的面向
包括去檢測一個模型
它能夠識別內容的能力
識別誰在說話的能力
識別他是怎麼說的能力
甚至是識別這句話背後語意的能力
從全方位來檢測一個
self-supervised learning的模型
它在理解人類語言上的能力
而且我們還有一個Toolkit
這個Toolkit裡面就包含了
各式各樣的
self-supervised learning的模型
還有這些
self-supervised learning的模型
它可以做的
各式各樣語音的下游的任務
然後把連結放在這邊給大家參考
講這些只是想告訴大家說
self-supervised learning的技術
不是只能被用在文字上
在這個影像上 在語音上
都仍然有非常大的空間可以使用
self-supervised learning的技術
好 那這個
self-supervised learning的部分呢
這個BERT跟GPT我們就講到這邊
