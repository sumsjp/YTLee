臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
各位同學大家好，我們來上課吧
今天的規劃是這樣子
就是我們等一下，會先公告 final
那 final 有三個選擇，所以是需要花時間跟大家講一下的
所以，今天的規劃是
我們等一下上課，大概上到 1: 20 以後下課
不對，腦殘了，到 11:20 吧
不是 11:20，大概是 10:20 的時候下課
然後，等一下
剩下的時間就讓助教來把
三個 final 都跟大家講完
那今天我們要講的是 Word Embedding
我們之前已經講了 Dimension Reduction
那 Word Embedding 其實是 Dimension Reduction 一個
非常好、非常廣為人知的應用
如果我們今天要你用一個 vector
來表示一個 word，你會怎麼做呢？
最 typical 的作法
叫做 1-of-N encoding
每一個 word，我們用一個 vector來表示
這個 vector 的 dimension，就是這個世界上
可能有的 word 數目
假設這個世界上可能有十萬個 word
那 1-of-N encoding 的 dimension
就是十萬維
那每一個 word，對應到其中一維
所以，apple 它就是第一維是 1，其他都是 0
bag 就是第二維是 1，cat 就是第三維是 1，以此類推，等等
如果你用這種方式來描述一個 word
你的這個 vector 一點都不 informative
你每一個 word，它的 vector 都是不一樣的
所以從這個 vector 裡面，你沒有辦法得到任何的資訊
你沒有辦法知道說
比如說，bag 跟 cat
bag 跟 cat，沒什麼關係
比如說，cat 跟 dog
它們都是動物這件事
你沒有辦法知道
那怎麼辦呢？
有一個方法就叫做建 Word Class
也就是你把不同的 word
有同樣性質的 word
把它們 cluster 成一群一群的
然後就用那一個 word 所屬的 class 來表示這個 word
這個就是我們之前，在做 Dimension Reduction 的時候
講的 clustering 的概念
比如說，dog, cat 跟 bird，它們都是class 1
ran, jumped, walk 是 class 2
flower, tree, apple 是class 3，等等
但是，光用 class 是不夠的
我們之前有講過說，光做 clustering 是不夠的
因為，如果光做 clustering 的話呢
我們找了一些 information
比如說
這個是屬於動物的 class
這是屬於植物的 class
它們都是屬於生物
但是在 class 裡面，沒有辦法呈現這一件事情
或者是說，class 1 是動物
而 class 2 代表是，動物可以做的行為
但是， class 3 是植物
class 2 裡面的行為是 class 3 裡面沒有辦法做的
所以，class 2 跟 class 1 是有一些關聯的
沒有辦法用 Word Class 呈現出來
所以怎麼辦呢？
我們需要的，是 Word Embedding
Word Embedding 是這樣
把每一個 word
都 project 到一個 high dimensional 的 space 上面
把每一個 word 都 project 到一個
high dimensional space 上面
雖然說，這邊這個 space 是 high dimensional
但是它其實遠比 1-of-N encoding 的 dimension 還要低
1-of-N encoding，你這個 vector 通常是
比如說，英文有 10 萬詞彙，這個就是 10 萬維
但是，如果是用 Word Embedding 的話呢
通常比如說 50 維、100維這個樣子的 dimension
這個是一個從 1-of-N encoding 到 Word Embedding
這是 Dimension Reduction 的 process
那我們希望在這個 Word Embedding 的
這個圖上
我們可以看到的結果是
同樣，類似 semantic，類似語意的詞彙
它們能夠在這個圖上
是比較接近的
而且在這個 high dimensional space 裡面呢
在這個 Word Embedding 的 space 裡面
每一個 dimension，可能都有它特別的含意
比如說，假設我們現在做完 Word Embedding 以後
每一個 word 的這個
Word Embedding 的 feature vector長的是這個樣子
所以，可能就可以知道說
這個 dimension 代表了
生物和其他的東西之間的差別
這個 dimension 可能就代表了，比如說
這是會動的，跟動作有關的東西
動物是會動的，還有這個是動作
跟動作有關的東西
和不會動的，是靜止的東西的差別，等等
那怎麼做 Word Embedding
Word Embedding 是一個 unsupervised approach
也就是，我們怎麼讓 machine
知道每一個詞彙的含義，是什麼呢？
你只要透過讓 machine 閱讀大量的文章
你只要讓 machine 透過閱讀大量的文章
它就可以知道，每一個詞彙
它的 embedding 的 feature vector 應該長什麼樣子
這是一個 unsupervised 的 problem
因為我們要做的事情就是
learn 一個 Neural Network
找一個 function
那你的 input 是一個詞彙
output 就是那一個詞彙所對應的 Word Embedding
它所對應的 Word Embedding
的那一個 vector
而我們手上有的 train data
是一大堆的文字
所以我們只有Input，我們只有 input
但是我們沒有 output ，我們沒有 output
我們不知道
每一個 Word Embedding 應該長什麼樣子
所以，對於我們要找的 function
我們只有單項
我們只知道輸入，不知道輸出
所以，這是一個 unsupervised learning 的問題
那這個問題要怎麼解呢？
我們之前有講過
一個 deep learning base 的 Dimension Reduction 的方法
叫做 Auto-encoder
也就是 learn 一個 network，讓它輸入等於輸出
這邊某一個 hidden layer 拿出來
就是 Dimension Reduction 的結果
在這個地方
你覺得你可以用 Auto-encoder 嗎 ?
給大家一秒鐘時間的想一想
你覺得這邊可以用 Auto-encoder 的同學，舉手一下
你覺得不能用 auto-encoder 的同學請舉手一下，謝謝
把手放下，大多數的同學都覺得
不能用 Auto-encoder 來處理這一問題
沒錯，這個問題你沒辦法用 Auto-encoder 來解
你沒辦法用 Auto-encoder 來解
這件事情有點難解釋
或許讓大家自己回去想一想
你可以想想看
如果你是用 1-of-N encoding 當作它的 input
如果你用 1-of-N encoding 當作它的 input
對 1-of-N encoding 來說，每一個詞彙都是 independent
你把這樣子的 vector 做 Auto-encoder
你其實，沒有辦法 learn 出
任何 informative 的 information
所以，在 Word Embedding 這個 task 裡面
用 Auto-encoder 是沒有辦法的
如果你這一邊 input 是 1-of-N encoding
用 Auto-encoder 是沒有辦法的
除非你說，你用這個
你用 character，比如說你用
character 的 n-gram 來描述一個 word
或許，它可以抓到一些字首、字根的含義
不過基本上，現在大家並不是這麼做的
那怎麼找這個 Word Embedding 呢
這邊的作法是這樣子
它基本的精神就是
你要如何了解一個詞彙的含義呢
你要看這個詞彙的 contest
每一個詞彙的含義
可以根據它的上下文來得到
舉例來說
假設機器讀了一段文字是說
馬英九520宣誓就職
它又讀了另外一段新聞說
蔡英文520宣誓就職
對機器來說，雖然它不知道馬英九指的是什麼
他不知道蔡英文指的是什麼
但是馬英九後面有接520宣誓就職
蔡英文的後面也有接520宣誓就職
對機器來說，只要它讀了大量的文章
發現說，馬英九跟蔡英文後面都有類似的 contest
它前後都有類似的文字
機器就可以推論說
蔡英文跟馬英九代表了某種有關係的 object
他們是某些有關係的物件
所以它可能也不知道他們是人
但它知道說，蔡英文跟馬英九這兩個詞彙
代表了，可能有同樣地位的東西
那怎麼來體現這一件事呢
怎麼用這個精神來找出 Word Embedding 的 vector 呢
有兩個不同體系的作法
一個做法叫做 Count based 的方法
Count based 的方法是這樣
如果我們現在有兩個詞彙，wi, wj
它們常常在同一個文章中出現
它們常常一起 co-occur
那它們的 word vector
我們用 V(wi) 來代表
wi 的 word vector
我們用 V(wj) 來代表，wj 的 word vector
如果 wi 跟 wj，它們常常一起出現的話
V(wi) 跟 V(wj) 它們就會比較接近
那這種方法
有一個很代表性的例子
叫做 Glove vector
我把它的 reference 附在這邊
給大家參考
那這個方法的原則是這樣子
假設我們知道
wi 的 word vector 是 V(wi)
wj 的 word vector 是 V(wj)
那我們可以計算它的 inner product
假設 Nij 是 wi 跟 wj
它們 co-occur 在同樣的 document 裡面的次數
那我們就希望為 wi 找一組 vector
為 wj 找一個組 vector
然後，希望這兩個
這兩件事情
越接近越好
你會發現說，這個概念
跟我們之前講的 LSA 是
跟我們講的 matrix factorization 的概念
其實是一樣的
另外一個方法
叫做 Prediction based 的方法
我發現我這一邊拼錯了
這應該是 Prediction based 的方法
那我不知道說
就我所知，好像沒有人
很認真的比較過說
Prediction based 方法
跟 Count based 的方法
它們有什麼樣非常不同的差異
或者是誰優誰劣
如果你有知道這方面的 information，或許
你可以貼在我們的社團上面
我講一下， Prediction based 的方法是怎麼做的呢
Prediction based 的方法，它的想法是這樣
我們來 learn 一個 neural network
它做的事情是 prediction，predict 什麼呢？
這個 neural network 做的事情是 given 前一個 word
假設給我一個 sentence
這邊每一個 w 代表一個 word
given w(下標 i-1)，這個 prediction 的 model
這個 neural network，它的工作是要
predict 下一個可能出現的 word 是誰
那我們知道說，每一個 word
我們都用 1-of-N encoding，可以把它表示成一個 feature vector
所以，如果我們要做 prediction 這一件事情的話
我們就是要 learn 一個 neural network
它的 input
就是 w(下標 i-1) 的 1-of-N encoding 的 feature vector
1-of-N encoding 的 vector
那它的 output 就是
下一個 word, wi 是某一個 word 的機率
也就是說，這一個 model 它的 output
它 output 的 dimension 就是 vector 的 size
假設現在世界上有 10 萬個 word
這個 model 的 output 就是 10 萬維
每一維代表了某一個 word
是下一個 word 的機率
每一維代表某一個 word
是會被當作 wi
它會是下一個 word, wi 的機率
所以 input 跟 output 都是 lexicon size
只是它們代表的意思是不一樣的
input 是 1-of-N encoding，output 是下一個 word 的機率
那假設這就是一個
一般我們所熟知的
multi-layer 的 Perceptron，一個 deep neural network
那你把它丟進去的時候
你把這個 input feature vector 丟進去的時候
它會通過很多 hidden layer
通過一些 hidden layer，你就會得到一些 output
接下來，我們把第一個 hidden layer 的 input 拿出來
我們把第一個 hidden layer 的 input 拿出來
假設第一個 hidden layer 的 input
我們這一邊寫作，它的第一個 dimension 是 Z1
第二個 dimension 是 Z2，以此類推
這邊把它寫作 Z
那我們用這個 Z
就可以代表一個 word
input 不同的 1-of-N encoding
這邊的 Z 就會不一樣
所以，我們就把這邊的 Z
拿來代表一個詞彙
你 input 同一個詞彙
它有同樣的 1-of-N encoding
在這邊它的 Z 就會一樣
input 不同的詞彙，這邊的 Z 就會不一樣
所以，我們就用這個 Z
這一個 input 1-of-N encoding 得到 Z 的這個 vector
來代表一個 word
來當作那一個 word 的 embedding
你就可以得到這一個現象
你就可以得到這樣的 vector
為什麼用這個 Prediction based 的方法
就可以得到這樣的 vector 呢
Prediction based 的方法
是怎麼體現我們說的
可以根據一個詞彙的上下文
來了解一個詞彙的涵義，這一件事情呢？
這邊是這樣子的
假設我們的 training data 裡面呢
有一個文章是
告訴我們說，馬英九跟蔡英文宣誓就職
另外一個是馬英九宣誓就職
在第一個句子裡面
蔡英文是 w(下標 i-1)，宣誓就職是 w(下標 i)
在另外一篇文章裡面
馬英九是 w(下標 i-1)，宣誓就職是 w(下標 i)
那你在訓練這個 Prediction model 的時候
不管是 input 蔡英文，還是馬英九
不管是 input 蔡英文還是馬英九的 1-of-N encoding
你都會希望 learn 出來的結果
是宣誓就職的機率比較大
因為馬英九和蔡英文後面
接宣誓就職的機率都是高的
所以，你會希望說 input 馬英九跟蔡英文的時候
它 output，是 output 對應到宣誓就職那一個詞彙
它的那個 dimension 的機率是高的
為了要讓
蔡英文和馬英九雖然是不同的 input
但是，為了要讓最後在 output 的地方
得到一樣的 output
你就必須再讓中間的 hidden layer 做一些事情
中間的 hidden layer 必須要學到說
這兩個不同的詞彙
必需要把他們 project 到
必須要通過這個
必須要通過 weight 的轉換
通過這個參數的轉換以後
把它們對應到同樣的空間
在 input 進入 hidden layer 之前
必須把它們對應到接近的空間
這樣我們最後在 output 的時候
它們才能夠有同樣的機率
所以，當我們 learn 一個 prediction model 的時候
考慮一個 word 的 context這一件事情
所以我們把這個 prediction model 的
第一個 hidden layer 拿出來
我們就可以得到
我們想要找的這種 word embedding 的特性
那你可能會想說
如果只用 w(下標 i-1)去 predict w(下標 i)
好像覺得太弱了
就算是人，你給一個詞彙
要 predict 下一個詞彙
感覺也很難
因為，如果只看一個詞彙，
下一個詞彙的可能性，是千千萬萬的
是千千萬萬的
那怎麼辦呢？怎麼辦呢？
你可以拓展這個問題
比如說，你可以拓展說
我希望 machine learn 的是 input 前面兩個詞彙
w(下標 i-2) 跟 w(下標 i-1)
predict 下一個 word, w(下標 i)
那你可以輕易地把這個 model 拓展到 N 個詞彙
一般我們，如果你真的要 learn 這樣的 word vector 的話
你可能會需要你的 input
可能通常是至少 10 個詞彙
你這樣才能夠 learn 出
比較 reasonable 的結果
那我們這邊用 input 兩個 word 當作例子
那你可以輕易地把
這個 model 拓展到 10 個 word
那這邊要注意的事情是這個樣子
本來，如果是一般的 neural network
你就把 input w(下標 i-2) 和 w(下標 i-1) 的
1-of-N encoding 的 vector，把它接在一起
變成一個很長的 vector
直接丟都到 neural network 裡面
當作 input 就可以了
但是實際上，你在做的時候
你會希望 w(下標 i-2) 的
這個跟它相連的 weight
跟和 w(下標 i-1) 相連的 weight
它們是被 tight 在一起的
所謂 tight 在一起的意思是說
w(下標 i-2) 的第一個 dimension
跟第一個 hidden layer 的第一個 neuron
它們中間連的 weight
和 w(下標 i-1) 的第一個 dimension
和第一個 hidden layer 的 neuron，它們之間連的weight
這兩個 weight 必須是一樣的
所以，我這邊故意用同樣的顏色來表示它
這一個 dimension，它連到這個的 weight
跟第一個 dimension，它連到這邊的 weight
它必須是一樣的
所以，我這邊故意用同樣的顏色來表示他
這一個 dimension，它連到它的 weight
跟它連到它的 weight，必須是一樣的
以此類推
希望大家知道知道我的意思
那為什麼要這樣做呢？
為什麼要這樣做呢？
一個顯而易見的理由是這樣
一個顯而易見的理由是說
如果，我們不這麼做
如果我們不這麼做，你把不同的 word
你把同一個 word 放在
w(下標 i-2) 的位置跟放在 w(下標 i-1) 的位置
通過這個 transform 以後
它得到的 embedding 就會不一樣
如果，你必須要讓這一組 weight
和這一組weight 是一樣的
那你把一個 word 放在這邊，通過這個 transform
跟把一個 weight 放在這邊，通過一個 transform
它們得到的 weight 才會是一樣的
當然另外一個理由你可以說
我們做這一件事情的好處是
我們可以減少參數量
因為 input 這個 dimension 很大，它是十萬維
所以這個 feature vector，就算你這一邊是50 維
它也是一個非常非常、碩大無朋的 matrix
有一個已經覺得夠卡了
所以，有兩個更是吃不消
更何況說，我們現在 input 往往是 10 個 word
所以，如果我們強迫讓
所有的 1-of-N encoding
它後面接的 weight 是一樣的
那你就不會隨著你的 contest 的增長
而需要這個更多的參數
或許我們用 formulation 來表示
會更清楚一點
現在，假設 w(下標 i-2) 的 1-of-N encoding 就是 X2
w(下標 i-1) 的 1-of-N encoding 就是 X1
那它們的這個長度
都是 V 的絕對值
它們的長度我這邊都寫成 V 的絕對值
那這個 hidden layer 的 input
我們把它寫一個 vector, Z
Z 的長度，是 Z 的絕對值
那我們把這個 Z 跟
X(i-2) 跟 X(i-1) 有什麼樣的關係
Z 等於 X(i-2) * W1 + X(i-1) * W2
你把 X(i-2) * W1 + X(i-1) * W2，就會得到這個 Z
那現在這個 W1 跟 W2
它們都是一個 Z 乘上一個 V dimension 的 weight matrix
那在這邊，我們做的事情是
我們強制讓 W1 要等於 W2
要等於一個一模一樣的 matrix, W
所以，我們今天實際上在處理這個問題的時候
你可以把 X(i-2) 跟 X(i-1) 直接先加起來
因為 W1 跟 W2 是一樣的
你可以把 W 提出來
你可以把 X(i-1) 跟X(i-2) 先加起來
再乘上 W 的這個 transform
就會得到 z
那你今天如果要得到一個 word 的 vector 的時候
你就把一個 word 的 1-of-N encoding
乘上這個 W
你就可以得到那一個 word 的 Word Embedding
那這一邊會有一個問題，就是我們在實做上
如果你真的要自己實做的話
你怎麼讓這個 W1 跟 W2
它們的位 weight 一定都要一樣呢
事實上我們在 train CNN 的時候
也有一樣類似的問題
我們在 train CNN 的時候
我們也要讓 W1 跟 W2
我們也要讓某一些參數，它們的 weight
必須是一樣的
那怎麼做呢？這個做法是這樣子
假設我們現在有兩個 weight, wi 跟 wj
那我們希望 wi 跟 wj，它的 weight 是一樣的
那怎麼做呢？
首先，你要給 wi 跟 wj 一樣的 initialization
訓練的時候要給它們一樣的初始值
接下來，你計算 wi 的
wi 對你最後 cost function 的偏微分
然後 update wi
然後，你計算 wj 對 cost function 的偏微分
然後 update wj
你可能會說 wi 跟 wj
如果它們對 C 的偏微分是不一樣的
那做 update 以後
它們的值，不就不一樣了嗎？
所以，如果你只有列這樣的式子
wi 跟 wj 經過一次 update 以後，它們的值就不一樣了
initialize 值一樣也沒有用
那怎麼辦呢？
我們就把 wi 再減掉
再減掉 wj 對 C 的偏微分
把 wj 再減掉 wi 對 C 的偏微分
也就是說 wi 有這樣的 update
wj 也要有一個一模一樣的 update
wj 有這樣的 update
wi 也要有一個一模一樣的 update
如果你用這樣的方法的話呢
你就可以確保 wi 跟 wj，它們是
在這個 update 的過程中
在訓練的過程中
它們的 weight 永遠都是被 tight 在一起的
永遠都是一樣
那要怎麼訓練這個 network 呢？
這個 network 的訓練
完全是 unsupervised 的
也就是說，你只要 collect 一大堆文字的data
collect 文字的 data 很簡單
就寫一個程式上網去爬就好
寫一個程式爬一下
八卦版的 data
就可以爬到一大堆文字
然後，接下來就可以 train 你的 model
怎麼 train，比如說這邊有一個句子就是
潮水退了，就知道誰沒穿褲子
那你就讓你的 model
讓你的 neural network input "潮水" 跟 "退了"
希望它的 output 是 "就" 這個樣子
你會希望你的 output 跟"就" 的 cross entropy
"就" 也是一個 1-of-N encoding 來表示
所以，你希望你的 network 的 output
跟 "就" 的 1-of-N encoding
是 minimize cross entropy
然後，再來就 input "退了 " 跟 "就"
然後，希望它的 output 跟 "知道" 越接近越好
然後 output "就" 跟 "知道"
然後就，希望它跟 "誰" 越接近越好
那剛才講的
只是最基本的型態
其實這個 Prediction based 的 model
可以有種種的變形
目前我還不確定說
在各種變形之中哪一種是比較好的
感覺上，它的 performance
在不同的 task上互有勝負
所以，很難說哪一種方法一定是比較好的
那有一招叫做
Continuous bag of word, (CBOW)
那 CBOW 是這個樣子的
CBOW 是說，我們剛才是拿前面的詞彙
去 predict 接下來的詞彙
那 CBOW 的意思是說
我們拿某一個詞彙的 context
去 predict 中間這個詞彙
我們拿 W(i-1) 跟 W(i+1) 去 predict Wi
用 W(i-1) 跟 W(i+1)去 predict Wi
那 Skip-gram 是說
我們拿中間的詞彙去 predict 接下來的 context
我們拿 Wi 去 predict W(i-1) 跟 W(i+1)
也就是 given 中間的 word，我們要去 predict 它的周圍
會是長什麼樣子
講到這邊大家有問題嗎？
講到這邊常常會有人問我一個問題
假設你有讀過 word vector 相關的文獻的話
你可能會說
其實這個 network 它不是 deep 的阿
雖然，常常在講 deep learnin g 的時候
大家都會提到 word vector
把它當作 deep learning 的一個 application
但是，如果你真的有讀過 word vector 的文獻的話
你會發現說
這個 neural network，它不是 deep 的
它其實就是一個 hidden layer
它其實是一個 linear 的 hidden layer
了解嗎？就是
這個 neural network，它只有一個 hidden layer
所以，你把 word input 以後，你就得到 word embedding
你就直接再從那個 hidden layer，就可以得到 output
它不是 deep 的，為什麼呢？
為什麼？常常有人 問我這個問題
那為了回答這個問題
我邀請了 Tomas Mikolov 來台灣玩這樣
Tomas Mikolov 就是 propose word vector 的作者
所以，如果你有用過 word vector 的 toolkit 的話
你可能有聽過他的名字
那就問他說，為什麼這個 model不是 deep 的呢？
他給我兩個答案
他說，首先第一個就是
他並不是第一個 propose word vector 的人
在過去就有很多這樣的概念
那他最 famous 的地方是
他把他寫的一個非常好的 toolkit 放在網路上
他在他的 toolkit 裡面，如果你看他的 code 的話
他有種種的 tip
所以，你自己做的時候做不出他的 performance 的
他是一個非常非常強 的 engineer
他有各種他自己直覺的 sense
所以你自己做，你做不出他的 performance
用他的 toolkit，跑出來的 performance 就是特別好
所以，這是一個
他非常厲害的地方
他說，在他之前其實就有很多人做過
word vector，也有提出類似的概念
他說他寫的，他有一篇 word vector 的文章跟 toolkit
他想要 verify 最重要的一件事情是說
過去其實其他人就是用 deep
他想要講的是說，其實這個 task
不用 deep 就做起來了
不用 deep 的好處就是減少運算量
所以它可以跑很大量、很大量、很大量的 data
那我聽他這樣講
我就想起來，其實過去確實是
有人已經做過 word vector
過去確實已經有做過 word vector 這件事情
只是那些結果沒有紅起來
我記得說，我大學的時候
就看過類似的 paper
我大學的時候就有看過
其實就是一樣，就是 learn 一個 Prediction model
predict 下一個 word 的做法
只是那個時候是 deep
在我大學的時候
那時候 deep learning 還不紅
我看到那一篇 paper 的時候
他最後講說我 train 了這個 model
我花了 3 週，然後我沒有辦法把實驗跑完
所以結果是很好的
就其他方法，他可以跑很多的 iteration
然後說這個 neural network 的方法
我跑了 5 個 epoch，花了 3 週，我實在做不下去
所以，performance 沒有特別好
而且想說，這是什麼荒謬的做法
但是，現在運算量不同
所以，現在要做這一件事情呢
都沒有問題
其實像 word embedding 這個概念
在語音界，大概是在 2010 年的時候開始紅起來的
那個時候我們把它叫做 continuous 的 language model
一開始的時候
也不是用 neural network 來得到這個 word embedding的
因為 neural network 的運算量比較大
所以，一開始並不是選擇 neural network
而是用一些其他方法來
一些比較簡單的方法來得到這個 word 的 embedding
只是，後來大家逐漸發現說
用 neural network 得到的結果才是最好的
過去其他不是 neural network 的方法
就逐漸式微
通通都變成 neural network based 的方法
還有一個勵志的故事
就是Tomas Mikolov 那個
word vector paper不是非常 famous 嗎？
它的 citation，我不知道，搞不好都有 1 萬了
他說他第一次投那 一篇 paper 的時候
他先投到一個，我已經忘記名字的
很小很小的會，accept rate 有 70%
然後就被 reject 了
他還得到一個 comment，就是這是什麼東西
我覺得這東西一點用都沒有
所以，這是一個非常勵志的故事
那我們知道說
word vector 可以得到一些有趣的特性
我們可以看到說
如果你把同樣類型的東西的 word vector 擺在一起
比如說，我們把這個 Italy
跟它的首都 Rome 擺在一起
我們把Germany 跟它的首都 Berlin 擺在一起
我們把 Japan
跟它的首都 Tokyo 擺在一起
你會發現說
它們之間是有某種固定的關係的
或者是，你把一個動詞的三態擺在一起
你會發現說，動詞的三態
同一個動詞的三態
它們中間有某種固定的關係
成為這個三角形
所以從這個 word vector 裡面呢
你可以 discover 你不知道的 word 跟 word 之間的關係
比如說，還有人發現說
如果你今天把
兩個 word vector 和 word vector 之間，兩兩相減
這個結果是把 word vector 跟 word vector 之間兩兩相減
然後 project 到一個 2 dimensional 的 space 上面
那你會發現說，在這一區
如果今天 word vector 兩兩相減
它得到的結果是落在這個位置的話
那這兩個 word vector 之間，它們就有，比如說
某一個 word 是包含於某一個 word 之間的關係
比如說，你把 (這一邊這個字比較小)
比如說，你把海豚跟會轉彎的白海豚相減
它的 vector 落在這邊
你把演員跟主角相減，落在這一邊
你把工人跟木匠相減，落在這邊
你把職員跟售貨員相減，落在這一邊
你把羊跟公羊相減，落在這邊
如果，某一個東西是
屬於另外一個東西的話
你把它們兩個 word vector 相減
它們的結果呢，會是很類似的
所以用 word vector 的這一個的概念
我們可以做一些簡單的推論
舉例來說， 因為我們知道說
比如說，hotter 的 word vector
減掉 hot 的 word vector 會很接近
bigger 的 word vector 減掉 big 的 word vector
或是 Rome 的 vector 減掉 Italy 的 vector
會很接近 Berlin 的 vector 減掉 Germany 的 vector
或是 King 的 vector 減掉 queen 的 vector 會很接近
uncle 的 vector 減掉 aunt 的 vector
如果有人問你說，羅馬之於義大利
就好像 Berlin 之於什麼？
智力測驗都會考這樣的問題
機器可以回答這種問題了
怎麼做呢？因為我們知道說
今天這個問題的答案
Germany 的 vector 會很接近 Berlin 的 vector
減掉 Rome 的 vector 加 Italy 的 vector
因為這 4 個 word vector 中間有這樣的關係
所以你可以把 Germany 放在一邊
把另外三個 vector 放在右邊
所以 Germany 的 vector 會接近 Berlin 的 vector
減掉 Rome 的 vector 再加上 Italy 的 vector
所以，如果你要回答這個問題
假設你不知道答案是 Germany 的話
那你要做的事情就是
計算 Berlin 的 vector
減掉 Rome的 vector，再加 Italy 的 vector
然後看看它跟哪一個 vector 最接近
你可能得到的答案就是 Germany
這邊有一個 word vector 的 demo
就讓機器讀了大量 PTT 的文章以後
它就像這樣
那 word vector 還可以做很多其他的事情
比如說，你可以把不同的語言的 word vector
把它拉在一起
如果，你今天有一個中文的 corpus
有一個英文的 corpus
你各自去、分別去 train 一組 word vector
你會發現說
中文跟英文的 word vector
它是完全沒有任何的關係的
它們的每一個 dimension
對應的含義並沒有任何關係，為什麼？
因為你要 train word vector 的時候
它憑藉的就是上下文之間的關係
所以，如果你今天的 corpus 裡面
沒有中文跟英文的句子混雜在一起
沒有中文跟英文的詞彙混雜在一起
那 machine 就沒有辦法判斷
中文的詞彙跟英文的詞彙他們之間的關係
但是，今天假如你已經事先知道說
某幾個詞彙
某幾個中文的詞彙和某幾個英文的詞彙
它們是對應在一起的
那你先得到一組中文的 vector
再得到一組英文的 vector
接下來，你可以再 learn 一個 model
它把中文和英文對應的詞彙
比如說，我們知道 "加大" 對應到 "enlarge"
"下跌" 對應到 "fall"
你把對應的詞彙，通過這個 projection 以後，
把它們 project 在 space上的同一個點
把它們 project 在 space 上面的同一個點
那在這個圖上，綠色的然後下面又有
這個綠色的英文的代表是
已經知道對應關係的中文和英文的詞彙
然後，如果你做這個 transform 以後
接下來有新的中文的詞彙和新的英文的詞彙
你都可以用同樣的 projection
把它們 project 到同一個 space 上面
比如說，你就可以自動知道說
中文的降低跟的英文的 reduce
它們都應該落在這個位置
都應該落在差不多的位置等等這樣
你就可以自動做到
比如說，類似翻譯這個樣子的效果
那這個 embedding不只限於文字
你也可以對影像做 embedding
這邊有一個很好的例子
這個例子是這樣做的
它說，我們先已經找到一組 word vector
比如說，dog 的 vector、horse 的 vector
auto 的 vector 和 cat 的 vector
它們分佈在空間上是這樣子的位置
接下來，你 learn 一個 model
它是 input 一張 image
output 是跟一個跟 word vector
一樣 dimension 的 vector
那你會希望說
狗的 vector 就散佈在狗的周圍
馬的 vector 就散佈在馬的周圍
車輛的 vector 就散佈在 auto 的周圍
那假設有一些 image
你已經知道他們是屬於哪一類
你已經知道說這個是狗、這個是馬、這個是車
你可以把它們 project 到
它們所對應到的 word vector 附近
那這個東西有什麼用呢？
假如你今天有一個新的 image 進來
比如說，這個東西，它是個貓
但是你不知道它是貓
機器不知道它是貓
但是你通過它們的 projection
把它 project 到這個 space 上以後
神奇的是你就會發現它可能就在貓的附近
那你的 machine 就會自動知道說
這個東西叫做貓
當我們一般在做影像分類的時候
大家都已經有做過作業三
作業三就是影像分類的問題
在做影像分類的問題的時候
你的 machine 其實很難去處理
新增加的，它沒有辦法看過的 object
舉例來說，作業 3 裡面
我們就先已經訂好 10 個 class
你 learn 出來的 model
就是只能分這 10 個 class
如果今天有一個新的東西
不在這10個 class 裡面
你的 model 是完全是無能為力 的
它根本不知道它叫做什麼
但是，如果你用這個方法的話
就算有一張 image
是你在 training 的時候，你沒有看過的 class
比如說，貓這個 image
它從來都沒有看過
但是如果貓的這個 image
可以 project 到 cat 的 vector 附近的話
你就會知道說，這一張 image 叫做 cat
如果你可以做到這一件事，就好像是
machine 先閱讀了大量的文章以後
它知道說，每一個詞彙
指的是什麼意思
它知道說，狗啊，貓啊，馬啊
它們之間有什麼樣的關係
它透過閱讀大量的文章，先了解詞彙間的關係
接下來，在看 image 的時候
它就可以根據它已經閱讀得到的知識
去 mapping 每一個 image
所應該對應的東西
這樣就算是它看到它沒有看過的東西
它也可能可以把它的名字叫出來
那剛才講的呢
都是 word embedding
也可以做 document 的 embedding
不只是把一個 word 變成一個 vector
也可以把一個 document 變成一個 vector
那怎麼把一個 document 變成一個 vector 呢
最簡單的方法，我們之前已經講過了
就是把一個 document 變成一個 word
然後，用 Auto-encoder
你就可以 learn 出
這個 document 的 Semantic Embedding
但光這麼做是不夠的
我們光用這個 word 來描述一篇 document
是不夠的，為什麼呢？
因為我們知道說，詞彙的順序
代表了很重要的含
舉例來說
這一邊有兩個詞彙，有兩個句子
一個是： white blood cells destroying an infection
另外一個是：an infection destroying white blood cells
這兩句話，如果你看它的 bag-of-word 的話
它們的 bag-of-word 是一模一樣的
因為它們都有出現有這 6 個詞彙
它們都有出現這 6 個詞彙
只是順序是不一樣的
但是因為它們的順序是不一樣的
所以上面這一句話
白血球消滅了傳染病，這個是 positive
下面這句話，它是 negative
雖然說，它們有同樣的 bag-of-word
它們在語意上，完全是不一樣的
所以，光只是用 bag-of-word
來描述一張 image 是非常不夠的
用 bag-of-word 來描述 一篇 document 是非常不足的
你用 bag-of-word 會失去很多重要的 information
那怎麼做呢？
我們這一邊就不細講
這邊就列了一大堆的 reference 給大家參考
上面這 3 個方法，它是 unsupervised
也就是說你只要 collect
一大堆的 document
你就可以讓它自己去學
那下面這幾個方法算是 supervised
因為，在這一些方法裡面
你需要對每一個 document
進行額外的 label
你不用 label 說，每一個 document 對應的 vector是什麼
但是你要給它其他的 label
才能夠 learn 這一些 vector
所以下面，不算是完全 unsupervised
我把 reference 列在這邊，給大家參考
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
