好 那我們來上課吧
那這一堂課啊
我們要講的是
Deep Reinforcement Learning
也就是 RL
那我想這個 RL 啊
Reinforcement Learning 啊
大家一定一點都不陌生
因為你知道很多很潮的應用
AlphaGo 等等
它背後呢
用的就是 RL 的技術
那 RL 可以講的技術啊
非常非常地多
它不是在一堂課裡面可以講得完的
我甚至覺得說
如果有人要把它開成一整個學期的課
可能也是有這麼多東西可以講
所以今天啊
這堂課的目的
並不是要告訴你有關 RL 的一切
而是讓大家有一個基本的認識
大概知道 RL 是什麼樣的東西
那 RL 相關的課程
你其實在網路上可以找到
非常非常多的參考的資料
那 RL 如果要講得非常地艱澀
其實也是可以講得非常地艱澀的
可是今天這一堂課啊
我們儘量避開太過理論的部分
我期待這堂課可以讓你做到的
並不是讓你聽了覺得
哇 RL 很困難啊
搞不清楚在做什麼
而是期待讓你覺得說
啊 RL 原來就只是這樣而已
我自己應該也做得起來
希望這一堂課可以達到這一個目的
好 那什麼是 Reinforcement Learning 呢
到目前為止啊
我們講的幾乎都是 Supervised Learning
假設你要做一個 Image 的 Classifier
你不只要告訴機器
它的 Input 是什麼
你還要告訴機器
它應該輸出什麼樣的 Output
然後接下來呢
你就可以 Train 一個 Image 的 Classifier
那在多數這門課講到目前為止的技術
基本上都是基於 Supervised Learning 的方法
就算是我們在講 Self Supervised Learning 的時候
我們其實也是
很類似 Supervised Learning 的方法
只是我們的 Label
不需要特別僱用人力去標記
它可以自動產生
或者是我們在講 Auto-encoder 的時候
我們雖然說它是一個 Unsupervised 的方法
我們沒有用到人類的標記
但事實上
我們還是有一個 Label
只是這個 Label
不需要耗費人類的力量來產生而已
好 但是 RL 就是另外一個面向的問題了
在 RL 裡面
我們遇到的問題是這樣子的
我們
機器當我們給它一個輸入的時候
我們不知道最佳的輸出應該是什麼
舉例來說
假設你要叫機器學習下圍棋
用 Supervised Learning 的方法
好像也可以做
你就是告訴機器說
看到現在的盤勢長這個樣子的時候
下一步應該落子的位置在哪裡
但是問題是
下一步應該落子的位置到底應該在哪裡呢
哪一個是最好的下一步呢
哪一步是神之一手呢
可能人類根本就不知道
當然你可以說
讓機器閱讀很多職業棋士的棋譜
讓機器閱讀很多高段棋士的棋譜
也許這些棋譜裡面的答案
也許這些棋譜裡面給某一個盤勢
人類下的下一步
就是一個很好的答案
但它是不是最好的答案呢
我們不知道
在這個你不知道正確答案是什麼的情況下
往往就是 RL 可以派上用場的時候
所以當你今天
你發現你要蒐集有標註的資料很困難的時候
正確答案人類也不知道是什麼的時候
也許就是你可以考慮使用 RL 的時候
但是 RL 在學習的時候
機器其實也不是一無所知的
我們雖然不知道正確的答案是什麼
但是機器會知道什麼是好
什麼是不好
機器會跟環境去做互動
得到一個叫做 Reward 的東西
這我們等一下都還會再細講
所以機器會知道
它現在的輸出是好的還是不好的
來藉由跟環境的互動
藉由知道什麼樣的輸出是好的
什麼樣的輸出是不好的
機器還是可以學出一個模型
好 那接下來呢
這是今天這份投影片的 Outline
首先呢
我們會從最基本的 RL 的概念開始
那在介紹這個 RL 概念的時候
有很多不同的切入點啦
也許你比較常聽過的切入點是這樣
比如說從 Markov Decision Process 開始講起
那我們這邊選擇了一個比較不一樣的切入點
我要告訴你說
雖然如果你自己讀 RL 的文獻的話
你會覺得
哇 RL 很複雜哦
跟一般的 Machine Learning 好像不太一樣哦
但是我這邊要告訴你說
RL 它跟我們這一門課學的 Machine Learning
是一樣的框架
我們在今天這個這學期
一開始的第一堂課就告訴你說
Machine Learning 就是三個步驟
那 RL 呢
RL 也是一模一樣的三個步驟
等一下會再跟大家說明
好 在今天
在這個本學期這一門課的第一開始
就告訴你說
什麼是機器學習
機器學習就是找一個 Function
Reinforcement Learning
RL 也是機器學習的一種
那它也在找一個 Function
它在找什麼樣的 Function 呢
那 Reinforcement Learning 裡面呢
我們會有一個 Actor
還有一個 Environment
那這個 Actor 跟 Environment
會進行互動
你的這個 Environment
你的這個環境啊
會給 Actor 一個 Observation
會給
那這個 Observation 呢
就是 Actor 的輸入
那 Actor 呢
看到這個 Observation 以後呢
它會有一個輸出
這個輸出呢
叫做 Action
那這個 Action 呢
會去影響 Environment
這個 Actor 採取 Action 以後呢
Environment 就會給予新的 Observation
然後 Actor 呢
會給予新的 Action
那這個 Observation 是 Actor 的輸入
那這個 Action 呢
是 Actor 的輸出
所以 Actor 本身啊
它就是一個 Function
其實 Actor
它就是我們要找的 Function
這個 Function 它的輪入
就是環境給它的 Observation
輸出就是這個 Actor 要採取的 Action
而今天在這個互動的過程中呢
這個 Environment
會不斷地給這個 Actor 一些 Reward
告訴它說
你現在採取的這個 Action
它是好的還是不好的
而我們今天要找的這個 Actor
我們今天要找的這個 Function
可以拿 Observation 當作 Input
Actor 當作 Output 的 Function
這個 Function 的目標
是要去 Maximizing
我們可以從 Environment
獲得到的 Reward 的總和
我們希望呢
找一個 Function
那用這個 Function 去跟環境做互動
用 Observation 當作 Input
輸出 Action
最終得到的 Reward 的總和
可以是最大的
這個就是 RL 要找的 Function
那我知道這樣講
你可能還是覺得有些抽象
所以我們舉更具體的例子
那等一下舉的例子呢
都是用 Space Invader 當作例子啦
那 Space Invader 就是一個非常簡單的小遊戲
那 RL 呢
最早的幾篇論文
也都是玩
讓那個機器呢
去玩這個 Space Invader 這個遊戲
在 Space Invader 裡面呢
你要操控的是下面這個綠色的東西
這個下面這個綠色的東西呢
是你的太空梭
你可以採取的行為
也就是 Action 呢 有三個
左移 右移跟開火
就這三個行為
然後你現在要做的事情啊
就是殺掉畫面上的這些外星人
畫面上這些黃色的東西
也就是外星人啦
然後你開火
擊中那些外星人的話
那外星人就死掉了
那前面這些東西是什麼呢
那個是你的防護罩
如果你不小心打到自己的防護罩的話
你的防護罩呢
也是會被打掉的
那你可以躲在防護罩後面
你就可以擋住外星人的攻擊
然後接下來呢 會有分數
那在螢幕畫面上會有分數
當你殺死外星人的時候
你會得到分數
或者是在有些版本的 Space Invader 裡面
會有一個補給包
從上面橫過去 飛過去
那你打到補給包的話
會被加一個很高的分數
那這個 Score 呢
就是 Reward
就是環境給我們的 Reward
好 那這個遊戲呢
它是會終止的
那什麼時候終止呢
當所有的外星人都被殺光的時候就終止
或者是呢
外星人其實也會對你的母艦開火啦
外星人擊中你的母艦 你也是會
這個你就被摧毀了
那這個遊戲呢
也就終止了
好 那這個是介紹一下
Space Invader 這一個遊戲
好 那如果你今天呢
要用 Actor 去玩 Space Invader
大概會像是什麼樣子呢
現在你的 Actor 啊
Actor 雖然是一個機器
但是它是坐在人的這一個位置
它是站在人這一個角度
去操控搖桿
去控制那個母艦
去跟外星人對抗
而你的環境是什麼
你的環境呢
是遊戲的主機
遊戲的主機這邊去操控那些外星人
外星人去攻擊你的母艦
所以 Observation 是遊戲的畫面
所以對 Actor 來說
它看到的
其實就跟人類在玩遊戲的時候
看到的東西是一樣的
就看到一個遊戲的畫面
那輸出呢
就是 Actor 可以採取的行為
那可以採取哪些行為
通常是事先定義好的
在這個遊戲裡面
就只有向左 向右跟開火
三種可能的行為而已
好 那當你的 Actor 採取向右這個行為的時候
那它會得到 Reward
那因為在這個遊戲裡面
只有殺掉外星人會得到分數
而我們就是把分數定義成我們的 Reward
那向左 向右其實並不會
不可能殺掉任何的外星人
所以你得到的 Reward 呢
就是 0 分
好 那你採取一個 Action 以後呢
遊戲的畫面就變了
遊戲的畫面變的時候
就代表了有了新的 Observation 進來
有了新的 Observation 進來
你的 Actor 就會決定採取新的 Action
你的 Actor 是一個 Function
這個 Function 會根據輸入的 Observation
輸出對應的 Action
那新的畫面進來
假設你的 Actor
現在它採取的行為是開火
而開火這個行為正好殺掉一隻外星人的時候
你就會得到分數
那這邊假設得到的分數是 5 分
殺那個外星人
得到的分數是 5 分
那你就得到 Reward 等於 5
那這個呢
就是拿 Actor 去玩
玩這個 Space Invader 這個遊戲的狀況
好 那這個 Actor 呢
它想要學習的是什麼呢
我們在玩遊戲的過程中
會不斷地得到 Reward
那在剛才例子裡面
做第一個行為的時候
向右的時候得到的是 0 分
做第二個行為
開火的時候得到的是 5 分
那接下來你採取了一連串行為
都有可能給你分數
而 Actor 要做的事情
我們要學習的目標
我們要找的這個 Actor 就是
我們想要 Learn 出一個 Actor
這個 Actor
這個 Function
我們使用它在這個遊戲裡面的時候
可以讓我們得到的 Reward 的總和會是最大的
那這個就是拿 Actor 去
這個就是 RL 用在玩這個小遊戲裡面的時候
做的事情
好 那其實如果把 RL 拿來玩圍棋
拿來下圍棋
其實做的事情跟小遊戲
其實也沒有那麼大的差別
只是規模跟問題的複雜度不太一樣而已
那如果今天你要讓機器來下圍棋
那你的 Actor 就是就是 AlphaGo
那你的環境是什麼
你的環境就是 AlphaGo 的人類對手
那 AlphaGo 的輸入是什麼
你的這個 Actor 的輸入是什麼
你的 Actor 的輸入就是棋盤
棋盤上黑子跟白子的位置
那如果是在遊戲的一開始
棋盤上就空空的
空空如也
上面什麼都沒有
沒有任何黑子跟白子
那這個 Actor 呢
看到這個棋盤呢
它就要產生輸出
它就要決定它下一步
應該落子在哪裡
那如果是圍棋的話
你的輸出的可能性就是有 19×19 個可能性
那這 19×19 個可能性
每一個可能性
就對應到棋盤上的一個位置
好 那假設現在你的 Actor
決定要落子在這個地方
那這一個結果
就會輸入給你的環境
那其實就是一個棋士
然後呢 這個環境呢
就會再產生新的 Observation
因為這個李世石這個棋士呢
也會再落一子
那現在看到的環境又不一樣了
那你的 Actor 看到這個新的 Observation
它就會產生新的 Action
然後就這樣反覆繼續下去
你就可以讓機器做下圍棋這件事情
好 那在這個
在這個下圍棋這件事情裡面的 Reward
是怎麼計算的呢
在下圍棋裡面
你所採取的行為
幾乎都沒有辦法得到任何 Reward
在下圍棋這個遊戲裡
在下圍棋這件事情裡面呢
你會定義說
如果贏了
就得到 1 分
如果輸了就得到 -1 分
也就是說在下圍棋這整個
這個你的 Actor 跟環境互動的過程中
其實只有遊戲結束
只有整場圍棋結束的最後一子
你才能夠拿到 Reward
就你最後
最後 Actor 下一子下去
贏了
就得到 1 分
那最後它落了那一子以後
遊戲結束了
它輸了
那就得到 -1 分
那在中間整個互動的過程中的 Reward
就都算是 0 分
沒有任何的 Reward
那這個 Actor 學習的目標啊
就是要去最大化
它可能可以得到的 Reward
好 剛才講的也許你都已經聽過了
那這個是 RL 最常見的一種解說方式
那接下來要告訴你說
RL 跟機器學習的 Framework
它們之間的關係是什麼
開學第一堂課就告訴你說
Machine Learning 就是三個步驟
第一個步驟
你有一個 Function
那個 Function 裡面有一些未知數
Unknown 的 Variable
這些未知數是要被找出來的
第二步
訂一個 Loss Function
第三步
想辦法找出未知數去最小化你的 Loss
第三步就是 Optimization
而 RL 其實也是一模一樣的三個步驟
我們先來看第一個步驟
第一個步驟
我們現在有未知數的這個 Function
到底是什麼呢
這個有未知數的 Function
就是我們的 Actor
那在 RL 裡面
你的 Actor 呢
就是一個 Network
那我們現在通常叫它 Policy 的 Network
那在過去啊
在還沒有把 Deep Learning 用到 RL 的時候
通常你的 Actor 是比較簡單的
它不是 Network
它可能只是一個 Look-Up-Table
告訴你說看到什麼樣的輸入
就產生什麼樣的輸出
那今天我們都知道要用 Network
來當做這個 Actor
那這個 Network
其實就是一個很複雜的 Function
這個複雜的 Function 它的輸入是什麼呢
它的輸入就是遊戲的畫面
就是遊戲的畫面
這個遊戲上面
這個黑子跟這個
這個不是下圍棋啦
所以沒有黑子跟白子啊
那這裡面如果是拿這個
玩 Space Invader 當作例子的話呢
這個遊戲畫面上的 Pixel
像素
就是這一個 Actor 的輸入
那它的輸出是什麼呢
它的輸出就是
每一個可以採取的行為
它的分數
每一個可以採取的 Action 它的分數
舉例來說 輸入這樣的畫面
給你的 Actor
你的 Actor 其實就是一個 Network
它的輸出可能就是給
向左 0.7 分
向右 0.2 分
開火 0.1 分
那事實上啊
這件事情跟分類是沒有什麼兩樣的
你知道分類就是輸入一張圖片
輸出就是決定這張圖片是哪一個類別
那你的 Network 會給每一個類別
一個分數
你可能會通過一個 Softmax Layer
然後每一個類別都有個分數
而且這些分數的總和是 1
那其實在 RL 裡面
你的 Actor 你的 Policy Network
跟分類的那個 Network
其實是一模一樣的
你就是輸入一張圖片
輸出其實最後你也會有個 Softmax Layer
然後呢
你就會 Left、Right 跟 Fire
三個 Action 各給一個分數
那這些分數的總和
你也會讓它是 1
那至於這個 Network 的架構呢
那你就可以自己設計了
要設計怎麼樣都行
比如說如果輸入是一張圖片
欸 也許你就會想要用 CNN 來處理
不過在助教的程式裡面
其實不是用 CNN 來處理啦
因為在我們的作業裡面
其實在玩遊戲的時候
不是直接讓我們的 Machine 去看遊戲的畫面
讓它直接去看遊戲的
讓它直接去看遊戲的畫面比較難做啦
所以我們是讓
看這個跟現在遊戲的狀況有關的一些參數而已
所以在這個助教的
在這個作業的這個 Sample Code 裡面呢
還沒有用到 CNN 那麼複雜
就是一個簡單的 Fully Connected Network
但是假設你要讓你的 Actor
它的輸入真的是遊戲畫面
欸 那你可能就會採取這個 CNN
你可能就用 CNN 當作你的 Network 的架構
甚至你可能說
我不要只看現在這一個時間點的遊戲畫面
我要看整場遊戲到目前為止發生的所有事情
可不可以呢
可以
那過去你可能會用 RNN 考慮
現在的畫面跟過去所有的畫面
那現在你可能會想要用 Transformer
考慮所有發生過的事情
所以 Network 的架構是你可以自己設計的
只要能夠輸入遊戲的畫面
輸出類似像類別這樣的 Action 就可以了
那最後機器會決定採取哪一個 Action
取決於每一個 Action 取得的分數
常見的做法啊
是直接把這個分數
就當做一個機率
然後按照這個機率
去 Sample
去隨機決定要採取哪一個 Action
舉例來說 在這個例子裡面
向左得到 0.7 分
那就是有 70% 的機率會採取向左
20% 的機率會採取向右
10% 的機率會採取開火
那你可能會問說
為什麼不是用（00：18：47）呢
為什麼不是看 Left 的分數最高
就直接向左呢
你也可以這麼做
但是在助教的程式裡面
還有多數 RL 應用的時候 你會發現
我們都是採取 Sample
採取 Sample 有一個好處是說
今天就算是看到同樣的遊戲畫面
你的機器每一次採取的行為
也會略有不同
那在很多的遊戲裡面這種隨機性
也許是重要的
比如說你在做剪刀石頭布的時候
如果你總是會出石頭
就跟小叮噹一樣
那你就很容易被打爆
如果你有一些隨機性
就比較不容易被打爆
那其實之所以今天的輸出
是用隨機 Sample 的
還有另外一個很重要的理由
那這個我們等一下會再講到
好 所以這是第一步
我們有一個 Function
這個 Function 有 Unknown 的 Variable
我們有一個 Network
那裡面有參數
這個參數就是 Unknown 的 Variable
就是要被學出來的東西
這是第一步
然後接下來第二步
我們要定義 Loss
在 RL 裡面
我們的 Loss 長得是什麼樣子呢
我們再重新來看一下
我們的機器跟環境互動的過程
那只是現在用不一樣的方法
來表示剛才說過的事情
好 首先有一個初始的遊戲畫面
這個初始的遊戲畫面
被作為你的 Actor 的輸入
你的 Actor 那就輸出了一個 Action
比如說向右
輸入的遊戲畫面呢
我們叫它 s1
然後輸出的 Action 呢
就叫它 a1
那現在會得到一個 Reward
這邊因為向右沒有做任何事情
沒有殺死任何的外星人
所以得到的 Reward 可能就是 0 分
採取向右以後
會看到新的遊戲畫面
這個叫做 s2
根據新的遊戲畫面 s2
你的 Actor 會採取新的行為
比如說開火
這邊用 a2
來表示看到遊戲畫面 s2 的時候
所採取的行為
那假設開火恰好殺死一隻外星人
和你的 Actor 就得到 Reward
這個 Reward 的分數呢
是 5 分
然後採取開火這個行為以後
接下來你會看到新的遊戲畫面
那機器又會採取新的行為
那這個互動的過程呢
就會反覆持續下去
直到機器在採取某一個行為以後
遊戲結束了
那什麼時候遊戲結束呢
就看你遊戲結束的條件是什麼嘛
舉例來說
採取最後一個行為以後
比如說向右移
正好被外星人的子彈打中
那你的飛船就毀了
那遊戲就結束了
或者是最後一個行為是開火
把最後一隻外星人殺掉
那遊戲也就結束了
就你執行某一個行為
滿足遊戲結束的條件以後
遊戲就結束了
那從遊戲開始到結束的這整個過程啊
被稱之為一個 Episode
那在整個遊戲的過程中
機器會採取非常多的行為
每一個行為都可能得到 Reward
把所有的 Reward 通通集合起來
我們就得到一個東西
叫做整場遊戲的 Total Reward
好 那這個 Total Reward 呢
就是從遊戲一開始得到的 r1
一直得
一直加
累加到遊戲最後結束的時候
得到的 rt
假設這個遊戲裡面會互動
T 次
那麼就得到一個 Total Reward
我們這邊用 R
來表示 Total Reward
其實這個 Total Reward 又有另外一個名字啊
叫做 Return 啦
你在這個 RL 的文獻上
常常會同時看到 Reward 跟 Return
這兩個詞會出現
那 Reward 跟 Return 其實有點不一樣
Reward 指的是你採取某一個行為的時候
立即得到的好處
這個是 Reward
把整場遊戲裡面所有的 Reward 通通加起來
這個叫做 Return
但是我知道說
很快你就會忘記 Reward 跟 Return 的差別了
所以我們等一下就不要再用 Return 這個詞彙
我們直接告訴你說
整場遊戲的 Reward 的總和
就是 Total 的 Reward
而這個 Total 的 Reward 啊
就是我們想要去最大化的東西
就是我們訓練的目標
那你可能會說
欸 這個跟 Loss 不一樣啊
Loss 是要越小越好啊
這個 Total Reward 是要越大越好啊
所以有點不一樣吧
但是我們可以說在 RL 的這個情境下
我們把那個 Total Reward 的負號
負的 Total Reward
就當做我們的 Loss
Total Reward 是要越大越好
那負的 Total Reward
當然就是要它越小越好吧
就我們完全可以說負的 Total Reward
就是我們的 Loss
就是 RL 裡面的 Loss
好 那在進入第三步之前
也許我們可以看一下怎麼回答
同學們有沒有問題
好
那個
有同學建議這個休息的時候可以關麥
好 那個
這個我研究一下怎麼關麥
其實我還不知道要怎麼關麥
好 講理論不好嗎
講理論
沒有不好
那你可以看一下過去
有關 RL 這個部分的錄影
那我等一下在投影片裡面
其實也有附上相關的錄影
那裡面是有講到比較多理論的地方
那我這今天呢
我是期待說從更淺顯的角度呢
來跟大家講 RL 這件事情
那你永遠可以在過去的上課錄影
找到比較偏理論的內容
好 那如果大家暫時還沒有其他問題的話呢
我們就繼續講有關 Optimization 的部分
好 那我們再把這個環境跟
Agent 互動的這一件事情啊
再用不一樣的圖示
再顯示一次
好 這個是你的環境
你的環境呢
輸出一個 Observation
叫做 s1
這個 s1 呢
會變成你的 Actor 的輸入
你的 Actor 呢
接下來就是輸出 a1
然後這個 a1 呢
又變成環境的輸入
你的環境呢
看到 a1 以後
又輸出 s2
然後這個互動的過程啊
就會繼續下去
s2 又輸入給 Actor
它就輸出 a2
a2 又輸入給 Environment
它就輸出給
它就產生 s3
它這個互動呢
一直下去
直到滿足遊戲中止的條件
好 那這個 s 跟 a 所形成的這個 Sequence
就是 s1 a1 s2 a2 s3 a3 這個 Sequence
又叫做 Trajectory
那我們用 τ 來表示 Trajectory
好 那根據這個互動的過程
Machine 會得到 Reward
你其實可以把 Reward 也想成是一個 Function
我們這邊用一個綠色的方塊來代表
這個 Reward 所構成的 Function
那這個 Reward 這個 Function
有不同的表示方法啦
在有的遊戲裡面
也許你的 Reward
只需要看你採取哪一個 Action 就可以決定
不過通常我們在決定 Reward 的時候
光看 Action 是不夠的
你還要看現在的 Observation 才可以
因為並不是每一次開火你都一定會得到分數
開火要正好有擊到外星人
外星人正好在你前面
你開火才有分數
所以通常 Reward Function 在定義的時候
不是只看 Action
它還需要看 Observation
同時看 Action 跟 Observation
才能夠知道現在有沒有得到分數
所以 Reward 是一個 Function
這個 Reward 的 Function
它拿 a1 跟 s1 當作輸入
然後它產生 r1 作為輸出
它拿 a2 跟 s2 當作輸入
產生 r2 作為輸出
把所有的 r 通通結合起來
把 r1 加 r2 加 r3 一直加到 rT
全部結合起來就得到 R
這個就是 Total Reward
也就是 Return
這個是我們要最大化要去 Maximize 的對象
好 那這個 Optimization 的問題
它長得是什麼樣子呢
這個 Optimization 的問題是這個樣子
你要去找一個 Network
其實是 Network 裡面的參數
你要去 Learn 出一組參數
這一組參數放在 Actor 的裡面
它可以讓這個 R 的數值越大越好
就這樣
結束了
整個 Optimization 的過程就是這樣
你要去找一個 Network 的參數
讓這邊產生出來的 R 越大越好
那乍看之下
如果這邊的
這個 Environment Actor 跟 Reward
它們都是 Network 的話
這個問題其實也沒有什麼難的
這個搞不好你現在都可以解
它看起來就有點像是一個 Recurrent Network
這是一個 Recurrent Network
然後你的 Loss 就是這個樣子
那只是這邊是 Reward 不是 Loss
所以你是要讓它越大越好
你就去 Learn 這個參數
用 Gradient Descent 你就可以讓它越大越好
但是 RL 困難的地方是
這不是一個一般的 Optimization 的問題
因為你的 Environment
這邊有很多問題導致說
它跟一般的 Network Training 不太一樣
第一個問題是
你的 Actor 的輸出是有隨機性的
這個 a1 它是用 Sample 產生的
你定同樣的 s1 每次產生的 a1 不一定會一樣
所以假設你把 Environment Actor 跟 Reward
合起來當做是一個巨大的 Network 來看待
這個 Network 可不是一般的 Network
這個 Network 裡面是有隨機性的
這個 Network 裡面的某一個 Layer 是
每次產生出來結果是不一樣的
這個 Network 裡面某一個 Layer 是
它的輸出每次都是不一樣的
另外還有一個更大的問題就是
你的 Environment 跟 Reward
它根本就不是 Network 啊
它只是一個黑盒子而已
你根本不知道裡面發生了什麼事情
Environment 就是遊戲機
那這個遊戲機它裡面發生什麼事情你不知道
你只知道說你輸入一個東西會輸出一個東西
你採取一個行為它會有對應的回應
但是到底是怎麼產生這個對應的回應
我們不知道
它只是一個黑盒子
而 Reward 呢
Reward 可能比較明確
但它也不是一個 Network
它就是一條規則嘛
它就是一個規則說
看到這樣子的 Optimization 跟這樣的 Action
會得到多少的分數
它就只是一個規則而已
所以它也不是 Network
而且更糟
而且更麻煩的地方是
往往 Reward 跟 Environment
它也是有隨機性的
如果是在電玩裡面
通常 Reward 可能比較不會有隨機性
因為規則是定好的
對有一些 RL 的問題裡面
Reward 是有可能有隨機性的
但是在 Environment 裡面
就算是在電玩的這個應用中
它也是有隨機性的
你給定同樣的行為
到底遊戲機會怎麼樣回應
它裡面可能也是有亂數的
它可能每次的回應也都是不一樣
如果是下圍棋
你落同一個子
你落在
你落子在同一個位置
你的對手會怎麼樣回應
每次可能也是不一樣
所以環境很有可能也是有隨機性的
所以這不是一個一般的 Optimization 的問題
你可能不能夠用我們這門課已經學過的
訓練 Network 的方法來找出這個 Actor
來最大化 Reward
所以 RL 真正的難點就是
我們怎麼解這一個 Optimization 的問題
怎麼找到一組 Network 參數
可以讓 R 越大越好
其實你再仔細想一想啊
這整個問題跟 GAN 其實有異曲同工之妙
它們有一樣的地方
也有不一樣的地方
先說它們一樣的地方在哪裡
你記不記得在訓練 GAN 的時候
在訓練 Generator 的時候
你會把 Generator 跟 Discriminator 接在一起
然後你希望去調整 Generator 的參數
讓 Discriminator 的輸出越大越好
今天在 RL 裡面
我們也可以說這個 Actor 就像是 Generator
Environment 跟 Reward 就像是 Discriminator
我們要去調整 Generator 的參數
讓 Discriminator 的輸出越大越好
所以它跟 GAN 有異曲同工之妙
但什麼地方不一樣呢
在 GAN 裡面你的 Discriminator
也是一個 Neural Network
你了解 Discriminator 裡面的每一件事情
它也是一個 Network
你可以用 Gradient Descent
來 train 你的 Generator
讓 Discriminator 得到最大的輸出
但是在 RL 的問題裡面
你的 Reward 跟 Environment
你可以把它們當 Discriminator 來看
但它們不是 Network
它們是一個黑盒子
所以你沒有辦法用
一般 Gradient Descent 的方法來調整你的參數
來得到最大的輸出
所以這是 RL
跟一般 Machine Learning不一樣的地方
但是我們還是可以把 RL 就看成三個階段
只是在 Optimization 的時候
在你怎麼 Minimize Loss
也就怎麼 Maximize Reward 的時候
跟之前我們學到的方法是不太一樣的
好 那這個就是有關
RL 跟 Machine Learning 三個步驟的關係
好 我們看一下大家有沒有問題問
好 有一位同學問說
為什麼負的 Total Reward 等於 Loss
好 為什麼負的 Total Reward 會等於 Loss 呢
我們在 Training 的時候啊
在我們之前講過的
所有的 Deep Learning 的 Training 裡面
我們都是定義了一個 Loss
要讓這個 Loss 越小越好
在 RL 裡面呢
我們是定義了一個 Total Reward R
然後我們要讓那個 R 越大越好
但是要讓 R 越大越好這件事情
我們完全可以反過來說
就是我們要讓負的 R
就是 R 乘上一個負號
越小越好
所以我們就可以說
R 乘上一個負號就是 RL 的 Loss
如果以前學的模型沒有固定
Random Seed 的話也算是有隨機性嗎
嗯 這兩個隨機性是不一樣的
我們在之前的學模型的時候
沒有固定 Random Seed
你是 Training 的時候有隨機性
就是你沒有固定 Random Seed
你可能 Initialize 的 Parameter 不一樣
所以你每次訓練出來的結果不一樣
但是 RL 是在 Testing 的時候就有隨機性
也就是說不是 Training 的時候有隨機性喔
是測試的時候就已經有隨機性了
所以如果拿一般的 Training 來比喻的話
就是你在你 Network Train 好以後
你拿這個 Network 在 Testing 的時候
你想要使用它
你把你要把這個 Network
使用在 Testing 的狀況下
但發現說你給同樣的 Input
每次輸出都不一樣
這個才是 RL 的隨機性
所以 RL 是說你 Train 好一個 Actor
Actor 你 Learn 好了
Actor 參數都是固定的
但你拿這個 Actor 去跟環境互動的時候
每次的結果都是不一樣的
因為你的環境就算是看到同樣的輸入
它每次給輸出也可能是不一樣的
所以 RL 是一個隨機性特別大的問題啦
所以你可以想見這個作業有可能是
也確實是特別困難的
不過我覺得一個作業的難度啊
有時候不好說
因為 RL
如果今天你沒有任何網路上參考資料的話
它可能是最難的
但另外一方面 RL 又蠻容易找到
各式各樣的 GitHub 上的 Code
所以好像又沒有那麼難
但是 RL 的隨機性是會非常非常大的
就算是同樣的 Network
你每次測試的時候
結果都可以是不一樣
a1 下方寫 （00：34：47）是寫錯了嗎
這個我投影片後來有
可能是有改了一下
如果你有不清楚的地方你再問我好了
就是這個投影片我剛才在上課前改了一下
之後會把新的投影片再釋出
好
好 大家還有問題要問一下嗎
好 如果大家暫時沒有問題的話
那我們就再繼續
那我們就是在講到一個段落呢
我們再休息
好 那接下來啊
我們就要講一個拿來解 RL
拿來做 Optimization 那一段常用的一個演算法
叫做 Policy Gradient
那如果你真的想知道
Policy Gradient 是哪裡來的
你可以參見過去上課的錄影
對 Policy Gradient 有比較詳細的推導
那今天我們是從另外一個角度
來講 Policy Gradient 這件事情
好 那在講 Policy Gradient 之前
我們先來想想看
我們要怎麼操控一個 Actor 的輸出
我們要怎麼讓一個 Actor
在看到某一個特定的 Observation 的時候
採取某一個特定的行為呢
我們怎麼讓一個 Actor
它的輸入是 s 的時候
它就要輸出 Action a^ 呢
那你其實完全可以把它想成一個分類的問題
也就是說假設你要讓 Actor 輸入 s
輸出就是 a^
假設 a^ 就是向左好了
假設你要讓
假設你已經知道
假設你就是要教你的 Actor 說
看到這個遊戲畫面向左就是對的
你就是給我向左
那你要怎麼讓你的 Actor 學到這件事呢
那也就說 s 是 Actor 的輸入
a^ 就是我們的 Label
就是我們的 Ground Truth
就是我們的正確答案
而接下來呢
你就可以計算你的 Actor
它的輸出跟 Ground Truth 之間的 Cross-entropy
那接下來你就可以定義一個 Loss
假設你希望你的 Actor
它採取 a^ 這個行為的話
你就定一個 Loss
這個 Loss 等於 Cross-entropy
然後呢
你再去 Learn 一個 θ
然後這個 θ 可以讓 Loss 最小
那你就可以讓這個 Actor 的輸出
跟你的 Ground Truth 越接近越好
你就可以讓你的 Actor 學到說
看到這個遊戲畫面的時候
它就是要向左
這個是要讓你的 Actor
採取某一個行為的時候的做法
但是假設你想要讓你的 Actor
不要採取某一個行為的話
那要怎麼做呢
假設你希望做到的事情是
你的 Actor 看到某一個 Observation s 的時候
我就千萬不要向左的話怎麼做呢
其實很容易
你只需要把 Loss 的定義反過來就好
你希望你的 Actor 採取 a^ 這個行為
你就定義你的大 L 等於 Cross-entropy
然後你要 Minimize Cross-entropy
假設你要讓你的 Actor
不要採取 a^ 這個行為的話
那你就把你就定一個 Loss
叫做負的 Cross-entropy
Cross-entropy 乘一個負號
那你去 Minimize 這個 L
你去 Minimize 這個 L
就是讓 Cross-entropy 越大越好
那也就是讓 a 跟 a^ 的距離越遠越好
那你就可以避免你的 Actor 在看到 s 的時候
去採取 a^ 這個行為
所以我們有辦法控制我們的 Actor
做我們想要做的事
只要我們給它適當的 Label 跟適當的 Loss
所以假設我們要讓我們的 Actor
看到 s 的時候採取 a^
看到 s' 的時候不要採取 a^' 的話
要怎麼做呢
這個時候你就會說
Given s 這個 Observation
我們的 Ground Truth 叫做 a^
Given s' 這個 Observation 的時候
我們有個 Ground Truth 叫做 a^'
那對這兩個 Ground Truth
我們都可以去計算 Cross-entropy
好 我們都可以去計算 Cross-entropy
e1 跟 e2
然後接下來呢
我們就定義說我們的 Loss
就是 e1 減 e2
也就是說我們要讓這個 Case
它的 Cross-entropy 越小越好
這個 Case 它的 Cross-entropy 越大越好
然後呢
我們去找一個 θ 去 Minimize Loss
得到 θ⋆
那就是一個可以在 s
可以在看到 s 的時候採取 a^
看到 s' 的時候採取 a^' 的 Actor
所以藉由很像是在
Train 一個 Classifier 的這種行為
藉由很像是現在 Train 一個 Classifier
的這種 Data
我們可以去控制一個 Actor 的行為
好 我剛才講到這邊
有沒有同學要問問題
好 有一個同學問了一個非常好的問題
就是如果以 Alien 的遊戲來說的話
因為只有射中 Alien 才會有 Reward
這樣 Model 不是就會一直傾向於射擊嗎
對 這個問題我們等一下會來解決它
之後的投影片就會來解決它
然後又有另外一個同學
問了一個非常好的問題就是
哇 這樣不就回到 Supervised Learning 了嘛
這個投影片上看起來
就是在訓練一個 Classifier 而已啊
我們就是在訓練 Classifier
你只是告訴它說
看到 s 的時候就要輸出 a^
看到 s' 的時候就不要輸出 a^
a^'
這不就是 Supervised Learning 嗎
這就是 Supervised Learning
這個就是跟 Supervised Learning Train 的
Image Classifier 是一模一樣的
但等下我們會看到它跟
一般的 Supervised Learning 不一樣在哪裡
好 那我們就再繼續再稍微看一段
我們再休息
那所以呢
如果我們要訓練一個 Actor
我們其實就需要蒐集一些訓練資料
就蒐集訓練資料說
我希望在 s1 的時候採取 a^1
我希望在 s2 的時候不要採取 a^2
但可能會問說
欸 這個訓練資料哪來的
這個我們等一下再講訓練資料哪來的
所以你就蒐集一大堆的資料
這個跟 Train 一個 Image 的 Classifier 很像的
這個 s 你就想成是 Image
這個 a^ 你就想成是 Label
只是現在有的行為是想要被採取的
有的行為是不想要被採取的
你就蒐集一堆這種資料
你就可以去定義一個 Loss Function
有了這個 Loss Function 以後
你就可以去訓練你的 Actor
去 Minimize 這個 Loss Function
就結束了
你就可以訓練一個 Actor
期待它執行我們的行為
期待它執行的行為是我們想要的
而你甚至還可以更進一步
你可以說每一個行為並不是只有好或不好
並不是有想要執行跟不想要執行而已
它是有程度的差別的
有執行的非常好的
有 Nice to have 的
有有點不好的
有非常差的
所以剛才啊
我們是說每一個行為就是要執行 不要執行
這是一個 Binary 的問題
這是我們就用 ±1 來表示
但是現在啊
我們改成每一個 s 跟 a 的 Pair
它有對應的一個分數
這個分數代表說
我們多希望機器在看到 s1 的時候
執行 a1^ 這個行為
那比如說這邊第一筆資料跟第三筆資料
我們分別是定 +1.5 跟 +0.5
就代表說我們期待機器看到 s1 的時候
它可以做 a1^
看到 s3 的時候它可以做 a3^
但是我們期待它看到 s1 的時候
做 a1^ 的這個期待更強烈一點
比看到 s3 做 a3^ 的期待更強烈一點
那我們希望它在看到 s2 的時候
不要做 a2^
我們期待它看到 sN 的時候
不要做 aN^
而且我們非常不希望
它在看到 sN 的時候做 aN^
有了這些資訊
你一樣可以定義一個 Loss Function
你只是在你的原來的 Cross-entropy 前面
本來是 Cross-entropy 前面
要嘛是 +1 要嘛是 -1
現在改成乘上 An 這一項
改成乘上 An 這一項
告訴它說有一些行為
我們非常期待 Actor 去執行
有一些行為我們非常不期待 Actor 去執行
有一些行為如果執行是比較好的
有一些行為希望儘量不要執行比較好
但就算執行了也許傷害也沒有那麼大
所以我們透過這個 An 來控制說
每一個行為我們多希望 Actor 去執行
然後接下來有這個 Loss 以後
一樣 Train 一個 θ
Train 下去你就找一個 θ⋆
你就有個 Actor 它的行為是符合我們期待的
那接下來的難點就是
要怎麼定出這一個 a 呢
這個就是我們接下來的難點
就是我們接下來要面對的問題
我們還有另外一個要面對的問題是
怎麼產生這個 s 跟 a 的 Pair 呢
怎麼知道在 s1 的時候要執行 a1
或在 s2 的時候不要執行 a2 呢
那這個也是等一下我們要處理的問題
