臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw/
我們今天要講的是用 GAN 來 improve sequence generation
那 sequence generation 的 task 它有非常多的應用
那我們先講一下怎麼用 GAN 來 improve 這個 conditional sequence generation
然後接下來我們會講說
今天假設有了 GAN 的技術以後
其實我們可以做到 unsupervised conditional generation
我們上次有講過， 如果有 Unsupervised conditional generation 的話
你可以做比如說 image 的 style transfer
把 image 在不同風格間作轉換
或者是做 voice style transfer 也就是 speaker conversion
把 A 的聲音轉成 B 的聲音等等
那今天我們要講，有的 GAN 技術以後
我們不只可以用 GAN 來 improve conditional sequence generation
我們還可以做到 Unsupervised conditional generation
那我們先講一下 conditional sequence generation 指的是什麼
那其實只要是要產生一個 sequence 的 task
都是 conditional sequence generation
舉例來說語音辨識可以看作是一個 conditional sequence generation 的 task
你需要的是一個 generator
input 是聲音訊號，output 就是語音辨識的結果
就是這一段聲音訊號所對應到的文字
或者是說假設你要做翻譯，你要做 translation 的話呢
那你的 input，假設你要中翻英，你的 input 是中文
那你的 output 就是翻譯過的結果
是一串 word sequence
或者是說我們在作業二裡面有做一個 chatbot， 那其實 chatbot
也是一個 conditional sequence generation 的 task
它的 input 是一個句子
它的 output 是另外一個 sequence，是另外一個句子
其實你可以用今天我們在這一堂課學到的技術
來 improve 你在作業二的時候，所學出來的 chatbot
那我們之前有講過說，其實這些 task
語音辨識，翻譯或 chatbot，你是怎麼解它的呢
你都是用 sequence to sequence 的 model 來解它的
所以實際上這邊這個圖上所畫的 generator
它們都是 sequence to sequence 的 model
只是我們原來在 train seq2seq model 的時候
我們有講過說怎麼 train 的
大家在作業二都已經 train 了 seq2seq model
所以你都知道要怎麼 train
那今天要講的是用一個不一樣的方法
用 GAN 的技術來 train 一個 seq2seq model
那為什麼我們會要用到 GAN 的技術或 其他的技術來 train seq2seq model 呢
我們先來看看在作業二裡面
我們 train seq2seq model 的方法
有什麼不足的地方
那我們都知道在作業 2-2 裡面，我們就 train 了一個 chatbot
一個 chatbot 它是一個 seq2seq model， 它裡面有一個 encoder
有一個 decoder
那在這邊這個 seq2seq model 就是我們的 generator
那這個 encoder 會吃一個 input 的句子，這邊用 c 來表示
那它會 output 另外一個句子 x
encoder 吃一個句子，之於 decoder 會output 一個句子 x
那我們知道説要 train 這樣子的 chatbot
你需要收集一些 training data
所謂的 training data 就是人的對話
所以你今天告訴 chatbot 説，在這個 training data 裡面
A 説 How are you 的時候，B 的回應是 I'm good
所以 chatbot 必須學到說
當 input 的句子是 How are you 的時候
它 output 這個 I'm good 的 likelihood
應該越大越好
或者假設你不知道 maximum likelihood 是什麼的話
這邊的意思就是說
今天假設正確答案是 I'm good
那你在用 decoder 產生句子的時候
第一個 timestamp 產生 I'm
假設 I'm 算是一個 word，產生 I'm 的機率要越大越好
那在第二個 timestamp 產生 good 的機率要越大越好
那這麼做顯然有一個非常大的問題
就是我們看兩個可能的 output
假設今天有一個 chatbot
它 input How are you 的時候，它 output 是 Not bad
有另外一個 chatbot
它 input How are you 的時候，它 output 是 I'm John
那如果從人的觀點來看
Not bad 是一個比較合理的 answer
I'm John 是一個比較奇怪的 answer
但是如果從我們 training 的 criteria 來看
從我們在 train 這個 chatbot 的時候
希望 chatbot 要 maximize 的 object， 希望 chatbot 學到的結果來看
事實上 I'm John 是一個比較好的結果
為什麼呢？因為 I'm John 跟 I'm bad 比起來
你至少第一個 word 的還是對的
那如果是 Not bad
你兩個 word 都是錯的
所以從這個 training 的 criteria 來看， 假設你 train 的時候
是 maximum likelihood
或是其實 maximum likelihood 就是 minimize 每一個 timestamp 的 cross entropy
不管，這兩個其實是 equivalent 的東西啦
其實 maximum likelihood 就是 minimize cross entropy
所以假設有一天
這個是一個真實的問題，就有人去某個大公司面試
然後人家問他說
train 這個 classifier 的時候
有時候我們會說我們是 maximum likelihood
有時候我們會說我們是在 minimize cross entropy
這兩者有什麼不同呢？
如果你答一些，這兩個東西有點像
但他們中間有微妙的不同， 你就錯了這樣，這個時候你就是要說
他們兩個就是一模一樣的東西
所以 maximum likelihood 跟 minimize cross entropy
是一模一樣的東西，剛才講的是真正的例子
某人去面試某一個大家都知道的
全球性的科技公司，是真的被問了這個問題
那我們現在來看一下我們今天要講的東西
我們先講一下怎麼去 improve 這個 seq2seq 的 model
那這邊呢會講兩個 improve 的方法
我們其實會先講說，怎麼用 reinforcement learning
來 improve conditional generation 這件事情
然後接下來我們才會講說
怎麼用 GAN 來 improve conditional generation
之所以要講 RL，是因為等一下你會發現說
用 GAN 來 improve conditional generation 這件事情
其實跟 RL 是非常像的
你甚至可以說使用 RL
來 improve seq2seq 的 chatbot
可以看作是 GAN 的一個 special case
那等一下我們繼續看下去，你就會比較清楚
假設我們今天要 train 一個 seq to seq 的 model
你不想要用 train maximum likelihood 的方法
來 train seq to seq model，因為我們剛才講說
用 maximum likelihood 的方法，有很明顯的問題
那我們現在如果想引入 RL
來 train seq to seq model 的話
我們等一下都用 chatbot 來做例子， 其實我們等一下討論的技術
不是只限於 chatbot 而已
其實任何 seq to seq model，舉例來說作業 2-1 做的
tracation generation 也可以用到等一下討論的技術
不過我們等一下舉例的時候
我們都假設我們是要做 chatbot 就是了
那今天假設你要 train 一個 chatbot
你不要 maximum likelihood 的方法
你想要 Reinforcement learning 的方法
那你會怎麼做呢？那你的作法可能是這樣
你就讓這個 chatbot 去胡亂在線上跟人講話
就有一個人說 How are you，chatbot 就回答 bye-bye， 人就會給 chatbot 一個很糟的評價
chatbot 就知道說這樣做是不好的
再下一次他跟人對話的時候，人說 Hello
chatbot 説 Hi，人就覺得說它的回答是對的， 就給它一個 positive 的評價
chatbot 就知道說它做的事情是好的
那 chatbot 在跟人互動的過程中呢
他會得到 reward，那我今天把這個問題想的單純一點
就是人說一個句子，然後 chatbot 就做一個回應
然後人就會給 chatbot 一個分數
那今天 chatbot 要做的事情
就是希望透過互動的過程
它去學習怎麼 maximize 它可以得到的分數
或者是說我們用這頁投影片 來說明一下我們現在的問題是什麼樣子
我們有一個 chatbot，它 input 一個 sentence c
他要 output 一個 response x，它就是一個 seq to seq model
接下來有一個人，人其實也可以看作是一個 function
人這個 function 做的事情是什麼呢？
人這個 function 做的事情就是
input 一個 sentence c
還有 input 一個 response x
然後給於一個評價，給於一個 reward
這個 reward 我們就寫成 R(c, x)
但如果你熟係 conditional generation 的話
就記得我們在講這個我們作業 3-2， 就是要做 conditional generation 嘛
對不對，要 input 文字，然後 output 對應的圖片
那你會發現說這個圖
跟用 GAN 做 conditional generation
其實是非常像的
唯一的不同只是說
在原來的 conditional generation 裡面
這一個如果用 GAN 做 conditional generation 的話
這個綠色的方塊，它是一個 discriminator
我們說 discriminator 切記它不要只吃 generator 的 output
它要同時吃 generator 的 input 跟 output
才能給於評價
今天人也是一樣，人來取代那個 discriminator
人他就不用 train，或者是說你可以說人已經 train 好了
人有一個腦，然後在數十年的成長歷程中你其實已經 train 好了， 所以你不用再 train
然後給你一個 input sentence c，給你一個 response x，然後你可以給於一個評價
我們接下來要做的事情，chatbot 要做的事情就是
它希望去調整這個 seq to seq model 裡面內部的參數
希望去 maximize 人會給他的評價，這邊寫成 R(c, x)
這件事情怎麼做呢？我們要用的技術
其實就是 policy gradient
policy gradient 我們其實在 machine learning 的最後幾堂課其實是有說過的
那也許你記得，也許你忘了， 我們這邊以 chatbot 做例子
來很快地複習一下，policy gradient 是怎麼做的
那我們有一個 seq to seq model， 它的 input 是 c output 是 x
接下來我們有另外一個 function，這個 function 是人
人吃 c 跟 x，然後 output 一個 R
那我們現在要做的事情是什麼呢
我們要去調 encoder 跟 generator 的參數
這個 encoder 跟 generator 合起來是一個 seq to seq model
他們合起來的參數，我們叫做 theta
我們希望調這個 theta 去 maximize human 這個 function 的 output
那怎麼做呢
我們先來計算一個東西， 這個東西是給定某一組參數 theta 的時候
假設固定 theta，把 theta 固定起來的時候
這個時候這個 seq to seq model 這個 chatbot
會得到的期望的 reward 有多大
假設這個 theta 是固定的， 然後計算一下這個 seq to seq model
它會得到的期望的 reward 是有多大
那這個東西怎麼算呢？
首先我們先 summation over 所有可能的 input c
然後乘上每一個 c 出現的機率
因為 c 可能有各種不同的 output
比如說人可能說 How are you，人可能說 Good morning
人可能說 Good evening，你有各種各樣的 input
你有各種各樣的 c
但是每一個 input 出現的機率
可能是不太一樣的， 比如說 How are you 相較於其他的句子
也許它出現的機率是特別大的
因為人特別常對 chatbot 說這個句子
接下來，summation over 所有可能的回應 x
當你有一個 c 的時候，當你有一個 input c 的時候
再加上假設這個 chatbot 的參數 theta， 我們已經知道的時候
接下來你就可以算出一個機率
這個機率是在 given c，given 這組參數的情況下
chatbot 會回答某一個答覆 x 的機率有多少
那你說這邊會什麼會是一個機率呢？給一個 input c
為什麼 output 會是一個機率呢？
你想想看喔，我們今天在 train seq to seq model 的時候
每一個 timestamp 我們不是其實要做一個 sampling 嘛
我們 train 一個 seq to seq model 的時候
每一次給同樣的 input，它的 output，不見得是一樣的
假設你在做 sampling 的時候
你從那個就是我們的 decoder 的 output 是一個 distribution
你要把 distribution 變成一個 token 的時候
如果你是採取 sampling 的方式
那你 chatbot 的每一次 output 都會是不一樣的
所以今天給一個 c，每一次 output 的 x
其實是不一樣的，所以給一個 c
我們其實得到的是一個 x 的機率
那你說假設你不是用 sampling 的方式
你是用 Arg Max 的方式呢？
其實這樣也可以，如果是用 Arg Max 的方式
給一個 c，那你一定會得到一模一樣的 x
但我們可以說，那個 x 出現的機率就是 1
其他的 response 出現的機率都是 0
其他的 x 出現機率都是 0
總之給你一個 c，在參數 x, c theta 知道的情況下
你可以把 chatbot 可能的 output 看作是一個 distribution
這邊寫成 Ptheta of (x, given c)
當 chatbot 回答一個 x 的時候
當給一個 c，chatbot 產生一個 x 的時候
接下來人就會給一個 reward R(c, x)
那這一整項，這一整項 summation over 所有的 c
summation over 所有的 x
這邊乘上 c 的機率，這邊乘上 x 出現的機率
在 weighted by 這個 reward
其實就是 reward 的期望值，對不對
這一項就是某一個 chatbot
它的參數是 theta，它可以得到的 reward 的期望值
那接下來我們要做的事情是什麼呢？
接下來我們要做的事情就是
我們要調這個 theta，要調這個 chatbot 的參數 theta
讓 reward 的期望值，越大越好
那這件事情怎麼做呢？
這件事情怎麼做呢？
我們先把這個 reward 的期望值稍微做一下整理
這邊是 summation over c
所以我們可以看作是
根據 p of c 這個 distribution，取期望值
這邊是 summation over x
然後乘上 Ptheta of (x, given c)
所以這邊可以看作是
對這一個 distribution
這一個 x 是從這個 distribution sample 出來的
然後取期望值，那取期望值的對象是 R(c, x)
那接下來這邊這項沒什麼了不起的
就把這兩個 sample 放在一起
這邊我發現了一個錯，有發現嗎？
這邊這個應該是 c
其實是這樣子的，我在做投影片的時候，本來都是 h
而做到某個地方我突然想把它改成 c
然後前面就有地方沒有改到這樣子，這個是一個 c
那這邊的意思就是說
我們從 P of c 裡面 sample 出一個 c 出來
我們從這個機率裡面 sample 出一個 x 出來
然後取 R(c, x) 的期望值
你應該可以注意找一下
我相信有很多地方的 c 都沒有改到
然後接下來你的問題就是
這個期望值要怎麼算？
這個期望值要怎麼算？你要算這個期望值
你實際上做法
甚至 theoretical 作法
你要 summation over 所有的 c
summation over 所有的 x
但是在實作上
你根本無法窮舉所有 input
你根本無法窮舉所有可能 output
所以實作上是怎麼做的？
實作上就是做 sampling
假設這個 distribution 你知道，假設這個 distribution 你知道
那這兩個 distribution 我們知道嗎？
這兩個 distribution 我們知道
P of c，人會說什麼句子
你就從你的 database 裡面 sample 看看
你就從你的 database 的句子裡面 sample， 你就知道人常輸入什麼句子
那這個機率，你只要知道參數，他就是給定的
所以我們根據這兩個機率
我們去做一些 sample， 我們去 sample 大 N 筆的 c 跟 x 的pair
比如說上百筆的 c 跟 x 的pair
所以本來這邊應該是要取一個期望值
但實際上我們並沒有辦法真的去取期望值
我們真正的做法是
做一下 sample，sample 出大 N 筆 data
這大 N 筆 data，每一筆都去算它的 reward
把這大 N 筆 data 的 reward 全部平均起來
我們用這個東西來 approximate 期望值
而這一項就是期望的 reward 的 approximation
然後再來你會遇到的問題是說
那我們現在要對 theta
我們要對 theta 做 optimization
我們要找一個 theta 讓 R bar 這一項越大越好
那意味著說我們要拿 theta 去對 R bar 算它的 gradient
但是問題是在這項裡面，我們說 R bar 就等於這項
這項裡面沒有 theta 啊
沒有 theta 你根本沒有辦法對 theta 算 gradient，
heta 不見了，不知不覺間
本來這邊都好像還有 theta
這邊有theta，這邊有theta
不知不覺間，它就不見了
它到哪裡去了呢？
它被藏到 sampling 的這個 process 裡面去了
當你改變 theta 的時候
你會改變 sample 到的東西，但在這式子裡面
theta 就不見了
你根本就不知道要怎麼對這個式子算 theta 的 gradient
所以怎麼辦呢？實作上的方法是這個樣子的
這一項如果把它 approximate 成這一項的話
就會沒有辦法算 gradient 了
所以怎麼辦？先把對這一項算 gradient
再做 approximation
先對這一項算 gradient
這一項算 gradient 是怎麼樣呢？
只有這個 Ptheta of (x, given c)
跟 theta 是有關的
所以你對 R bar 取 gradient 的時候
那你只需要把 gradient 放到 Ptheta 的前面就好了， 因為只有這一項和 theta 是有關係的
接下來呢，唯一的 trick 是
對這一個式子，他的分子和分母的地方
都同乘 Ptheta of (x, given c)，分子分母同乘一樣的東西
當然對結果是沒有任何影響的
那我們知道說，假設你有一個 function f of x
dx 分之 d log (f of x) 會等於 (f of x) 分之 1， dx 分之 d (f of x)
反正微分的式子告訴我們反正就是這個樣子， 所以今天這個式子
Ptheta of (x, given c) 乘上 gradient Ptheta of (x, given c) 除以 Ptheta of (x, given c) 其實就是
其實這一項
就是 gradient Ptheta of (x, given c) 除以 Ptheta of (x, given c)
其實就是 gradient log Ptheta of (x, given c)
這兩項是一樣的
那接下來呢，接下來我們知道說
前面這邊，這 summation over c 乘上 P of c
可以看作是在取 expectation 的時候
對從 P of c 這個 distribution 裡面去 sample c 出來
這邊 summation over x
可以看做是從 Ptheta of (x, given c) 這個 distribution 裡面去 sample x 出來
那你要取期望值的對象是什麼呢？ 要取期望值的對象是 R(c, x)
R(c, x) 乘上 gradient log Ptheta of (x, given c)
乘上 gradient log Ptheta (x, given c)
所以這一項，當你要對 R bar 做 gradient 的時候
你要去 approximate 這一項的話，你是怎麼算的呢？
實際上你的做法是這樣子的
所以這一項怎麼算，這一項就是
這個 summation 把它換做 sampling
你就 sample 大 N 項
每一項都去算 R of (ci, xi)
再乘上 gradient log Ptheta of (xi, given ci)
把它們平均起來就是這一項 expectation 的 approximation
那假設前面那些東西你沒有聽懂的話呢， 那就算了，所以我們實際上是怎麼做的呢？
實際上的做法是， 我們今天怎麼 update 我們的參數 theta
你 update 的方法是，原來你的參數叫做 theta old
然後你用 gradient ascent 去 update 它
加上某一個 gradient 的項， 你得到新的 model，theta new
接下來呢，gradient 這一項怎麼算？
gradient 這一項算法就是， 去 sample N 個 pair 的 ci 跟 xi 出來
去 sample N 個 pair 的 ci 跟 xi 出來
然後接下來你把 R of (ci, xi) 去乘上
gradient log Ptheta of (xi, given ci)，就結束了
那這一項假設前面的推導你聽不懂的話
其實這一項它是非常的直覺的
怎麼說它非常的直覺呢？
它的直覺的解釋是這樣
這一個 gradient 到底代表什麼意思呢？
這個 gradient 所代表的意思是說
假設今天 given ci, xi
也就是說有人對 machine 說了 ci 這個句子
machine 回答 xi 這個句子
然後人給的 reward 是 positive 的
人說如果輸入 ci 回答 xi
它是好的，給你一個 positive 的 reward
那我們就要增加 given ci 的時候，xi 出現的機率
這個非常直覺， 如果 given ci 產生 xi 你的 reward 是 positive 的
given ci 產生 xi 是好的， 那你就要增加 given ci 產生 xi 的機率
反之如果 R of (ci, xi) 是 negative 的
當人對 chatbot 説 ci
chatbot 回答 xi，然後得到負面的評價的時候
這個時候我們就應該調整參數 theta
讓這一項機率，也就是 given ci
回答 xi 的這個機率呢，越小越好
它的精神就是這樣
所以實作上的時候，實際上如果你要用 policy gradient 這個技術
來 implement 一個 chatbot
讓它在 reinforcement learning 的情境中
可以去學習怎麼和人對話的話
實際上你是怎麼做的呢？實際上你的做法是這個樣子
你有一個 chatbot 它的參數叫做 Theta(t)
然後你把你的 chatbot 拿去跟人對話，然後他們就講了很多
這個是一個 sampling 的 process
你先用 chatbot 跟人對話
做一個 sampling 的 process
在這個 sampling 的 process 裡面呢？
當人說 c1 chatbot 回答 x1 的時候， 會得到 reward R of (c1, x1)
當輸入 c2 回答 x2 的時候，會得到 reward R of (c2, x2)
那你會 sample 出 N 筆 data
每一筆 data 都會得到一個 reward
N 筆 data N 個 reward
那接下呢，接下來你做的事情是這樣
你有一個參數 Theta(t)，你要 update 這個參數
讓它變成 Theta(t+1)
那怎麼 update 呢？你要把它加上
對這個 R bar 的 gradient
那這個 R bar 的 gradient 這一項到底怎麼算呢？
這一項式子就列在這邊
那這個式子的直觀解釋我們剛才講過說
如果 R of (ci, xi) 是正的
那就增加這一項的機率
如果 R of (ci, xi) 是負的，就減少這一項的機率
但是你這邊要注意
每次你 update 完參數以後
你要從頭回去，再去 sample data
因為這個 R bar 它是在 given 參數是 theta 的情況下
所算出來的結果
那你今天一但 update 你的參數， 從 theta(t) 變成 theta(t+1)
這一項就不對了
所以你本來參數 theta(t)，一但你 update theta(t+1) 以後
你就要回過頭去再重新收集參數
所以這跟一般 train 的 gradient decent 非常不同
因為一般 gradient decent，你就算 gradient
然後就可以 update 參數
然後就可以馬上再算下一次 gradient，再 update 參數
但是如果你 apply reinforcement learning 的時候， 你的持續做法是
每次你 update 完參數以後
你就要去跟使用者再互動
然後才能再次 update 參數
所以每次 update 參數的時間呢
需要的 effort 是非常大的
每 update 一次參數
你就要跟使用者互動 N 次
才能 update 下一次參數
所以在 policy gradient 裡面
update 參數這件事情
是非常寶貴的，就這一步是非常寶貴的
絕對不能夠走錯這樣子，你一走錯
你就要要你要重新再去跟人互動，才能夠走回來
那你也有可能甚至就走不回來，所以
下一週會講到一些新的技術呢
來讓這一步做得更好
不過這是我們下週才要再講的東西
那這邊是把 reinforcement learning 跟 maximum likelihood 呢
做一下比較
在做 maximum likelihood 的時候
你有一堆 training data
這些 training data 告訴我們說
今天假設人說 c1
chatbot 最正確的回答是 x1 hat
對不對，我們就會有 labeled 的 data 嘛
大家都做過作業 2-2 你應該知道怎麼回事
就你有 input c1 output x1 hat，
input cN 正確答案就是 xN hat
這是 training data 告訴我們的
在 training 的時候，你就是 maximize 你的 likelihood
怎麼樣 maximize 你的 likelihood 呢？
你希望 input ci 的時候
output xi hat 的機率越大越好
input 某個 condition，input 某個 input 的時候
input 某個輸入的句子的時候
你希望正確的答案出現的機率越大越好
那算 gradient 的時候很單純
你就把這個 log Ptheta 前面呢
加上一個 gradient，你就算 gradient 了
這個是 maximum likelihood
那我們來看一下 reinforcement learning
在做 reinforcement learning 的時候呢
你也會得到一堆 c 跟 x 的 pair
但這些 c 跟 x 的 pair，它並不是正確的答案
這些 x 並不是人去標的答案
這些 x 是機器自己產生的
就人輸入 c1 到 cN
機器自己產生了 x1 到 xN
所以有些答案是對的，有些答案有可能是錯的
接下來呢我們說
我們在做reinforcement learning 的時候
我們是怎麼計算 gradient 的呢？
我們是用這樣的式子來計算 gradient
所以我們實際上的作法呢
我們這個式子的意思就是把這個 gradient log Ptheta 前面乘上 R(c, x)
就如果你比較這兩個式子的話
你會發現說他們唯一的差別是
在做 reinforcement learning 的時候
你在算 gradient 的時候， 每一個 x 跟 c 的 pair 前面都乘上 R(c, x)
如果你覺得這個 gradient 算起來不太直觀
那沒關係，我們根據這個 gradient
反推 objective function
我們反推說什麼樣的 objective function
在取 gradient 的時候，會變成下面這個式子
那如果你反推了以後，你就會知道說
什麼樣的 objective function 取 gradient 以後會變成下面這個式子呢
你的 objective function 就是
summation over 你 sample 到的 data
每一筆 sample 到的 data，你都乘上 R (c, x)
然後你去計算每一筆 sample 到的 data 的 log 的 likelihood
你去計算每一筆 sample 到的 data 的 log Ptheta
再把它乘上 R (c, x)，就是你的 objective function
把這個 objective function 做 gradient 以後，你就會得到這個式子
那我們在做 reinforcement learning 的時候
我們每一個 iteration
其實是在 maximize 這樣一個 objective function
那如果你把這兩個式子做比較的話，那就非常清楚了
今天右邊這個 reinforcement learning 的 case
可以想成是每一筆 training data 都是有 weight
而在 maximum likelihood case 裡面
每一筆 training data 的 weight 都是一樣的
每一筆 training data 的 weight 都是 1
那在 reinforcement learning 裡面
每一筆 training data 都有不同的 weight
這一個 weight 就是那一筆 training data 得到的 reward
也就是說今天輸入一個 ci
機器回答一個 xi
如果今天機器的回答正好是好的
這個 xi 是一個正確的回答
那我們在 training 的時候就給那筆 data 比較大的 weight
如果今天 xi 是一個不好的回答
代表這筆 training data 是錯的
我們 even 會給它一個 negative 的 weight
這個就是 maximum likelihood
和 reinforcement learning 的比較
理論上並沒有特別的限制
就看你今你 R 想要訂怎麼樣
你用 policy gradient，都可以去 maximize R
但是在實作上，會有限制
我們剛才不是講到說，如果 R 是正的
你就要讓機率越大越好
那你今天會不會遇到一個問題就是，假設 R 永遠都是正的
今天這個 task R 就是正的，你做的最差
也只是得到的分數比較小而已，它永遠都是正的
那今天不管你採取什麼樣的行為
machine 都會說我要讓機率上升，聽請來有點怪怪的
但是在理論上這樣未必會有問題， 為什麼說理論上這樣未必會有問題呢？
你想想看喔，你要 maximize 的這一項，是一個機率
它是一個機率，它的和是 1
所以今天就算是所有不同的 xi
他前面乘的 R 是正的
他終究是有大有小的
對不對，你不可能讓所有的機率都上升
因為機率的和是 1，你不可能讓所有機率都上升
所以變成說，如果 weight 比較大的
就比較 positive 的
就上升比較多
如果 weight 比較小的，比較 negative 的
它就可能反而是會減少的
就算他是正的，但如果他值比較小
它可能也是會減小，因為 constrain 就是它的和要是 1
但是你今天在實作上並沒有那麼容易， 因為在實作上會遇到的問題是
你不可能 sample 到所有的 x
所以到時候就會變成說，假設一筆 data 你沒有 sample 到
其他人只要有 sample 到都是 positive 的 reward， 沒 sample 到的
反而就會機率下降而 sample 到的都會機率上升
這個反而不是我們要的
所以其實今天在設計那個 reward 的時候
你其實會希望那個 reward 是有正有負的
你 train 起來會比較容易
那假設你的 task reward 都是正的
實際上你會做的一件事情是， 把那個 reward 通通都減掉一個 threshold
讓它變成是有正有負，這樣你 train 起來會容易很多
這個是講了 maximum likelihood 跟 reinforcement learning 的比較
但是你知道實作上要做什麼 reinforcement learning 根本就是不太可能的
你有沒有看過一篇文章就是講說
當有一個人寫一篇網路文章，然後說
當有人問他說某一個 task 用 reinforcement learning 好不好的時候，他的回答都是不好這樣
那多數的時候他都是對的
就是你要做 reinforcement learning 一個最大的問題就是
機器必須要跟人真的互動很多次
才能夠學得起來
你不要看今天那些什麼，google 或者是，Deep mind
或者是 OpenAI 他們在玩那些什麼 3D 遊戲都玩得嚇嚇叫這樣
那個 machine 跟環境互動的次數都可能是上千萬次
或者是上億次
那麼多互動的次數
除了在電玩這種 simulated 的 task 以外，在真實的情境
幾乎是不可能發生
所以如果你要用 reinforcement learning 去 train 一個 chatbot
幾乎是不可能的，因為在現實的情境中
人沒有辦法花那麼多力氣
去跟 chatbot 做互動
所以後來就有人就想了一個 Alpha Go style training
也就是說我們 learn 兩個 chatbot
讓它們去互講這樣子
就有一個 bot 説 How are you， 另外一個說 see you
然後它再說 see you，它說 see you
然後陷入一個無窮迴圈永遠都跳不出來這樣子
但它們有時候可能也會說出比較正確的句子
因為我們知道說機器在回應的時候
其實是有隨機性的所以問它同一個句子， 每次的回答不見得是一樣的
接下來你再去訂一個 evaluation 的 function
因為你還是不可能說讓兩個 chatbot 互相對話
然後產生一百萬則對話以後， 人再去一百萬則對話每一個去給它 feedback 説
講得好還是不好
你可能會設計一個 evaluation function， 這個就是人自己訂好
這邊我其實應該換成 c 啦，這邊其實應該換成 c 啦
不過我其實覺得也沒差就是了
你就訂一個 evaluation function
給一則對話，然後看說這則對話好不好
這則對話好不好
但是這種 evaluation function 是人訂的
你其實沒有辦法真的訂出太複雜的 function， 就只能定義一些很簡單的，就是
比如說陷入無窮迴圈，就是得到負的 reward
說出 I don't know，就是得到負的 reward
你根本沒有辦法真的訂出太複雜的 evaluation function
所以用這種方法還是有極限的
所以接下來要解這個問題，你可以引入 GAN 的概念
GAN 和 RL 有什麼不同呢？
在 RL 裡面，你是人給 feedback
在 GAN 裡面，你變成是 discriminator 來給 feedback
我們一樣有一個 chatbot，一樣吃一個句子
output 另外一個句子
現在有一個 discriminator，這個 discriminator
其實就是取代了人的角色
它吃 chatbot 的 input 跟 output
然後吐出一個分數
那這個跟 typical 的 conditional GAN 就是一樣的
我們知道說， 就算是別的 task
什麼 image 的生成，你做的事情也是一樣的， 你就是有一個 discriminator
它吃你的 generator 的 input 跟 output， 接下來給你一個評價
那在 chatbot 裡面也是一樣，你有一個 discriminator
它吃 chatbot input 的 sentence 跟 output sentence
然後給予一個評價
那這個 discriminator 呢
你要給他大量人類的對話
讓他知道說真正的人類的對話
真正的當這個chatbot 換成一個人的時候
他的 c 跟 x 長什麼樣子
那這個 discriminator 就會學著鑑別說
這個 c 跟 x 的 pair
是來自於人類，還是來自於 chatbot
然後 discriminator 會把他學到的東西
feedback 給 chatbot， 或者是說 chatbot 要想辦法騙過這個 discriminator
那這跟我們之前上週講的 conditional GAN
還是上上週講的 conditional GAN
反正就是一模一樣的事情就是了
那這個 algorithm 是什麼樣子呢？
其實這個 discriminator 的 output
就可以想成是人在給 reward，你要把這個 discriminator
想成其實就是一個人
只是這個 discriminator 和人不一樣的地方是
它不是完美的，所以他也是要去更新它自己的參數的
那整個 algorithm 其實就跟傳統的 GAN
是一樣的，傳統 conditional GAN 是一樣的
你有 training data，這些 training data
就是一大堆的正確的 c 跟 x 的 pair
然後你一開始你就 initialize 一個 G
其實你的 G 就是你的 generator 你的 chatbot， 然後 initialize 你的 discriminator D
在每一個 training 的 iteration 裡面
你從你的 training data 裡面
sample 出正確的 c 跟 x 的 pair
你從你的 training data 裡面 sample 出一個 c prime
然後把這個 c prime 丟到你的 generator 也就是 chatbot 裡面
讓它回一個句子 x tilde
那這個 c prime, x tilde 就是一個 native 的一個 example
接下來discriminator 要學著說
看到正確的 c 跟 x
給它比較高的分數
看到錯誤的 c prime 跟 x tilde
給它比較低的分數
至於怎麼 train 這個 discriminator
你可以用傳統的方法
量 js divergence 的方法
你完全也可以套用 WGAN，都可以這樣
都可以，你這邊可以用 WGAN，都是沒有問題的
那接下來的問題是說
我們知道在 GAN 裡面你 train discriminator 以後
接下來你就要 train 你的 chatbot，也就是 generator
那 train generator 他的目標是什麼呢？
你要 train 你的 generator
這個 generator 的目標呢
就是要去 update 參數
然後呢你 generator 產生出來的 c 跟 x 的 pair
能讓 discriminator 的 output 越大越好
那這個就是 generator 要做的事情
這邊要做的事情，跟我們之前看到的 chatbot
跟我們之前看到的 conditional GAN，其實是一模一樣的
我們說 generator 要做的事情
其實就是要去騙過 discriminator
但是這邊我們會遇到一個問題
什麼樣的問題呢
如果你仔細想一想
你的 chatbot 的 network 的架構的話
我們的 chatbot 的 network 的架構它是一個 seq to seq 的 model
它是一個 RNN 的 generator
我們看 chatbot 在 generate 一個 sequence 的時候
它 generate sequence 的 process 是這樣子的
那這個我們在講 seq to seq model 的時候我們已經講過了
他是怎麼generate 的呢？
一開始你給它一個 condition
這個 condition 可能是從 attention based model 來的
給它一個 condition，然後呢它 output 一個 distribution
那根據這個 distribution 它會去做一個 sample
就 sample 出一個 token
sample 出一個 word
然後接下來你會把這個 sample 出來的 word
當作下一個 timestamp 的 input
再產生新的 distribution，再做 sample
再當做下一個 timestamp 的 input，再產生 distribution
那大家在作業 2 已經 implement 過這種 model
所以我相信這個對你來說並不陌生
然後我們說我們要把 generator 的 output 呢
丟給 discriminator
你對這個 discriminator 的架構，你也是自己設計
反正只要可以吃兩個 sequence
注意一下這個 discriminator
剛才前一頁的圖，我只有畫說它吃這個
chatbot 的 output，但它其實不能夠只吃 chatbot 的 output
它是同時吃 chatbot 的 input 和 output
我想大家應該了解這個意思吧，在做 conditional GAN 的時候
你的 discriminator 要同時吃你的 generator 的 input 和 output
所以其實這個 discriminator
是同時吃了這個 chatbot 的 input 跟 output
就是兩個 word sequence
那至於這個 discriminator network 架構要長什麼樣子
這個就是看你高興，看你高興
你可以說你就 learn 一個 RNN
然後你把 chatbot input 跟 output 把它接起來
變成一個很長的 sequence
然後 discriminator 把這個很長的 sequence 就讀過
然後就吐出一個數值
這樣也是可以的
有人說我可以用 CNN，反正只要吃兩個 sequence
可以吐出一個分數
怎麼樣都是可以的
那反正 discriminator 就吃一個 word sequence
接下來他吐出一個分數
當我們知道說假設我們今天要 train generator 去騙過 discriminator
我們要做的事情是，update generator 的參數
update 這個 chatbot seq to seq model 的參數
讓 discriminator 的 output 的 scalar 越大越好
這件事情你仔細想一下，你有辦法做嗎？
你可以想說這個很簡單啊
就是把 generator 跟 discriminator 串起來就變成一個巨大的 network
然後我們要做的事情就是
調這個巨大的 network 的前面幾個 layer 讓
這個 network 最後的 output 越大越好
但是你會遇到的問題是
你發現這個 network 其實是沒有辦法微分的
為什麼它沒有辦法微分？
你想想看這整個 network 裡面
有一個 sampling 的 process，
這跟我們之前在講 image 的時候
是不一樣的，那我覺得這個其實是當你用文字
你要用 GAN 來做 natural language 的 processing
跟你用 GAN 來做 image processing 的時候
一個非常不一樣的地方
在 image 裡面
當你用 GAN 來產生一張影像的時候
你可以直接把產生的影像
丟到 discriminator 裡面
所以你可以把 generator 跟 discriminator 合起來
看作是一個巨大的 network
但是今天在做文字的生成的時候
你生成出一個 sentence
這個 sentence 是一串 sequence，是一串 token
你把這串 token 丟到 discriminator 裡面
這中間
你要得到這個 token 的時候這中間有一個 sampling 的 process
當一整個 network 裡面有一個 sampling 的 process 的時候
它是沒有辦法微分的
為什麼呢，一個簡單的解釋是
你想想看所謂的微分的意思是什麼？
微分的意思是你把某一個參數小小的變化一下
看它對最後的 output 的影響有多大
這兩個相除，就是微分
那今天假設一個 network 裡面有 sampling 的 process
你把裡面的參數做一下小小的變化
對 output 的影響是不確定的
因為中間有個 sampling 的 process
所以你每次得到的 output 是不一樣的
你今天對你整個 network 做一個小小的變化的時候
它對 output 的影響是不確定的
所以你根本就沒有辦法算微分出來
或者是另外一個更簡單的解釋就是，你回去用
TensorFlow 或 PyTorch implement 一下
看看如果 network 裡面有一個 sampling 的 process
你跑不跑得動這樣子
你應該是會得到一個 error 這樣
但是你應該是跑不動的
結果就是這樣，反正無論如何
今天你把這個 seq to seq model
跟你的 discriminator 接起來的時候
你是沒有辦法微分的
所以接下來真正的難點就是
怎麼解這個問題
那我在文獻上看到，大概有三類的解法
一個是 Gumbel-softmax
一個就是給 discriminator continuous input
然後另外一個方法就是做 reinforcement learning
那 Gumbel-softmax 我們就不解釋，不解釋
那它其實 implement 其實也是蠻簡單的， 但是我發現用在 GAN 上目前沒有那麼多
所以我們就不解釋，總之 Gumbel-softmax 就是想了一個 trick
讓本來不能微分的東西，somehow 變成可以微分
如果你有興趣的話，你再自己研究 G-S 是怎麼做的
那另外一個很簡單的方法就是
給 discriminator continuous 的 input
你說今天如果問題是在這一個 sampling 的 process
那我們何不就避開 sampling process 呢
我們今天給這個 discriminator
這一個 distribution
這 discriminator 不是吃這些 word sequence
不是吃這個 discrete token
來得到分數，而是吃這些 word distribution
來得到分數
那今天如果我們把這一個 seq to seq model
跟這個 discriminator 串在一起
你就會發現說它變成一個是可以微分的 network 了
因為現在沒有那一個 sampling process 了
問題就解決了
但是實際上問題並沒有這麼簡單，因為你仔細想想看
當你今天給你的 discriminator 一個 continuous input 的時候
你會發生什麼樣的問題，你會發生的問題是這樣
Discriminator 不是會看 real data 跟 fake data
去決定說，它會看 real data 跟 fake data
然後去給它一筆新的 data 的時候， 它會決定它是 real 還是 fake 的
當你今天給 discriminator word distribution 的時候， 你會發現說
real data 跟 fake data 它在本質上就是不一樣的
因為對 real data 來說
它是 discrete token
或者是說每一個 discrete token
我們其實是用一個 1 one-hot 的 vector 來表示它
我想 1 one-hot vector 大家應該都知道
所以對一個 discrete token
我們是用 1 one-hot vector 來表示它
所以一個真正的 sentence
一個 real sentence
對 discriminator 來說，它看到的 real sentence 就長這樣
而對 generator 來說
它每次只會 output 一個 word distribution
它每次 output 的都是一個 distribution
當它把它的 output 丟給 discriminator 的時候
discriminator 看到的結果是這樣子
所以對 discriminator 來說
要分辨今天的 input 是 real 還是 fake 的
太容易了，他完全不需要管這個句子的語義
都不用管，它完全不管句子的語義，它只要一看說
是不是 one-hot
就知道說它是 real 還是 fake 的，就結束了
所以如果你直接用這個方法
來 train GAN 的話，你會發現會遇到什麼問題呢？ 你會發現說
generator 很快就會發現說 discriminator
判斷一筆 data 是 real 還是 fake 的準則
是看說今天你的每一個 output
是不是 one-hot 的
所以 generator 唯一會學到的事情就是， 迅速的變成 one-hot
它會想辦法趕快把某一個
隨便選一個 element 誰都好，也不要在意語意了
因為就算你考慮語意
也很快會被 discriminator 發現
因為 discriminator 就是要看說是不是 one-hot
所以今天隨便選一個 element， 想辦法趕快把它的值變到 1
其他都趕快壓成 0
然後產生的句子完全不 make sense，然後就結束了
你會發現所以今天直接讓 discriminator 吃 continuous input 是不夠的
是沒有辦法真的解決這個問題
那其實還有一個解法是說，也許用一般的 GAN
train 不起來，但是你可以試試看用 WGAN
為什麼在這個 case 用 WGAN，是有希望的呢？
因為今天呢 WGAN 我們不是說在 train 的時候， 你會給你的 model 一個 constrain
你要去 constrain 説你的 discriminator， 一定要是 1-Lipschitz function
因為你有這個 constrain，所以你的 discriminator 它的
手腳會被綁住， 所以它就沒有辦法馬上分別出 real sentence
跟 generated sentence 的差別
他的視線是比較模糊的，它是比較看不清楚的
因為它有一個 1-Lipschitz function constrain
所以他是比較 fuzzy 的
所以它就沒有辦法馬上分別這兩者的差別
所以今天假設你要做這個 conditional generation 的時候呢
如果你是要做這種 sequence generation
然後你要用的方法是讓 discriminator 吃 continuous input
WGAN 是一個可以的選擇
如果你沒有用 WGAN 的話，應該是很難把它做起的
因為 generator 其實學不到語意相關的東西，它只學到說
output 必須要像是 one-hot，才能夠騙過 discriminator
所以這個是第二個 solution，給它 continuous input
第三個 solution 呢
就是套用 RL
我們剛才已經講過說，假設這個 discriminator
換成一個人的話，你知道怎麼去調你 chatbot 的參數
去 maximize 人會給予 chatbot 的 reward
那今天把人換成 discriminator
solution 其實是一模一樣的
怎麼解這個問題呢？
也就是說現在呢，discriminator 就是一個 human
我們說人其實就是一個 function 嘛， 然後看 chatbot 的 input output 給予分數
所以 discriminator 就是我們的人
它的 output，它的 output 那個 scalar
discriminator output 的那個數值，就是 reward
然後今天你的 chatbot 要去調它的參數， 去 maximize discriminator 的 output
也就是說本來人的 output 是 R(c, x)
那我們只是把它換成 discriminator 的 output D of (c, x)，就結束了
接下來怎麼 maximize D of (c, x)
你在 RL 怎麼做，在這邊就怎麼做，結束，就這樣子
所以呢，我們說在這個 RL 裡面是怎麼做的呢？
你讓 theta 去跟人互動，然後得到很多 reward
接下來套右邊這個式子，你就可以去 train 你的 model
現在我們唯一做的事情，是把人呢，換成另外一個機器
就是換成 discriminator
本來是人給 reward
現在換成 discriminator 給 reward
我們唯一做的事情，就是把 R 換成 D
所以右邊也是一樣，把 R 換成 D
當然這樣跟人互動還是不一樣， 因為人跟機器互動很花時間嘛
那如果是 discriminator，它要跟 generator 互動多少次
反正都是機器，你就可以讓他們真的互動非常多次
但是這邊只完成了 GAN 的其中一個 step 而已
我們知道說在 GAN 的每一個 iteration 裡面
你要 train generator
你要 train discriminator 再 train generator
再 train discriminator，再 train generator
今天這個 RL 的 step 只是 train 了 generator 而已
接下來你還要 train discriminator
怎麼 train discriminator 呢？
你就給 discriminator 很多人真正的對話
你給 discriminator 很多 現在你的這個 generator 產生出來的對話
你給 discriminator 很多 generator 產生出來的對話
給很多人的對話
然後 discriminator 就會去學著分辨說這個對話是 real 的
是真正人講的，還是 generator 產生的
那你就可以學出一個 discriminator
那你學完 discriminator 以後
再重頭去 train
因為你的 discriminator 不一樣了
這邊給的分數當然也不一樣了
你 train 好 discriminator 以後
再回頭去 train generator
再回頭去 train discriminator
這兩個 step 就反覆地進行
這個就是用 GAN 來 train seq to seq model 的方法， 用 GAN 來 train chatbot 的方法
那其實還有很多的 tip，那這邊也稍跟大家講一下
那如果我們看這個式子的話，你會發現有一個問題
什麼樣的問題呢？
這個式子跟剛才那個 RL 看到的式子是一樣的
我們只是把 R 換成了 D
那你問這邊這個式子有什麼樣的問題呢？
今天假設 ci 是 what is your name
然後 xi 是 I don't know，這可能不是一個很好的回答
所以你得到的 discriminator 給它的分數是負的
當 discriminator 給它的分數是負的的時候， 我們希望調整我們的參數 theta
讓 log Ptheta of (xi, given ci) 的值越小越好
讓 log Ptheta of (xi, given ci) 的值變小
那我們再想想看，Ptheta (xi, given ci)
到底是什麼樣的東西呢？它其實是一大堆 term 的連成
對不對，它其實是一大堆 term 的連成
也就是說，我們今天實際上在做 generation 的時候
我們每次只會 generate 一個 word 而已，對不對
我們假設 I don't know 這邊有三個 word， 第一個 word 是 x1
第二個 word 是 x2，第三個 word 是 x3
這個 Ptheta of (xi, given ci)
實際上是 P of 在 given condition ci 的前提下
產生 xi 的機率， 乘上 given condition 跟 xi 產生 x2 的機率
再乘上 given condition x1, x2 我用 x1:2 代表 x1, x2
given condition x1, x2 產生 x3 的機率
你把這三項機率相乘，就會 Ptheta of (xi, given ci)
那這邊取 log，取log，取log，取log，所以變成相加
這個大家 ok 吧，實際上這個機率的算法
是這個樣子的，那你說讓這個機率下降
意思就是你希望這一項也下降，你希望這一項也下降
你希望這一項也下降，你希望他們每一項都下降
但是我們看看 P of (ci, given x1) 是什麼， P of (ci, given x1) 是
given condition 是 what is your name 的時候
產生 I 的機率
那如果輸入 what is your name?
一個好的答案其實可能是比如說 I am John 或 I am Mary
所以今天問 What is your name? 的時候， 給你的 condition: What is your name? 的時候
你其實回答 I 當作句子的開頭
是好的，但是你在 training 的時候，你卻告訴 chatbot 説
看到 What is your name? 的時候，回答 I 這個機率，應該是下降的
看到 What is your name? 你已經產生 I
產生 don't 的機率要下降，這項是合理的
產生 I don't 再產生 know 的機率要下降是合理的
但是 given What is your name? 產生 I 的機率，其實是不合理的
那你可能會這樣覺得說，那這個 training 不是有問題嗎？
理論上這個 training 不會有問題，為什麼？
因為今天你的 output
其實是一個 sampling 的 process
所以今天在另外一個 case
當你輸入 What is your name 的時候
機器的回答可能是 I am John
這個時候機器就會得到一個 positive 的 reward
也就是 discriminator 會給機器一個 positive 的評價
這個時候 model 要做的事情就是 update 它的參數
去 increase log Ptheta (xi, given ci)
它要去 increase Ptheta (xi, given ci)
那 Ptheta (xi, given ci)，是這三個項的乘
是這三個項的相乘，而第一項是 P of (I, given ci)
那今天我們會希望 P of (I, given ci) 的值越大越好
當你輸入 What is your name? sample 到 I don't know 的時候
P of given ci 產生 I 的機率，要偏小
當你 sample 到 I am John 的時候
你希望這個機率上升
那如果你今天 sample 的次數夠多
這兩項就會抵消，那就沒事了
但問題就是在實作上，你永遠 sample 不到夠多的次數
所以在實作上這個方法是會造成一些問題的
所以怎麼辦呢？今天的 solution 是這個樣子
我們今天希望當輸入 What is your name? sample 到 I don't know 的時候
machine 可以自動知道說
在這三個機率裡面
雖然 I don't know 整體而言是不好的
但是造成 I don't know 不好的原因
並不是因為在開頭 sample 到了 I
在開頭 sample 到 I，是沒有問題的
是因為之後你產生了 don't 跟 know，所以才做得不好
所以希望機器可以自動學到說
今天這個句子不好，到底是哪裡不好，是因為產生這兩個 word 不好
而不是產生第一個 word 不好
那所以你今天會改寫你的式子
本來你的式子是這樣，對一整個句子
given 一個 ci，產生一個完整的句子 xi，他會有一個分數 D of (ci, xi)
現在你給每一個 generation step
都不同的分數
今天在給定 condition ci，已經產生 x1 到 t-1
已經產生前 t-1 個 word 的情況下
產生的 word xt， 他到底有多好或多不好
我們換另外一個 measure 叫做 Q
來取代 D
這個 Q 它是對每一個 timestamp 去做 evaluation
它對這邊每一次 generation 的 timestamp 去做 evaluation
而不是對整個句子去做 evaluation
那接下來問題就是， 這件事情要怎麼做呢？
我們就沒打算要細講
那你可以自己去查一下文件，因為反正我們作業裡面沒這個東西，所以大概你也沒興趣知道就是了
你如果想知道的話，你就自己查一下文獻
那有不同的作法， 那這其實是一個還可以尚待研究中的問題
一個作法就是做 Monte Carlo
跟 Alpha Go 的方法非常地像
你就想成是在做 Alpha Go， 你去 sample 接下來會發生到的狀況
然後去估測每一個 generation
每一個 generation 就像是在棋盤上下一個子一樣
可以估測每一個 generation 在棋盤上落一個子的勝率
那這個方法最大的問題就是
它需要的運算量太大，所以在實作上你會很難做
那有另外一個運算量比較小的方法
這個方法他的縮寫叫做 REGS，不過反正這個方法
在文獻上看到的結果就是他不如 MC
那我們自己也有實作過，覺得他確實不如 MC
但 MC 的問題就是，他的運算量太大了
所以這個仍然是一個目前可以研究的問題
那還有另外一個技術可以improve 你的 training， 這個方法
叫做 RankGAN，那我們今天就不大算講 RankGAN
那你可能說不打算講為什麼要放在這邊呢？
之後你就知道了
那這邊是講一些我們自己的觀察啦
講一些我們自己的觀察
今天到底當你把 maximum likelihood
換到 GAN 的時候，有什麼樣的不同呢？
因為大家都已經做過作業 2-2
所以你已經用過 End to End 的技術 train 過 chatbot
事實上如果你有 train 過 chatbot 的話，你會知道說
今天 train 完以後，chatbot 非常喜歡回答一些沒有很常
然後非常 general 的句子
通常它的回答要嘛就是 I'm sorry
要嘛就是 I don't know，這樣講來講去都是那幾句
那我們實際統計一下
我們用一個 benchmark corpus 叫做 Open subtitle
來 train 一個 end to end 的 chatbot 的時候
其實有 1/10 的句子
他都會回答 I don't know 或是 I'm sorry
這聽起來其實是沒有非常 make sense
那如果你要解這個問題，我覺得 GAN 就可以派上用場
為什麼今天會回答 I'm sorry 或 I don't know 呢？
我的猜測是，這些 I'm sorry 或 I don't know 這些句子
對應到影像上，就是那些模糊的影像
我們有講過說， 為什麼我們今天在做影像生成的時候要用 GAN
而不是傳統的 supervised learning 的方法，是因為
今天在做影像的生成的時候
你可能同樣的 condition
你有好多不同的對應的 image
比如說火車有很多不同的樣子
那機器學習的時候，它是會產生所有火車的平均
然後看起來是一個模糊的東西
那今天對一般的 training 來說
假設你沒有用 GAN 去 train 一個 chatbot 來說
也是一樣的，因為輸入同一個句子
在你的 training data 裡面，有好多個不同的答案
對 machine 來說他學習的結果就是希望去 同時 maximize 所有不同答案的 likelihood
但是同時 maximize 所有答案的 likelihood 的結果
就是產生一些奇怪的句子
那我認為這就是導致為什麼 machine
今天用 end to end 的方法，用 maximum likelihood 的方法
train 完一個 chatbot 以後它特別喜歡說 I'm sorry
或者是 I don't know
那用 GAN 的話，一個非常明顯你可以得到的結果是，用 GAN 來 train 你的 chatbot 以後，他比較喜歡講長的句子，那它講的句子會比較有內容，就這件事情算是蠻明顯的，那一個比較不明顯的地方是我們其實不確定說，產生比較長的句子以後，是不是一定就是比較好的對話，但是蠻明顯可以觀察到説，當你把原來 MLE 換成 GAN 的時候，它會產生比較長的句子
那其實各種不同的 seq to seq model 包括
在作業 2-1 做的 video capturing generation，其實你都可以用上 GAN 的技術，所以這邊的 technical message，就前面的東西假設你都沒有聽到的話，那今天這堂課要講的東西就是，如果你今天在 train seq to seq model 的時候，你其實可以考慮加上 GAN，看看 train 的會不會比較好
剛才講個 conditional sequence generation，那還是 supervised 的，你要有 seq to seq model 的 input 跟 output
接下來要講 Unsupervised conditional sequence generation
那這邊會舉三個例子
那我們先講 Text style transformation
那我們今天已經看過滿坑滿谷的例子是做 image style transformation
把什麼梵谷的畫，轉成
把風景照轉成梵谷的畫風轉成浮世繪的畫風等等
就作業 3-3 大家要做的事情
那其實在文字上，你也可以做 style 的 transformation
什麼叫做文字的 style 呢？
這邊舉一個簡單的例子是說
我們可以把正面的句子，算做是一種 style
負面的句子，算做是另一種 style
接下來你只要 apply cycle GAN 的技術
把兩種不同 style 的句子，當作兩個 domain
你就可以用 unsupervised 的方法
你並不需要兩個 domain 的文字句子的 pair
你並不需要知道說這個 positive 的句子應該對應到哪一個 negative 的句子
你不需要這個資訊，你只需要兩堆句子
一堆 positive，一堆 negative
你就可以直接 train 一個 style transformation
那我們知道說其實要做這種你要知道 一個句子是不是 positive 的
其實還蠻容易的，因為我們在 ML 的作業 5 裡面
你就會 train 一個 RNN
那你就把你 train 過那個 RNN 拿出來，然後給他一堆句子
然後如果很 positive，就放一堆，很 negative 就放一堆
你就自動有 positive 跟 negative 的句子了
那這個技術怎麼做呢？
我們就完全不需要多講，這個是我們上週就看過的
cycle GAN 的圖
那你今天要把 image style transformation
換成 text style transfer
唯一做的事情就是影像換成文字
所以我們就把 positive 的句子算是一個 domain
negative 的句子算是另外一個 domain
用 cycle GAN 的方法 train 下去就結束了
那你這邊可能會遇到一個問題是，我們剛才有講到說
如果今天你的 generator 的 output，是 discrete 的
你沒有辦法直接做 training
假設你今天你的 generator output 是一個句子
句子是一個 discrete 的東西
你用一個 sampling 的 process
你才能夠產生那個句子
當你把這兩個 generator
跟這個 discriminator 全部串在一起的時候
你沒辦法一起 train
那怎麼辦呢？有很多不同的解法
我們剛才就講說有三個解法，一個是
用 Gumbel-softmax
一個是給 discriminator continuous 的東西
第三個是用 RL，那就看你愛用哪一種，
在我們的實驗裡面，我們是用 continuous 的東西
怎麼做呢？其實就是把每一個 word
用它的 word embedding 來取代
你把每一個 word，用它的 word embedding 來取代以後
每一個句子，就是一個 vector 的 sequence
那 word embedding 它並不是 one-hot， 它是continuous 的東西
現在你的 generator，是 output continuous 的東西
這個 discriminator 跟這個 generator 就可以吃這個 continuous 的東西
當作 input
所以你只要把 word 換成 word embedding
你就可以解這個 discrete 的問題
你可能會問說這個東西有什麼用
這個東西具體而言就是沒有什麼用
正向會轉負向喔，就是會 learn 兩個 network
就是在做 cycle GAN 的時候你會 learn 兩個 network
一個就是正向轉負向
但我們只拿負向轉正向這個出來 demo
就沒有拿正向轉負向出來這個 demo
因為我不希望讓你的人生變得更黑暗這樣子
那我們上次講到說這種 unsupervised 的 transformation 有兩個做法
一個就是 cycle GAN 系列的做法
那我們剛才看到哪個 Text style transfer
是用 cycle GAN 系列的做法
那也可以有另外一個系列的做法
就是你把不同 domain 的東西
都 project 到同一個 space
然後再用不同 domain 的 decoder，把它解回來
Text style transfer 也可以用這樣子的做法
你唯一做的事情
就只是把本來你的 x domain 跟 y domain 可能是真人的頭像
跟二次元人物的頭像，把他們換成正面的句子
跟負面的句子，就結束了
當然我們有說，今天如果是產生文字的時候
你會遇到一些特別的問題就是因為，文字是 discrete 的
所以今天這個 discriminator， 沒有辦法吃 discrete 的 input
如果它吃 discrete 的 input 的話
它會沒有辦法跟 decoder jointly trained
所以怎麼解呢？在文獻上我們看過的一個作法是
當然你可以用 RL，Gumbel-softmax 等等不同的解法
但我在文獻上看到一個有趣的解法是
有人說這個 discriminator
這個是 MIT CSAIL lab 做的
我如果沒記錯的話
這 discriminator 不要吃 decoder output 的 word
它吃 decoder 的 hidden state
就 decoder 也是一個 RNN 嘛
那 RNN 每一個 timestamp 就會有一個 hidden vector
這個 decoder 不吃最終的 output
它吃 hidden vector，hidden vector 是 continuous 的
所以就沒有那個 discrete 的問題，這是一個解法
然後我們說這個今天你要讓這兩個不同的 encoder
可以把不同 domain 的東西 project 到同一個 space
你需要下一些 constrain， 那在上週我們講了很多各式各樣不同的 constrain
那我發現說那些各式各樣不同的 constrain
還沒有被 apply 到文字的領域
所以這是一個未來可以做的事情
我現在看到唯一做的技術只有說有人 train 了一個 classifier
那這個 classifier，就吃這兩個 encoder 的 output
那這兩個 encoder 要盡量去騙過這個 classifier
這個 classifier 要從這個 vector 判斷說這個 vector 是來自於哪一個 domain
我把文獻放在這邊給大家參考
那接下來我要講的是說
用 GAN 的技術來做 Unsupervised Abstractive summarization
那這個怎麼做呢？
那怎麼 train 一個 summarizer 呢？
怎麼 train 一個 network 它可以幫你做摘要呢？
那所謂做摘要的意思是說
假設你收集到一些文章
那你有沒有時間看，你就把那些文章直接丟給 network
希望它讀完這個文章以後，自動地幫你生成出摘要
當然做摘要這件事，從來不是一個新的問題
因為這個顯然是一個非常有應用價值的東西，所以他從來不是一個新的問題，50-60 年前就開始有人在做了
只是在過去的時候，machine learning 的技術還沒有那麼強
所以過去你要讓機器學習做摘要的時候
通常機器學做的事情是
extracted summarization
這邊 title 寫的是 abstractive summarization
還有另外一種作摘要的方法叫做 extracted summarization
extracted summarization 的意思就是說， 給機器一篇文章
那每一篇文章機器做的事情就是判斷這篇文章的這個句子
是重要的還是不重要的
接下來他把所有判斷為重要的句子接起來
就變成一則摘要了
那你可能會說用這樣的方法
可以產生好的摘要嗎？
那這種方法雖然很簡單
你就是 learn 一個 binary classifier 決定一個句子是重要的還是不重要的
但是你沒有辦法用這個方法，產生真的非常好的摘要
因為為什麼呢？因為就像我們國小老師都有告訴我們說
你今天在寫課文摘要的時候
你要用自己的話，來寫摘要
你不能夠把課文裡面的句子就直接抄出來
當作摘要，你要自己 understanding 這個課文以後
看懂這個課文以後，用自己的話，來寫出摘要
那過去 extracted summarization
做不到這件事，但是今天多數我們都可以做 abstractive summarization，怎麼做？
learn 一個 seq2seq model
收集一大堆的文章，每一篇文章都有人標的摘要
然後 seq2seq model 硬 train 下去，就像你 train 一個 chatbot
或 train 一個 video capturing generator 一樣
train 下去就結束了，給它一個新的文章
它就會產生一個摘要
而且這個摘要是機器用自己的話說出來的
不見得是文章裡面現有的句子
但是這整套技術最大的問題就是
你要 train 這個 seq2seq model
你顯然需要非常大量的資料
到底要多少資料才夠呢？ 很多同學會想要自己 train 一個 summarizer
然後他去網路上收集比如說 10 萬篇文章
10 萬篇文章它通通有標註摘要，他覺得已經很多了
train 下去結果整個壞掉，為什麼呢？
這時候我就會告訴你說， 你要 train 一個 abstractive summarization 系統
通常至少要 1 百萬個 examples，才做得起來
沒有一百萬個 examples
機器可能連產生符合文法的句子都做不到
但是如果有上百萬個 examples
對機器來說，要產生合文法的句子，其實不是一個問題
但是這個 abstractive summarization 最大的問題就是
要收集大量的資料，才有辦法去訓練
所以怎麼辦呢？我們就想要提出一些新的方法
我們其實可以把文章視為是一種 domain
把摘要視為是另外一種 domain
現在如果我們有了 GAN 的技術
我們可以在兩個 domain 間
直接用 unsupervised 的方法互轉
我們並不需要兩個 domain 間的東西的 pair
所以今天假設我們把文章視為一個 domain
摘要視為另外一個 domain
我們不需要文章和摘要的 pair，只要收集一大堆文章
收集一大堆摘要當作範例告訴機器說
摘要到底長什麼樣子，這些摘要不需要是這些文章的摘要
只要收集兩堆 data
機器就可以自動在兩個 domain 間互轉
你就可以自動地學會怎麼做摘要這件事
而這個 process 是 unsupervised 的
你並不需要標注這些文章的摘要
你只需要提供機器一些摘要， 作為範例就可以了
那這個技術怎麼做的呢？
這個技術就跟 cycle GAN 是非常像的
我們 learn 一個 generator
這個generator 是一個 seq2seq model
這個 seq2seq model 吃一篇文章
然後 output 一個 word sequence
output 一個 比較短的 word sequence
但是假設只有這個 generator
你沒辦法 train
因為 generator 根本不知道說 output 什麼樣的 word sequence
才能當作 input 的文章的摘要
所以接下來
你就要 learn 一個 discriminator
這個 discriminator 的工作是什麼呢？
這個 discriminator 的工作就是，他看過很多人寫的摘要
但這些摘要不需要是這些文章的摘要
Discriminator 看過很多人寫的摘要
他知道人寫的摘要是什麼樣子
接下來他就可以給這個 generator feedback
讓 generator output 出來呢 word sequence
看起來像是摘要一樣
就跟我們之前講說什麼風景畫轉梵谷畫一樣
你需要一個 discriminator，看説一張圖是不是梵谷的圖
把這個資訊 feedback 給 generator
generator 就可以產生看起來像是梵谷的畫作
梵谷 style 的畫作
那這邊其實一樣，你只需要一個 generator
一個 discriminator，discriminator 給這個 generator feedback
就可以希望它 output 出來的句子，看起來像是 summary
但是在講 cycle GAN 的時候我們有講過說
光是這樣的架構是不夠的
因為 generator 可能會學到產生看起來像是 summary 的句子
就人寫的 summary 可能有某些特徵
比如說它都是比較簡短的
也許 generator 可以學到產生一個簡短的句子， 但是跟輸入是完全沒有關係的
那怎麼解這個問題呢？
就跟 cycle GAN 一樣，你要加一個 reconstructor
在做 cycle GAN 的時候我們說
我們把 x domain 的東西轉到 y domain
接下來要 learn 一個 generator，把 y domain 的東西轉回來
這樣我們就可以迫使，x domain 跟 y domain 的東西
是長得比較像的
我們希望 generator output，跟 input 是有關係的
所以在做 unsupervised abstractive summarization 的時候
我們這邊用的概念，跟 cycle GAN 其實是一模一樣的
你 learn 另外一個 generator
我們這邊稱為 reconstructor
他的工作是，吃第一個 generator output 的 word sequence
把這個 word sequence，轉回原來的 document
那你在 train 的時候你就希望
原來輸入的文章，被縮短
以後要能被擴寫回原來的 document
這個跟 cycle GAN 用的概念是一模一樣
那你其實可以用另外一個方法來理解這個 model
你說我有一個 generator
這個 generator 把文章變成簡短的句子
那你有另外一個 reconstructor 它把簡短的句子變回原來的文章
如果這個 reconstructor 可以把簡短的句子
變回原來的文章，代表說這個句子
有原來的文章裡面重要的資訊
因為這個句子有原來的文章裡面重要的資訊
所以你就可以把它當作一個摘要
在 training 的時候，這個 training 的 process 是 unsupervised
因為你只需要文章就好
你只需要輸入和輸出的文章越接近越好
所以並不需要給機器摘要
你只需要提供給機器文章就好
那這個整個 model，這個 generator 跟 reconstructor 合起來
可以看作是一個 seq2seq2seq auto-encoder 這樣
一般你都會 train 一個 seq2...， 你就一般你 train auto-encoder 就 input 一個東西
把它變成一個 vector，把這個 vector 變回原來的 object
比如說是個 image 等等，那現在是 input 一個 sequence
把它變成一個短的 sequence
再把它解回原來長的 sequence
這樣是一個 seq2seq2seq auto-encoder
那一般的 auto-encoder 都是用一個 latent vector 來表示你的資訊
那我們現在不是用一個人看不懂的 vector 來表示資訊
我們是用一個句子來表示資訊
這個東西希望是人可以讀的
但是這邊會遇到的問題是，假設你只 train 這個 generator 跟這個 reconstructor
你產生出來的 word sequence 可能是人沒有辦法讀的
他可能是人根本就沒辦法看懂的
因為機器可能會自己發明奇怪的暗語
因為 generator 跟 reconstructor，他們都是 machine 嘛
所以他們可以發明奇怪的暗語， 反正只要他們彼此之間看得懂就好
那人看不懂沒有關係，比如說台灣大學
它可能就縮寫成灣學，而不是台大
反正只要 reconstructor 可以把灣學解回台灣大學其實就結束了
所以為了希望 generator 產生出來的句子是人看得懂的
所以我們要加一個 discriminator
這個 discriminator 就可以強迫說
generator 產生出來的句子
一方面要是一個 summary
可以對reconstructor 解回原來的文章
同時 generator output 的這個句子
也要是 discriminator 可以看得懂的
覺得像是人類寫的 summary
那這個就是 unsupervised abstractive summarization 的架構
那可以做到什麼樣的程度呢？
這邊可以跟大家講一下就是說，在 training 的時候， 因為這邊 output 是 discrete 的嘛
所以你當然是需要有一些方法來處理這種 discrete output
那我們用的就是 reinforced algorithm 就是了
那我們來看一下可以做到什麼樣的程度， 這是一些真正的例子
那有人可能會想說用 unsupervised learning 有什麼好處
因為你用 unsupervised learning 永遠贏不過 supervised learning
supervised learning 就是 unsupervised learning 的 upper bound
但 unsupervised learning 的意義何在
那所以我們以下用這個實驗來說明一下
unsupervised learning 的意義
那我們知道說，那這邊這個縱軸是 ROUGE 的分數
總之就是用來衡量摘要的一個方法啦
值越大，代表我們產生的摘要，越好就是了
那黑色的線是什麼？黑色的線是 supervised learning 的方法
那今天在做 supervised learning 的時候
需要 380 萬筆 training example
380 萬篇文章跟它的摘要
你才能夠 train 出一個好的 summarization 的系統
那用 380 萬篇文章 train 出來的結果呢
是黑色的這一條線
那這邊我們用了不同的方法來做這個，來 train 這個 GAN
我們有用 WGAN 的方法
有用 reinforcement learning 的方法
分別是藍線跟橙線得到的結果其實是差不多的
看起來 WGAN 是差一點
那橙色的結果，用 reinforcement learning 的結果是比較好的
那今天如果在完全沒有 label 情況下， 得到的結果是這個樣子
那當然跟 supervised 的方法，還是差了一截
但是今天你可以用少量的 summary
再去 fine tune unsupervised learning 的 model
就是你先用 unsupervised learning 的方法把你的model 練得很強
再用少量的 label data 去 fine tune
那它的進步就會很快，舉例來說， 我們這邊只用 50 萬筆的 data
得到的結果就已經跟 supervised learning 的結果一樣了
所以這邊你只需要原來的 1/6 或者更少的 data
其實就可以跟用全部的 data 得到一樣好的結果
所以 unsupervised learning 帶給我們的 好處就是你只需要比較少的 data
比較少的 label data， 就可以跟過去大量 label data 的時候得到的結果
也許是一樣好的， 那這就是 unsupervised learning 的妙用
這邊舉最後一個例子是 unsupervised machine translation，那至於怎麼做，我們今天就不細講
因為我們今天可以把，不同的語言
就視為是不同的 domain
就假設你要英文轉法文
你就要把英文視為一個 domain
法文視為另外一個 domain
然後就可以用 unsupervised learning 的方法把英文轉成法文，法文轉成英文
那就做到摘要了，就結束了這樣， 不是摘要，做到翻譯就結束了
所以你就可以做 unsupervised 的翻譯
那這個方法聽起來還蠻匪夷所思的
真的能夠做得到嗎？其實 facebook 在今年的 ICLR
就發了兩篇這種 paper
看起來還真的是可以的
細節我們就不講，細節你可以想像就很像那個 cycle GAN 這樣
就我們前面講過的方法
只是前面我們有說拿兩種不同 image 當作兩個不同的 domain
兩種不同的語音當作兩個不同的 domain
現在只是把兩種語言當作兩個不同的 domain
然後讓機器去學兩種語言間的對應
硬做看看做不做的起來，這個是文獻上的結果
這個虛線代表 supervised learning 的方法
縱軸是 BLEU score，是拿來衡量摘要好壞的方法
總之 BLEU 越高，代表摘要做得越好
橫軸是訓練資料的量，從 10^4 一直到 10^7
你發現說呢，如果 supervised learning 的方法
這邊是不同語言的翻譯，英文轉法文
法文轉英文，德文轉英文，英文轉德文，四條線
代表四種不同語言的 pair，語言組合間的翻譯
那你發現訓練資料越多，當然結果就越好
這個沒有什麼特別稀奇的，橫線是這個橫線是什麼？
橫線是 unsupervised learning 的方法
用 10^7 的 data 去 train 的
unsupervised learning 的方法
但是你並不需要兩個語言間的 pair
做 supervised learning 的時候
你需要兩個語言間的 pair
但做 unsupervised learning 的時候
就是兩堆句子，不需要他們之間的 pair，然後硬做
得到的結果呢，你今天只要
unsupervised learning 的方法
有 10 million 的 sentences
你的 performance，就可以跟 supervised learning 的方法
只用 10 萬筆 data，是一樣好的
所以假設你手上沒有 10 萬筆 data pair
unsupervised 方法其實還可以贏過
supervised learning 的方法， 這個結果是我覺得還頗驚人的
接下來我們就想說，既然兩種不同的語言可以做
那語音跟文字間可不可以做呢？
把語音視為是一個 domain
把文字視為是另外一個 domain
然後你就可以 apply 類似 GAN 的技術
在這兩個 domain 間，互轉，硬做，硬做
這樣看看機器能不能夠學得起來
如果假設今天機器可以學會說
給它一堆語音給它一堆文字
它就可以自動學會怎麼把聲音轉成文字的話
你就可以做 unsupervised 的語音辨識了
未來機器可能在日常生活中，聽人講話
然後它自己再去網路上
看一下人寫的文章，就自動學會，語音辨識了
有人可能會想說，這個聽起來也是還蠻匪夷所思的
這個東西到底能不能夠做到呢？
我先覺得是有可能的，如果翻譯可以做到
這件事情也是有機會的
unsupervised 語音辨識也是有機會的
這邊舉一個非常簡單的例子，假如說所有聲音訊號的開頭
都是某個樣子，比如說都有 P1 這個 pattern
我們用 P 代表一個 pattern，就 P1 這個 pattern
那機器在自己去讀文章以後發現説
所有的文章都是 The 開頭
它就可以自動 mapping 到說 P1 這種聲音訊號
這種聲音訊號的 pattern，就是 The 這樣
那這是一個過度簡化的例子
實際上做不做得起來呢？這個是實際上得到的結果
我們用的聲音訊號來自於 TIMIT 這個 corpus
用的文字來自於 WMT 這個 corpus
那這兩個 corpus 是沒有對應關係的
就並不是說這些文字就是這些語音的辨識的結果，不是
這是兩堆不相關的東西，一堆語音講自己的
文字講自己的，兩堆不相關的東西
然後接下來，用類似 cycle GAN 的技術，硬轉
看能不能夠把聲音訊號硬是轉成文字
這是一個實驗的結果，縱軸是辨識的正確率
那其實是 Phoneme recognition，不是辨識出文字
你是辨識出音標而已，辨識出文字還是比較難
直接辨識出音標而已
那這個橫軸代表說訓練資料的量
如果是 supervised learning 的方法，當然訓練資料的量越多
performance 越好
這兩個橫線是什麼呢？
這兩個橫線就是用 unsupervised 的方法硬做得到的結果
那硬做其實有得到 36% 的正確率
你會想 36% 的正確率，這麼低
這個 output 結果應該人看不懂吧，是，人看不懂
但是它是遠比 random 好的
真的是遠比 random 好的
所以就算是在完全 unsupervised 的情況下
只給機器一堆文字，一堆語音，它還是有學到東西了
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
