那上次講的是比較有關理論的部分
那這邊要講的是在實驗上真正的觀察
那這個也是作業 1-2 的其中一題
就是我們試著用一些你想的到的方法
來 visualize training 的時候
這個 loss surface 長甚麼樣子
還有 training process 長甚麼樣子
那今天這個圖當然不是
這邊這個圖當然不是一個 network 真正的 error surface
它只是一個示意圖
希望大家可以想辦法看能不能夠 visualize network 的參數
還有它的 loss 之間的關係
但是怎麼 visualize 呢
network 的參數有成千上萬個
它並不是二維的空間 並不是只有兩個參數
那要怎麼 visualize network 的參數
跟它的 loss 之間的關係呢
那以下是*** 用的一個方法
那這個其實是我看到比較早的一篇
跟 network error surface visualization 有關係的文章
應該是在14 還是 15 年的時候就發表了
這是 *** 做的
它這邊怎麼說呢 它說
我們沒有辦法 visualize network 的所有參數
跟 loss 之間的關係
但是我們可以 visualize 參數在某一個方向上的變化
跟 loss 之間的關係
我們把參數往某一個方向改變的時候
我們去觀察它對 loss 的影響
但是要選擇哪一個方向呢
今天在一個 network 的參數所組成的 space 裡面
你可以選擇的方向有無窮多個
好那今天在 visualize 的時候
**** 他選擇的方向是
從 training 的 initial 的參數
到最後你 train 到停止的時候
你找出來的那一組參數
中間的這個方向
也就是說我們一開始 training 的時候
有一組 initialize 的參數這個寫作 θ0
train 到最後
你的 network 停下來了 你 training process 停下來了
你得到θ*
接下來我們在 θ0 跟 θ* 之間連一條線
當然實際上在 training 的時候
network 並不是從 θ0 直接跑到 θ*
它從 θ0 跑到 θ* 中間呢
可能有一大堆曲曲折折的關係
當然這件事情是發生在一個非常高維的空間上
只是現在我們把它畫在二維平面上
這個 θ0 跟 θ* 都是高維空間中的一個 point
我們只是把它畫在二維的空間上而已
那有了 θ0 有了 θ* 有了高維空間中的兩個點
我們就把高維空間的這兩個點連在一起
接下來我們 visualize 這一條連線上面 loss 的變化
所以就得到右上角這個圖
0 這邊代表 θ0 這個參數它的 loss
那這個 1.0 這邊
代表 θ* 跟你最後 train 出來的 solution 它的 loss
那你還可以再順著這個方向繼續向右
把這邊這一條連線上面的每一個點
它的 loss 都算出來放在這邊
放在這邊就會得到這個曲線
然後你從這個方向還可以繼續
再順著這個方向繼續往右走
你走到 θ0+2(θ*-θ0)
那你就得到右邊的這個部分
那上面這邊有三條曲線 這三條曲線分別是
三個不同的 network maxout network relu 還有 sigmoid
這三種不同的 network 它們的 error surface
那這個圖告訴我們甚麼呢
這個圖告訴我們事情其實是
這個 從這個地方 從 training 的起始
一直到 training 的結束
其實這個 error surface 看起來是頗為平坦
只有非常少的例外舉例來說
我們最後 training 的時候停在 我們看藍色這條線
藍色這條線是一個 maxout network
training 的時候是停在這個地方
但實際上有一個更低的地方是在這裡
所以顯然這個地方在用 gradient descent
並沒有真的找到一個 local minima
因為這個點它的 loss 是更低的
它比較更像一個 local minima
那你光看這個圖你並不能確定說
這邊是不是一個 local minima
因為它只是某一個方向上 loss 的變化
在這個螢幕上這個方向上看起來它是 local minima
但是也許垂直這個螢幕輸入輸出的地方
它原來還可以 loss 還可以再下降
其實它是一個 settle point
也是有可能的 我們不知道
因為我們只看了某一個方向上 loss 的變化而已
但是這個圖告訴我們甚麼 這個圖告訴我們說
其實 local minima 好像沒有我們想像的那麼容易出現
從這個地方到這個地方
我們本來預期中間有很多的坑坑洞洞
它會讓 network 走到一些坑洞以後就陷進去了
但事實並不是這樣
從 θ0 到 θ* 中間它是非常平坦的
network 從這邊走走走
如果你用 gradient descent 從這邊滑下來
你 even 可以從起始的地方就滑到終點
可能在 training 的時候你根本就不需要像我們想像的一樣
繞了遠路 順著 gradient 的方向 才從θ0 回到 θ*
這是 ***給我們的一個觀察
好那剛才看的是一個一般的 fully**** 的 network
那我沒記錯的話實驗是做在 MNIST 上面
那有另外一個實驗是看 CNN 它是做在這個 CIFAR10 上面
然後一樣我們看 CNN 的時候會發現說
左邊這個 這邊是 training 起始的地方
中間這個是 training 停止的地方
就是你最後 training 停止的時候你得到 network 的參數
注意你最後 training 停止的時候你得到 network 的參數
它不見得是 local minima
甚至不見得是 critical point
所以如果你讀 *** paper 的話 你會發現他寫的時候是很小心的
他都沒有告訴你說這個地方叫 local minima 還是 critical point
他都只說那是我的 algorithm 的 solution
因為首先你要確定它是不是 local minima
檢查 *** 才知道它是不是 local minima
不檢查你怎麼能夠說它是一個 local minima 呢
再來假設你要確認它是不是一個 critical point
你只要確定你的微分是 0
如果你不確定微分是 0
gradient 是 0 你其實也不能夠說它是 critical point
所以 *** 從來沒有說過這個點叫做 critical point
他只說它就是 algorithm 的 solution
好所以從 initial 的地方到最後 algorithm 的 solution
中間的變化也是頗為平坦的
然後再往更右邊去
再往更右邊去會發現說 loss 就急遽的上升
所以確實這個最後找到的 solution
看起來有點像是一個 local minima
雖然不能百分之百保證它是 但是看起來有點像
那有人會說會不會是因為我們的解析度不夠
如果解析度高一點也許就會看到很多的 local minima
這邊*** 提供的一個解析度比較高的圖 他把這個位置
放很大然後來觀察
然後發現說確實有一點點高低起伏
這個點好像是比旁邊還要低
就是如果你用 gradient descent 因為這個地方稍微高一點
所以你用 gradient descent 你不可能
因為 gradient descent 就是順著那個坡度走
所以它沒有辦法 除非你有 momentum
不然你沒有辦法逆勢而為走過這個地方
所以 machine 在走的時候它應該還是有繞路
就是它並不是完全順著這條路走
它還是有繞路避開這個山坡的地方
所以 network 才能夠走到 solution
助教上週也有講說如果他把解析度放得非常非常大
他其實是有看到一些高低起伏的
那個就留給大家作業的時候觀察
看看 *** 是不是也在騙我們這樣
也有可能就是比如說它的 network 太 shallow 了
所以才沒有觀察到那些高低起伏
也許當 network 非常深的時候
你就可以觀察到一些崎嶇的變化
好那這個是更多的實驗
因為我剛才發現說很難找到 local minima
非常難找到 local minima
他說如果我們
他說今天如果隨便找兩個點
隨便找兩個點然後把它們連在一起
把它們連在一起
你會發現他們曲線的變化看起來像是這個樣子
非常平坦你很難找到一些高低起伏的地方
那這邊是另外一個 case 它說
它的 0.0 跟 1.0 分別是兩個 solution
所以從兩個不同的 initialization train 下去
會得到兩個不同的最終的參數的 solution
然後在這兩個參數的 solution 中間連線的這個地形
這邊我們就可以看到一個高起來的地方
所以看起來像是說這個 solution 有點像是一個 local minima
另外一個 solution 也有點像是一個 local minima
然後他們之間有一個高起來的小山丘把它們分開在兩邊
這個就是一些觀察
好那剛剛觀察的是只有一個維度而已
只有一個維度而已
那如果觀察兩個維度呢
怎麼觀察兩個維度呢
這邊的做法是說一個維度是從 θ0 到 θ*
這個代表的是 projection 這一維
那另外一個維度是
現在你的參數跟這一條藍色的線中間的距離
這個叫做 residual
所以你今天實際上在 train 的時候
你是從 θ0 走到 θ*
這條藍色的線的座標是 projection 這一維的座標
是 projection 這一維的座標
然後這個 residual 這一維的座標代表的是現在走的這條線
現在走的這條線它跟藍線中間的距離
那要注意一下因為今天在走的時候
舉例來說在這個點
你的 residual 是這個點和藍線的距離
這邊是這個點和這條線的距離 他們的方向是不一樣
但是它們都是正的值 我不知道大家知不知道我的意思
所以這個 residual 這個方向並不是高維空間中的某一個固定的方向
其實不斷的在旋轉這樣
我不知道大家知不知道我的意思就假設這個是起始的地方
這個是終止的地方起始到終止的地方是這樣子
那你在 train network 的參數的時候
在高維空間從這點走到這一點
他並不是這樣走直線過去的
它是歪歪曲曲的走 它可能有的時候網上偏一點
再下來 有時候往右邊偏一點再進來再往左邊偏一點
你會發現他們曲線的變化看起來像是這個樣子
