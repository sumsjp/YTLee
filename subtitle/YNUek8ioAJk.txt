臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
好，接下來我要來講一下 Generation 的 model
有關 Generation 的 model
你可以看一些很好的 reference
這個是 OpenAI 寫的一篇科普的文章
那我把它列在這一邊
在這篇文章裡面呢
它開頭是引用費曼的話
這句話，來歷是什麼呢？
據說是，費曼過世以後
有人去他的辦公室，拍一下他的黑板
他在黑板上留下了這一句話
這個是他辦公室的黑板
這一句話寫在左上角 ，他說
What I cannot create, I do not understand.
所以，如果一個東西
他沒有辦法，他不知道怎麼產生它的話
他就不算是真的完全理解
所以，對 machine 來說 ，或許也是一樣
比如說 ，現在 machine 在影像處理上
machine 可以做到分類
你可以讓他見識貓和狗的不同
但他或許並不真的了解
貓是什麼， 或許它並不真的了解狗是什麼
也許在未來，有一天
machine 它可以自己畫出一隻貓的時候
它對於貓這個東西的概念
或許就不一樣了
而這個是現在一個非常熱門的主題
有很多相關的研究
我們就來稍微講一下
我們稍微來 overview 一下
相關的研究
在這些研究裡面
大概可以分成 3 個方法
這邊分別是 PixelRNN
VAE (Variational Autoencoder)
和 Generative Adversarial Network (GAN)
那這些方法呢，其實都非常的新
其中最舊的是 VAE
VAE 是 2013 年提出來的
像 GAN 是 2014 年提出來
它提出來還不到兩年
你知道，我們一般在上課的時候
尤其是在必修課裡面
你聽到的每一個東西往往都是，比如說
50 年前，100 年前 propose 出來的
但是在 machine learning，這邊這個領域裡面
它的變化是非常非常快速的
所以，有很多很多
未來可能會成為非常經典的方法
它是這幾年，才不斷地推陳出新
所以，我試著在課程裡面
cover 一些比較新的技術
讓大家知道說
這個領域它的變化，是非常非常快的
那我們先來講一下 PixelRNN
我們今天就先講一下這些方法
大概是怎麼運作的
然後下一次，再細講它的原理
我們來講一下 ，這個 Pixel RNN
雖然我還沒有講過 RNN 是什麼
但我相信這個方法，你是可以聽得懂的
因為它非常的，非常的直覺
非常的直覺
假設我們今天要產生一個
我們今天的目標是要讓 machine 自己畫一張圖出來
比如說，我們今天要讓 machine 畫出一個
解析度是 3*3，有 9 個 pixel 的 image
怎麼做呢？
我們讓 machine 每一次畫一個 pixel
它每一次就點一個點
點完 9 個點，就畫出一張圖了
那怎麼做呢？
假設我們先隨機給它第一個
隨機給這個 image ，塗一個紅色的 pixel
接下來呢，你就 learn 一個 model
它 input ，就是已經在圖上的紅色 pixel
它的 output，就是接下來
要吐出什麼顏色的 pixel
假設，它吐出藍色的 pixel
那就把藍色的 pixel 再擺到 image 上面
那你說，怎麼描述一個 pixel 呢？
一個 pixel 不就是 RGB 三個顏色所構成的嗎？
所以，一個 pixel 就是一個三維的 vector
所以，你就是 learn 一個 neural network
它可以吃一個三維的 vector
然後，output 另外一個 3 維的 vector
這個 3 維的 vector，3個值
就代表了，這個 pixel 的 RGB 3 個值
你要把這個 neural network 的 output
轉成一個顏色，然後，把它塗上去就結束了
接下來，這個圖上有兩個點
那你就再用同一個 model
它 input 紅色和藍色的 pixel
接下來，它就 output 下一個 pixel
比如說，它是淺藍色的 pixel
那這邊呢，你可能會有個問題就是
neural network 不是 input 一個 pixel length 的 vector
output 另外一個 pixel length 的 vector 嗎？
比如說，你 input 3 維
它就固定只能 input 3 維
你怎麼 input 一個 pixel
又用同一個 network，input 2個 pixel 呢？
這個就是 RNN 可以處理的地方
RNN 它可以處理 variable length 的 input
因為我們還沒有講過 RNN
如果，你覺得你無法理解
怎麼處理一個 variable length 的 pixel 的話
那你就想像說
還沒有 pixel 的地方，我就補零
就想像說，我這一邊空白的地方
我其實是，就是補一個白色的 pixel 而已
那接下來呢
你給它紅色、藍色跟淺藍色的 pixel
接下來，它吐出一個灰色的 pixel
那你就再把灰色的 pixel 放到圖上
接下來，重複剛才的步驟，給它這 4 個 pixel
然後，把下一個 pixel 再補上去
那你就把 9 個 pixel 都補完
就畫完一張圖了
那要 train 這種 network 很簡單
它完全就是 unsupervised
你不需要任何的 label ，你想在作業 3 裡面
你要知道某一些 image，它的 label 是什麼
你才能夠 train 一個 image 的 classifier
這一邊不用，你就收集一大堆的 image
然後，machine 就會知道說
看到第一個 pixel 是這個顏色
第二個 pixel 應該是甚麼
看到第一個和第二個 pixel 這個顏色
第三個應該是甚麼
看到1, 2, 3 個pixel
第四個 pixel 應該是甚麼
這個就叫 PixelRNN
你可能想說，這招這麼簡單
它會 work 嗎？
也讓我蠻驚奇的
它其實是 work 的
而且在不同的 generate image 的方法裡面
現在，就我所知
應該是 PixelRNN 它產生的圖
是最清晰的
其他的方法產生的圖
沒有辦法像 PixelRNN 這麼清晰
這個是，DeepMind paper 的一個例子
這個是真實的 image，就是一隻狗
如果我們現在把這一隻狗的下半身
遮掉，把下半身遮掉
接下來，我們就要讓 machine
去 predict 說，given 這些 pixel
接下來這隻狗的下半身
應該要長什麼樣子
machine 把狗的下半身畫出來
那 machine 畫出了這樣子的狗
還有這樣子的狗
有看起來看像猩猩的狗
還有看起來像雞的狗這樣
然後，這個方法
其實也可以被用在
不只是影像上
它也可以被用在語音上
非常著名的例子呢
就是 WaveNet
我們知道 WaveNet 可以做語音合成
但是 WaveNet 到底是怎麼做的呢？
其實，它的概念非常簡單
就是 input 一大堆的
這邊底下的每一個 node
藍色的圈圈，代表的就是 wave form
wave form 大家知道嗎？就是
反正，你就把聲音取進來
然後，直接去取樣
然後看它的 amplitude
等於完全不用做任何 Fourier transform
通通都不要，這邊每一個藍色的點
就是聲音訊號上面的一個 sample
好，那今天就是這樣
我們等這個動畫跑完
給定前面一段的聲音訊號
然後，predict下一個 sample 的結果
然後把它放下來，再 predict 下一個 sample
再放下來，再 predict 下一個 sample
再放下來 ，就這樣
然後接下來就是
硬 train 下去，就結束了
然後，你就可以合出一段聲音
這個是在語音上的應用
在影像上也有應用
在 影像上，你也可以做一樣的事情
只要讓 machine 看過很多的 video
然後，它就可以 predict 說
給它一段 video，接下來會發生甚麼事情
比如說， 在 Google 的 demo 裡面
它會 DEMO 說
它的那些影像都是機器人手背上的影像
它在機器人手背上裝了一個攝影機
所以，它可以錄說
機器人的手背看到的影像
你就可以看到說
影像的前半段是
機械人的手伸向一塊抹布
然後，影像的後半段
就是機器人把抹布拿起來
把抹布拿起來的那一些影像
是機器自己腦補的結果
並不存在於現實生活中
機器自己 predict 接下來會發生什麼事情
那如果你想要練習
這個 generation 的話
一個最簡單的 application，當然是做 MNIST
不過做 MNIST，我覺得有一點虛弱
generate digit 有點虛弱，而且那個有點簡單
所以，我 create 另外一個 task
如果你想要練習 generative model 的話
你可以用這個 task
這一個 task 是 Pokémon Creation
這邊，我們有 792 張寶可夢的小圖
我們要讓 machine 看過這些小圖以後
它自己產生新的寶可夢出來，這一邊的口號就是
用創造代替捕抓這樣
過去我們是抓寶可夢
在野外抓寶可夢
像我們在實驗室裡面
是你創造新的寶可夢出來
image 的 source 是來自於 Pokémon 的 Wiki
我看了一下它的 copyright
它的 image 都可以任意使用
和變造的，所以應該是沒有版權的問題
原來的 image 是 40*40 的 pixel
其實，我覺得 40*40 有點稍微太大
所以，我把它 cut 小一點
只取了中間的部分
中心的部分，變成 20*20 的 image
那看起來大概像是這樣子
這個是喵喵
這個是，我也不知道它是什麼
我只知道它好像跟果然翁在一起
其實，我沒有把動畫仔細看完
反正就是這些，你可以
雖然說，影像的解析度沒有很高
但是，還是可以很清楚的看到說，它就是寶可夢
那這邊有一些 tips
因為，我其實自己實際做了一下
我覺得有一些 tips
首先，就是你可能很直覺的覺得說
一個 image、一個 pixel 應該就是用一個三維的 vector
用三個數字來描述它
每一個數字代表了 RGB
比如說，這樣的 pixel
你可能就用 3 個 vector
你可能用三個數值
50，150 和 100 來描述它
但是，我覺得實際上這樣做以後呢
我發現這樣做，結果感覺不是太好
那我覺得，原因是因為
你用這樣做，你產生的圖
都很灰，看起來都灰灰髒髒的
我覺得一個原因是因為
如果你要產生非常鮮明的顏色的話
非常鮮明的顏色往往是
某一個RGB 裡面的某一個 value 特別高
其他都接近零
像灰色，就是三個 RGB 的 value 都差不多
就是灰色
所以，如果你要這個
那它產生非常鮮明的顏色的話
你這三個值，要差得夠大才行
但是，你在 learn 的時候
你往往沒有辦法真的讓，比如說
G 是 0、B 是 0
你往往做不到這一件事
尤其，你今天如果是 output
你 output 把 0 到 255
normalize 到 0 到 1 之間的話
output 你用 sigmoid function 的話
sigmoid function 的值，通常都落在 0.5 左右
通常都落在中間，它很難落在這種極端的值
所以，它產生的值
都是 RGB 3 個值差不多
所以，你看起來每一張圖，看起來都是灰色，
棕色，這感覺不太好看
所以，我就稍微做了一下前處理
怎麼做前處理呢？
我把每一個 pixel
都用一個 1-of-N encoding 的 vector 來表示
但是 1-of-N encoding 裡面
每一個 dimension
就代表了一個顏色
也就是說每一個 pixel
都是用一個 vector來表示
這個 vector 的每一個 dimension
比如說，第一個 dimension 就代表黃色
第二個 dimension 就代表藍色
第三個 dimension 就代表綠色
第四個 dimension 就代表黑色，等等
也就是說，不讓它產生 RGB
去合成顏色
而是直接讓它產生一個顏色
所以，比如說
這個顏色進來， 這個綠色進來
那它就是這一維是 1
然後其他是 0
所以，一個 pixel，我們用這個方式來描述它
但是，這邊會有一個問題就是
你可能的顏色太多了
RGB 每一個都有 255 種可能
256種可能
256 的 3 次方
有太多可能的顏色
怎麼辦呢？
我先做 clustering
把同樣接近的顏色 cluster 在一起
也就是說
這個綠，跟這個綠，跟這個綠
它們算是同一個顏色
都用上面這個綠來表示它
那我就把在 corpus 裡面
出現次數比較少的這個顏色
跟出現次數比較多的顏色呢
把它合併起來
所以，我們得到了 167 個不同的顏色
那我 release 了這個 corpus
大家可以任意使用
這個 corpus 是這個樣子
原來 image 在這一邊，然後，我裁過的結果在這邊
那我這邊就直接存 pixel
所以你也不用做 preprocessing 了，都幫你做好了
在這個 data 裡面
每一個 row，就代表了一張 image
然後，每一個數字，就代表了一個顏色
就代表了一個顏色
那怎麼知道每個數字代表什麼顏色呢？
它的 mapping 在這一邊
比如說，0，它的 RGB 就都是 255, 255, 255
所以，0 代表白色
然後，比如說，這一邊
這個 2，2 代表三個都是49, 49, 49
這個其實是一個灰色，等等
那我等一下就做這個 PixelRNN 的實驗
我發現，其實，你可能會懷疑說只有 700 多張圖
你 train 得起來嗎？
我 train 得起來，是 train 得起來
但問題就是，你可以得出什麼正常的好的結果嗎？
我發現驚人的就是
其實是可以的
然後，我沒有花太多時間調參數
我就用了一個一層的 LSTM
還有 512 個 cell
你不知道 LSTM 是什麼也沒有關係
反正，就是 learn 一個很簡單的 model
做出來的結果，就還算尚可
我相信，你認真調一下參數
一定可以做得比我好的，好的更多
這個是實驗的結果
首先要強調的是
我特意留了 3 張寶可夢
是 machine 沒有看過的
你在 training data 裡面
是沒有這三張寶可夢
接下來，我給 machine 看這個寶可夢的前半部
把剩下的半部，把它蓋起來
只給 machine 看前半部
然後，讓它 predict 這個寶可夢應該長什麼樣子
這個，你覺得它是什麼呢？
它看起來像是個兔子
紅色的兔子
然後，把它的下半身 generate 出來
它長這樣
它原來是有穿吊帶褲的兔子
但它其實是長這樣子
那你覺得這隻是什麼呢？
這一隻是甚麼？
感覺像是鐵甲蛹之類的東西
那 machine generate 出來結果是這個樣子
它是一個長的綠毛蟲
這個還有一個尾巴，這樣
它是一個綠毛蟲
但實際上，它是一隻蜥蜴
然後，這個呢，這個是什麼？
這個是什麼，這個 machine generate 出來
它覺得是一個類似狗或是狐狸的東西
它的臉很長
然後，這個是它的腳
這個是它的腳
machine還是要配色，你看
你看本來
本來這一邊它是沒有很多深紫色
但是到腳這邊，幫它塗一下深紫色這樣
我覺得還蠻強的，還蠻強的
但實際上，它其實是長這個樣子
如果，我們今天給它看
只有 25% 的圖 ，它會得到什麼
只給它看一個耳朵
它會覺得接下來是什麼
它產生這個臘腸兔這個樣子
臘腸兔
而且其實，我覺得 machine 還算蠻強的
你看它有手
它有手，它都把手畫出來
臘腸的兔，嘴巴跟眼睛，這個樣子
我覺得還蠻強的
這個給它看一個角
它會覺得是甚麼呢？
它覺得是鐵甲蛹這樣
類似鐵甲蛹，看起來像右邊
自己產生了一個，看起來很不爽的眼睛
鼻子，嘴巴這樣
這個，給它看上面一點點
也不知道是什麼東西
然後，它判斷出來，它覺得是這個樣子
你看它產生一個臉
它產生一個兔子的臉，這個是它的耳朵
這個是眼睛
這個是眼睛，想要翻白眼
然後，這個是它的鼻子
它產生一個臉
所以，我就覺得還蠻強的
而且，我覺得在做這種 task
有一個難點就是，你很難 evaluate 這個 task
因為，所謂創造這件事情
是無法 evaluate 的
你不能說，我覺得這個臘腸兔也很合理
我覺得搞不好還比這個蛞蝓更合理一點
就是說，它建得出來，並不一定要跟 ground truth 一樣
它建出來的結果跟 ground truth不一樣
並不代表說，它畫的是錯的
所以，像這種 task
是很難被，很難被 evaluate 的
這是現在做這種
generation 的一個難點
剛才，是有給 machine 一個開頭
然後讓它畫下去
接下來，你就什麼都不給他
讓它從頭開始畫
但是，如果你只是什麼都不給它
讓它從頭開始畫的話
它可能每一張 image 都是一樣的
所以你要故意加一些 random
也就是說，它在 predict 下一個 pixel 的時候
不見得是選機率最高的，那個 pixel
它會有一定的機率，選一個
機率比較低的顏色出來
畫在圖上面，這樣它每一次畫的圖
都才會有點不一樣
那其實蠻多都是不知道是在做什麼的圖
比如說，像這個我都不知道在做什麼
這個看起來像是鳥的嘴巴
那，這個也許，也許是兩個眼睛
我也不知道它在畫什麼
這個是，看起來像是個兔子，像是個兔子
難道這個是它的臉嗎
難道這個是它戴了一個頭盔
我也不知道它在畫甚麼
有一些，我覺得比較清楚
像這個，我覺得它是一隻飛鳥
這是它的手
這個是一個大嘴鳥
這是它的嘴巴這樣
這個是一隻比較慘的鴕鳥
它是一個戴綠帽的駝鳥
然後，這個是地鼠
這個畫得還不錯，它還有配色
淺藍色配白色，這是個地鼠
它是有眼睛的
這個也是另外一隻比較大隻的地鼠
它頭上有一些藍藍紅紅的
它是比較大隻的地鼠
然後，這個是，我們胡亂給它取個名字
叫做岩漿模糊好了
它下面是岩漿，上面是雲
叫岩漿雲模糊好了
它上面，這個是有眼睛
不要小看它，它是有眼睛的，兩個眼睛都點出來了
這個是飛行骷髏
它也是有眼睛的
我發現這個 model 還蠻強的
它還蠻喜歡點眼睛出來
然後，還有其他方法
細節我們或許下次再講
我們可以稍微看一下它的結果
這個東西那叫做 Variational Autoencoder
那我們之前，已經有講過說
已經有講過 Auto-encoder
我們之前講過 Auto-encoder
那我們之前在講 Auto-encoder 的時候
我們說，你現在呢
Auto-encoder 的 training criteria
就是 input 一張 image
通過 encoder 就變成一個 code
再通過 decoder 把 image 解回來
那你希望 input 跟 output
越接近越好
希望 input 跟 output 越接近越好
那你 learn 完這個 Auto-encoder 以後
你其實就可以把這 decoder 拿出來
你就把 decoder 拿出來
然後，你再給它 input 一個
random 的東西
你再給它 input 一個
你 generate 一個 random 的 vector
把這個 random vector 當作是 一個 code
你這個 code 10 維
你就 generate 一個 10 維的 vector
然後，希望丟到這個 decoder 裡面
它 output ，就可以是一張完整的 image
但，實際上這麼做呢
你得到的 performance 通常不一定很好
你要用一個方法叫做
Variation Autoencoder ，VAE
你得到的結果會比較好
這個 VAE 怎麼做呢？
它的結構跟 Auto-encoder 非常像
它只是在中間加了一些神妙的小 trick
為什麼要加神妙的小 trick
或許我們下一次再講
這個神妙的小 trick 是什麼
它說，input一個 encoder
然後，這 encoder 跟 decoder 的部分維持原狀
維持原狀不動
但是，現在在 encoder 的地方
我們不是直接 output code
我們先 output 兩個 vector
假設你的 code
你打算要做的 code 是三維的話
那你 output 的這兩個 vector 呢
也都是 3 維
這個 vector 是 m1, m2, m3
這個 vector 是 σ1, σ2, σ3
接下來，你用 normal distribution
去 generate 另外一個也是 3 維的 vector
它的 3 維分別是 e1, e2, e3
接下來，你把 σ1, σ2, σ3 取 exponential
跟 e1, e2, e3 相乘
然後，把它跟 m1, m2, m3 加起來
你得到這個 c1, c2, c3
這個東西呢，才是你的 code
這邊每一個 c 呢
這個 ci = exp(σi) * ei + mi
然後，丟到 decoder 裡面
希望說 decode 能夠 minimize reconstruction error
你希望你的 decoder 可以 minimize
reconstruction error
但是，光只有這麼做是不夠的
它還有第二項
這一項是說
這一項非常的神妙
我們之後再解釋
這一項是怎麼樣啊
你看，覺得很怪
這一項是，我們把它的三個 dimension 加起來
然後我們要去 minimize：1 + σi - (mi)^2 - exp(σi)
有這麼一項
minimize reconstruction error
同時 minimize 這一項
那你用 VAE 做出來結果怎麼樣呢？
這個是 OpenAI 做的結果
他們是做在 Cifar-10 上面
那你會發現說，其實用 VAE 得到的圖呢
它是不太清楚的
這些圖你都
你看得出來，好像想要畫點什麼
但是你又搞不清楚，它到底在畫什麼
但是用 VAE 跟剛才的 PixelRNN 有什麼不一樣的地方呢
用 VAE 的話
你可以做以下的事情
你可以 control 你要 generate 的 image，理論上
你可以 control 你要 generate 的 image
什麼意思呢？
假設，我們現在把 VAE 用在 Pokémon creation 上面
那我們 training 的時候
就是 input 一個 Pokémon
然後 reconstruct 一樣的 Pokémon
然後 learn 出來這個 code
我們就設 10 維
learn 好 這一個 Pokémon 的 VAE 以後
我們就把 decoder 的部分拿出來
因為，現在我們有一個 decoder
你可以 input 一個 vector
所以，你在 input 的 時候，你可以這樣做
你可以說，我現在有 10 維的 vector
我固定其中 8 維
只選其中的 2 維出來
然後，在這兩維的 dimension 上面
我灑不同的點
然後，我們把每一點都丟到 decoder 裡面
看它合出來的 image 長什麼樣子
那如果我們做這一件事情的話
如果我們灑不同的點的話
你就可以看到說
這個 code 的每一個 dimension
分別代表什麼意思
如果，我們可以解讀說
code 的每一個 dimension 代表什麼意思
以後我們就可以把那個 code
當作像是一個拉桿一樣
每一個 dimension 當作一個拉桿一樣
你可以調整它
你就可以產生不同的寶可夢
理想上是這樣子
那我們用 VAE 產生出來的結果是這個樣子
你可能覺得沒有很好
首先，你都看不出來每一隻是什麼
沒錯， 用 VAE 做出來就是這個樣子
那這兩個 dimension
分別是移動了
變化了兩個這個 code 的 兩個 dimension 以後的結果
但是，你從這個
你從這個變化，你可以看得出來說
每一個 dimension 其實或許它真的是有些含意的
比如說，如果我們看這一個變化
這個變化
它本來是看起來像是一個站著的東西
然後，它後來就逐漸倒下來，逐漸倒下來
演化，變成類似魚的東西
然後，後來就塌下來
塌下來，變成一個趴在地上的東西
如果你從左邊到右邊
你就會發它站的越來越直
它本來是，感覺是有點前傾的
越往右它就站得越來越直，它頭上會出現一個帽子
站得越來越直
所以，你可以從這樣的 dimension 變化發現說
它確實是有學到一些東西
如果我們可以讓圖產生更清楚的話
我們就可以控制這兩個 dimension
就可以產生不同的寶可夢
這邊是某兩個 dimension
這邊是另外兩個 dimension 的結果
這邊，這個的例子可能又更清楚一點
從右邊到左邊，本來是一個類倉鼠的樣子
後來就慢慢倒下來，倒下、倒下，然後變成一團布這樣
如果你看直的話
看直的話，它本來是一個倉鼠的樣子
倉鼠的樣子，從上面往下
你就會發現說，首先它的腳，越來越長
你就看到寶可夢的演化
它本來腳很短，後來越來越長、越來越長
後來變成有一隻很長的腳，看起來就變成像是鳥
後來就變成有兩隻很長的腳
後來又變成說，其實它是有四隻腳
看起來像似哺乳類的動物
這是它的演化
如果你看它的尾巴的話
本來尾巴很短
後來尾巴也越來越長
尾巴越來越長
如果你看它的頭的話
本來耳朵很短
後來耳朵越來越長
看起來就像是，長出了角一樣
就可以看到說，隨著 input 的 code 的不同
你就可以產生不同的寶可夢
但是，在這麼多圖裡面
你會發現都沒有清楚的，都沒看起來像樣的
那我先找到一個像樣的
就是在這一邊，這樣
我找到一個，這一邊這一隻
我想原來的寶可夢裡面應該是沒有這一隻
我看了，我翻了一下圖鑑
但是沒有很仔細翻，我覺得應該是沒有這一隻
它產生出來是這樣
看起來，它的頭像是一個海馬
然後它的尾巴，就叫做鱷魚
我們就胡亂叫它鱷魚海馬
然後，有人說可以用 VAE 來寫詩
怎麼用 VAE 來寫詩呢？
這個做法是這樣子的
本來這個 VAE 的 Auto-encoder
input 是一個 image，output 也是一個 image
現在只是把 input 跟 output 改掉
改成 input 一個 sentence
output 也是一個 sentence
不過，如果你要 input 處理 sentence 的 input
產生 sentence 的 output
這邊你需要用 RNN 才能夠做到
不過，這個我們之後再講
那怎麼讓 machine 寫詩呢？
這一邊的作法是這樣
你先胡亂選兩個句子
先胡亂選兩個句子
一個句子是 I went to the store to buy some groceries.
第二個句子是 "don't worry about it," she said.
接下來，你通過這個 encoder
這兩個句子，都找出它的 code
都找出它的 code
所以，這兩個句子就變成
在 code space 上面的兩個點
接下來呢，你把這兩個點相連
然後，中間等間隔的取一些點
再把中間等間隔取的點呢
你就把這個 code 丟到 decode 裡面，再還原出去
所以，把這一個點稍微偏一點以後
把 I went to the store to buy some groceries.
稍微偏一點以後
你得到的句子就是
I store to buy some groceries.
再偏一點以後
你就得到 I were to buy any groceries.
就這樣一直下來，一直下來
到這一邊呢
變成 "come with me," she said
然後 "talk to me," she said.
然後變成 "don't worry about it," she said.
你可能會說，這個算是寫詩嗎？
這個是這樣
如果你仔細看他的 paper 的話
他的 paper 裡面沒有提到寫詩這一件事情
在這個 blog 裡面說是
說是這樣子是在寫詩就是了'
我不知道這個 blog 是誰寫的
大概就是這個樣子
那 GAN 的部分
我們就下一次再講
我們先就休息10 分鐘，等一下來講一下 final
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
