好 那我們就開始上課吧
那第一堂課 是要簡單跟大家
介紹一下machine learning
還有deep learning的基本概念
等一下會講一個跟寶可夢
完全沒有關係的故事
告訴你機器學習
還有深度學習的基本概念
好那什麼是機器學習呢
我想必大家在報章雜誌上
其實往往都已經聽過機器學習
這一個詞彙 那你可能也知道說
機器學習就是跟今天很熱門的AI
好像有那麼一點關聯
那所謂的機器學習到底是什麼呢
顧名思義好像是說 機器他具備有
學習的能力 那些科普文章往往把
機器學習這個東西吹得玄之又玄
好像機器會學習以後 我們就有了
人工智慧 有了人工智慧以後
機器接下來就要統治人類了
那機器學習到底是什麼呢
事實上 機器學習概括來說
可以用一句話來描述機器學習這件事
什麼叫機器學習呢 機器學習就是
讓機器具備找一個函式的能力
那機器具備找函式的能力以後
他可以做什麼樣的事情呢
他確實可以做很多事 舉例來說
假設你今天想要叫機器做語音辨識
機器聽一段聲音
產生這段聲音對應的文字
那你需要的就是一個函式
這個函式的輸入是聲音訊號
輸出是這段聲音訊號的內容
那你可以想像說 這個可以把聲音訊號
當作輸入文字當作輸出的函式
顯然非常非常的複雜 他絕對不是
你可以用人手寫出來的方程式
這個函式他非常非常的複雜
人類絕對沒有能力把它寫出來
所以我們期待憑藉著機器的力量
把這個函式自動找出來 這件事情
就是機器學習
那剛才舉的例子
是語音辨識 還有好多好多的任務
我們都需要找一個很複雜的函式
舉例來說 假設我們現在要做
影像辨識 那這個影像辨識
我們需要什麼樣的函式呢
這個函式的輸入是一張圖片
他的輸出是什麼呢 他是這個
圖片裡面 有什麼樣的內容
或者是大家都知道的AlphaGo
其實也可以看作是一個函式
要讓機器下圍棋 我們需要的
就是一個函式 這個函式的輸入
是棋盤上黑子跟白子的位置
輸出是什麼 輸出是機器下一步
應該落子的位置 假設你可以
找到一個函式 這個函式的輸入
就是棋盤上黑子跟白子的位置
輸出就是下一步應該落子的位置
那我們就可以讓機器做自動下圍棋
這件事 就可以做一個AlphaGo
那隨著我們要找的函式不同
機器學習有不同的類別
那這邊介紹幾個專有名詞給大家
認識一下 第一個專有名詞
叫作Regression
Regression的意思是說
假設我們今天要找的函式
他的輸出是一個數值
他的輸出是一個 scalar
那這樣子的機器學習的任務
我們稱之為Regression
那這邊舉一個Regression的例子
假設我們今天要機器做的事情
是預測未來某一個時間的
PM2.5的數值 你要叫機器
做的事情是找一個函式
這個我們用f來表示
這個函式的輸出
是明天中午的PM2.5的數值
他的輸入可能是種種跟預測PM2.5
有關的指數 包括今天的PM2.5的數值
今天的平均溫度
今天平均的臭氧濃度等等
這一個函式可以拿這些數值當作輸入
輸出明天中午的PM2.5的數值
那這一個找這個函式的任務
叫作Regression
那還有別的任務嗎
還有別的任務 除了Regression以外
另外一個大家耳熟能詳的任務
叫作Classification
那Classification這個任務
要機器做的是選擇題 我們人類
先準備好一些選項 那這些選項
又叫作類別 又叫作classes
我們現在要找的函式它的輸出
就是從我們設定好的選項裡面
選擇一個當作輸出 那這個問題
這個任務就叫作Classification
舉例來說
現在每個人都有gmail account
那gmail account裡面有一個函式
這個函式可以幫我們偵測一封郵件
是不是垃圾郵件
這個函式的輸入是一封電子郵件
那他的輸出是什麼呢
你要先準備好你要機器選的選項
在偵測垃圾郵件這個問題裡面
可能的選項就是兩個
是垃圾郵件 或不是垃圾郵件
Yes或者是No
那機器要從Yes跟No裡面
選一個選項出來
這個問題叫作Classification
那Classification不一定只有兩個選項
也可以有多個選項 舉例來說
alpha go本身也是一個Classification
的問題 那只是這個Classification
他的選項是比較多的
那如果要叫機器下圍棋
你想做一個 alpha go的話
我們要給機器多少的選項呢
你就想想看 棋盤上有多少個位置
那我們知道棋盤上有19乘19個位置
那叫機器下圍棋這個問題 其實
就是一個有19乘19個選項的選擇題
你要叫機器做的就是找一個函式
這個函式的輸入是棋盤上
黑子跟白子的位置
輸出就是從19乘19個選項裡面
選出一個正確的選項
從19乘19個可以落子的位置裡面
選出下一步應該要落子的位置
那這個問題也是一個分類的問題
那其實很多教科書 在講機器學習的
種種不同類型的任務的時候
往往就講到這邊 告訴你說
機器學習 兩大類任務
一個叫作Regression
一個叫作Classification
然後就結束了
但是假設你對機器學習的認知
只停留在機器學習就是兩大類任務
Regression跟Classification
那就好像你以為說
這個世界只有五大洲一樣
你知道這個世界不是只有五大洲
對不對 這個世界外面是有一個
黑暗大陸的 這鬼滅之刃連載之前
我們就已經出發前往黑暗大陸了
鬼滅之刃連載以後
我們居然都還沒有到
可見這個黑暗大陸距離那麼遠
那在機器學習這個領域裡面
所謂的黑暗大陸是什麼呢
在於Regression跟Classification以外
大家往往害怕碰觸的問題
叫作Structured Learning
也就是機器今天不只是要做選擇題
不只是輸出一個數字 還要產生
一個有結構的物件 舉例來說
機器畫一張圖 寫一篇文章
這種叫機器產生有結構的東西的問題
就叫作Structured Learning
那如果要講得比較擬人化
比較潮一點 Structured Learning
你可以用擬人化的講法說
我就是要叫機器學會創造這件事情
好那到目前為止
我們就是講了三個機器學習的任務
Regression Classification跟Structured Learning
接下來我們要講的是 那我們說
機器學習就是要找一個函式
那機器怎麼找一個函式呢
那這邊要用個例子跟大家說明說
機器怎麼找一個函式
這邊的例子是什麼呢 這邊的例子
在講這個例子之前 先跟大家說一下
說這一門課有一個youtube的頻道
然後這個我會把上課的錄影
放到這個youtube的頻道上面
那這個頻道 感謝過去修過這門課的
同學不嫌棄 其實也蠻多人訂閱
所以我算是一個三流的youtuber
是沒有什麼太多流量
但是這邊也是有7萬多訂閱
那為什麼突然提到
這個youtube的頻道呢
因為我們等一下要舉的例子
跟youtube是有關係的
那你知道身為一個youtuber
youtuber在意的東西是什麼呢
youtuber在意的
就是這個頻道的流量對不對
假設有一個youtuber
是靠著youtube維生的
他會在意頻道有沒有流量
這樣他才會知道他可以獲利多少
所以我在想說
我們有沒有可能找一個函式
這個函式 他的輸入
是youtube後台的資訊
輸出是這個頻道
隔天的總點閱率總共有多少
假設你自己有youtube頻道的話
你會知道說在youtube後台
你可以看到很多相關的資訊
比如說每一天按讚的人數有多少
每一天訂閱的人數有多少
每一天觀看的次數有多少
我們能不能夠根據
一個頻道過往所有的資訊去預測
它明天有可能的觀看的次數是多少呢
我們能不能夠找一個函式
這個函式的輸入是youtube上面
youtube後台是我的資訊
輸出就是某一天
隔天這個頻道會有的總觀看的次數
有可能會說 為什麼要做這個
如果我有營利的話
我可以知道我未來可以賺到多少錢
但我其實沒有開營利
所以我也不知道
我為什麼要做這個就是了
完全沒有任何管用
我單純就是想舉一個例子而已
好那接下來我們就要問
怎麼找出這個函式呢
怎麼找這個函式F
輸入是Youtube後台的資料
輸出是這個頻道
隔天的點閱的總人數呢
那機器學習找這個函式的過程
分成三個步驟
那我們就用Youtube頻道
點閱人數預測這件事情
來跟大家說明這三個步驟
是怎麼運作的
第一個步驟是我們要寫出一個
帶有未知參數的函式
簡單來說就是 我們先猜測一下
我們打算找的這個函式F
它的數學式到底長什麼樣子
舉例來說
我們這邊先做一個最初步的猜測
這個F長什麼樣子呢
這個輸入跟y之間會什麼樣的關係呢
我們寫成這個樣子
y等於b加w乘以xₗ
這邊的每一個數值是什麼呢
這個y啊 就假設是今天吧
不過今天還沒有過完
所以我還不知道
今天總共的點閱次數是多少
所以這件事情是我們未知的
y是我們準備要預測的東西
我們準備要預測的是今天
2月26號這個頻道總共觀看的人數
那xₗ是什麼呢
xₗ是這個頻道
前一天總共觀看的人數
y跟xₗ都是數值
都是我們
這個y是我們要準備預測的東西
xₗ是我們已經知道的資訊
w那b跟w是什麼呢
b跟w是未知的參數
它是準備要透過資料去找出來的
我們還不知道w跟b應該是多少
我們只是隱約的猜測 但這個猜測
為什麼會有這個猜測呢
這個猜測往往就來自於
你對這個問題本質上的了解
也就是Domain knowledge
所以才會聽到有人說
這個做機器學習啊
就需要一些Domain knowledge
這個Domain knowledge
通常是用在哪裡呢
這個Domain knowledge就是用在
這個帶有未知數的函數的時候
所以我們怎麼知道說
這個能夠預測未來點閱次數的函式F
它就一定是前一天的點閱次數
乘上w 再加上b呢
我們先不知道 這是一個猜測
也許我們覺得說
這個今天的點閱次數
總是會跟昨天的點閱次數有點關聯
所以我們把昨天的點閱次數
乘上一個數值
但是總是不會一模一樣
所以再加上一個b做修正
當作是對於2月26號
點閱次數的預測
這是一個猜測
它不一定是對的
我們等一下回頭會再來修正這個猜測
好那現在總之
我們就隨便猜說
y等於b加w乘以xₗ
而b跟w是未知的
這個帶有未知的參數
這個Parameter中文通常翻成參數
這個帶有Unknown的Parameter的
這個Function 我們就叫做Model
所以我們常常聽到有人說
模型 Model這個東西
Model這個東西在機器學習裡面
就是一個帶有
未知的Parameter的Function
好那這個xₗ啊
是這個Function裡面我們已知的
已經知道的東西
它是來自於Youtube後台的資訊
我們已經知道2月25號
點閱的總人數是多少
這個東西叫做Feature
而w跟b是我們不知道的
它是Unknown的Parameter
那這邊我們也給w跟b
給他一個名字
這個跟Feature做相乘的未知的參數
這個w 我們叫它weight
這個沒有跟Feature相乘的
是直接加下去的
這個我們叫它Bias
那這個只是一些名詞的定義而已
等一下我們講課的時候
我們在稱呼
模型裡面的每一個東西的時候
會更為方便
好那這個是第一個步驟
好那第二個步驟是什麼呢
第二個步驟呢
是我們要定義一個東西叫做Loss
什麼是Loss呢
Loss它也是一個Function
那這個Function它的輸入
是我們Model裡面的參數
我剛才已經
把我們的Model寫出來了對不對
我們的Model叫做
y等於b加w乘以xₗ
而b跟w是未知的
是我們準備要找出來的
那所謂的L啊
所謂的這個Loss啊
它是一個Function
這個Function的輸入是什麼
這個Function的輸入
就是b跟w
所以L它是一個Function
它的輸入是Parameter
是model裡面的Parameter
那這個Loss 這個Function
輸出的值代表什麼呢
這個Function輸出的值代表說
現在如果我們把這一組未知的參數
設定某一個數值的時候
這筆數值好還是不好
那這樣講可能你覺得有點抽象
所以我們就舉一個具體的例子
假設現在我們給未知的參數的設定是
b這bias等於0.5k
這個w呢直接等於1
那這個Loss怎麼計算呢
如果我們b設0.5k
這個w設1
那我們拿來預測
未來的點閱次數的函式 就變成
y等於0.5k加1倍的xₗ
那這樣子的一個函式
這個0.5k跟1
他們所代表的這個函式
它有多少呢
這個東西就是Loss
那在我們的問題裡面
我們要怎麼計算這個Loss呢
這個我們就要從訓練資料來進行計算
在這個問題裡面
我們的訓練資料是什麼呢
我們的訓練資料是
這一個頻道過去的點閱次數
舉例來說
從2017年到2020年的點閱次數
每天的這個頻道的點閱次數都知道
這邊是假的數字啦
隨便亂編的
好 所以那我們知道
2017年1月1號
到2020年12月31號的
點閱數字是多少
接下來我們就可以計算Loss
怎麼計算呢
我們把2017年1月1號的點閱次數
代入這一個函式裡面
我們已經說我們想要知道
b設定為0.5k
w設定為1的時候
這個函式有多棒
當b設定為0.5k
w設定為1的時候
我們拿來預測的這個函數
是y等於0.5k加一倍的xₗ
那我們就把這個xₗ代4.8k
看它的預測出來的結果是多少
所以 根據這一個函式
根據b設0.5k
w設1的這個函式
如果1月1號
是4.8k的點閱次數的話
那隔天應該是4.8k乘1加0.5k
就是5.3k的點閱次數
那隔天實際上的點閱次數
1月2號的點閱次數我們知道嗎
從後台的資訊裡面 我們是知道的
所以我們可以比對一下
現在這個函式預估的結果
跟真正的結果
它的差距有多大
這個函式預估的結果是5.3k
真正的結果是多少呢
真正的結果是4.9k
它是高估了
高估了這個頻道可能的點閱人數
那就可以計算一下這個差距
計算一下估測的值
跟真實的值的差距
這邊估測的值用y來表示
真實的值用ŷ來表示
你可以計算y跟ŷ之間的差距
得到一個eₗ
代表估測的值跟真實的值之間的差距
那計算差距其實不只一種方式
我這邊把y跟ŷ相減
直接取絕對值
算出來的值是0.4k
好那我們今天有的資料
不是只有1月1號
跟1月2號的資料
我們有2017年1月1號
到2020年12月31號
總共三年的資料
那這個真實的值叫做Label
所以常常聽到有人說
機器學習都需要Label
Label指的就是正確的數值
這個東西叫做Label
那我們不是只能用1月1號
來預測1月2號的值
我們可以用1月2號的值
來預測1月3號的值
如果我們現在的函式是
y等於0.5k加一倍的xₗ
那1月2號
根據1月2號的點閱次數
預測的1月3號的點閱次數的
值是多少呢 是5.4k
以xₗ代4.9k進去
乘1在加0.5k 等於5.4k
接下來計算這個5.4k
跟真正的答案
跟Label之間的差距
Label是7.5k
看來是一個低估
低估了這個頻道
在1月3號的時候的點閱次數
才可以算出e₂
這個e₂是
y減y跟ŷ之間的差距
算出來是2.1k
那同一個方法
你就可以算過這三年來
每一天的預測的誤差
假設我們今天的Function
是y等於0.5k加一倍的xₗ
這三年來每一天的誤差
通通都可以算出來
每一天的誤差都可以給我們一個小e
好那接下來我們就把每一天的誤差
通通加起來
加起來然後取得平均
這個大N代表我們的
訓驗資料的個數
那我們訓練資料的個數
就是三年來的訓練資料
就365乘以3
每年365天 所以365乘以3
那我們算出一個L
我們算出一個大L
這大L是每一筆訓練資料的誤差
這個e相加以後的結果
這個大L就是我們的Loss
這個大L越大
代表我們現在這一組參數越不好
這個大L越小
代表現在這一組參數越好
那這個e啊就是計算這個
估測的值跟實際的值之間的差距
其實有不同的計算方法
在我們剛才的例子裡面
我們是算y跟ŷ之間
絕對值的差距
這一種計算差距的方法
得到的這個大L
得到的Loss叫 mean absolute error
縮寫是MAE
那在這MAE裡面
我們是算Y跟Y hat 相減以後的平方
如果你今天的e是用
相減y平方算出來的
這個叫mean square error
又叫MSE
那MSE跟MAE
他們其實有非常微妙的差別
通常你要選擇
用哪一種方法來衡量距離
那是看你的需求
和你對這個任務的理解
那在這邊呢我們就不往下講
反正我們就是選擇MAE
作為我們計算這個誤差的方式
把所有的誤差加起來
就得到Loss
那你要選擇MSE也是可以的
在作業裡面我們會用MSE
那有一些任務
如果y和ŷ它都是機率
都是機率分佈的話
在這個時候
你可能會選擇Cross-entropy
這個我們都之後再說
反正我們這邊就是選擇了MAE
那這個是機器學習的第二步
那我剛才舉的那些數字
不是真正的例子
但是在這一門課裡面
我在講課的時候
就是要舉真正的例子給你看
所以以下的數字
是真實的例子
是這個頻道真實的後台的數據
所計算出來的結果
那我們可以調整不同的w
我們可以調整不同的b
求取各種w 求取各種b
組合起來以後
我們可以為不同的w跟b的組合
都去計算它的Loss
然後就可以畫出以下這一個等高線圖
在這個等高線圖上面
越偏紅色系
代表計算出來的Loss越大
就代表這一組w跟b越差
如果越偏藍色系
就代表Loss越小
就代表這一組w跟b越好
拿這一組w跟b
放到我們的Function裡面
放到我們的Model裡面
那我們的預測會越精準
所以你就知道說
假設w在負0.25
這個b在負500
就代表說呢這個W在負0.25
b在負500 就代表說
這個頻道每天看的人越來越少
而且Loss這麼大
跟真實的狀況不太合
如果w代0.75 b代500
那這個正確率
這個估測會比較精準
那估測最精準的地方看起來
應該是在這裡啦
如果你今天w代一個很接近1的值
b帶一個小小的值
比如說100多
那這個時候估測是最精準的
那這跟大家的預期可能是比較接近的
就是你拿前一天的點閱的總次數
去預測隔天的點閱的總次數
那可能前一天跟隔天的點閱的總次數
其實是差不多的
所以w設1
然後b設一個小一點的數值
也許你的估測就會蠻精準的
那像這樣子的一個等高線圖
就是你試著試了不同的參數
然後計算它的Loss
畫出來的這個等高線圖
叫做Error Surface
那這個是機器學習的第二步
接下來我們進入機器學習的第三步
那第三步要做的事情
其實是解一個最佳化的問題
如果你不知道最佳化的問題
是什麼的話也沒有關係
我們今天要做的事情就是
找一個w跟b
把未知的參數
找一個數值出來
看代那一個數值進去
可以讓我們的大L
讓我們的Loss的值最小
那個就是我們要找的w跟b
那這個可以讓loss最小的w跟b
我們就叫做w*跟b*
代表說他們是最好的一組w跟b
可以讓loss的值最小
那這個東西要怎麼做呢
在這一門課裡面 我們唯一會用到的
Optimization的方法
叫做Gradient Descent
那這個Gradient Descent
這個方法要怎麼做呢
它是這樣做的
為了要簡化起見
我們先假設我們未知的參數只有一個
就是w
我們先假設沒有b那個未知的參數
只有w這個未知的參數
那當我們w代不同的數值的時候
我們就會得到不同的Loss
這一條曲線就是error surface
只是剛才在前一個例子裡面
我們看到的error surface
是二維的是2D的
那這邊只有一個參數
所以我們看到的這個error surface
是1D的
那怎麼樣找一個w
去讓這個loss的值最小呢
那首先你要隨機選取一個初始的點
那這個初始的點
我們叫做w₀
那這個初始的點往往真的就是隨機的
就是隨便選一個
真的都是隨機的
那在往後的課程裡面
我們其實會看到也許有一些方法
可以給我們一個比較好的w零的值
那我們先不講這件事
我們先當作都是隨機的
隨便擲個骰子隨機決定
w₀的值應該是多少
那假設我們隨機決定的結果
是在這個地方
那接下來你就要計算說
在w等於w0的時候
w這個參數對loss的微分是多少
那我假設你知道微分是什麼
這對你來說不是個問題
計算w對loss的微分是多少
如果你不知道微分是什麼的話
那沒有關係反正我們做的事情就是
計算在這一個點
在w₀這個位置的
這個error surface的切線斜率
也就是這一條藍色的虛線
它的斜率
那如果這一條虛線的斜率是負的
那代表什麼意思呢
代表說左邊比較高 右邊比較低
在這個位置附近
左邊比較高 右邊比較低
那如果左邊比較高右邊比較低的話
那我們要做什麼樣的事情呢
如果左邊比較高右邊比較低的話
我們就把w的值變大
那我們就可以讓loss變小
如果算出來的斜率是正的
就代表說左邊比較低右邊比較高
是這個樣子的
左邊比較低右邊比較高
如果左邊比較低 右邊比較高的話
那就代表我們把w變小了
w往左邊移 我們可以讓Loss的值變小
那這個時候你就應該把w的值變小
那假設你連斜率是什麼都不知道的話
也沒有關係
你就想像說有一個人站在這個地方
然後他左右環視一下
那這一個算微分這件事啊
就是左右環視
它會知道左邊比較高還是右邊比較高
看哪邊比較低
它就往比較低的地方跨出一步
那這一步要跨多大呢
這一步的步伐的大小取決於兩件事情
第一件事情是這個地方的斜率有多大
這個地方的斜率大
這個步伐就跨大一點
斜率小步伐就跨小一點
另外 除了斜率以外
就是除了微分這一項
微分這一項我們剛才說它就代表斜率
除了微分這一項以外
還有另外一個東西會影響步伐大小
這個東西我們這邊用η來表示
這個η叫做learning rate
叫做學習速率
這個learning rate 它是怎麼來的呢
它是你自己設定的
你自己決定這個η的大小
如果η設大一點
那你每次參數update就會量大
你的學習可能就比較快
如果η設小一點
那你參數的update就很慢
每次只會改變一點點參數的數值
那這種你在做機器學習
需要自己設定的東西
叫做hyperparameters
這個我們剛剛講說機器學習的第一步
就是訂一個有未知參數的function
而這些參數
這些未知的參數
是機器自己找出來的
請說
好 那你請說
好 這其實是一個好的問題
我複述一下這個問題
有同學問說
為什麼loss可以是負的呢
Loss這個函數是自己定義的
所以在剛才我們的定義裡面
我們說loss就是估測的值
跟正確的值
它的絕對值
那如果根據剛才loss的定義
那它不可能是負的
但是loss的這一個function
是你自己決定的
你可以說
我今天要決定一個loss function
就是絕對值再減100
那你可能就有負的
所以我這邊這一個curve
我這邊可能剛才忘了跟大家說明說
這個curve並不是一個真實的loss
它是我隨便亂舉的一個例子
因為在今天
我想要舉一個比較general 的case
它並不是一個真實任務的
Error surface
所以這個loss的這個curve
這個error surface
它可以是任何形狀
這邊沒有預設立場說
它一定要是什麼形狀
但是確實在真實
在剛才這一個如果loss的定義
就跟我剛才定的一樣是絕對值
那它就不可能是負值
但這個loss
這個function是你自己決定的
所以它有可能是負的
那既然有同學在這邊問問題
我們就在這邊停一下
看大家有沒有問題想問的
然後助教以後
會幫我按Youtube的直播
有人在直播上問問題嗎
如果有的話你就幫我唸一下
你先看好以後在唸給我聽
我們就先繼續講
我們講到一個段落
再來要繼續回答大家的問題
再問一下
現場同學有沒有同學想要問問題的
好
沒有的話就請容我繼續講
好那剛才講到那裡呢
剛才講到hyperparameter這個東西
hyperparameter是你自己設的
所以在機器學習的這整個過程中
你需要自己設定的這個東西
就叫做hyperparameter
那我們說我們要把w⁰往右移一步
那這個新的位置就叫做w¹
這一步的步伐是η乘上微分的結果
那如果你要用數學式來表示它的話
就是把w⁰減掉η乘上微分的結果
得到w¹
那接下來你就是反覆進行剛才的操作
你就計算一下w¹微分的結果
然後再決定現在要把w¹移動多少
然後再移動到w²
然後你再繼續反覆做同樣的操作
不斷的把w移動位置
最後你會停下來
什麼時候會停下來呢
往往有兩種狀況
第一種狀況是你失去耐心了
你一開始會設定說
我今天在調整我的參數的時候
我在計算我的微分的時候
我最多計算幾次
你可能會設說
我的上限就是設定100萬次
就我參數更新100萬次以後
我就不再更新了
那至於要更新幾次
這個也是一個hyperparameter
這個是你自己決定的
做一個deadline是明天
那你可能更新的次數就設少一點
對它下周更新的次數就設多一點
那還有另外一種理想上的
停下來的可能是
今天當我們不斷調整參數
調整到一個地方
它的微分的值就是這一項
算出來正好是0的時候
如果這一項正好算出來是0
0乘上learning rate η還是0
所以你的參數就不會再移動位置
那假設我們是這個理想的狀況
我們把w⁰更新到w¹
再更新到w²
最後更新到wᵗ有點卡
wᵗ卡住了
也就是算出來這個微分的值是0了
那參數的位置就不會再更新
那講到這邊
你可能會馬上發現說
Gradient Descent 這個方法
有一個巨大的問題
這個巨大的問題在這一個例子裡面
非常容易被看出來就是
我們沒有找到真正最好的解
我們沒有找到那個
可以讓Loss最小的那個w
在這個例子裡面
把w設定在這個地方
你可以讓loss最小
但是如果 Gradient Descent
是從這個地方
當作隨機初始的位置的話
也很有可能走到這裡
你的訓練就停住了
你就沒有辦法再移動w的位置
那這一個位置
這個真的可以讓loss最小的地方
叫做global 的minima
而這個地方叫做local 的minima
它的左右兩邊
都比這個地方的loss還要高一點
但是它不是
整個error surface上面的最低點
這個東西叫做local minima
所以常常可能會聽到有人講到
Gradient Descent
就會說Gradient Descent
不是個好方法
這個方法會有local minima的問題
沒有辦法真的找到global minima
但教科書常常這樣講
農場文常常這樣講
但這個其實只是幻覺而已
事實上
假設你有做過深度學習相關的事情
假設你有自己訓練network
自己做過Gradient Descent 經驗的話
其實local minima是一個假問題
我們在做Gradient Descent 的時候
真正面對的難題不是local minima
到底是什麼
這個 我們之後會再講到
在這邊你就先接受
先相信多數人的講法說
Gradient Descent
有local minima的問題
在這個圖上在這個例子裡面
顯然有local minima的問題
但之後會再告訴你說
Gradient Descent真正的痛點
到底是什麼
那剛才舉的
是只有一個參數的例子而已
那實際上我們剛才的模型有兩個參數
有w跟b
那有兩個參數的情況下
怎麼用Gradient Descent呢
其實跟剛才一個參數沒有什麼不同
若一個參數你沒有問題的話
你可以很快的推廣到兩個參數
我們現在有兩個參數
那我們給它兩個參數
都給它隨機的初始的值
就是w⁰跟b⁰
然後接下來呢
你要計算w跟loss的微分
你要計算b對loss的微分
計算是在w等於w⁰的位置
b等於b₀的位置
在w等於w₀的位置
b等於b⁰的位置
你要計算w對L的微分
計算b對L的微分
計算完以後
就根據我們剛才
一個參數的時候的做法
去更新w跟b
把w⁰減掉learning rate
乘上微分的結果得到w¹
把b⁰減掉learning rate
乘上微分的結果得到b¹
那有同學可能會問說
這個微分這個要怎麼算啊
如果你不會算微分的話
不用緊張
怎麼不用緊張呢
在deep learning 的framework裡面
或在我們作業一
會用的pytorch裡面
算微分都是程式自動幫你算的
你就co一行 就寫一行程式
自動就把微分的值就算出來了
你就算完全不知道自己在幹嘛
也還是可以把微分的值算出來
所以這邊
如果你根本就不知道微分是什麼
不用擔心
這一步驟就是一行程式
這個等一下之後在作業一的時候
大家可以自己體驗看看
那就是反覆同樣的步驟
就不斷的更新w跟b
然後期待最後
你可以找到一個最好的w
w*跟最好的b b*
那這邊呢就是舉一下例子
跟大家看一下說如果在這一個問題上
它操作起來是什麼樣子
假設
你隨便選一個初始的值在這個地方
那你就先計算一下w對L的微分
跟計算一下b對L的微分
然後接下來你就要更新w跟b
更新的方向就是w對L的微分
乘以η再乘以一個負號
b對L的微分
乘以η再乘以一個負號
算出這個微分的值
你就可以決定更新的方向
你就可以決定w要怎麼更新
w要怎麼更新
那把w跟b更新的方向結合起來
就是一個向量
就是這個紅色的箭頭
我們就從這個位置移到這個位置
然後再計算一次微分
然後你再決定要走什麼樣的方向
把這個微分的值乘上learning rate
再乘上負號
你就知道紅色的箭頭要指向那裡
你就知道怎麼移動w跟b的位置
一直移動一直移動一直移動
期待最後可以找出一組不錯的w跟b
那實際上真的用Gradient Descent
進行一番計算以後
這個是真正的數據
我們算出來的最好的w是0.97
最好的b是0.1k
跟我們的猜測蠻接近的
因為x₁的值可能跟y很接近
所以這個w就設一個接近1的值
b就設一個比較偏小的值
那loss多大呢
loss算一下是0.48k
也就是在2017到2020年的資料上
如果使用這一個函式
b代0.1k
w代0.97
那平均的誤差是0.48k
也就是它的預測的觀看人數誤差
大概是500人次左右
講到目前為止
我們就講了機器學習的三個步驟
第一個步驟
寫出一個函式
這個函式裡面是有未知數的
第二個步驟
定義一個叫做loss的function
第三個步驟
解一個Optimization的problem
找到一組w跟b讓loss最小
那w跟b的值剛才已經找出來的
那這組w跟b可以讓loss小到0.48k
但是這樣
是一個讓人滿意或值得稱道的結果嗎
也許不是
為什麼
因為這三個步驟合起來啊
叫做訓練
我們現在是在
我們已經知道答案的資料上
去計算loss
2017到2020年的資料
我們已經知道啦
我們其實已經知道2017到2020年
每天的觀看次數 所以
其實我們現在其實只是在自high而已
就是假裝我們不知道隔天的觀看次數
然後拿這一個函式來進行預測
發現誤差是0.48k
但是我們真正要在意的
是已經知道的觀看次數嗎
不是
我們真正要在意的是
我們不知道的
未來的觀看的次數是多少
所以我們接下來要做的事情是什麼呢
就是拿這個函式
來真的預測一下未來的觀看次數
那這邊
我們只有2017年到2020年的值
我們在2020年的最後一天
跨年夜的時候
找出了這個函式
接下來從2021年開始每一天
我們都拿這個函式
去預測隔天的觀看人次
我們就拿2020年的12月31號的
觀看人次
去預測2021年元旦的觀看人次
用2021年元旦的觀看人次
預測一下2021年元旦隔天
1月2號的觀看人次
用1月2號的觀看人次去預測
1月3號的觀看人次
每天都做這件事
一直做到2月14號
就做到情人節
然後得到平均的值
平均的誤差值是多少呢
這個是真實的數據的結果
在2021年沒有看過的資料上
這個誤差值是
我們這邊用 L' 來表示
它是0.58
所以在有看過的資料上
在訓練資料上
誤差值是比較小的
在沒有看過的資料上
在2021年的資料上
看起來誤差值是比較大的
那我們每一天的平均誤差
有580人左右
600人左右
只是能不能夠做得更好呢
在做得更好之前
我們先來分析一下結果
這個圖怎麼看呢
這個圖的橫軸是代表的是時間
所以0這個點 最左邊的點
代表的是2021年1月1號
最右邊點
代表的是2021年2月14號
然後這個縱軸啊
就是觀看的人次
這邊是用千人當作單位
紅色的線是什麼呢
紅色的線是真實的觀看人次
藍色的線是機器用這一個函式
預測出來的觀看人次
你有發現很明顯的
這藍色的線沒什麼神奇的地方
它幾乎就是
紅色的線往右平移一天而已
它其實也沒做什麼特別厲害的預測
就把紅色的線往右平移一天
因為這很合理
因為我們覺得
x₁也就是前一天的觀看人次
跟隔天觀看人次的
要怎麼拿前一天的觀看人次
去預測隔天的觀看人次呢
前一天觀看人次乘以0.97
加上0.1k 加上100
就是隔天的觀看人次
所以你會發現說
機器幾乎就是拿前一天的觀看人次來預測
隔天的觀看人次
但是如果你仔細觀察這個圖
你就會發現
這個真實的資料有一個很神奇的現象
它是有週期性的
它有神奇的週期性
你知道這個週期是什麼嗎
你知道它每隔七天就會有兩天特別低
兩天觀看的人特別少
那兩天是什麼日子呢
那我發現那兩天都固定
是禮拜五跟禮拜六
禮拜五跟禮拜六我可以了解
就禮拜五週末 大家出去玩
誰還要學機器學習
禮拜六誰還要學機器學習
那不知道為什麼
禮拜天大家去學機器學習
這個我還沒有參透為什麼是這個樣子
也許跟youtube背後
神奇的演算法有關係
比如說youtube都會推頻道的影片
也許youtube在推頻道的影片的時候
它都選擇禮拜五禮拜六不推
只推禮拜天到禮拜四
可是為什麼推禮拜天到禮拜四呢
這個我也不了解
但是反正看出來的結果
我們看真實的數據
就是這個樣子
每隔七天一個循環
每個禮拜五禮拜六
看的人就是特別少
所以既然我們已經知道每隔七天
就是一個循環
那這一個式子 這一個model
顯然很爛
因為它只能夠看前一天
如果說每隔七天它一個循環
我們應該要看七天對不對
如果我們一個模型
它是參考前七天的資料
把七天前的資料
直接複製到拿來當作預測的結果
也許預測的會更準也說不定
所以我們就要修改一下我們的模型
通常一個模型的修改
往往來自於你對這個問題的理解
也就是Domain Knowledge
所以一開始
我們對問題完全不理解的時候
我們就胡亂寫一個
y等於b 加wx₁
並沒有做得特別好
接下來我們觀察了真實的數據以後
得到一個結論是
每隔七天有一個循環
所以我們應該要把
前七天的觀看人次都列入考慮
所以我們寫了一個新的模型
這個模型長什麼樣子呢
這個模型就是y等於b加xⱼ
xⱼ代表什麼
這個下標j代表是幾天前
然後這個j等於1到7
也就是從一天前兩天前
一直考慮到七天前
那七天前的資料
通通乘上不同的weight
乘上不同的wⱼ
加起來
再加上bias
得到預測的結果
如果這個是我們的model
那我們得到的結果是怎麼樣呢
我們在訓練資料上的loss是0.38k
那因為這邊只考慮一天
這邊考慮七天
所以在訓練資料上
你會得到比較低的loss
這邊考慮了比較多的資訊
在訓練資料上你應該要得到更好的
更低的loss
這邊算出來是0.38k
但它在沒有看過的資料上面
做不做得好呢
在沒有看到的資料上
有比較好
是0.49k
所以剛才只考慮一天是0.58k的誤差
考慮七天是0.49k的誤差
那這邊每一個w跟b
我們都會用Gradient Descent
算出它的最佳值
它的最佳值長什麼樣子呢
這邊show出來
給你看 它的最佳值長這樣
當然機器的邏輯我是有點沒有辦法了解
我本來以為它會選七天前的數據
七天前的觀看人數
直接複製過來
我看來它沒有這樣選就是了
它的邏輯是前一天
跟你要預測的隔天的數值的關係很大
所以w₁*是0.79
那不知道為什麼 它還考慮前三天
前三天是0.12
然後前六天是0.3
前七天是0.18
不過它知道說
如果是前兩天前四天前五天
它的值會跟未來我們要預測的
隔天的值是成反比的
所以w₂ w₄跟w₅它們最佳的值
讓Loss可以在訓練資料上
是0.38k的值 是負的
但是w₁ w₃ w₆跟w₇是正的
我們考慮前七天的值
那你可能會問說
能不能夠考慮更多天呢
可以
那這個輕易的改考慮更多天
本來是考慮前七天
然後考慮28天會怎麼樣呢
28天就一個月
考慮前一個月每一天的觀看人次
去預測隔天的觀看人次
預測出來結果怎樣呢
訓練資料上是0.33k
那在2021年的資料上
在沒有看過的資料上是0.46k
看起來又更好一點 好 28天
好那接下來考慮56天會怎麼樣呢
在訓練資料上是稍微再好一點
是0.32k
在沒看過的資料上還是0.46k
看起來
考慮更多天沒有辦法再更進步了
看來考慮天數這件事
也許已經到了一個極限
好那這邊這些模型
它們都是把輸入的這個x
這個x 還記得它叫什麼嗎
它叫做feature
把feature乘上一個weight
再加上一個bias就得到預測的結果
這樣的模型有一個共同的名字
叫做Linear model
那我們接下來會看
怎麼把Linear model做得更好
