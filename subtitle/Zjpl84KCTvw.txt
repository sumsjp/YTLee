okay Neil so we are going to start so
let me introduce our uh keynote speaker
to everyone today so hello everyone so
first of all welcome to the Cod super
special session and hongi and today our
first keynote speaker is Dr Neil zador
uh Neil is co-founder and chief modeling
officer of the qai research lab uh I
believe many of you have heard the name
of qtip lab because Le B MOSI uh the
first Real Time full duplex spoken
language model I believe most of you
have played with it and found it very
very very very amazing and he was
previously at Google and when he was at
Google he also working on spoken
language model he has built lots of very
well-known models including sound stream
audio L and music L I believe all of you
have heard models so we are very very
happy today uh he can have time to be
with us online and share with us more
about speech and audio language model so
let's welcome our keynot speaker thank
you thanks thanks for having me uh
thanks for the nice
introduction uh so yes today I'm going
to give a an overview of the work I've
done with uh my teams at Google and at
Cai on N language models
and uh to start before I would like to
say a few words about qai so we just got
a nice introduction that maybe I could
say a few words about it it's a it's a
lab we created in Paris a year ago and
it's a nonprofit research lab forus on
open science which means uh our entire
mission is about making research
publishing papers and open sourcing
codes and models we get generous
donations from various philanthropists
and we are always recruiting looking for
good talents uh working mostly on
multimodal large language models so now
back to the topic of Interest the idea
of audio language models was to to
create Foundation models for generative
audio so the first motivation for the
work I've done was to try to get a a
generic way of training Audio models
that could then be used for various
generative task for example text to
audio or audio to audio or even
unconditioned generation so at the time
there were a lot of uh models for audio
generation the main ones that was that
were very popular were generative
adversarial
networks however when we thought about
the properties of such an audio
Foundation model we thought the the
following characteristic where what we
wanted so we wanted to do generative PR
training on massive unlabeled data
because that's what was easily available
to have generic pre-training loss in
particular we really didn't want to have
something that was specific to speech we
wanted to find a generic approach that
could work for audio music speech that
could model very complex long-term
dependencies while learning high level
structure and having zero sh
abilities so when we look into the
literature what we saw it was that these
kind of models were kind of rising in
text right so it was around the time of
of gpt2 and gpt3 and we were starting to
see that uh language models will provide
such things for text
so that's where our inspiration came
from and so if we look at how we train a
text based language model basically you
are all familiar with it but for the
sake of the demonstration I will give a
brief recap uh you give it a sequence of
wordss or a sequence of tokens and you
train it to predict the the next
one so then is the question is how do we
go from a text language model to a noal
language
model and here the the first difficulty
we faced is that audio has a much higher
samp ra and text right so we can have a
few characters per second of of of
speech in when represented as text while
even as a low at a low quality such as
16 K Earth we going to have dozens of
thousands of
samples yet back in 2016 there was a
first early Approach at applying uh at
applying long wage models for audio
prediction in particular it was wavenet
so in the context of wave net the main
idea was has to have a language modeling
of the wave form after quantization so
language model is a classification model
it's trying to predict the next class so
since the waveform is a continuous value
what they propos is to discretize the
waveform into a finite set of values
with scalar quantization and then train
an auto regressive model for next
discretized waveform value
prediction Transformers did not exist at
the time so to model long context they
reli on dilated convolutions that allow
to a receptive field that grows
exponentially with the number of
layers this was a groundbreaking work in
text to speech in particular because For
the First Time a purely waveform based
model could have performed traditional
TTS
systems however it was very slow which
means because the main issue was when
you did inference you had to produce at
16 coer 16,000 values per second which
was just ridiculously long sequence and
the second issue was that it did not
learn high level stricture so it's not
in the paper but it's on the wet blog
post wet is a text to spech system
however the authors also showed a
demonstration of how it behaves when you
don't condition in on text so they also
run inference in pure audio mode without
conditioning and in that context you get
this kind of
audio just a
please let me know if you couldn't hear
well but basically the idea here is that
the quality is very high uh it clearly
sounds
like natural speech however it's
bubbling so the model was able to learn
uh you know the natural sound of a voice
but didn't learn language from
scratch so this led to an which is that
instead of predicting the waveform
directly what if we add kind of pseudo
characters that are going to be the most
similar possible to text characters but
that will represent audio what does it
mean that they will be close to text
first they should be low frame rate
because in particular we don't want to
predict dozens of thousands of values
per seconds they should model the
semantic information so the ey level
structure while allowing for a
constructing high quality audio so
ideally we would have an encoder that
takes an audio transforms it into these
Pudo characters where we can perform
next second prediction and then a
decoder will convert this uh generated
uh tokens back
to so I think it's very similar work in
uh in that regard was the work by the
team of Emanuel dupu at Facebook AI it's
called textless NLP and the main IDE is
the following they start from a Hubert
model which is a self-supervised uh
Speech model
and it's trained for with a mass
language model uh Criterion for
representation learning originally ubert
was proposed for pre-training for speech
recognition so for discriminative task
it was not meant to be used for
generative task what they showed is that
when you have trained ybert what you can
do is pick a layer in the middle of the
architecture and project your R into
that representation space now you have a
continuous semantic representation of
your remember that we need a discrete
representation because we want to train
a language model so what they do is that
they train a k means that is going to go
from a discrete eding to a to a from
continu someing sorry to a discrete
representation and then you can train a
language model for next token prediction
in this representation space so the big
advantages were the following it's still
it's fully unsupervised because ybert is
unsupervised K is unsupervised and then
NE language model isn't supervised it's
very low frame rate as well because
unlike wave net instead of modeling AIO
at 16 khz you are at 50 Herz now which
is much more reasonable and I think the
most impressive and
groundbreaking result is that for the
first time a model could learn the
structure of language so grammar lexicon
uh even some kind of semantics and you
could learn it without having seen any
text just by listening to
speech in some parts of the country
where white stones in The Lakes are so
large that a common liquid flattened all
the ground the ground is greater than
the top and the beaches are so white so
it's not a very interesting sentence
however it's sentence that is grammar
grammar correct and Carries some kind of
semantics it's it's kind of coherent
right so it was really one of the most
impressive work I think and and a very
early work that kind of influenced
everything that came
afterwards however it came with
fundamentally limitations the first one
is that the tokens were very speech
specific youber was train on clean
speech it was only meant to model clean
speech also the decoder was speaker
specific the thing is that the ubert
tokens were too compressed to model both
the linguistic content and the speaker
identification and the speaker identity
so what happened is that it was the task
of the decoder to reconstruct the
speaker voice so it was not trivial to
have a model that could work on any
voice any kind of recording conditions
and so on and so forth
so which means that the kind of the
semantic part was solded back then but
the acoustic part the idea of having
acoustic tokens that could U model all
the rich information that is not
linguistic was still
missing so it's around the time where we
developed Soundstream so Soundstream was
kind of the first neural codc that could
compete with uh with traditional codex
in particular it was a generative
adversar networks with vector ization in
the middle in particular we introduced
residual Vector quantization in this
paper and this allowed to have very high
quality even at lobit tra so to give you
an idea that's the kind of of quality
you'll get with
Opus and with Sound Stream
so when we developed Soundstream the
main idea was not generative models we
developed Soundstream for realtime
communication the idea was to have a
codex that could be used in a video
conference like this one and AOW for
high quality however when we read the
paper about uh text L nip it gave us an
ID so now we add tokens that instead of
being speaker specific and speech
specific could be used for any kind of
uh of audio so what if we just trained a
language mod to predict the next
Soundstream
tokens we were very confident that it
would work and it didn't work so in the
case of speech here is what you will get
S and for Annie outside that it was as
though a dark cloud was conion she can
as sorry so the quality is pretty high
uh but the speed is still bbing so we
are back to the problem of wet one thing
we observed though is that when we did
continuation the voice of the speaker
was perfectly preserved and this gave us
the idea to furthermore later on invent
uh zero shot voice cing from a few
seconds and we also observe the same
thing with piano so with piano here
you're going to to hear four seconds of
real audio and then the continuation
[Music]
so this was pretty disappointing and
also exciting because so obviously it's
piano right it's uh it's natural piano
however there is no musical structure in
it so we had sold the kind of the
quality aspect but we lost the high
level structure from the semantic token
of textas NLP so we came up with a very
simple idea just combine both so we
introduced the naming of semantic and
acoustic tokens what we call semantic
tokens where these kind of tokens that
model high level information but cannot
reconstruct high quality audio while the
acoustic tokens coming from Soundstream
will rather model all the acoustic
details and we propose the hierarchical
modeling of of these tokens so a first
language model would take would just
predict the semantic tokens based on the
past
then the key idea of odm is that the
semantic tokens will serve the role of
scaffold holding to predict acoustic
tokens so we we will use them as a
prefix to predict acoustic tokens and
then we will just throw away the
semantic tokens and reconstruct AIO from
acoustic tokens so your question may be
okay but what's the point of semantic
tokens here semantic tokens they serve
kind of a guidance uh uh a conditioning
to the generation of acoustic tokens to
guarantee that the ey level structure is
going to be
preserved this allowed us to propose the
first model for zero shot voice and
continuation the idea was the following
we would have a three second promp of
audio some images like some
Sensations we would trans transform it
into semantic and acoustic tokens then
the first agage of will predict the
future semantic tokens and now we would
have the uh the semantic
tokens both the the the prompt and the
generated one so there is a mistake in
the figure and we will predict uh sorry
there's no mistake in the figure and we
will predict the future acous
tokens and that would allow us to now
have the first model for speech and
music continuation that would theow for
keeping the high quality speaker
identity and so on and so
forth nay nay lording answered wolf we
know not how to call you Lord or lady we
have lived too long in the forest and
are now
[Music]
so this was a very exciting result for
us but obviously that was not our end
goal right because our main
motivation was to do conditioned
generation as I said in the beginning
the continuation task was kind of a
debugging task force you know it was a
way of developing the models
architecture the tokens and so on but
when we saw how good we could do speech
continuation we decided to work also on
text to speech and when we saw that we
could do p piano continuation we
realized that we could have developed
the first text to music system which I
will present later now let's start with
text to speech in that context an actual
Tas that is much more interesting than
just speech continuation is to have a
text and generate the corresponding
audio like this one
oh what a grand thing to happen on a raw
day
so the idea of how we address text to
speech is the following if we look at
what we showed in audm without text so a
pure audio model that has never seen
text it Tak semantic tokens and predict
acoustic tokens so in a way what we
could do is just take text as input and
produce acoustic tokens right because
the semantic tokens carries the
linguistic information uh we could just
replace them with text
this was done by Val uh which is kind of
this idea of conditioning a model with
text converted to phon and then have
language model that is going to produce
acoustic tokens directly that can then
be ma back to audio so a limitation of
this kind of approach is that if we look
at the training data so audm was trained
on audio only data both for for music or
for speech it was Audio
Only but now you need to have of
transcript and speech right so you need
transcribed audio so the problem is that
if you want to handle multiple speakers
and new conditions you you need
annotated pairs in that domains this is
a limitation because label data is rare
in most languages and you don't want the
diversity of generated data to be
dependent on the diversity of labeled
data to give you a concrete example
let's say I train my teex to speech
system only on male voices if I wanted
now to work with female voices I will
need to collect transcribe from female
voices so it's a big
limitation so we saw an opportunity here
because unannotated speech is widely
available and so we proposed an approach
that could not only require a minimal
amount of transcribed data in particular
we were able to train a TTS system with
only 15 minutes of data of parallel data
sorry but also what we showed is that we
completely decoupled the
diversity from the label data which
means that we are able to only have
label data in one speaker and have the
model gener
anybody the main idea was the following
now we have again achal model but it's a
bit different thanm the first stage goes
from text to semantic tokens and the
second stage goes from semantic tokens
to acoustic tokens and here what we
observe is an asymmetry in the nature of
the data that you need in particular the
first stage requires parallel data
because since you go from text to
semantic tokens it means that you need
to have parallel text and audio however
since the second part is only from
semantic to acoustic tokens it's totally
unsupervised so you can train it on
audio only data with very high
volumes the main question here then is
what is exactly modeled by semantic
tokens with respect to acoustic tokens
to probe this kind of information uh a
simple experiment is the following you
take a ground toio you compute ground
through semantic tokens and you generate
various acoustic tokens right so you
always keep the same semantic and you
just U repeat temperature sampling for
the acoustic tokens so here is what you
see this Transit spring and lighting up
are beautiful a Glam so that's the
ground truth and here has various
generation of acoustic
tokens this transient spring and
lighting up are beautiful a glamour of
beguiling our
senses this transient spring and
lighting up are beautiful clim so it's
pretty clear what happens here is that
every time we change the acoustic tokens
the voice changes the recording
condition changes and so on however the
text Remains the Same so that's how we
we we observe that semantic tokens will
model the linguistic contents and the
acoustic tokens will model everything
else
so this gave us this key observation in
particular since the speaker ident
identity is modeled by the acoustic
tokens
there is no problem to train the first
stage on single speaker data because
even if we train even if our par data as
a single speaker it's the second stage
that is going to bring the model its
ability to speak in various uh voices so
as long as the second stage is
multispeaker the first stage can be
single
speaker and this gave us this kind of of
ability spare TTS with 24 hours of
parallel data spare TTS with 12 hours of
parallel data spare TTS with 2 hours of
parallel data spare TTS with 30 minutes
of parallel data spare TTS with 15
minutes of parallel
data
so what we observe is that and for all
the metrics I rather would have you
refer to the to the paper for the sake
of time and rather go across the many uh
many contributions but not only the uh
the linguistic content is preserved even
with very small parallel data what is
very interesting here is that the the
quality the audio quality and the
speaker diversity is completely
independent from the amount of parallel
data so we are able to completely
decouple both
aspects
sorry then uh similarly to what we did
with odm and then was done by Valley uh
we could do voice prompting from a few
seconds now the main difference instead
is that we could just have
uh a text to control what he said so the
input to the model will be a text and an
audio point to see the signs of
adolescence that made and we could
generate this text with the
voice and yesterday things went on just
as
usual so in terms of uh quality what was
interesting is that despite having 15
minutes of parallel data instead of
60,000 hours we are able to have speaker
similarity almost on par with Valley an
a quality that was judg to be much
better to be fair there were a lot of
confounding factors because we were not
using the the the same to codec and so
on however it was to show that you know
we were able to to reduce the amount of
par data to very small amounts which
could be very useful for low resource
languages then again what I said from in
the beginning and what something that is
very important to me is uh from the
beginning both when we develop
Soundstream and audm our priority was to
find generic principles that could work
for any audio we really didn't want to
have something that is specific to
speech uh in particular traditional text
to speech with neural networks will be
very speech specific you would have a
phon duration model and a pro model and
so on and so forth if you have this kind
of uh prior knowledge it helps for your
task but it doesn't transfer to other
task that's why we kept audm and Sound
Stream as generic as possible there is
pretty much nothing that is specific to
spe
to illustrate that we applied the same
ID from text to speech to text to music
so in that context instead of having
something you want to synthesize you
have uh a prompt and you want to
generate the
corresponding so again now the model is
exactly almost exactly the same but we
need new type of data so instead of
having transcribed speech we need pairs
of caption and music however it was very
clear from the beginning that we could
not train a text to music system with 15
minutes of parel data because the
mapping from text to semantic tokens in
speech was kind of an easy mapping
because both that were very close in
terms of the information they they
contain however a high level caption
like 19's uh house
music is kind of far to the
corresponding music in terms of
representation so this map is much more
complex to
learn much more ambiguous so it requires
more data this kind of data is very
difficult to collect and obtain it was
the case back then in 2023 it's still
the case today uh in particular there is
no existing
database uh that will be of a large
volume like we would have with a lib
light or where you would have with a
lion for images so it was a bit
challenging to find this and what we did
is that we relied on uh Ed multimodal
eddings that were developed at Google
called Mulan um my understand is that it
will be close to something like clap
that is used widely rather than Mulan
and the main idea of this kind of model
is the following the idea is
that you learn a representation space
where if I give an audio music audio and
the corresponding text that describe
this audio this will be closed in the
eding space and if they don't represent
the same information it should be far
intoing space so it's a multimodal iding
space where you can map a song both as
its audio version or it's this textual
description and both representations
will be ideally very close in the
representation space and the nice thing
about this is that once you have trained
this multimodal embeddings you can train
your text music system without text
because let's say I want to train my
model to predict audio from barin 19
house so so I don't have text and audio
music so what I do is that I just s my
unlabeled music I project it into a
Mulan eddings and it becomes a
conditioning to my language model so
during training I don't I don't need
text you can see there is no text in
this diagram I can only train on audio
but then at inference we want the user
to be able to provide a textual
description so now instead of computing
the conditioning to the language model
with audio we pass the text description
through Mulan and since Mulan represents
both audio and text with similar onings
it just works to train on audio only and
then uh run inference in kind of an
outof domain way where you give text as
input instead of AIO so I'm going to
play a few samples but every time I do
this presentation it's less and less
impressive because uh now you haveo and
udio that are extremely impressive
models please uh remember that back then
except for refusion I think there was
pretty much nothing uh so this was kind
of the first war that could do very
complex and Rich description and turn
them into music
[Music]
so to show how much the odm framework
was a generic framework that could be
used for a lot of things we also did
another fun that is called reverse Koke
where you go from a voice to an
instrumental so in that context you pass
it to
[Music]
aella so it takes time to to
when and you generate the corresponding
instrumental
so
all we
are
so going very uh because I'm I'm running
a bit out of time to just to go quickly
here's the main idea is that now you
need parallel acapella and instrument to
it's very simple you take a data set of
music you apply a source separator like
DX and then you separate the acapella
from the instrumental and you train the
model to predict the instrumental from
the acapella and now you have your
reverse K music model one thing that was
very important to us was to bring human
in the loop uh in this context in
particular automatic of evaluation of
Music generation is limited uh for
people who have worked on this topic
it's one of the most frating aspect is
that you cannot really trust the
automatic Matrix so you spend a lot of
time listening to audio um in particular
subjectivity is something that is very
hard to model yet is very important in
evaluation so we will need ideally to
integrate humans in the loop like was
done uh like is done for text language
models so what we did is that we
released music LM to the world for a web
uh app and we made a specific design
Choice when we made the UI is that we
decided to allow the user to give a
trophy for so every time they would pass
a prompt they will it will generate two
audio and the user could pick their
favorite one they could not give two
trophies they could give one trophy if
there there is one audio that they
preferred and that would allow us to get
a preference because what we learned
from our colleagues working on
reinforcement learning is that it's much
easier to exploit preference rather than
absolute gradings like giving a a rating
from one to five let's say we collected
about 300,000 preferences and we're able
to do lhf for music generation for the
first time and the resulting models is
what is currently used uh in all the
apis of Google for music like music
ethics DJ and so on and so forth so all
of this uh is uh is using this kind of
feedback group so I sto your your figure
because so I presented mostly the work I
did with my team but obviously it's it
contributed to a much wider effort
across industry I am proud to think that
what we did has inspired a lot of work
uh for example the work of my friends at
Facebook working on onod music gen um
also work across industry and because
Beyond research notbook LM relies on
audm Soo in its first version also
relied on audm so it had kind of it
became one of the main approach to audio
generation along with uh diffusion so
basically now you have this two families
on model either audio language models or
diffusion models and I'm very happy to
have been able to contribute uh to the
development of UL W models so a year ago
now I want to say a few words about I've
been doing atai I left Google and uh
with the goal of doing a new project
that will be a groundbreaking project in
AIO and we decided to work on a on a
realtime conversation for a specific
reason even
though we had you know with VM we were
able to do basically any kind of audio
generation task test to music text to
audio speech announcement speech
separation text to speeech so we did
pretty much everything we could there
was one task where we had made
absolutely no progress and nobody seemed
to have made progress which was to have
a real time full duplex
conversation
so what would be new about that is that
if you look unlike uh standard voice AIS
if you look at the standard pipeline to
how you do voicei is the following so
you have a voice activity detection that
is going to uh decide when to record
your query let's say I want to ask what
is the capital city of France then when
the voice activity detection detects
that I'm done with my question is going
to an automatic speech recognition model
is going to transcribe my text into uh
uh my speech into text then a large
language model is going to produce an
answer and the text to speech system is
going uh to produce a spoken answer like
uh it's Paris there are two main
limitation with that the first one is
that it's pretty slow right so you get
typically between three to 5 Seconds of
latency and the second issue is that you
lose all the nonlinguistic information
like the emotion and so on and so forth
so because you go through this
bottleneck of text so let's say now you
would like to make an assistant that
trains you to do job interviews it's a
bit challenging because uh when you
train for a j job interview you can do
it right now let's say with ch GPT but a
real job interview can be very stressful
uh the people can try to challenge you
so you they are not going to patiently
wait 5 Seconds every time to answer you
you know the conversation can go pretty
fast and it can also evaluate your
emotional behavior and so on and so
forth so if you wanted to create an
assistant to train for job interviews
you could not use existing voice series
another thing is that now I guess you
are all familiar with the advaned voice
mode of chat GPT so it's a model that
reduce the latency to pretty much you
know a few hundred of milliseconds
virtually zero let's say however it's
still limited by the way it represents
the
conversation any voice conversation
model is have duplex so it's like a
talky Wy in the sense that it considers
that a conversation is a clean
segmentation between a user term then a
AI term then a user term and so on and
so farth however a real human
conversation looks like what you see
below you have interruptions you have
back channeling so some some sometimes
are going to say but the person should
keep talking you know it can go very
fast can be very chaotic it's more like
being on a phone with a family member so
statistically if you are on the phone
with a family member up to 20% of your
speech is going to be overla so most of
20% of the spoken time will have two
people speaking at the same time so this
completely breaks the Assumption of the
F duplex and one use case for this for
example is let's say you would like to
play video games with a lot of AI in the
team speak uh the situation here in
World of Warcraft is very chaotic a lot
of things happen it's very you know it's
a very comp environment uh so you need
your model to be very comfortable with
the ambiguity of a complex audio if
overlapping speech and so on and so
forth and to give you an idea of what it
could look like uh here is a video of me
trying to play a uh mmpg with Mushi
hello what's up hi Mushi uh let's
pretend we Are wizards and we are
fighting the giant dragons that is
coming right to us okay okay I'll be the
wizard and you be the fighter okay how
do we start okay so as a wizard what
kind of spell can you cast to protect us
well I have a spell called Fireball that
can create a ball of fire and throw it
at our enemy okay can you cast it right
now against the dragon it's coming
closer to us sure I'll cast the spell
okay on my side I will start attacking
it with my with my bow okay sounds good
uh so now so for the sake of time I will
not play the entire conversation but
basically it's you know it gives you an
idea of how we could uh we could do that
so when we go from Mod to mhi one
problem is that we need we have much
stronger requirements than continuation
because we need to be real time and
streaming so we cannot model semantic
and acoustic token
separately the model needs to be much
smarter thanm so it needs to rely on an
LM backbone and we need to have an
explicit modeling of both user and a
speech so that the model is going to to
understand what is their speech and what
is the user speech I'm running a bit out
of time so I'll be very quickly but the
main idea is that we use the
hierarchical model inspired from unio
and AQ Transformer where instead of
having this flatten model of tokens our
large model predicts a continuous
embedding that becomes the input to a
much smaller Transformer that predicts
the tokens for for a single step so now
we have a model that is going to scale
to much longer sequences and that can
operate in real time the second thing is
that if we want to go from continuation
to dialog it's very easy to do Al duplex
you have your sequence of tokens and
you're just going to concatenate the
tokens of the user and the tokens of the
AI it's it works but it cannot model
overlap so the simple idea is that we
have two stream of tokens now as input
instead of one we always model one
stream for the tokens of the user and
one stream for the tokens of the system
so that both can speak at the same time
both can be silent at the same time and
so on and so forth uh I will skip on the
uh a fun demo unless I have a I think I
have only a few minutes left but
so then there was a question of how we
create the data to train this model and
so we created synthetic data by hiring a
voice artist and we we wrote with our
text llm a lot of strips for her and we
trained it allowed us to train a t time
I'm not chatting but rather being
controlled by text I can express more
than 70 emotions and speaking Styles
like Whispering or maybe I could sing a
song I can sound terrified so we created
this emotional TTS uh system and we used
it to generate unlimited amounts
basically of synthetic data uh that look
like like this hello what's going
on do you watch a drama series if so
which one yeah I do there are so many
good on again for the sake of time I
rather have you read the paper but B the
first multistream TTS as well that can
create full duplex synthetic speech that
is used then to train Moshi so to
summarize because I'm a bit late Moshi
is the First full dlex voicei ever so we
start with online demo on mh. chat then
we release wa weights and code and a 60
page research paper on archive it can
run locally on your MacBook uh with u uh
these uh two simple commands and as a
conclusion uh I would say that audio
language models are very dear to my
heart because they we showed that they
are strong Foundation models and they
are also natural candidates for
multimodality because what you can do is
you can tokenize images text audio and
then have a single model uh predicting
this uh all these modalities with a
unified architecture which is basically
our Approach at qai the Approach at
gimini and so on and so forth what is
next and that will be my final word one
thing I have not been able to uh make
progress on so far and I'm really
looking forward to happen is emergence
in terms of zero shot abilities now the
what we see in text language models they
are able to do any kind of task with
only few shot prompting or even
sometimes zero shot prompting it will be
amazing to to show that the speech model
that was trained in unsupervised fashion
can do impersonation can do voice
transformation and so on and so forth in
the zero shot manner thanks a lot for
your attention and if there is time I'm
happy to take a few questions thank
you okay let's still take probably one
or two questions so any question from
the
audience okay there okay we have a
question
behind uh hi NE thanks for your great
work and uh I observed in your uh audio
l you tried the uh
self-supervised features with to Vector
bir but in MH I observed that you use
wave LM so is there any difference
between the two or any other sub
supervised the
representations
so wel B bir is not open source so I
could use it when I was at Google since
then I think there was a open source
application but by meta but the
basically the main idea is that we could
have used wave Tober or WM I don't think
it would have much uh made much uh
differences one thing that was very
important for mhi is that we needed the
semantic toen to be coal because the
well tot is B directional so it cannot
work in a streaming fashion you cannot
use it in a realtime conversation
because you cannot uncode it until you
have some context in the future so
that's why we did a distillation from WM
into coal tokens that can be computed in
a swimming fashion one thing that is
interesting is that uh we also tried
dising whisper activations and it didn't
work at all despite whisper being
state-ofthe-art for years so there is
probably something to explore
there okay thank
you oh any quick question okay the last
one the the one at the thir third role
okay okay yeah I know we probably you
have more questions but we have limited
of time sorry thank you for your
presentation uh I have a question
regarding MOSI so the last day training
of MOSI is training on 2,000 hours of
data
right uh from
Fisher and I think um so the it seems
like the text input is different from
what is uh used for pretraining the
helium temporal
Transformer so do you think it's any
mismatch or like it does the how much
does the pre-training help for
MOSI okay
so question so we the last stage is not
fisher fisher is used at some point to
uh just to because it's it's the only
existing uh F duplex data we had however
we used Fisher mostly to train uh
multistream text to speech system and
then the final step is purely synthetic
data so it's fake conversation between
Mushi and the user it's around 100,000
hours of data in this fashion obviously
it really changes from the distribution
of text that is used to train the llm
backboard what you can see in our model
is that Moshi it's much worse on
question answering than it's L&M
backbone and I think one theory for that
is that there is much more high quality
content in text Data than in speech so
in text you have stack overflow stack
exchange Wikipedia you know you have a
lot of sources of high density of
information very informative in audio
it's much more difficult because if you
wanted to synthesize the entire
Wikipedia it would already represent a
huge volume so that's why we we want to
make progress on how we merge the
knowledge of the text model along with
the speaking
abilities I see thank you thank you okay
I think due to Tha limitation we have to
stop at here let's s Neil again to give
us a wonderful and insightful talk thank
you very much thank you thanks a lot for
having me very happy to be here thank
