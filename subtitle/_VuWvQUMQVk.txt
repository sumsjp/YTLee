臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw
剩下的部分，其實還是需要講的
等一下助教會講作業
請容我再講十分鐘
因為等下這個是跟作業有關係的
那這個是，其實今天
在 deep learning 的領域，有這麼一個推論
這個推論是這樣
幾乎所有的 local minimum
他的 loss 跟 global 的 optimum
都是差不多的
也就是說，你今天在做 optimization 的時候
卡到、走到 local minimum 也不用驚慌
因為 local minimum 的 loss 跟 global minimum 的 loss
應該是差距不大的
而我們今天又知道說
learning 跟 optimization 不一樣
你應是找一個
在你的 training set 上，loss 最小的東西
搞不好你只是 over-fitting 而已啊
找一個 local minimum
他的 loss 跟 global minimum 差不多
其實搞不好也就夠了
所以，在 deep learning 有這麼樣的一個傳說
這個傳說，是怎麼來的呢？
其實這個傳說，我第一次聽到的時候是
Yann Lecun 在 07 年的一個，還是在 06 年的一個演講
距今已經十年了
或者是超過十年了，所以
十年前 Yann Lucan 就已經這麼說了
他就告訴我們說，train deep learning 的時候不用害怕
當你卡在 local minima 的時候和 global minima 的值
應該是差不多的
那這個說法是怎麼來的呢？
以下是一個簡化的版本
但其實這些東西現在都已經有證明了
當年 Yann Lacun 十年前講的時候
應該是沒有證明的，憑著直覺他居然知道這些事情
但是十年後，我看到 2017 年的時候有一篇 paper
已經有對這樣子的假說做了一些證明
那 Yann Lacun 是怎麼說的呢？
這邊是一個簡化的版本
這個簡化的版本是這樣
我們今天在 train 的時候
我們用 gradient descent，最後會停在一個 critical point
這個 critical point，他可能是一個 saddle point
也可能是一個 local minima
那他到底是 saddle point，還是一個 local minima 呢？
我們剛才說，我們要分析 Hessian, H
我們現在假設 network 有 N 個參數
那 Hessian, H 就是一個 N*N 的 matrix
我們又剛才講過說，一個 N*N 的、對稱的矩陣
他會有 N 個，彼此之間是 orthogonal 的 eigenvector
所以我們現在把 eigenvector 列出來就是 v1, v2...vN
每一個 eigenvector 會對應一個 eigne value
這邊寫作 λ1, λ2...λN
如果所有 eigenvalue 都是正的
這個 critical point 就是 local minima
所有 eigenvalue 都是負的
他跟 local maxima 有正有負
他就是 saddle point
現在我們隨便走到一個 critical point
他的 matrix, H，他到底應該是 local minima
還是 saddle point，還是 local maxima 呢？
這邊引入了一個有點奇怪的假設
假設今天這些 λ
是從某一個，比如說 Gaussian distribution sample 出來
他有一半的機率是正的
有一半的機率是負的
所以，你不要問我說這個是哪來的這樣
先假設你相信說
這些 λ，就你拿一個 λ1 出來
他的 eigenvalue 應該是正的還是負的呢
機率各一半一半
假設這個前提你相信的話
如果今天 network 只有一個參數
今天他走到一個 critical point
因為只有一個參數
所以只有一個 eigenvector
你只有一個 eigenvalue
他有 1/2 的機率是正的，有 1/2 的機率是負的
那意味著說，他有 1/2 的機率是 local minima
有 1/2 的機率是 local maxima
除非正好是 0，不然他不可能是 saddle point
所以，今天在只有一個參數的情況下
有一半的機率是 local minima
有一半的機率是 local maxima
幾乎不可能是一個￼ saddle point
假設有兩個參數
你有兩個 λ，λ1 跟 λ2
今天如果兩個 λ 都是正的
那你有 1/4 的機率發生這件事情
那他是一個 local minima
兩個 λ 都負的有 1/4 的機率發生這件事情
那他是一個 local maxima
兩個 λ，一個正、一個負
那他是一個 saddle point
saddle point 發生的機率有 1/2
所以發現有兩個參數的時候
saddle point 發生的機率就已經大過 local minima
假設今天有十個參數呢
有十個參數你就會發現說
有非常低的機率是 local minima
因為你要全部的
你有 λ1 到 λ10
然後每次擲骰子，決定他是正的還是負的
是1/2，你擲個銅板決定他是正的還是負的
你要全部都是正的，他才是 local minima
全部都是正的機率是 1/1024
全部都是負的機率也是 1/1024
通常你只要有其中一個正的、有一個負的
他就是 saddle point，所以通通、幾乎你每次
走到一個 critical point 的時候，他都是一個 saddle point
所以，這整套說法告訴我們說
假設一個 network 他的參數很多
network 參數越多
當我們今天碰到一個 critical point 的時候
他就越不可能是一個 local minima
他就越有可能是一個 saddle point
當你今天的參數非常非常多的時候
你走到一個 critical point
他是 local minima 的機率是幾乎不可能的
幾乎一定是 saddle point
而且這個假設
還有下半部
這個下半部是說
我們剛才說 λ 有 1/2 的機率他是正的
有 1/2 的機率他是負的
接下來，我們再進一步想像說
這個機率並不是 1/2
這個機率跟你現在的 loss 是有關係的
我們假設這個機率我們把它寫成 p
然後我們說，今天這個 p 是跟
你現在 loss function 的那個 loss
我這邊寫 error，但意思是一樣的
就跟 loss function 的 loss 是有關係的
假設現在 loss 越大
那 p 的值就越大
p 是負的 eigenvalue 出現的機率
負的 eigenvalue 代表說有某一條路可以讓你往下走
這個假設也是滿合理的，你想想看
loss 大就是你剛開始的時候
剛開始的時候，你在 loss 比較高的地方
應該會有很多條路
你找到一個 critical point，應該會有很多條路讓你
再往 loss 更低的地方走
所以你走到一個 loss 很低的地方的時候
可能 loss 就沒有再更低的地方了
所以，所有的路可能都是會讓 loss 變高的
所以這個假設也是滿合理的，就是
如果今天你所在的 error 的 surface
你是在 training 剛開始的時候，loss 很大的時候
這個時候呢
比較有可能出現 negative 的 eigen value
當你今天 train 到後來，loss 已經很低了
出現 negative 負的 eigenvalue 的機率就很小
所以今天 loss，這邊用 ε 代表 loss
今天隨著 ε，loss 的不同
你走到一個 critical point
他的 eigenvalue 的分佈啊
是可能會有像下面的變化
如果今天，他的 loss 很大
那你的分佈就是這樣
他有幾乎一半的機率是正的，幾乎一半的機率是負的
如果今天隨著 loss 越來越小
那你的這個，這個 λ 代表 eigenvalue
你的 eigenvalue 就會逐漸往正的那邊偏
當你的 loss 真的很小的時候
eigenvalue 就很容易是正的
eigenvalue 就很容易是正的，代表他很容易是一個
critical point
很容易是一個 local minima
所以這個理論告訴我們說
saddle point 比較容易出現在 loss 大的地方
local minima 比較容易出現在 loss 低的地方
所以，對於 deeo learning
error surface 的想像是長這樣子的
所有的 local minima
可能有一個 global minima 在這個地方
所有的 local minima，他的 loss 都這麼低
可能就跟 global minima 差不多低了
而所有的 saddle point
他的 loss 都很高
所有的 saddle point 都會出現在 loss 比較高的地方
所以，如果你今天走到一個 local minima
那這個 local minima 的 loss
可能就跟 global minima 的 loss 差不多
而這件事情可能就已經足夠好
你也不一定要去找出 global minimal
也許 local optimal 的 loss 已經夠低了
那這件事情，剛才就只是一個假設而已
但是，首先在實驗上
有一些實驗室可以佐證的
而這個是 Banjo 的 paper
他 train 了三個 network
這三個 network 呢
紅色的這個他的 error 很低
綠色的這個，error 是 23%
藍色的這個，error 更高，是 28%
現在他找到了一個 critical point
這三個 critical point，loss 分別是這個樣子
找到 critical point 以後，你就
把他的 eigenvalue 解出來
把那三個 Hessian 的 eigenvalue 解出來
藍色的 critical point，eigenvalue 的分佈是這樣子
中線黑色這條線代表 eigenvalue 是 0
藍色的 critical point，他的 eigenvalue 有一部分是負的
有一部分是正的，代表他是一個 saddle point
綠色的，有一部分 eigenvalue 是負的
有一部分 eigenvalue 是正的
代表他是一個 saddle point
紅色的這個 critical point
你會發現說，他的 loss 很低
然後，他的 eigenvalue 幾乎通通都是正的
紅色這個，他的 eigenvalue 都是正的
代表他是一個 local minima
而 local minima 是出現在 loss 很低的地方
這邊只分析了三個點，當然你可以分析更多的點
那這個其實是作業要大家做的一件事
看看說這個 Banjo 講的
Banjo 有沒有在騙我們這樣
這個實驗室什麼？這個實驗的縱軸是 training error
你就 train 一個 network，train 很多次
你每次 initialize 的值都不同
最後收斂的地方就會不太一樣
你就 train 很多次，找到很多的參數
找到很多的 critical point
那這些 critical point 各自有不同的 error
把他們記錄下來
接下來，每一個 critical point
你都去算出他的 Hessian
然後去算出那個 Hessian 的 eigenvalue
那假設你找到的那個 Hessian
他的 positive eigenvalue 越多的話
代表他越像是一個 local minima
因為你很難正好最後的 eigenvalue 通通都是正的
是有正有負，只是比例不同而已
假設今天某一個 Hessian 算出來
正的 eigenvalue 的比例越高
代表說他越像是一個 local minima
反之，如果負的越多
代表他越像是一個 saddle point
現在就把每一個 critical point
他像是 local minima 的比例把他畫出來
你就會發現說，如果今天一個 critical point
他越像是 local minima
他的 error 就越低
或者說，一個 critical point 他越像是 local minima
他的 error 就越低
剛才講的，其實比較像是猜測，那實際上呢
剛才講的那件事情
是有理論的證明的
那實際上理論的證明就
把它放在文獻裡，留給大家做參考
那理論上的證明給我們的
是圖上的實線
這個實線告訴我們什麼，這個實線告訴我們說
1 - local minima 的 degree
也就是負的 eigenvalue 佔的比例
會正比於 (ε/c - 1)^2/3
這個 c 一定要小於 ε
你可能想說 2/3 這個次方哪來的
你就看到 paper 他證出來就是這樣子
他證出了這樣子
那這個需要一些假設啦，這個假設到底
合不合理，就是你未來要繼續研究這樣子
那至少根據這些假設就證出來說
一個 critical point 他像不像是 local minima 的程度
確實跟 ε 是成正比的
畫出圖來，就是這個實線
那這個點點是實際上做實驗的結果
跟實線其實是滿接近的
還有其他的假說，在早年，14 年的時候
Yann Lacun 他就把
spin-glass 的 model 跟 neural network，把它連結在一起
我不知道大家知不知道 spin-glass 的 model
他是物理上的一個研究得比較透徹的 model
然後，他假設說
network 跟 spin-glass 的 model 是非常像的
他用了七個 assumption
那七個 assumption 有很多是不合理的這樣
意思是，告訴你說
network 跟 spin-glass model 是一樣的
spin-glass model 有這些這些特質
所以，network 應該也有這些這些特質
但是要在假設成立的前提下這樣講就是了
我就不打算講這部分
那七個假設有很多是很奇怪的
所以這個留給大家研究
但是，他自己做了一些實驗，他說
我們左邊是 spin-glass 的 model，我們就不要管他
右邊是說，我們來 train 一個 network
那 network 有比較小的
只有 25 個 neuron 的
有比較大的，有 500 個 neuron 的
然後，train, train, train，看最後 loss 卡在哪裡
假設今天會卡住，就是走到 local minima 就會卡住
你會發現說，今天如果是一個比較小的 network
他有可能卡在，他有可能 loss 降的很低
但它的分布很廣，他有可能卡在 loss 很低的地方
隨著 network 越來越大，他就變得越來越集中
如果 network 非常大的時候，他的 loss
都會幾乎集中在某一個區域
所以，好像顯示說 local minima
他的 loss 通通集中在某一個 value 一樣
這邊還有另外一個理論
這個也是最新的，這個是 CVPR 2017 的 paper
這篇 paper 告訴我們說
只要一個 network 夠大，但是他還沒有告訴我們說多大
只要大到某一個程度
我們有一個 global optimization 的方法
我們可以用 gradient descent
找到 global optimal，無視 optimization
只要 network 夠大
但是他這邊其實也有一些比較不 pratical 的假設
他有一個假設是說
他假設 network 的架構必須是長這個樣子的
也就是說，他有很多條支路
有很多條支路
而支路和支路之間是不相通的
我們知道一般的 network 是 fully-connected 的
並不是長這個樣子的
如果我們今天把中間的每一條路簡化成
只有一個 neuron 的話
他就變成只有一個 hidden-layer 的 network
他變成一個正常的 fully-connected network
但是，證明只有一個 hidden layer 的 network
可以找到 global optimal
其實沒有特別厲害，因為你知道說
SVM 他也可以看作是
一個 hidden layer 的 neural network
他已經可以找到 global optimal
他整個 algorithm 看起來有點像是 boosting
他每次會加一條支路進去，加一條支路進去
直到那個 error 變成 0 為止
這邊就是想引用一些文獻
最早的，有關 Hessian
最早有關 Hessian 的一些猜測
出現在這三篇 paper 裡面
分別是 2014 年的 paper，還有一篇 2015 年的 paper
那第一篇這個是，Yann Lacun
下面這一篇，The Loss Surface of Multilayer Networks
是試圖把 spin-glass model 跟 network 硬是連在一起
上面這兩篇，就是
講了一下 saddle point
才是真正的問題的所在
然後，local minima 比較不是問題
你知道裡面其實沒有證明
他都只是引用一個物理學的文獻
告訴你說應該是這樣
看到物理學的文獻跟 network 也是沒什麼太多的關係
就是他裡面都講得比較模糊
但是，這篇 paper 就有直接證明告訴你說
過去的那些猜測是對的
然後剛才 CVPR 的 paper 我也列在這邊
他就想了一個演算法可以找到 global 的 optimum
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw
