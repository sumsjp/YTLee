接下來要講的就是，怎麼讓機器進一步成為一個通才呢?
那這樣怎麼希望機器能夠閱讀我們要它做的任務的敘述
並且我們提供一些範例，它根據我們要它解的題目 或者是任務的敘述跟範例，就要做我們想做的事情?
那這樣機器的行為就更像是人類了
大家在考英文指考的時候是不是都會看到像這樣子的題目的敘述
然後看到像這樣子的題目的範例，給你一個敘述 給你一個範例，希望你就知道接下來要怎麼做的
那這種對機器而言
這種給題目敘述 就要能夠回答叫做 Instruction learning
那給範例就要能夠回答叫做 In-context learning
那這個就是ChatGPT系列想要達成的目標
那其實OpenAI並不是這幾年才突然想做這件事
他們一直都想做這件事
比如說2021年的機器學習上課錄影 就已經有一個很像的投影片
告訴你說GPT它想要做的事情 就是讓機器讀題目的敘述
然後再看範例，就能夠回答問題
不過在2021年那個時候會覺得這個想法太狂了
大概二三十年後才會看到結果吧 沒有想到這麼快就可以看到結果了
當初在這個影片的標題上還取說這叫GPT的野望
代表說這個應該是很難達成的事情
有人常常問一個問題
GPT系列能不能夠成為一個專才跟BERT一樣
針對不同的任務去做微調呢? 其實是可以的，一定是可以的
但是為什麼GPT系列沒有選擇跟BERT一樣去微調參數呢?
我其實沒有很好的答案，以下是想了兩個可能
第一個是，一開始OpenAI對於人工智慧的技術就有比較高的期待
讓機器成為專才這件事就是他不屑做的
他一開始就想開發成能夠當作通才的模型
他一開始就想開發一個模型可以看得懂人類的指令
我不知道OpenAI是不是一開始就有這麼高瞻遠矚的想法
另外一個可能就是，BERT都已經把微調那個路線做得差不多了
也就是專才那個路線已經被BERT封死了差不多了
GPT再去做微調，你也不過只能跟BERT做得差不多
有什麼樣特別厲害的地方，所以一定要另闢蹊徑
微調這條路已經被佔去了
所以GPT系列也只好另闢蹊徑，走一條不一樣的路
我覺得不管是哪一個理由，其實都蠻能夠編成故事的
我相信未來一定有人可以編各式各樣的故事
比如說OpenAI的研究人員一開始也是想要微調的
但是發現BERT已經被微調了，所以他決定要另闢蹊徑
然後他的主管就不斷地壓迫他，所以就跟BERT一樣做微調就好了
然後他就不肯
GPT系列是2018年就開始有了，GPT-1，2018年就開發出來了
從GPT-1到GPT-3，從2018年到2020年三年間
這個模型都是個廢物
就算是GPT-3如此巨大
但解各種任務都跟BERT不在同一個量級上
在train這麼大的模型到底有什麼用呢?
我相信搞不好OpenAI內部的研究人員受到金主爸爸很大的壓力
金主爸爸就壓著他們脖子說，你給我就跟BERT做一樣的東西就好了
這聽起來比較實際，但是他們就是抵住了各種壓力
抵死不退，然後才有今天的ChatGPT這樣
以上的故事是我隨便亂編的
但是我覺得搞不好真的就是這個樣子 這可以成為一個很不錯的勵志的故事
我們就先來看一下機器怎麼根據範例來做學習
根據範例來做學習這件事情叫做In-context learning
假設我們要語言模型做某一個任務，比如說我們現在要它做情感分析
也就是給它一個句子，它要說這個句子是正面的還是負面的
你直接給它句子是沒有用的，因為就算它有情感分析的能力
你直接給它一個句子，它也不知道要幹嘛
是要做翻譯嗎?是要做摘要嗎?它不知道要幹嘛
所以你得告訴它說，現在要做情感分析
那怎麼告訴它現在要做情感分析呢?
提供給它一些例子
就告訴機器說，今天天氣很好，是正面的，今天運氣很差，是負面的
這朵花很美，正面，我真的是累了，負面
把這些句子通通串起來，再加上我感到非常高興這個句子
通通丟到語言模型裡面，當作是一個文字接龍。
這些句子也是文字接龍的一部分，然後讓語言模型輸出
接下來應該輸出哪一個詞彙，希望它就會輸出正面
希望它可以領悟到說，根據這些例子
領悟到說，現在就是要做情感分析
所以給你這個句子，你應該輸出正面
講到這邊，大家可能會非常困惑
機器真的能夠從例子中學習嗎?
如果你沒有什麼機器學習的概念，你可能會覺得說
這個不就跟人一樣，看一些例子本來就可以學會啊
但是如果你有機器學習的概念，你可能會非常懷疑說
這可不是跑gradient descent
如果你說，你把這些例子拿去跑gradient descent
去微調語言模型，機器可以學會一點東西
當然可以
但是這個不是跑gradient descent
它只是input而已，機器真的能從這些input
就做類似學習的事情，得到正確的答案嗎?
所以有人就很懷疑
這篇文章叫做《Rethinking the Role of Demonstration: What Makes In-Context Learning Work?》
有人就很懷疑說，機器真的能從這些例子裡面學到東西嗎?
所以在這篇文章裡面就做了一個神秘的實驗，它故意給機器錯的答案
一樣給它例子
但是這些例子的標註是錯的 然後看看機器會有什麼樣的反應
我們就來看一下實驗結果吧
這邊上面是一堆分類任務的正確率的平均 下面是一堆多選題的任務的正確率的平均
每一塊就是不同的模型
從小的，比如說GPT-2，一直到GPT-3
藍色是沒有給範例，沒有給範例
機器表現當然很差，這沒有什麼問題
黃色是給了範例，然後範例裡面的標註是正確的
紅色是給了範例，然後範例裡面的標註是錯的
最有趣的地方是，範例給了錯誤的標註以後
其實正確率並沒有真的下降很多
從這個實驗看起來，還是有一點點的下降了
但是神奇的事情是沒有下降很多
看來給機器這些範例，它從範例裡面似乎並沒有做真正的學習
接下來，這個作者又做了另外一個實驗是
如果我們給機器一些句子
但這些句子的domain
跟我們現在要測試的domain非常不一樣
假設我們現在要做的是情感分析
有很多情感分析的Benchmark Opus
但是我們這邊的這些句子 不是從情感分析的Benchmark Opus拿出來的
而是比如說從Wikipedia上面隨便sample來的句子
如果我們給機器這些奇奇怪怪的句子，那結果會怎麼樣呢?
這個是實驗的結果，黃色的就是給正確的答案
紅色的是給錯誤的答案，紫色這一條是給無關的句子 藍色這一條是完全沒有給例子
所以你發現說，如果你給機器的例子
它裡面的句子跟你現在要解的任務，是來自於不同的domain
它們的內容差異非常大，那你發現說 根據範例來學習，就會沒有效
所以看起來，給機器的那些demonstration 給機器的那些範例，它的domain是很重要的
所以這篇文章裡面，作者就給了一個推測
這些語言模型本來就會做情感分析
你給它這個句子，它本來就知道它是正面的
它需要的是什麼?它其實不需要這些例子來做學習
它不需要你教它說這叫正面，這叫負面，它早就會了
那這些例子有什麼用?
這些例子是啟動它，讓它知道說現在要做情感分析
而不是做其他的事情，比如說翻譯或摘要
那有另外一個額外的旁證是說
像這種in-context learning的方法 其實你這邊例子給了多了，也沒什麼用
因為in-context learning的這些例子，它最重要的目標
並不是讓機器根據這些例子做學習
而是要喚醒機器，告訴它說現在要執行什麼樣的任務
而語言模型本來就有執行這些任務的能力了
我們需要的只是透過這些例子，喚醒它 讓它知道現在要解什麼樣的任務
所以在文獻上，你會發現說 其實in-context learning你給的範例的數目沒有那麼的重要
那這邊的縱軸是在不同任務上面的表現 這邊的橫軸是給不同的範例的數目
0個、4個、8個、16個跟32個
那一般在做一般的supervised learning
比如說微調模型參數的時候，你給這麼少的例子
就從4增加到32，這個對於機器來說差別非常大
所以如果你今天是從4、8、16到32
如果你今天做的是微調整個模型的參數
你的例子從4、8、16增加到32 照理說你的正確率會上升非常多
但是對於in-context learning這種方法，你會發現說
很快就收斂了，到4個8個例子的時候 其實機器就不能夠再做得更好了
它並不是透過這些例子來進行學習，這些例子主要是喚醒它的記憶
告訴它說現在要做什麼樣的事情 所以例子給多了，可能幫助也不大
但是其實也有不一樣的聲音
就剛才那篇paper告訴你說 in-context learning就沒有learn到什麼
只是要喚醒機器的記憶而已
但是也有另外一個系列的論文，我把reference列在這邊
告訴你說，其實in-context learning 也是有機會讓機器做到learning這一件事情的
你可以自己再詳細讀這篇論文，看看它們是怎麼說的
怎麼說in-context learning 也可以達成類似gradient descent的效果
在前天有一篇文章被放到arxiv上
我昨天在兩個不同的場合都有人跟我提到這篇文章
這篇文章是這樣說的
它也是想要深入研究in-context learning
但它得到的結論
跟我們剛才看到的前一篇rethinking那篇文章 得到的結論是略有不同的
所以我昨天聽到這篇文章以後 就趕快做了投影片來跟大家分享
這篇文章是怎麼說的呢? 它說它做了很多不同的任務，然後試了很多不同的模型
在這個圖上，顏色越深的代表模型越大 所以由上到下是模型由大到小
橫軸是什麼?橫軸是我們今天在做in-context learning的時候
給機器的例子裡面有百分之多少是錯的 從25%到100%，有百分之多少是錯的
你會發現，從這個圖上看來
有很多模型都是給它越多錯的例子，它做的結果就越差
你會發現，尤其是大型的模型
顏色最深的就是大型的模型
最大的那些模型受到 錯誤的例子的影響是最大的
所以從這篇文章看起來 機器還是有從那些例子裡面學到東西的
這篇文章，這個是Google的paper
他並不是想要告訴你說前面的文章的結論是錯的
那篇paper又提到前面的Rethinking the Role of In-context Learning那篇文章
它是說，in-context learning這件事情
機器要從範例學習這件事情，可能要是非常大的模型才會發生
比如說這邊PaLM，它是540個billion的參數
它可是GPT-3的再又三倍大
這種巨大的模型，它確實就會受到這個例子很大的影響
但如果是小的模型，受到錯誤的例子的影響，就比較小
所以過去的論文之所以會得到結論說
in-context learning裡面資料是不是對的沒什麼用
那是因為過去看的是小的模型
如果看大的模型，機器真的可以從範例中進行學習
另外一個機器可以從範例中進行學習的例子是這樣
你看這個灰色這個虛線
灰色這個曲線代表什麼?
灰色這個虛線代表的是random的結果
今天你隨便亂猜是50%
今天如果是有100%的錯誤率
假設我們在做sentiment analysis
本來正面的文章通通被改成負面 本來負面的文章通通都改成正面
如果今天在這個例子裡面，機器得到的正確率其實是低於50%
是不是代表它不只沒有亂猜，它還從錯誤的資料中進行學習
本來有一個正面的句子，它知道你要我把正面的句子都標成負面
雖然這個只是很奇怪 跟它之前在做pre-train的時候可能學的都不一樣
但沒關係，你既然給它這樣的例子，它就照著給你學下來
所以正面的句子它會標負面，負面的句子它會標正面
所以你看到這些特別大的模型
在100%錯誤的標註的時候，它的正確率是低於50%的
意味著它可以從錯誤的資料中進行學習
它知道你給它正面的句子，其實要不要負面
你給它負面的句子其實要不要正面
這是另外一個機器有從in-context learning學到東西的例子
在這篇文章裡面，還做了一個更瘋狂的事情
直接拿大型語言模型來做分類的問題 怎麼做?
直接給機器一些feature
這是某一筆資料的feature 這是某一筆資料的label
有兩種label，Bar跟Foo
這個是什麼並不重要，反正你就當作class1跟class2
這是class1的example的feature 這是class2的example的feature
給機器一堆例子，它大概給了十幾個例子
然後再給機器一個input，給它一個feature 看看能不能得一些正確的答案出來
只能不能夠直接當作像PyTorch或Tensorflow一樣的framework
直接就來訓練一個分類的模型
這個語言模型的參數都沒有改，這個語言模型就是一個現成的語言模型
你只是給它讀了這些句子，看看它能不能夠自動地變成一個分類器
結果還真的是可以，太神秘了
這邊的橫軸是問題的難度，就是你的輸入
從一個dimension的feature 一直到64個dimension的feature
PaLM只有做1到8是因為如果feature越長 那你input就越長
然後PaLM它可以輸入的長度是有限的 所以它只做到8個feature而已
但是你可以發現說，這些模型學到的正確率是高過於隨機亂猜的
如果你跑SVM，當然SVM還是比這些大型語言模型更強 它本來就不擅長做分類
但是這邊想要表達的意思是說，你看
在某一些狀況下，它跟SVM的performance居然只差了一點點
它自己就學了一個分類的演算法，自動可以拿來做分類
就是這麼神奇
所以這是一個大型語言模型神秘的in-context learning的能力
當然，你可能會說，我們今天在做pre-training的時候
你從來沒有教機器說看到這些例子 你就要按照這些例子來做我們想做的事情
所以in-context learning結果不怎麼好
也是可以想像的
所以你可以讓機器去學習怎麼做in-context learning
這個部分的做法其實就比較直覺
你可以告訴機器說，給你一些例子
然後再給你一個句子，根據這些例子你就要知道做翻譯
給你一些範例，給你一個問題 你知道根據這些範例你就要做問答
期待今天給你不一樣任務的範例
機器知道說，這個不一樣的任務的範例
是要告訴它做natural NLI(natural language inference)
所以你可以讓機器去學習做in-context learning這件事情
當然，讓機器有學習做in-context learning 會比完全沒有學做in-context learning還要結果好得多
我剛才講的例子，機器並沒有去學著做in-context learning這件事
一個大型語言模型只學了文字接龍，直接拿出來以後
就有神秘的in-context learning的能力
剛才我們講的是透過範例來學習
更進一步我們要講機器怎麼透過題目的敘述來學習
其實透過範例來學習，對人類來說還是沒有那麼的自然
今天要操控機器，你還得去找一些範例，感覺有點麻煩
能不能更進一步讓機器透過敘述 叫你做翻譯就做翻譯
叫你做摘要就做摘要呢?
能不能夠讓機器直接閱讀任務的敘述 就知道我們要它幹嘛呢?
如果今天要讓機器可以看到任務的指示，就做對應的事情
其實直接使用一個預訓練的模型是不夠的
直接使用一個文字接龍的模型，現在看起來效果是很差的
所以文字接龍的模型還是需要經過一些微調
叫做instruction tuning以後，才能夠看得懂人類的指令
那什麼是instruction tuning呢?
就在訓練的時候，你給機器不同的指令
比如說請做翻譯，然後再告訴它說 我說請做翻譯的時候
再告訴它說請做摘要的時候，這是正確答案
期待在測試的時候，給一個前所未有的指令
機器自動知道這個指令是什麼意思，然後給出合理的回應
這個就是instruction tuning的概念
這樣的概念其實也不是全新的點子
很多人都試過這個概念了，一個知名的模型叫做T0
你看這個文章是2021年的文章，是兩年前的文章
兩年前就已經有很多人在做instruction tuning了
不是只有openAI在做instruction tuning 這個T0是Hugging Face的paper
也有其他人在做instruction tuning
那T0做的事情是什麼呢?跟我前面那張投影片講的是一樣的
你今天在訓練的時候，你給機器做摘要的指令 叫它做摘要
你給機器做情感分析的指令，叫它做情感分析
你給機器問答的指令，叫它做問答
期待在測試的時候，你直接叫它做natural language inference
natural language inference中文翻譯應該是自然語言推論
你直接叫它做推論，期待它也可以給你正確的答案
這個就是T0
另外一個知名的模型叫做FLAN
這個應該是Google的paper 也是差不多在2021年的年底的時候所發表的論文
在2021年那個時候，有很多人在嘗試做instruction tuning這件事情
如果你要做instruction tuning，做像FLAN或者像T0這種事情
那起手式通常是什麼呢?
起手式就是先去收集一大堆自然語言處理的任務
所以這個是從FLAN的paper截出來的
它就收集了各式各樣自然語言處理的任務
比如說翻譯，就有八個不同的dataset
摘要有十一個dataset等等
收集各式各樣自然語言處理的任務，還有標注的資料集
接下來，你需要把這些任務改寫成指令
什麼意思呢? 假設現在我們要做的任務是推論「 自然語言推論」
在自然語言推論裡面，機器要給它一個前提
然後給它一個假設，然後它回答說 這個前提跟假設有沒有矛盾
如果在一般的模型，不是instruction-based的模型
那你就是給它讀這兩個句子，期待它可以得到正確的結果
但是我們現在要叫機器做的事情，是看懂人類下的指令
所以這邊要問的問題就是
當人類想要叫機器做自然語言推論的時候 你會怎麼跟機器說話?
接下來就有各種不同的說法
在FLAN這邊paper裡面，每一個NLP的任務 它們都想了十個不同的描述方式
比如說，第三個描述的方式就是
如果我今天要叫機器做自然語言推論
那我可能就會說，請讀下面的文章
然後你要決定一下假設是否可以推導出前提
前提: 是什麼... 假設: 是什麼...，然後選項是什麼...
期待機器可以得到正確的答案
當然你也可以有別的問法
比如說，我現在的問法就是
我把前提先寫出來
然後接下來就說，基於上述的文句
我們能不能夠得到結論
就是把假設的句子放在這邊，期待機器可以得到正確的答案
簡單來說，你要想辦法把自然語言推論這個任務
用人類的語言把它描述出來
然後變成一個dataset，然後去教你的機器
看看大型語言模型，可不可以自動學到
看這些指令，就做它該做的事情
結果怎麼樣呢? 結果還真的是可以的
這個是FLAN那邊配合的結果
他們做了三種不同的測試任務
包括自然語言推論，Reading Comprehension 還有Closed-Book QA
這邊要強調一下
當它的測試任務是natural language inference的時候
訓練資料裡面就沒有natural language inference
這樣大家了解了吧，就是說 如果訓練資料裡面已經有natural language inference的任務
那你在測試的時候也有natural language inference的任務
機器可以看懂你要叫它做推論的指令，這聽起來沒什麼稀奇的
所以這邊如果測試的時候是natural language inference的任務
訓練的時候就沒有natural language inference的任務
然後看看機器能不能在它從來沒有看過的指令的情況下
自動知道說人要叫它做什麼事情
那你來看一下這個表現，這個數值就是越高越好
黃色的是GPT-3 GPT-3這邊應該是只有給它指令得到這樣的結果
GPT-3加few-shot就是有給它指令
也做In-context learning，得到結果是紅色這個bar
那FLAN呢?
FLAN是有做instruction tuning
所以你有拿一堆訓練的任務
教機器說看到人給這樣的指示的時候你要做什麼
雖然這些測試的任務的指示沒有看過
但在機器在訓練的時候已經看過各式各樣不同的指示了
GPT-3是沒有看過這些指示的，但是FLAN有看過
有學過怎麼根據人的指示來做合理的回應
可以得到的結果是比GPT-3 few-shot做In-context learning
還有GPT-3 zero-shot根本沒做instruction tuning的結果還要好
所以看起來機器是有機會學會 根據人下給它的指令做事情
然後它學到的東西可以generalize到 沒有看過的指令上面
這個是instruction tuning
