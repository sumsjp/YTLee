臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw/
我們要講一個東西，叫做 fGAN， 那這一段是這個樣子，這一段數學比較多
如果你聽不懂的話，不用太在意，就算了，因為這個東西有點用不上
為什麼，我們這邊要講的是什麼？
我們現在要講的是說，我們說
我們定某種 objective function， 就是在量 js divergences
那我們能不能夠量其他的divergence 呢？
fGAN 就是要告訴我們怎麼量其他的 divergences
那我之所以會說這招不太有用的關係， 原因就是，用不同的 divergences
就 fGAN 可以讓你用不同的 f divergences 來量你 generated 的 example 跟 real example 的差距
但是用不同的 x divergences 的結果
是差不多的，所以這一招好像沒什麼特別有用的地方
但是我們還是跟大家介紹一下
因為這個在數學上，他感覺非常的屌這樣子
但是在實作上，好像沒什麼特別的不同
我們來講一下 fGAN，fGAN 想要告訴我們的是說
其實不只是用 js divergence，任何的f-divergence
你都可以放到 GAN 的架構裡面去
那我們就先來介紹一下，什麼是 f-divergence
f-divergence 是說我們現在假設有兩個 distribution， p and q
p of x 代表 x 從 p 這個 distribution sample 出來的機率
q of x 代表 x 從 q 這個 distribution sample 出來的機率
那 p 跟 q 這兩個 distribution 的 f-divergence 它長什麼樣子呢？
他的長相就是，我們可以把它寫成像這邊這個式子
p 是一個 distribution，q 是一個 distribution
然後你對 x 做積分， 然後把 q of x 乘上 f of (p of x 除以 q of x)
那 f 這個 function， 假設這個東西要是一個 f-divergence 的話
那 f 這個 function 它必須是 convex 的
這是第一個條件，第二個條件是，f of 1 必須要等於 0
有這兩個條件的話，這一個式子
就是某一種 f-divergence
你這個 f 放不同的 function，它就是不同的 divergence
然後會看到說你 f 放某一個 function
就是 KL divergence
放另外一個 function 就變成 inverse 的 KL divergence
那首先我們想要跟大家說明的是，p 跟 q
就為什麼這個式子，它可以看作是在衡量 p 跟 q 的差異呢？
為什麼這個式子，可以看作是 p 跟 q 的 divergence 呢？
首先第一個我們要跟大家說明的是
如果 p 跟 q 這兩個 distribution 是一模一樣的
那這個值會是 0，因為這個東西
這個 divergence 等於是在衡量 p 跟 q 兩個 distribution 之間的距離
所以如果這兩個 distribution 是一樣的，它們的距離應該為 0
所以我們現在先來看看，假設 p 跟 q 是一樣的
他們的距離是不是應該為 ０呢？
假設 p 跟 q 是一樣的，會發生什麼事呢？
假設 p 跟 q 是一樣的，這一項的值是 1
那我們說 f-divergence 是一個條件式
f 這個式子帶 1 要是 0，所以這邊就變成 0
這邊變成 0 意味著什麼？意味著這一整項都是 0
所以當 p 跟 q 是一樣的時候，它們距離會是 0
接下來要跟大家說明的地方是
0 是這個式子可以達到的最小的距離
也就是說如果 p 跟 q 有些不同，不是完全一模一樣
它們算出來的 f-divergence，就會大於 0
怎麼說呢？因為 f 它是一個 convex 的 function
這是 f-divergence 訂的條件， 你的 f 必須要是一個 convex function
如果是一個 convex function，這個式子
可以寫成這樣，這步跳的有點快，反正你自己回去 check 一下，反正就是這麼回事就對了
因為 f 是 convex 的關係， 所以這個式子會大於等於這個式子
這個式子會是他的一個 lower bound
接下來，你把 q，消掉，然後對 p of x 做積分
你就得到 f of 1，然後會得到 0，所以今天 p 跟 q
它們如果一模一樣的時候，你的 f-divergence 算出來是 0
如果 p 跟 q，略有不同，它們一定會大於等於 0
因為 p 跟 q 是，p 跟 q 的值一定會大於等於 0
所以 p 跟 q，略有不同的時候，他們的值就會大於等於 0
所以我們可以說這個式子， 它可以拿來量 p 跟 q 之間的差異
那接下來，我想告訴大家說，假設你 x 帶不同的式子， 你就得到不同的 divergence
舉例來說，如果你的 f，是 x log x
那帶進去以後，把這個 f 用 x log x 帶進去以後
你算出來就是 KL divergence，如果你 x 是 -log x
如果你把這個 -log x 帶進去，你算出來的就是 reverse KL divergence
如果你現在 f of x，帶 (x-1)^2
你算出來的就是 chi square 的 divergence
你可以帶不同的東西， 你就得到各式各樣的 f-divergence 的 measure 就是了
那接下來要講一個你可能沒有聽過的東西，叫做 Fenchel Conjugate
這個要講的意思是說呢，這邊要講的東西是說
每一個 convex 的 function 的 f，他都有另外一個 conjugate 的 function
叫做 f *，就每一個 f 他都有一個對應的夥伴，叫做 f*
這個 f 跟 f* 之間的關係
寫成一個這樣看起來很可怕的式子，看了你就有點頭痛，然後等一下你就會跟不上了
你看就覺得有點頭痛這樣，這個到底是什麼東西？
這個東西是這個樣子，這個東西是，我們有一個 f
那它的 conjugate 的 function，叫做 f*
這個 f* 他長得是這個樣子，它是由 f of x 所導出來的
f* 如果我們帶 t 進去，要怎麼算出他的值呢？
你就窮舉所有的 t，看哪一個 t 可以讓 (xt - f of x) 的值最大
就是我們要的值
這樣講可能有點抽象，所以我們就實際上舉一個例子
f* of t1 是多少？
你就把 t1 帶進去，然後窮舉各種不同 x 的值， 看看哪一個值，可以讓 t1 最大
舉例來說，如果你 x 帶 x1，你得到 x1t1 - f of x1
你 x 帶 x2，你得到 x2t1 - f of x2
你 x 帶 x3，你得到 x3t1 - f of x3
在看這些不同的 x 誰最大，發現 x1 最大
那我們得到的 f* of t1，就是這個值
同理呢，假設你想要知道，f* of t2 的值是多少
那你就把不同的 x 呢，你就把 t2 先帶進去
然後把不同的 x 也帶進去，你帶 x1 進去，得到這個值
帶 x2 進去，得到這個值
帶 x3 進去，得到這個值，看誰最大
最大的值就是 f* of t2
所以如果你今天要知道，f* of t 長什麼樣子
你就每一點，從 t0 到 t100，每一點，通通這個方法去算
你就可以把 f* of t1 描繪出來
那這樣有點麻煩，另外一個方法是說
我們現在把 (xt - f of x)，把它畫出來，我們帶不同的 x1
但是把 xt-f of x 畫出來，假設 x 是 x1 的話
長這樣，假設 x 是 x2 的話
長這樣，這個都是一直線嘛，因為假設
你的 t 是你唯一的變數，x 是固定的
xt - f of x，x 是固定的
要嘛是 x1， 要嘛是 x2，他都是固定的
這時候 xt - f of x 他都是直線
你帶不同的 x 進去，他就是不同的直線
帶 x1 進去是這樣，帶 x2 進去是這樣，帶 x3 進去是這樣
帶不同的值進去，他就是不同的直線
接下來，我們要找最大的那一個
給定一個 t，我們要找最大的那一個
最大的那個是什麼呢？
假設給定 t1，最大的那個就是在這個地方
假設給定 t2，最大的那個就是在這個地方
所以今天你要把 f* of t 畫出來
就是把所有不同的 x1 所造成的直線，通通畫出來
然後再取它們的 upper bound，再把他們的 upper bound 找出來
這個就是 f* of t
所以今天你會發現 f* of t 它一定是 convex 的
所以今天如果你畫很多條直線，隨便亂畫， 隨便亂畫，不管你怎麼隨便畫
最後你只要找的是 upper bound
你得到的 function 都是 convex 的
所以今天 f of x 是 convex，f* of t，其實也一定是convex
那這邊是想要舉一個例子跟大家說明一下我們剛才講的是什麼？
假設 f of x 是 xlogx
今天如果我們 x 帶 0.1，我們會得到這樣一條直線
x 帶 1，我們會得到這樣一條直線
x 帶 10，我們會得到這樣一條直線
把這些直線的 upper bound 通通串起來，我們會得到一個看起來像是這樣的直線
這不是直線，看起來像是這樣的曲線，所以這 x 要帶不同的值
你從 0.1 帶進去，得到一條線
0.1001 帶進去，也是一條線
0.1002 帶進去，也是一條線， 通通帶進去，你得到無窮無盡的線
把這些所有的線 upper bound 都找出來， 就是紅色這一條線
那你會發現說這條紅色的線，它看起來像是 exponential，沒錯！
這一條紅色的線，它是 exp(t-1)
所以 f of x 它的 conjugate，就是 exp(t-1)
這邊是舉一個很直觀的例子啦
那下一頁是一個 proof， 我們也許就可以把它跳過去，反正你就
怒算一波以後會發現說，假設 f of x 是 xlog x
怒算一波以後就會知道它是 exp(t-1)， 它的 conjugate 是 exp(t-1)
那講這麼多，好像都跟 GAN 沒半毛錢的關係
到底為什麼要講這些呢？
接下來，我們就要進入跟 GAN 有關的內容了
我們剛才知道說，f of x，有一個 conjugate
就是 f* of t
其實 f* of t 它本身的 conjugate
也就是 f of x，所以 f of x 跟 f* of t
他們其實是互為 conjugate
所以 f* of t 跟 f of x 之間的關係，可以寫成這樣的式子
所以接下來呢，假設你有一個convex function，叫做 f of x
你就可以把這個 convex function f of x， 換成右邊這樣的式子
你可能會想說，這不是把本來簡單的問題變複雜了嗎？ 到底在搞什麼這樣子
這邊告訴你說，這件事情跟 GAN 有關係的
所以假設我們有一個 f-divergence 的 function
f-divergence 的 function 你是對 x 做積分， q of x 乘上 f of (p/q)
那這個 f 是一個 convex function 對不對？
那我們說 convex function， f of x
就可以換成右邊這個看起來比較複雜的樣子
所以我們就把它換掉，把簡單的問題弄得更複雜，把他換掉
所以本來 x 是 p/q，x 這邊是 p/q，是 p/q， 所以將 p/q 帶進去，p/q 帶進去
所以得到這樣的一個式子
所以本來 f of (p/q) 就變成了
p/q 乘上 t，再減掉 f* of t，f* of t 是 f 的 conjugate
然後再對所有的 t 取一個 max，你就得到這一個值
接下來我要告訴你說，我們現在 learn 一個 D
他其實就是 discriminator，他這個 D 這個 function
它就是 input 一個 x，它 output 一個 scalar
它 output 的這個 scalar，就是這邊這個 t
所以我們把這個 t 用 Ｄof x 取代掉
所以我們希望說，我們可以 learn 出一個 function，這個 function
我們不要解這個 max 的 problem，本來應該是說
給你 p of x，給你 q of x，給你 f* 長什麼樣
窮舉所有的 t，看哪一個 t 可以讓這個值最大
那這個就是你在小括號裡面的值
希望大家聽得懂這樣子
那接下來，我們要找一個 discriminator
這個 discriminator 幫我們解這個 max 的 problem
這個 discriminator 怎麼幫我們解這個 max 的problem 呢？
他就是 input 一個 x
它告訴我們說，你現在 input 這個 x
以後到底哪一個 t，可以讓這個值最大
它這個 D 就是要做這件事
但是因為假設 D 的 capacity 是有限的
那你今天把這個 t 換成 p of x
這一項就會變成是 f- divergence 的一個 lower bound
然後接下來，你再把它展開
這邊 q 可以刪掉
所以變成 p of x 乘上 D of x 減掉 q of x 乘上 f* of (D of x)
那你就把這個式子列上來
因為我先說，我們要找一個 D
如果你隨便找一個 D
它會比這個 f-divergence 的值還要小
如果你找一個最好的 D
他預測出來的 p 是最準的
你就可以去逼近 f-divergence
所以我們找一個 D，它可以去 maximize 後面這一項
它就可以去逼近 f-divergence
所以 f-divergence 的這個式子，會等於找一個 D
它可以 maximize 後面這一項，後面這一項是什麼呢？
後面這一項是對所有 x 做積分 p of x 乘上 q of x
減掉對所有 x 做積分 q of x 乘上 f* of (D of x)
接下來我就是要跟你說它跟 GAN 是有什麼關係
這一個式子我們可以寫成是對 p of x 就是一個機率嘛
對 p of x 做積分乘上 D of x，就等於是用 p 這個 distribution
對 D of x 取期望值
這邊是用 f* of (D of x) 這個值
然後用這個 Q 的distribution
對 f* of (D of x) 取期望值，接下來
這邊我們只是換了一下名字
我們把 p 改成 p data，我們把 Q 改成 PG
所以，今天 p data 跟 PG 之間的 f-divergence
就可以寫成後面這個式子
你的 f-divergence 是什麼？
就會影響到這個 f* 你放的是什麼？
所以今天假如你的 f-divergence 是 KL divergence
那你就看 KL-divergence f* 是什麼？
KL divergence f 是 x logx
它的 f* 是 exp(t-1)， 所以這個 f* 就帶 exp(t-1)，就這樣子
可是你看這個式子，跟 GAN 看起來的式子
怎麼看起來這麼像呢？
你想想看我們今天在 train 一個 generator 的時候
我們要做的事情，就是去 minimize 某一個 divergence
某一個 f-divergence, KL divergence, js divergence, reverse KL divergence 等等
而這個 divergence，我們就可以把它寫成這個式子
隨著你要用什麼divergence，你這 f* 就換不同的式子
你就是在量不同的 divergence
而這個東西就是我們說在 train GAN 的時候
你要用 discriminator 去 maximize 你的 generator
要去 minimize 的 objective function V of (G,D)
只是 V of (G,D) 的定義不同
你就是在量不同的 divergence
你設某種樣子，他就是在量 js divergence
你設另外一種樣子，就是在量 KL divergence
這邊就是從 paper 上面的圖，它就告訴你說
各種不同的 divergence 的 objective function
都幫你列好了，選一個你自己喜歡的
那可以 optimize 不同的 divergence， 到底有什麼厲害的地方呢？
它厲害的地方是
也許這一招可以解決一個長期以來困擾大家的問題是
當你 train GAN 的時候
你會遇到一個現象叫做，Mode Collapse
所以 Mode Collapse 的意思是說
你的 real data 的 distribution 是比較大的
但是你 generate 出來的 example
它的 distribution 非常的小
舉例來說，你在做二次元人物生成的時候
如果你 update 的 iteration 太多
你得到的結果可能會是這個樣子
你會發現，某一張特定的人臉
它就開始蔓延了，它就開始蔓延
變得到處都是這樣，但它這些人臉， 其實是略有不同的，這個是比較偏黃
這個是比較偏紅，但是他們都是看起來就像是同一張人臉
也就是說你今天產生出來的 distribution
它會越來越小，它會越來越小
而最後會發現同一張人臉
不斷的反覆出現，這個 case，叫做 Model collapse
那有另外一個 case 比 mode collapse 稍微輕微一點
叫做 Mode dropping
意思是說你的 distribution 其實有很多個 mode
假設你 real distribution 是兩群
但是你的 generator 只會產生同一群而已
他沒有辦法產生兩群不同的 data
舉例來說，你可能 train 一個人臉產生的系統
你會發現說，它產生出來的人臉，長得是這個樣子
你仔細一看覺得，看起來都還可以， 也許沒什麼太大的問題
但你發現你在 update 一次參數以後， 在 update 一次 generator 參數以後
產生出來的 image，變成這樣
所以發現說，其實剛才在前面產生的 image
他沒有產生黃皮膚的人
他只有產生膚色比較偏白的人，他沒產生黃皮膚的人
但是你今天 update 一次
它就變成產生黃皮膚的人，就沒產生白皮膚的人
再 update 一次，它就變成產生黑皮膚的人
他每次都只能產生某一種膚色的人
你的感覺好像是不太 ok 的
那為什麼會發生這種現象呢？
一個遠古的猜測是
也許是因為我們 divergence 選得不好
如果今天你的 data 的 distribution
是藍色這樣子的分佈
你的 generator 的 distribution
它只能有一個 mixture
它是綠色的虛線分佈
如果你選不同的 divergence
你最後 optimize 的結果
你最後選出來可以 minimize divergence 的那個 generator distribution
你會發現就是不一樣
假設你現在去 minimize KL divergence
你用 maximum likelihood 的方法，去 minimize KL divergence
那你的 generator 最後認為最好的那個 distribution
長得是這個樣子
那你發現說假設你的 generator distribution 長的是這個樣子
你從它裡面去 sample data
你 sample 在 mixture-mixture 之間
結果反而會是差的
所以這個可以解釋為什麼，過去沒有 GAN 的時候
我們是在 minimize KL divergence
我們是在 maximize likelihood，我們產生的圖片會那麼模糊
也許就是因為我們產生的 distribution 是這個樣子的
我們在 sample 的時候
其實並不是真的在 data density 很高的地方 sample
而是會 sample 到 data density 很低的地方
所以這地方就對應到模糊的圖片
那有人就說，如果你覺得是，KL divergence 所造成的
那如果你換別的 divergence
比如說你換 reverse KL divergence
那你就會發現說，對 generator 來說
最好的 distribution 是
完全跟某個 mode 一模一樣
就因為如果你看這個 reverse KL divergence 的式子， 你就會發現說
對它來說，如果他產生出來的 data
是原來的 distribution
藍色 distribution 沒有涵蓋它 penalty 比較大
所以如果你今天選擇的是 reverse KL divergence
那你的那個 generator
它就會選擇集中在某一個 mode 就好
而不是分散在不同的 mode
而我們傳統的 GAN 的那個 js divergence
它比較接近 reverse KL divergence
這也許解釋了，為什麼你 train 一下 GAN 的時候
你會產生出來，你會有 mode collapse
或者是 mode dropping 的情形
因為對你的 generator 來說
產生這種 mode collapse 或 mode dropping 的情形
其實反而是比較 optimal 的，
所以今天 fGAN 厲害的地方就是
如果你覺得是 js divergence 的問題
那現在你可以換了，你可以換 KL divergence
但結果就是，換不同的 divergence， mode dropping 的 case
狀況還是一樣，所以看起來不是 mode dropping 或 mode collapse 的問題
並不完全是選擇不同的 divergence 所造成的
那你可能會問說，那我要怎麼解決 mode collapse 的問題呢？
你在做作業的時候，你很可能會遇到 mode collapse 的問題
我不是大家要產生 25 張圖嗎？
你的 generator 可能會產生出來的圖通通都是一樣的
那你這樣就會被扣分了，那要怎麼避免這個情形呢？
這邊有一個其實做得好的同學都會這麼做的， 就是做 Ensemble
什麼意思呢？今天要你產生25 張圖片， 你就 train 25 個 generator
然後你的每一個 generator 也許它都 mode collapse
也許你的 generator 1 只會產生這樣的圖
gnerator 2 只會產生這樣的圖
但沒有關係，你有 25 個 generator
所以實際上助教在 run 你的 code 的時候
其實是跑了 25 個 generator
每個 generator 會產生一張 image
但是對使用者來說，使用者並不知道你有很多個 generator
那所以你產生出來的結果，看起來就會 diverse
這是一個我覺得最有效可以避免 mode collapse 的問題
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
