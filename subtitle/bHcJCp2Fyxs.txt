Linear 的 Model
也許太過簡單了
怎麼說它太過簡單呢
我們可以想像說 x1 跟 y
也許它中間有比較複雜的關係
但是對 Linear Model 而言
對 Linear 的 Model 來說
x1 跟 y 的關係就是一條直線
隨著 x1 越來越高
y 就應該越來越大
你可以設定不同的 w
改變這條線的斜率
你可以設定不同的 b
改變這一條藍色的直線
跟 y 軸的交叉點
但是無論你怎麼改 w 跟 b
它永遠都是一條直線
永遠都是 x1 越大
y 就越大
前一天觀看的人數越多
隔天的觀看人數就越多
但也許現實並不是這個樣子啊
也許在 x1 小於某一個數值的時候
前一天的觀看人數跟隔天的觀看人數是成正比
那也許當 x1 大於一個數值的時候
這個物極必反
過了一個峰值以後
過了一個假設 x1 太大
前天觀看的人數太高
那隔天觀看人數就會變少
也說不定啊
也許 x1 跟 y 中間
有一個比較複雜的
像這個紅色線一樣的關係
但你不管怎麼擺弄你的 w 跟 b
你永遠製造不出紅色那一條線
你永遠無法用 Linear 的 Model
製造紅色這一條線
所以怎麼辦呢
顯然 Linear 的 Model 有很大的限制
這一種來自於 Model 的限制
叫做 Model 的 Bias
那其實我們剛才在課堂一開始的時候也叫做
也說 b 叫做 Bias
那這個地方有一點
在用詞上有一點 Ambiguous
所以這邊特別強調說呢
這個東西叫做 Model 的 Bias
它跟 b 的這個 Bias 不太一樣
它指的意思是說
我們今天的這個限制
所以它沒有辦法模擬真實的狀況
所以怎麼辦呢
我們需要寫一個更複雜的
更有彈性的
有未知參數的 Function
Linear 的 Model 顯然是不夠的
那怎麼辦呢
怎麼寫出一個更複雜的
有未知參數的 Function 呢
我們可以觀察一下紅色的這一條曲線
紅色的這條曲線啊
它可以看作是一個常數
再加上一群藍色的這樣子的 Function
那這個藍色的 Function
它的特性是這個樣子的
當輸入的值
當 x 軸的值小於某一個這個 threshold 的時候
它是某一個定值
大於另外一個 threshold 的時候
又是另外一個定值
那中間呢 有一個斜坡
所以它是先水平的
然後再斜坡
然後再水平的
那它其實有名字
它的名字我們等一下再講
這邊我們因為它是藍色的 Function
我們就先叫它藍方吧 這樣子
好 那所以呢 這個紅色的線啊
它可以看作是一個常數項加一大堆的藍方
好 那這個常數項
它的值應該要有多大呢
你就看這一條紅色的線啊
它跟 x 軸的交點在哪裡
好 那這個常數項呢
就設跟 x 軸的交點一樣大
那怎麼加上這個藍色的 Function 以後
變成紅色的這一條線呢
你就這樣子加
這個藍色 Function 啊
它的這個這個坡度啊
這個斜坡的起點
設在紅色 Function 的起始的地方
然後第二個
斜坡的終點設在第一個轉角處
所以這邊紅色方向有一個轉角
那你就有一個藍色的 Function
它的斜坡的終點設在紅色 Function 的第一個轉角
然後呢
你刻意讓這邊這個藍色 Function 的斜坡
跟這個紅色 Function 的斜坡
它們的斜率是一樣的
這個時候如果你把 0 加上 1
你就可以得到紅色曲線 紅色這個線段的
第一個這個到這個
第一個轉折點之前的數值
所以 0 加上 1
可以得到紅色線段第一個轉折點之前的部分
然後接下來
再加第二個藍色的 Function
怎麼加呢
你就看紅色這個線
第二個轉折點出現在哪裡
好 所以第二個藍色 Function
它的斜坡就在紅色 Function 的第一個轉折點
到第二個轉折點之間
第一個轉折點到第二個轉折點之間
那你刻意讓這邊的斜率跟這邊的斜率一樣
這個時候你把 0加 1+2
你就可以得到兩個轉折點這邊的線段
就可以得到紅色的這一條線 這邊的部分
然後接下來第三個部分
第二個轉折點之後的部分 怎麼產生呢
你就加第三個藍色的 Function
第三個藍色的 Function
它這個坡度的起始點
故意設的跟這個轉折點一樣
這邊的斜率
故意設的跟這邊的斜率一樣
好 接下來你把 0加 1+2+3 全部加起來
你就得到紅色的這個線
你就得到紅色這個線
所以紅色這個線
可以看作是一個常數
再加上一堆藍色的 Function
那你仔細想一下就會發現說
不管我畫什麼樣的 Piecewise Linear 的 Curves
什麼叫做 Piecewise Linear 的 Curves 呢
就是你現在這個 Curves 啊
它是有很多線段所組成的
它是有很多鋸齒狀的線段所組成的
這個叫做 Piecewise Linear 的 Curves
那你會發現說這些 Piecewise Linear 的 Curves
你有辦法用常數項
加一大堆的藍色 Function 組合出來
只是他們用的藍色 Function 不見得一樣
你要有很多不一樣的藍色 Function
加上一個常數以後
你就可以組出這些 Piecewise Linear 的 Curves
那如果你今天 Piecewise Linear 的 Curves 越複雜
也就是這個轉折的點越多啊
那你需要的這個藍色的 Function 就越多
所以呢
那講到這邊有人可能會說
那也許我們今天要考慮的 x 跟 y 的關係
不是 Piecewise Linear 的 Curves 啊
也許它是這樣子的曲線
那就算是這樣的曲線
也無所謂
我們可以在這樣的曲線上面
先取一些點
再把這些點 連起來
變成一個 Piecewise Linear 的 Curves
而這個 Piecewise Linear 的 Curves 跟原來的曲線
它會非常接近
如果你今天點取的夠多
或你點取的位置適當的話
你點取的夠多
這個 Piecewise Linear 的 Curves
就可以逼近這一個
連續的這一個曲線
就可以逼近這一個不是 Piecewise Linear
它是有角度的 有弧度的這一條曲線
所以我們今天知道一件事情
你可以用 Piecewise Linear 的 Curves
去逼近任何的連續的曲線
而每一個 Piecewise Linear 的 Curves
又都可以用一大堆藍色的 Function 組合起來
也就是說
我只要有足夠的藍色 Function 把它加起來
我也許就可以變成任何連續的曲線
所以今天
假設我們的 x 跟 y 的關係
它也許非常地複雜
那也沒關係
我們就想辦法寫一個帶有未知數的 Function
這個帶有未知數的 Function 它表示的
就是一堆藍色的 Function
加上一個 Constant
那我們接下來要問的問題就是
這一個藍色 Function
它的式子應該要怎麼把它寫出來呢
怎麼把這個藍色 Function 的式子寫出來呢
也許你要直接寫出它 沒有那麼容易
但是你可以用一條曲線來逼近它
用什麼樣的曲線來逼近它呢
用一個 Sigmoid 的 Function
來逼近這一個藍色的 Function
那 Sigmoid Function
它的式子長的是這個樣子的
它的橫軸輸入是 x1
輸出是 y
輸入的 x1
我們先乘上一個 w
再加上一個 b
再取一個負號
再取 Exponential
再加 1
這一串被放在分放在
放在分母的地方
把 1 除以 1 加上 Exponential -b+wx1
前面 你可以乘上一個 Constant 叫做 c
好 那如果你今天輸入的這個 x1 的值啊
趨近於無窮大的時候
會發生什麼事呢
如果這一項趨近於無窮大
那 Exponential 這一項就會消失
那當 x1 非常大的時候
這一條這邊就會收斂在這個高度是 c 的地方
那如果今天 x1 負的非常大的時候
會發生什麼事呢
如果 x1 負的非常大的時候
分母的地方就會非常大
那 y 的值就會趨近於 0
所以你可以用這樣子的一個 Function
來試著畫出這一條曲線
用這一條曲線來逼近這一個藍色的 Function
那這個東西它的名字叫做 Sigmoid
Sigmoid 是什麼意思呢
Sigmoid
如果你要 硬要翻成中文的話
可以翻成 S 型的
所以 Sigmoid Function 就是 S 型的 Function
因為它長得是 有點像是 S 型的哦
所以叫它 Sigmoid Function
那這邊我們之後都懶得把 Exponential 寫出來
我們就直接寫成這個樣子
就是 y 等於 c 倍的 Sigmoid
然後這個括號裡面放 b+w 乘以 x1
然後這個 b+wx1
實際上做的事情
就是把它放在 Exponential 的指數下
前面加一個負號
然後 1+Exponential 的負
b+wx1 放在分母的地方
然後前面乘上 c
就等於 y
好 所以我們可以用這個 Sigmoid Function
去逼近一個藍色的 Function
那其實這個藍色的 Function
比較常見的名字就叫做
Hard 的 Sigmoid 啦
只是我本來是想說一開始
我們是先介紹藍色的 Function
才介紹 Sigmoid
所以一開始說它叫做 Hard Sigmoid
有一點奇怪
所以我們先告訴你說
有一個 Sigmoid Function
它可以逼近這個藍色的 Function
那這個藍色的 Function
其實通常就叫做 Hard 的 Sigmoid
那我們今天我們需要各式各樣不同的
藍色的 Function
還記得嗎
我們要組出各種不同的曲線
那我們就需要各式各樣合適的藍色的 Function
而這個合適的藍色的 Function 怎麼製造出來呢
我們就需要調整這裡的 b 跟 w 跟 c
你可以調整 b 跟 w 跟 c
你就可以製造各種不同形狀的 Sigmoid Function
用各種不同形狀的 Sigmoid Function
去逼近這個藍色的 Function
舉例來說
如果你今天改 w 會發生什麼事呢
你就會改變斜率
你就會改變這個斜坡的坡度
你就會改變斜坡的坡度
如果你動了 b 會發生什麼事呢
你就可以把這一個 Sigmoid Function 左右移動
那就可以把它左右移動
如果你改 c 會發生什麼事呢
你就可以改變它的高度
所以你只要有不同的 w 不同的 b 不同的 c
你就可以製造出不同的 Sigmoid Function
把不同的 Sigmoid Function 疊起來以後
你就可以疊出各種不同的
你就可以去逼近各種不同的
Piecewise Linear 的 Function
然後 Piecewise Linear 的 Function
可以拿來近似各種不同的 Continuous 的 Function
所以今天啊
假設我們要把紅色的這條線
它的函數寫出來的話
那可能長什麼樣子呢
我們知道說紅色這條線 就是 0加 1+2+3
而這個 123 啊
它們都是藍色的 Function
所以它們的函式 就是有一個固定的樣子
它們都寫做 x1 乘上 w 再加上 b
去做 Sigmoid 再乘上 c1
只是 1 跟 2 跟 3
它們的 w 不一樣
它們的 b 不一樣
它們的 c 不一樣
如果是第一個藍色 Function
它就是 w1 b1 c1
第二個藍色 Function
我們就說它的
它用的是 w2 b2 c2
第三個藍色 Function
我們就說它用的是 w3 b3 c3
好 那我們接下來呢
就是把 0 跟 123 全部加起來以後
我們得到的函式
就長這一個樣子
我們把 1+2+3 加起來
這邊就是 Summation Over i
我們的 i 呢 等於 1 或 2 或 3
然後 Summation 裡面呢
就是 ci 乘上 Sigmoid
bi+wi 乘上 x1
所以這邊每一個式子
都代表了一個不同藍色的 Function
Summation 的意思
就是把不同的藍色的 Function 給它加起來
就是這邊 Summation 的意思
然後呢
別忘了加一個 Constant
這邊用 b 呢 來表示這個 Constant
所以今天啊 我們有一個
如果我們假
我們今天就寫出了一個這樣子的 Function
如果我們假設裡面的 b 跟 w 跟 c
它是未知的
它是我們未知的參數
那我們就可以設定不同的 b 跟 w 跟 c
設定不同的 b 跟 w 跟 c
我們就可以製造不同的藍色的 Function
製造不同的藍色的 Function 疊起來以後
就可以製造出不同的紅色的 Curves
製造出不同的紅色的 Curves
就可以製造出不同的 Piecewise Linear 的 Curves
就可以去逼近
各式各樣不同的 Continuous 的 Function
所以我們其實有辦法寫出一個
這個非常有彈性的
有未知參數的 Function
它長這個樣子就是 Summation 一堆 Sigmoid
但它們有不同的 c 不同的 b 不同的 w
好 那所以本來我們是 Linear 的 Model
y 等於 b+w 乘上 x1
它有非常大的限制
這個限制叫做 Model 的 Bias
那我們要如何減少 Model 的 Bias 呢
我們可以寫一個更有彈性的
有未知參數的 Function
它叫做 y 等於 b+Summation
ci Sigmoid bi+wix1
就本來這邊是 b+wx1
這邊變成 bi+wix1
然後我們有很多不同的 bi
有很多不同的 wi
它們都通過 Sigmoid 都乘上 ci
把它統統加起來再加 b 等於 y
我們只要帶入不同的 c 不同的 b 不同的 w
我們就可以變出各式各樣
就可以組合出各式各樣不同的 Function
好 那我們剛才其實已經進化到
不是只用一個 Feature 啊
我們可以用多個 Feature
我們這邊用 j 呢
來代表 Feature 的編號
舉例來說剛才如果要考慮前 28 天的話
j 就是 1 到 28
考慮前 56 天的話
j 就是 1 到 56
那如果把這個 Function
再擴展成我們剛才講的上面這個
比較有彈性的 Function 的話那也很簡單
我們就把 Sigmoid 裡面的東西換掉
本來這邊是 b+Summation Over j wj xj
那這邊呢
就把這一項放到這個括號裡面
改成 bi+Summation Over j wij xj
把本來放在這邊的東西放到 Sigmoid 裡面
然後呢這個每一個 Sigmoid 的 Function 裡面呢
都有不同的 bi 不同的 wij
然後取 Sigmoid 以後乘上 ci 就全部加起來
再加上 b 就得到 y
我們只要這邊 ci bi 跟 wij 在放不同的值
就可以變成不同的 Function
好 那如果講到這邊你還是覺得有點抽象的話
如果你看這個式子覺得有點頭痛的話
那我們用比較直觀的方式
把這個式子實際上做的事把它畫出來
它畫出來看起來像是這個樣子
好 我們先考慮一下 j 就是 1 2 3 的狀況
就是我們只考慮三個 Feature
舉例來說 我們只考慮前一天 前兩天
跟前三天的 Case
所以 j 等於 1 2 3
好 那所以輸入就是 x1 代表前一天的觀看人數
x2 兩天前觀看人數
x3 三天前的觀看人數
i 是什麼
i 是每一個 i 就代表了一個藍色的 Function
只是我們現在每一個藍色的 Function
都用一個 Sigmoid Function 來比近似它
所以每一個 i 就代表了一個 Sigmoid Function
或者是代表了一個藍色的 Function
好 那這邊呢
這個 1 2 3 就代表我們有三個 Sigmoid Function
那我們先來看一下
這個括號裡面做的事情是什麼
每一個 Sigmoid 都有一個括號
這個括號裡面做的事情是什麼呢
好 第一個 Sigmoid i 等於 1 的 Case 啊
就是把 x1 乘上一個 Weight 叫 w11
x2 乘上另外一個 Weight 叫 w12
x3 再乘上一個 Weight 叫做 w13
全部把它加起來
不要忘了再加一個 b
然後把 b 加起來
然後呢 這個得到的式子就是這個樣子
所以這邊我們用 wij 呢
來代表在第 i 個 Sigmoid 裡面
乘給第 j 個 Feature 的 Weight
第一個 Feature 它就是 w11
第二個 Features 就是乘 w12
第三個 Feature 都是乘 w13
所以三個 Features1 2 3
這個 w 的第二個下標就是 123
w 的第一個下標呢代表是
現在在考慮的是第一個 Sigmoid Function
那我們有三個 Sigmoid Function
好 那第二個 Sigmoid Function 呢
我們就不把它的 w 寫出來了
我們就不把它的 w 放在這個箭頭旁邊
不然會太擠
那第二個 Sigmoid Function
它的在括號裡面做的事情是什麼呢
它在括號裡面做的事情就是把 x1 x1 乘上 w21
把 x2 x2 乘上 w22
把 x3 x3 乘上 w23
統統加起來再加 b2
第三個 Sigmoid 呢
第三個 Sigmoid 在括號裡面做的事情
就是把 1 2 3 1 2 3 x1 x2 x3
分別乘上 w31 w32 跟 w33 再加上 b3
好 那我們現在為了簡化起見
我們把括弧裡面的數字啊
用一個比較簡單的符號來表示
所以這一串東西我們當作 r1
這一串東西我們當作 r2
這一串東西我們當作 我們叫它 r3
那這個 x1 x2 跟 x3 和 r1 r2 r3
中間的關係是什麼呢
你可以用矩陣跟向量相乘的方法
寫一個比較簡單的 簡潔的寫法
我們剛才已經知道說 r1 r2 r3
也就是括弧裡面算完的結果啊
三個 Sigmoid 括弧裡面算完的結果
r1 r2 r3 跟輸入的三個 Feature x1 x2 x3
它們中間的關係就是這樣
把 x1 x2 x3 乘上不同的 Weight
加上不同的 Bias
也就是不同的 b 會得到不同的 r
那這三個式子 這一連串的運算啊
其實我們可以把它簡化
就如果你熟悉線性代數的話
簡化成矩陣跟向量的相乘
把 x1 x2 x3 拼在一起變成一個向量
把這邊所有的 w 統統放在一起變成一個矩陣
把 b1 b2 b3 拼起來變成一個向量
把 r1 r2 r3 拼起來變成一個向量
那這是三個式子
你就可以簡寫成
有一個向量叫做 x
這個 x 乘上個矩陣叫做 w
這個 w 裡面有 9 個數值就是這邊的 9 個 w
就是這邊的 9 個 Weight
x 先乘上 w 以後再加上 b 就得到 r 這個向量
那這邊做的事情跟這邊做的事情是一模一樣的
沒有半毛錢的不同
只是表示的方式不一樣而已
只是本來寫三個數字裡面有一堆加加減減
有一堆還有什麼上標
結果還有什麼兩個下標什麼看起來就讓人頭大
那把它改成線性代數比較常用的表示方式
x 乘上矩陣 w 再加上向量 b
會得到一個向量叫做 r
好 那所以這邊這件事情哪
在這個括號裡面做的事情哪就是這麼一回事
把 x 乘上 w 加上 b 等於 r
r 呢就是這邊的 r1 r2 r3
我的電腦有點卡 微卡這樣子
沒辦法控制那個滑鼠
沒關係 我可以控制 控制了
這是 r1 r2 r3
好 那接下來這個 r1 r2 r3 哪
就要分別通過 Sigmoid Function
好 分別通過 Sigmoid Function
因為我們實際上做的值就是
做的事情就是把 r1 取一個負號
再乘 再做 Exponential 再加 1
然後把它放到分母的地方
1 除以 1+Exponential 負 r1 等於 a1
然後同樣的方法由 r2 去得到 a2
把 r3 透過 Sigmoid Function 得到 a3
所以這邊這個藍色的虛線框框裡面做的事情
就是從 x1 x2 x3 得到了 a1 a2 a3
好 接下來呢
我們這邊呢有一個簡潔的表示方法
是我們用 r 通過一個
叫做這個 Sigmoid 的 Function
我們用這個東西
我們這邊呢用這個符號呢
來代表通過這個 Sigmoid 的 Function
然後呢 所以我們得到了 a 這個向量
就把 r1 r2 r3 分別通過 Sigmoid Function
但我們直接用這個符號來表示它
然後得到 a1 a2 a3
然後接下來呢
接下來我們這個 Sigmoid 的輸出
還要乘上 ci 然後還要再加上 b
那我們這邊做的事情就是
把 a1 乘 c1 a2 乘 c2 a3 乘 c3
通通加起來再加上 b
最終就得到了 y
好 那這邊呢 如果你要用向量來表示的話
a1 a2 a3 拼起來叫這個向量 a
c1 c2 c3 拼起來叫一個向量 c
那我們可以把這個 c 呢
作 Transpose 作 Transpose
好 那 a 呢 乘上 c 的 Transpose 再加上 b
好再加上 b 我們就得到了 y
所以這一連串的運算哪
剛才寫的那一個我們說比較有彈性的式子
它整體而言做的事情就是 x 輸入是 x
我們的 Feature 是 x 這個向量
x 乘上矩陣 w 加上向量 b 得到向量 r
再把向量 r 透過 Sigmoid Function得 到向量 a
再把向量 a 跟乘上 c 的 Transpose 加上 b 就得到 y
所以這是上面這件事情
如果你想要用線性代數的方法來表示它
用向量矩陣相乘方法來表示它
欸 就長得一副這個樣子
那這邊的這個 r 就是這邊的 r
這邊的 a 就這邊的 a
所以我們可以把這一串東西
放到這個括號裡面
再把這個 a 呢 放到這裡來
所以把相同的東西併起來以後
整體而言就是長這個樣子
上面這一串東西
我們覺得比較這個
比較有彈性的這個 Function
如果你要線性代數來表示它的話
就是下面這個式子啦
x 乘上 w 再加上 b 通過 Sigmoid Function
乘上 c 的 Transpose 加 b 就得到 y
上面這一串就是下面這一串
就是我剛才寫的那個比較彈性的 Function
講來講去都是一樣的東西
只是不同的表示方式而已
上面這個是圖示化的表示方式
下面這個是線性代數的表示方式
其實都在講同一件事情
好 那接下來啊 接下來啊
在我們繼續講說
要怎麼把這些未知的參數找出來之前
我們先再稍微重新定義一下我們的符號
這邊的這個 x 是 Feature
這邊的 w b c 跟 b
這邊有兩個 b 啊
但是這兩個 b 是不一樣的
這邊這一個是一個向量
這邊是一個數值
然後你看它們的這個底色是不一樣的
這個是綠色 這個是灰色
顯示它們是不一樣的東西
我們把這個黃色的這個 w
把這個 b 把這個 c 把這個 b 統統拿出來
集合在這邊
它們就是我們的 Unknown 的 Parameters
就是我們的未知的參數
那我們把這些東西通通拉直
拼成一個很長的向量
我們把 w 的每一個 Row
或者是每一個 Column 拿出來
今天不管你是拿過 Row 或拿 Column 都可以啦
意思是一樣啦
你就把 w 的每一個 Column 或每一個 Row 拿出來
拼成一個長的向量
把 b 拼上來 把 c 拼上來 把 b 拼上來
這個長的向量
我們直接用一個符號叫做 θ 來表示它
θ 是一個很長的向量
裡面的第一個數值我們叫 θ1
第二個叫 θ2 這個叫 θ3
那 θ 裡面
這個向量裡面有一些數值是來自於這個矩陣
有些數值是來自於 b
有些數值來自於 c
有些數值來自於這邊這個 b
那我們就不分了
反正 θ 它統稱我們所有的未知的參數
我們就一律統稱 θ
好 那這邊我們就是換了一個新的
我們就重新改寫了機器學習的第一步
重新定了一個有未知參數的 Function
那接下來我們就要進入第二步跟第三步
那在我們進入之前
我們來看大家有沒有問題想要問的
好 那也看看線上人有要問問題嗎
嗯 好 我試著回答看看
我猜他的問題是說
我們其實要做 Optimization 這件事
找一個可以讓 Loss 最小的參數
有一個最暴力的方法就是
爆收所有可能的未知參數的值對不對
像我們剛才在只有 w 跟 b 兩個參數的前提之下
我根本就可以爆收所有可能的 w 跟 b 的值嘛
所以在參數很少的情況下
你不 甚至你有可能不用 Gradient Descent
不需要什麼 Optimization 的技巧
但是我們今天參數很快就會變得非常多
像在這個例子裡面參數有一大把
有 w b 有 c 跟 b 串起來
變成一個很長的向量叫 θ
那這個時候你就不能夠用爆收的方法了
你需要 Gradient Descent 這樣的方法
來找出可以讓 Loss 最低的參數
好 希望這樣回答到他的問題
好 在座還有同學有問題嗎
來 請說
可以非常 這是一個
欸 這位同學的問題是說
剛才的例子裡面有三個 Sigmoid
那為什麼是三個呢
能不能夠四個 五個 六個呢
可以 Sigmoid 的數目是你自己決定的
而且 Sigmoid 的數目越多
你可以產生出來的
Piecewise Linear 的 Function 就越複雜
就是假設你只有三個 Sigmoid
意味著你只能產生三個線段
但是假設你有越多 Sigmoid
你就可以產生有越多段線的
Piecewise Linear 的 Function
你就可以逼近越複雜的 Function
但是至於要幾個 Sigmoid
這個又是另外一個 Hyper Parameter
這個你要自己決定
我們在剛才例子裡面舉三個
那只是一個例子
也許我以後不應該舉三個
因為這樣會讓你誤以為說
Input Feature 是三個
Sigmoid也是三個
不是就是說
Sigmoid 幾個可以自己決定
好 這樣回
這樣大家還有問題想問嗎
欸 請說
跟什麼 Sigmoid
Hard 的 Sigmoid
首先它的 Function 你寫出來可能會比較複雜
你一下子寫不出它的 Function
但如果你可以寫得出它的 Function 的話
你其實也可以用 Hard Sigmoid
你想要用也可以
所以不是一定只能夠用
剛才那個 Sigmoid 去逼近那個 Hard Sigmoid
完全有別的做法
等一下我們就會講別的做法
好
大家還有問題想要問嗎
好 如果目前暫時沒有的話
就請容我繼續講下去
那你知道這門課是 6:20 才下課
所以只要講到 6:20 前都是可以的
那如果你有事想要早點離開
也沒有問題 我們課程都是有錄影
好 那接下來進入第二步了
我們要定 Loss
有了新的這個 Model 以後
我們 Loss 會不會有什麼不同
沒有什麼不同 定義的方法是一樣的
只是我們的符號改了一下
之前是 L ( w 跟 b )
因為 w 跟 b 是未知的
那我們現在接下來的未知的參數很多了
你再把它一個一個列出來
太累了
所以我們直接用 θ 來統設所有的參數
用 θ 來代表所有未知的參數
所以我們現在的 Loss Function 就變成 L( θ )
這個 Loss Function 要問的就是
這個 θ 如果它是某一組數值的話
會有多不好或有多好
那計算的方法
跟剛才只有兩個參數的時候
其實是一模一樣的
就你先給定某一組 w b c^T 跟 b 的值
你先給定某一組 θ 的值
假設你知道 w 的值是多少
把 w 的值寫進去 b 的值寫進去
c 的值寫進去 b 的值寫進去
然後呢你把一種 Feature x 帶進去
然後看看你估測出來的 y 是多少
再計算一下跟真實的 Label 之間的差距
你得到一個 e
把所有的誤差通通加起來
你就得到你的 Loss
接下來下一步就是 Optimization
Optimization 的 problem 跟前面講的有沒有什麼不同呢
沒有什麼不同 還是一樣的
所以就算我們換了一個新的模型
這個 Optimization 的步驟
Optimization 的演算法還是 Gradient Descent
看起來其實沒有真的太多的差別
我們現在的 θ 它是一個很長的向量
我們把它表示成 θ1 θ2 θ3 等等等
我們現在就是要找一組 θ
這個 θ 可以讓我們的 Loss 越小越好
可以讓 Loss 最小的那一組 θ
我們叫做 θ 的 Start
好 那怎麼找出那個 θ 的 Start 呢
我們一開始要隨機選一個初始的數值
這邊叫做 θ^0
你可以隨機選
那之後也可能會講
也會講到更好的找初始值的方法
我們現在先隨機選就好
好 那接下來呢你要計算微分
你要對每一個未知的參數
這邊用 θ1 θ2 θ3 來表示
你要為每一個未知的參數
都去計算它對 L 的微分
那把每一個參數都拿去計算對 L 的微分以後
集合起來它就是一個向量
那個向量我們用 g 來表示它
這邊假設有 1000 個參數
這個向量的長度就是 1000
這個向量裡面就有 1000 個數字
這個東西有一個名字
就我們把每一個參數對 L 的微分集合起來以後
它有一個名字
這個向量有一個名字叫做 Gradient
那很多時候你會看到
Gradient 的表示方法是這個樣子的
你把 L 前面放了一個倒三角形
這個就代表了 Gradient
這是一個 Gradient 的簡寫的方法
那其實我要表示的就是這個向量
L 前面放一個倒三角形的意思就是
把所有的參數 θ1 θ2 θ3
通通拿去對 L 作微分
就是這個 L 倒三角形的意思
那後面放 θ0 的意思是說
我們這個算微分的位置
是在 θ 等於 θ0 的地方
在 θ 等於 θ0 的地方
我們算出這個 Gradient
算出這個 g 以後
接下來呢我們就要 Update 我們的參數了
要更新我們的參數了
更新的方法
跟剛才只有兩個參數的狀況是一模一樣的
只是從更新兩個參數
可能換成更新成 1000 個參數
但更新的方法是一樣的
本來有一個參數叫 θ1
那上標 0 代表它是一個起始的值
它是一個隨機選的起始的值
把這個 θ10 減掉 learning rate 乘上微分的值
得到 θ11
代表 θ1 更新過一次的結果
θ20 減掉微分乘以
減掉 learning rate 乘上微分的值
得到 θ21
以此類推
就可以把那 1000 個參數統統都更新了
那這邊有一個簡寫啦
就是你會把這邊所有的 θ 合起來當做一個向量
我們用 θ0 來表示
這邊呢你可以把 learning rate 提出來
那剩下的部分 微分的部分
每一個參數對 L 微分的部分
叫做 Gradient 叫做 g
所以 θ0 減掉 learning rate 乘上 g
就得到 θ1
把這邊的所有的這個 θ 通通集合起來
把這邊所有的 θ 通通集合起來
就叫做 θ1
θ0 減掉⋯⋯ θ0 是個向量
減掉 learning rate 乘上 g
g 也是一個向量會得到 θ1
那假設你這邊參數有 1000 個
那 θ0 就是 1000 個數值
1000 維的向量
g 是 1000 維的向量
θ1 也是 1000 維的向量
好 那整個操作就是這樣啦
就是由 θ0 算 Gradient
根據 Gradient 去把 θ0 更新成 θ1
然後呢再算一次 Gradient
然後呢根據 Gradient 把 θ1 再更新成 θ2
再算一次 Gradient 把 θ2 更新成 θ3
以此類推直到你不想做
或者是你算出來的這個 Gradient
是零向量 是 Zero Vector
導致你沒有辦法再更新參數為止
不過在實作上你幾乎不太可能
作出 Gradient 是零向量的結果
通常你會停下來就是你不想做了
好 那但是實作上
那這邊是一個實作的 Detail 的 Issue
之所以會在這邊就提它
是因為助教的程式裡面有這一段
所以我們必須要講一下
免得去看助教的程式的時候覺得有點困惑
實際上我們在做 Gradient Descent 的時候
我們會這麼做
我們這邊有大 N 筆資料
我們會把這大 N 筆資料分成一個一個的 Batch
就是一包一包的東西 一組一組的
怎麼分
隨機分就好
好 所以每個 Batch 裡面有大 B 筆資料
所以本來全部有大 N 筆資料
現在大 B 筆資料一組 大 B 筆資料一組
一組叫做 Batch
怎麼分組
隨便分就好
那本來我們是把所有的 Data 拿出來算一個 Loss
那現在我們不這麼做
我們只拿一個 Batch 裡面的 Data
只拿第一筆 Data 出來算一個 Loss
我們這邊把它叫 L1
那跟這個 L 呢以示區別
因為你把全部的資料拿出來算 Loss
跟只拿一個 Batch 拿出來
的資料拿出來算 Loss
它不會一樣嘛
所以這邊用 L1 來表示它
但是你可以想像說假設這個 B 夠大
也許 L 跟 L1 會很接近 也說不定
所以實作上的時候
每次我們會先選一個 Batch
用這個 Batch 來算 L
根據這個 L1 來算 Gradient
用這個 Gradient 來更新參數
接下來再選下一個 Batch 算出 L2
根據 L2 算出 Gradient
然後再更新參數
再取下一個 Batch 算出 L3
根據 L3 算出 Gradient
再用 L3 算出來的 Gradient 來更新參數
所以我們並不是拿大 L 來算 Gradient
實際上我們是拿一個 Batch 算出來的 L1 L2 L3
來計算 Gradient
那把所有的 Batch 都看過一次
叫做一個 Epoch
每一次更新參數叫做一次 Update
所以你在文獻上
常常會有人聽到 Update 這個詞彙
常常有人聽到 Epoch 這個詞彙
那 Update 跟 Epoch 是不一樣的東西
每次更新一次參數叫做一次 Update
把所有的 Batch 都看過一遍
叫做一個 Epoch
為了要讓為了要
那至於為什麼要分一個一個 Batch
那這個我們下週再講
但是為了讓大家更清楚認識
Update 跟 Epoch 的差別
這邊就舉一個例子
假設我們有 10000 筆 Data
也就是大 N 等於 10000
假設我們的 Batch 的大小是設 10
也就大 B 等於10
接下來問你
我們在一個 Epoch 中
總共 Update 了幾次參數呢
那你就算一下這個大 N 個 Example
10000 筆 Example
總共形成了幾個 Batch
總共形成了 10000 除以 10
也就是 1000 個 Batch
所以在一個 Epoch 裡面
你其實已經更新了參數 1000 次
所以一個 Epoch 並不是更新參數一次
在這個例子裡面一個 Epoch
已經更新了參數 1000 次了
那第二個例子
就是假設有 1000 個資料
Batch Size 設 100
那其實 Batch Size 的大小也是你自己決定的
所以這邊我們又多了一個 HyperParameter
所謂 HyperParameter 剛才講過
就是你自己決定的東西
人所設的東西不是 機器自己找出來的
叫做 HyperParameter
我們今天已經聽到了
Learning rate 是個 HyperParameter
幾個 Sigmoid 也是一個 HyperParameters
Batch Size 也是一個 HyperParameter
好 1000 個 Example
Batch Size 設 100
那1個 Epoch 總共更新幾次參數呢
是 10 次
所以有人跟你說
我做了一個 Epoch 的訓練
那你其實不知道它更新了幾次參數
有可能 1000 次
也有可能 10 次
取決於它的 Batch Size 有多大
好 那我們其實還可以對模型做更多的變形
剛才有同學問到說
咦 這個 Hard Sigmoid 不好嗎
為什麼我們一定要把它換成 Soft 的 Sigmoid
你確實可以不一定要換成 Soft 的 Sigmoid
有其他的做法
舉例來說這個 Hard 的 Sigmoid
我剛才說它的函式有點難寫出來
其實也沒有那麼難寫出來
它可以看作是兩個 Rectified Linear Unit 的加總
所謂 Rectified Linear Unit 它就是長這個樣
就是它有一個水平的線
走到某個地方有一個轉折的點
然後變成一個斜坡
那這種 Function 它的式子
寫成 c 乘上 max(0, b + wx1)
這個 max(0, b + wx1) 的意思就是
看 0 跟 b + wx1 誰比較大
比較大的那一個就會被當做輸出
所以如果 b + wx1 小於 0
那輸出就是0
如果 b + wx1 大於 0
輸出就是 b + wx1
那總之這一條線
可以寫成 c max(0, b + wx1)
每條不同的 w 不同的 b 不同的 c
你就可以挪動它的位置
你就可以改變這條線的斜率
那這種線呢在機器學習裡面
我們叫做 Rectified Linear Unit
它的縮寫叫做 ReLU
名字念起來蠻有趣的
它真的就唸ReLU
那你把兩個 ReLU 疊起來
就可以變成 Hard 的 Sigmoid
對不對
我們把這樣子的一個 ReLU
疊這樣子的一個 ReLU
把他們加起來 它就變成 Hard Sigmoid
所以我們能不能用 ReLU 呢
可以
所以如果我們不要用 Sigmoid
你想要用 ReLU 的話
就把 Sigmoid 的地方
換成 max(0, bi + Σj wij * xi)
那本來這邊只有 i 個 Sigmoid
但我想說你要 2 個 ReLU
才能夠合成一個 Hard Sigmoid 嘛
所以這邊有 i 個 Sigmoid
那如果 ReLU 要做到一樣的事情
那你可能需要 2 倍的 ReLU
因為 2 個 ReLU 合起來
才是一個 Hard Sigmoid
所以要 2 倍的 ReLU
所以我們把 Sigmoid 換成 ReLU
這邊就是把一個式子換了
因為要表示一個 Hard 的 Sigmoid
表示那個藍色的 Function 不是只有一種做法
你完全可以用其他的做法
好 那這個 Sigmoid 或是 ReLU
他們在機器學習裡面
我們就叫它 Activation Function
他們是有名字的
他們統稱為 Activation Function
當然還有其他常見的
還有其他的 Activation Function
但 Sigmoid 跟 ReLU
應該是今天最常見的 Activation Function
那哪一種比較好呢
這個我們下次再講
哪一種比較好呢
我接下來的實驗都選擇用了 ReLU
顯然 ReLU 比較好
至於它為什麼比較好
那就是下週的事情了
好 接下來呢就真的做了這個實驗
這個都是真實的數據 你知道嗎
真的做了這個實驗
好 如果是 Linear 的 Model
我們現在考慮 56 天
訓練資料上面的 Loss 是 0.32k
沒看過的資料 2021 年資料是 0.46k
如果用 10 個 ReLU
好像沒有進步太多
這邊跟用 Linear 是差不多的
所以看起來 10 個 ReLU 不太夠
100 個 ReLU 就有顯著的差別了
100 個 ReLU 在訓練資料上的 Loss
就可以從 0.32k 降到 0.28k
有 100 個 ReLU
我們就可以製造比較複雜的曲線
本來 Linear 就是一直線
但是 100 個 ReLU 我們就可以產生 100 個
有 100 個折線的 Piecewise Linear Function
在測試資料上也好了一些
接下來換 1000 個 ReLU
1000 個 ReLU
在訓練資料上 Loss 更低了一些
但是在沒看過的資料上
看起來也沒有太大的進步
好 接下來還可以做什麼呢
我們還可以繼續改我們的模型
舉例來說
剛才我們說從 x 到 a 做的事情是什麼
是把 x 乘上 w 加 b
再通過 Sigmoid Function
不過我們現在已經知道說
不一定要通過 Sigmoid Function
通過 ReLU 也可以
然後得到 a
我們可以把這個同樣的事情
再反覆地多做幾次
剛才我們把 w x 乘上 w 加 b
通過 Sigmoid Function 得到 a
我們可以把 a 再乘上另外一個 w’
再加上另外一個 b’
再通過 Sigmoid Function
或 ReLU Function
得到 a’
所以我們可以把 x
做這一連串的運算產生 a
接下來把 a 做這一連串的運算產生 a’
那我們可以反覆地多做幾次
那要做幾次
欸 這個又是另外一個 Hyper Parameter
這是另外一個你要自己決定的事情
你要做兩次嗎 三次嗎 四次嗎 一百次嗎
這個你自己決定
不過這邊的 w 跟這邊的 w’
它們不是同一個參數喔
這個 b 跟這邊的 b’
它們不是同一個參數喔
是增加了更多的未知的參數
好 那就是接下來就真的做了實驗了
我們就是每次都加 100 個 ReLU
那我們就是 Input Features
就是 56 天前的資料
如果是只做一次 只做一次
就那個乘上 w 再加 b
再通過 ReLU 或 Sigmoid
這件事只做一次的話
這是我們剛才看到的結果
兩次 哇 這個 Loss 降低很多啊
0.28k 降到 0.18k
沒看過的資料上也好了一些
三層 哇 又有進步
從 0.18k 降到 0.14k
所以從一層到 從就是乘一次 w
到通過一次 ReLU
到通過三次 ReLU
我們可以從 0.28k 到 0.14k
在訓練資料上
在沒看過的資料上
從 0.43k 降到了 0.38k
看起來也是有一點進步的
好 那這個是那個真實的實驗結果啦
就我們來看一下
今天有做通過三次 ReLU 的時候
做出來的結果怎麼樣
那橫軸剛才已經看過了
就是時間 就是日子
縱軸是觀看的人次 是千人
紅色的線代表的是真實的數據
藍色的線是預測出來的數據
那你會發現說
欸 在這種低點的地方啊
你看紅色的數據是每隔一段時間
就會有兩天的低點
在低點的地方
機器的預測還算是蠻準確的
它都準確抓到說這兩天就是低的
這兩天都是低的
這兩天就是低的
這兩天就是低
那這邊有一個神奇的事情
這個機器高估了真實的觀看人次
尤其是在這一天
這一天有一個很明顯的低谷
但是機器沒有預測到這一天有明顯的低谷
它是晚一天才預測出低谷
那你知道是怎麼回事嗎
閏年 不是
因為還沒有到 2 月 28 號啊
欸 大家有什麼想法嗎
對 過年啊
這一天最低點是什麼
這天最低點就是除夕啊
誰除夕還學機器學習 對不對
好 所以當然對機器來說
你不能怪它
它根本不知道除夕是什麼
它只知道看前 56 天的值
來預測下一天會發生什麼事
所以它不知道那一天是除夕
所以你不能怪它預測地不準
這一天就是除夕
好 那到目前為止
我們講了很多各式各樣的模型
那我們現在還缺了一個東西
你知道缺什麼東西嗎
缺一個好名字
你知道這個外表啊是很重要的
一個死臭酸宅穿上西裝以後就潮了起來
或者是織蓆販履的
說他是漢左將軍宜城亭侯中山靖王之後
也就潮了起來 對不對
所以我們的模型也需要一個好名字
所以它叫做什麼名字呢
這些 Sigmoid 或 ReLU 啊
它們叫做 Neuron
我們這邊有很多的 Neuron
很多的 Neuron 叫什麼
很多的 Neuron 就叫做 Neural Network
Neuron 是什麼
Neuron 就是神經元
人腦中就是有很多神經元
很多神經元串起來就是一個神經網路
跟你的腦是一樣的
接下來你就可以到處騙麻瓜說
看到沒有 這個模型就是在模擬人們腦 知道嗎
這個就是在模擬人腦
這個就是人工智慧
然後麻瓜就會嚇得把錢掏出來
但是啊 這個把戲在 80 90 年代的時候
已經玩過了這樣
Neural Network 不是什麼新的技術
80 90 年代就已經用過了
當時已經把這個技術的名字搞到臭掉了
Neural Network 因為之前吹捧得太過浮誇
所以後來大家對 Neural Network 這個名字
都非常地感冒
它就像是個髒話一樣
寫在 Paper 上面都註定會被
就會註定害你的 Paper 被拒絕
所以後來為了要重振 Neural Network 的雄風
所以怎麼辦呢
需要新的名字
怎麼樣新的名字呢
這邊有很多的 Neuron
每一排 Neuron 我們就叫它一個 Layer
它們叫 Hidden Layer
有很多的 Hidden Layer 就叫做 Deep
這整套技術就叫做 Deep Learning
好 我們就把 Deep Learning 講完了
就是這麼 就是這麼回事
就是這樣來的
好 所以人們就開始
把類神經網路越疊越多 越疊越深
12 年的時候有一個 AlexNet
它有 8 層 它的錯誤率是 16.4%
兩年之後 VGG 19層
錯誤率在影像辨識上進步到 7.3 %
這個都是在影像辨識上一個
這個基準的資料庫上面的結果
後來 GoogleNet 有錯誤率降到 6.7%
有 22 層
但這些都不算是什麼
Residual Net 有 152 層啊
它比 101 還要高啊
但是這個 Residual Net 啊
其實要訓練這麼深的 Network 是有訣竅的
這個我們之後再講
但是講到這邊
如果你仔細思考一下
我們一路的講法的話
你有沒有發現一個奇妙的違和的地方
不知道大家有沒有發現
什麼樣違和的地方呢
我們一開始說
我們想要用 ReLU 或者是 Sigmoid
去逼近一個複雜的 Function
實際上只要夠多的 ReLU 夠多的 Sigmoid
就可以逼近任何的 連續的 Function 對不對
我們只要有夠多的 Sigmoid
就可以知道夠複雜的線段
就可以逼近任何的 Continuous 的 Function
所以我們只要一排 ReLU 一排 Sigmoid
夠多就足夠了
那升的意義到底何在呢
把 ReLU Sigmoid Function 反覆用
到底有什麼好處呢
為什麼不把它們直接排一排呢
直接排一排也可以表示任何 Function 啊
所以把它反覆用沒什麼道理啊
所以有人就說把 Deep Learning
把 ReLU Sigmoid 反覆用
不過是個噱頭
你之所以喜歡 Deep Learning
只是因為 Deep 這個它名字好聽啦
ReLU Sigmoid 排成一排
你只可以製造一個肥胖的 Network
Fat Neural Network
跟 Deep Neural Network 聽起來
量級就不太一樣
Deep 聽起來就比較厲害啦
Fat Neural Network 還以為是死肥宅 Network
就不不不不厲害這樣子
那到底 Deep 的理由
為什麼我們不把 Network 變胖
只把 Network 變深呢
這個是我們日後要再講的話題
好 那有人就說
那怎麼不變得更深呢
剛才只做到 3 層
應該要做得更深嘛
現在 Network 都是疊幾百層的啊
沒幾百層都不好意思說
你在叫做 Deep Learning 對不對
好 所以要做更深
所以確實做得更深 做 4 層
4 層在訓練資料上
它的 Loss 是 0.1k
在沒有看過 2021 年的資料上
是如何呢 是 0.44k
慘掉了 欸 怎麼會這樣子呢
在訓練資料上
3 層比 4 層差
4 層比 3 層好
但是在沒看過的資料上
4 層比較差
3 層比較好
在有看過的資料上
在訓練資料上
跟沒看過的資料上
它的結果是不一致的
這種訓練資料跟測試
這種訓練資料跟沒看過的資料
它的結果是不一致的狀況
這個狀況叫做 Overfitting
那你常常聽到有人說
機器學習會發生 Overfitting 的問題
指的就是在訓練資料上有變好
但是在沒看過的資料上沒有變好這件事情
但是做到目前為止
我們都還沒有真的發揮這個模型的力量
你知道我們要發揮這個模型的力量
和 2021 的資料到 2 月 14 號之前的資料
我們也都已經手上有了
所以我們要真正做的事情是什麼
我們要做的事情就是預測未知的資料
但是如果我們要預測未知的資料
我們應該選 3 層的 Network
還是 4 層的 Network 呢
舉例來說 今天是 2 月 26 號
今天的觀看人數我們還不知道
如果我們要用一個 Neural Network
用我們已經訓練出來的 Neural Network
去預測今天的觀看人數
你覺得應該要選 3 層的
還是選 4 層的呢
好 這個我們來問一下大家的意見吧
你覺得應該選 3 層的同學舉手一下
好 手放下
好 覺得應該選 4 層的同學舉手一下
好 比較少
好 至於怎麼選模型
這個是下週會講的問題
大家 但是大家都非常有 Sense
知道我們要選 3 層的
多數人都決定要選 3 層的
你可能會說 我怎麼不選 4 層呢
4 層在訓練資料上的結果比較好啊
可是我們並不在意訓練資料的結果啊
我們在意的是沒有看過的資料
而 2 月 26 號是沒有看過的資料
我們應該選一個在訓練的時候
沒有看過的資料上表現會好的模型
所以我們應該選 3 層的 Network
那你可能以為這門課就到這邊結束了
其實不是 我們真的來預測一下
2 月 26 號應該要有的觀看次數是多少
但是因為其實 YouTube 的統計
它沒有那麼及時
所以它現在只統計到 2 月 24 號
沒關係 我們先計算一下 2 月 25 號的
觀看人數是多少
這個 3 層的 Network 告訴我說
2 月 25 號這個頻道的總觀看人次
應該是 5250 人
那我們先假設 2 月 25 號是對的
但實際上我還不知道 2 月 25 號對不對
因為 YouTube 後台統計的數據還沒有出來啊
但我們先假設這一天都是對的
然後再給我們的模型去預測 2 月 26 號的數字
得到的結果是 3.96k 有 3960 次
那它為什麼這邊特別低
因為模型知道說
這個禮拜五觀看的人數
就是比較少啊
所以它預測特別低
聽起來也是合理的
但是你覺得這個預測
跟這邊的 0.38k 比起來
哪一個會比較準確呢
你覺得 你覺得我們
我們下週來看看
2 月 26 號實際的值是多少
但是你覺得這個值
它跟真實值的誤差
會小於 0.38k 的同學舉手一下
覺得大於 0.38k 的同學舉手一下
哇 好 手放下
大家都對我這麼沒有信心這樣
好 我們就來看看這個
下一週誤差會有多少
當然我想應該是不會準啦
因為看這麼多人都覺得誤差會大
你們回去每個人都去點那個影片的話
哇 誤差就大了啊
今天講這麼久
其實就是騙大家去點影片而已啦
好 那今天其實就講了深度學習
那今天講的不是一般的介紹方式
如果你想要聽一般的介紹方式
過去的課程影片也是有的
我就把連結附在這邊
然後深度學習的訓練
會用到一個東西叫 Backpropagation
其實它就是比較有效率
算 Gradients 的方法
跟我們今天講的東西沒有什麼不同
但如果你真的很想知道
Backpropagation 是什麼的話
影片連結也附在這邊
好 今天上課就上到這邊
謝謝大家 謝謝
