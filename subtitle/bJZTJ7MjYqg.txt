好,那現在這一堂課呢,要跟大家講的是一個大型語言模型未來的趨勢
用大型語言模型打造AI agent
今天多數人呢,在使用AI幫我們做事的時候
我們往往只要求AI做一個步驟
舉例來說,你想要去GPT幫你做翻譯
你就給他你要翻譯的句子
他就給你翻譯的結果
或你想用ChatGPT幫你畫圖
你就說請幫我畫圖
ChatGPT就呼叫DALLE去給你一張圖出來
但是今天我們人類可以做更複雜的任務
而這些任務往往是需要多個步驟才能夠完成的
舉例來說我現在接到一個任務
是要舉辦朋友間的聚餐
那我可能需要先做一下計劃
就我要先調查時間
調查大家有空的時間
統計一下最多人有空的時間
然後我就訂我們常常吃的餐廳A
然後所以我可能接下來
就是先執行第一個步驟
發出一個Google表單給我的朋友們
然後我的朋友們就會填寫Google表單
我過兩天之後
再來統計一下時間
看最多人有空的時間是在週四晚上
然後呢我就跟餐廳A訂位
但是餐廳A說不好意思週四晚上沒有位子了
所以我就得訂另外一家餐廳
那這個時候我可能會用Google Map查詢一下
還有沒有其他可以去的餐廳
然後呢Google Map會給我一些餐廳的推薦
那我可能就選了另外一家餐廳B
完成舉辦朋友間聚餐這個任務是需要多步驟的
所以我需要先做一個規劃
而這些步驟它的先後順序是重要的
我不能先訂餐廳再來調查時間
我一定要先調查時間才能夠訂餐廳
不然沒有人來就糟糕了
而我在執行的過程中這個計劃趕不上變化
原來我計劃要訂餐廳A
但是可能餐廳A沒有位置了
我只好改訂另外一家餐廳
這樣這種多步驟的複雜的任務
今天我們有沒有可能讓AI來辦到這件事呢
如果AI可以辦到這件事
在這一堂課裡面
我們就稱之為AI Agent
那我知道Agent這個詞彙呢
在不同的領域可能有不同的用法
那甚至就算是在談論AI的時候
講到AI Agent的時候
每個人可能也都有不同的想像
但這一堂課裡面
我所謂的AI Agent指的就是
讓AI能夠執行多步驟的複雜的任務
能夠做計劃
而且能夠修改計劃
那今天AI有沒有能力做到這樣子的事情呢
也許你現在還沒有看到太多AI
可以做到類似的事情
但是有了大型語言模型之後
憑藉著大型語言模型的能力
也許不久的將來
AI agent就會出現在我們的日常生活中
你可以想想看
我今天所做的這些步驟
包括做計畫
包括調整計畫
包括使用工具
這些都是今天的語言模型
有機會 或已經能夠做到的事情
那今天假設你想要體驗一些AI Agent的話
有一些現成的AI Agent
那我就把幾個比較知名的AI Agent
列在這一頁投影片上
其中最知名的也許就是AutoGPT吧
你可以給他下一個任務
比如說幫我做個網頁
接下來就不要管他了
讓他自己去想辦法
他會偶爾上網搜尋啊
偶爾使用一些工具啊
偶爾自己reflection啊
想辦法完成你的任務
啊不過他是否可以真的完成你的任務
那個就不好說了
Auto GPT曾經一度非常非常的紅
不過很多人覺得他也許有點過譽了
有個網紅
下了一個指令給Auto GPT
然後就跑了一個晚上
最後什麼都沒有跑出來
把所有OpenAI的錢都噴光了
然後那網紅就非常生氣
結果Auto GPT
他畢竟還是有一些能力的極限
幾天語言模型他有一些能力的極限
你叫他自己想辦法解決任務的時候
很多時候他還是解決不了的
但是AI agent會是一個未來的趨勢
未來語言模型幫人類做事的時候
不會再侷限於一問一答
也許未來你不會再期待語言模型馬上給你一個答案
而是語言模型有能力自己跟環境互動
最終想辦法解決問題
那要講到這個AI可以自主運作的這件事情
也許最知名的例子就是我們之前有提過的
由AI村民所構成的Stanford小鎮
那我之前的上課錄影也已經講過這一段
希望大家都有看那一段錄影
那除了讓這個AI活在這個小鎮裡面以外
還有人讓AI玩Minecraft
讓AI還活在我的世界
活在Minecraft裡面
AI就會自己想辦法做各式各樣的探索
打造各式各樣的工具
你可以看一篇叫做Voyager的論文,這個是去年五月的論文
它會展示說它如何讓一個AI在Minecraft的世界裡
AI自己不斷地做各式各樣的探索,學習新的任務
那這是那篇paper裡面的一個截圖啦
那不同的這個icon就代表AI學會了不同的技能
那不同的線代表不同的方法
那這邊論文提出來的方法當然是最好的橙色這一條線
它要展示說它的方法真的非常厲害
AI很快的就學會用木製工具,很快馬上就學會用石製工具,馬上就學會鐵器工具
最終它的模型可以學會打造出鑽石劍,其他的方法都打造不出鑽石劍
它的模型會越來越強,學到的技能會越來越多,大概繼續玩下去,很快就會打到終界龍了
那剛才舉的例子都是讓AI活在虛擬的世界中
未來也很有可能這些AI agents
他們是有實體的可以跟物理的世界互動
那最近最廣受矚目的一個例子就是Figure 1
大家應該都看過這個影片了
所以我們這邊就不播放給大家看
有這個機器人
他背後有一個大型語言模型
所以你可以對他下指令
叫他清理桌子或拿點吃的給我
它是有辦法能夠做到的
那其實用語言模型來操控機器人
不是最近才有的觀念
Figure01它真正厲害的地方
不是拿語言模型操控機器人
而是它那個機器人的機械手臂跟手指
非常的靈活
那至於用語言模型操控機器人這件事情
其實早在有ChatGPT之前
就已經有人嘗試類似的方向了
這邊是引用一篇叫做Inner Monologue的論文
它是2022年7月的論文
是ChatGPT之前上古時代的論文
那我們來看inner monologue運作起來是怎麼樣的
就這個機器人背後有一個語言模型
所以人下一個指令 給我一個汽水
語言模型就開始想要做什麼
語言模型說去找汽水
然後機器人就去找汽水
看到汽水了
看到一杯可樂
我想把可樂拿起來
重點是在這個過程中遇到困難
這個人很壞啊
把他的這個可樂搶走了
沒可樂怎麼辦呢
他自己反思一下
發現做不到這件事
那他就跟人說
看到一個orange soda
喝orange soda好不好呢
人說不好要喝點別的東西
所以機器人只好
再去找其他的飲料
所以你就會看到說
是一個用大型語言模型來操控機器人的例子
那也有人嘗試用大型語言模型開自駕車
你可以看一下Talk2Drive這篇paper
這是去年年底的論文
這個圖就講得很清楚了
它背後是一個Large Language Model
那人類司機下一個指令
那這個Large Language Model
就會根據各種的情報
比如說天氣、交通狀況等等
去開自駕車
那它並不是真的去操控方向盤
它並不是說輸出左轉方向盤踩煞車這樣的指令
它是會寫一段程式碼
用那一段程式碼去操控自駕車的各個零件
然後人呢可以提供給這個自駕車回饋
比如說不要超速啊
或者是路邊停車的時候要小心一點啊等等
人可以提供各式各樣的回饋
然後這個機器呢就會把回饋存起來
日後在做決策的時候
會根據人過去所提供的回饋來進行決策
好那講到這個AI agent
那他實際上背後可能是怎麼運作的呢
我們現在來講一下
AI agent 運作的可能原理
好那右邊呢是AI agent
左邊呢是外界的環境
那AI agent 會有一個終極的目標
他終極要達成的任務
那他可能會有記憶
這個記憶代表他過去在跟環境互動的時候
所獲得的經驗
那他會從環境瞭解現在的狀態
比如說他可以透過文字的輸入
比如說他可能有視覺
他可以看得到外面的世界
比如說他有聽覺
他可以聽到其他人的聲音
或聽到外面世界發生什麼事情
或甚至假設他有實體的話
那他可能有觸覺
他可能可以感受到
現在碰到一個很燙的東西
或碰到一個很冰的東西等等
總之他可以從外界透過一些感知器
透過一些sensor 瞭解外界的狀態
那根據他的終極目標記憶跟狀態
這個AI agent可以產生一個計劃
那這個計劃就是AI agent的短期目標
也就要達到這個終極目標之前
需要採取的行動
那根據這個短期目標
這個AI agent就可以執行他的行為
那這個行為可以是輸出文字
也可以是產生圖
也可以是說話
那如果機械手臂的話
也許你可以期待這個AI agent
還可以去操作那個機械手臂
那至於要怎麼操作機械手臂
也有可能是產生一段程式碼
用這段程式碼來運作機械手臂
或者是跟其他的agent
其他的AI互動等等
那期待說AI agent
透過一系列從環境得到的輸出
透過一系列的記憶跟計畫
那他可以採取適當的行動
那他採取行動以後呢
會對世界造成一些影響
會改變現在外面世界的狀態
所以他可以根據外面世界的狀態
修改他的記憶
他可以從外面世界的狀態學習到新的經驗
修改他的記憶
他也有可能可以修改他的計劃
他不會計劃一定以後
就永遠要照著那個計劃執行
那這樣子太不靈活了
太死板了
他可能可以根據外面世界的狀態的變化
來修改他的記憶
那這個是今天我們想像中
一個AI agent應該具有的能力
那今天這些語言模型是否具有這些能力呢
它是否可以根據記憶做回復、修改記憶
它是否能制定計畫並修改計畫呢
其實今天的大型語言模型
對於產生記憶或修改計畫等等
算是有一定程度的能力了
所以我會說AI agent其實就在不遠的地方
也許你今天這個時間點
還沒有看到太多AI Agent的應用
但我相信在一兩年之內
AI Agent應該會滿地跑
那講到記憶這件事情
大家知道說
ChatGPT其實是沒有長期記憶的
今天ChatGPT能夠記得的事情
都發生在同一則對話裡面
一旦你按了開始新對話
你之前跟他講過的事情
他一概不知道一切都要從頭來過
但事實上呢
ChatGPT OpenAI也曾經一度聲稱
他們在打造有記憶能力的ChatGPT
那他們有一個Blog就是講說
他們準備要釋出有記憶能力的ChatGPT
甚至Twitter上有一些人講說
我拿到的這個版本它是可以有記憶的
但不知道為什麼後來這個功能就不見了
我手頭是還沒有看到這個功能出現在我的ChatGPT帳號裡面啦
如果有人手上有的話,記得跟我講一下
但是總之,OpenAI顯然在打造有記憶力的ChatGPT
有記憶力的ChatGPT運作起來可能是怎麼樣子呢?
當你跟ChatGPT對話完之後
它可能會把對話裡面的關鍵訊息進行摘要
比如說它知道你現在正在準備期中考試
接下來當你開始新的對話
原來的ChatGPT一旦開始新的對話
一切過去對話的內容都會清空
但這一次他有做一些摘要
所以他知道過去對話的內容裡面
有哪些重點
所以也許你下一次跟他對話的時候
他會先對他過去的記憶做一下
RAG搜尋一些跟你相關的資訊
那他看到你跟他講話的時候
他甚至可能會問你說
那你期中考考得怎麼樣等等
所以有可能給ChatGPT記憶
那如果ChatGPT有記憶的話
它就可以更加的個人化
那雖然現在OpenAI的ChatGPT
還沒有真正釋出記憶的功能
但是其實也有很多人試圖在以ChatGPT為基礎
打造有記憶能力的GPT
舉例來說你可以看一篇paper
叫做Memory-GPT
它就會展示大型語言模型
怎麼加上RAG讓它有這個記憶的能力
好 那以上呢 講的是一些對於AI Agent的想像
那如果你覺得講到這邊很模糊 沒有很具體的話
那我們來舉一個具體的例子
這邊又要講一個福利連的莫名其妙的故事了
這個故事是這個樣子的
大家都知道在福利蓮裡面有一級魔法師考試
一級魔法師考試的其中一關
就是要攻略靈洛的王座這一個迷宮
那在進入這個迷宮之前 因為進入這個迷宮很危險
所以在進入這個迷宮之前
主考官呢 就發給每人一個逃脫用哥列姆
逃脫用哥列姆是什麼呢
他是一個裝在瓶子裡面的泥人
如果你遇到危險的時候 就打破這個瓶子
哥列姆呢 就會救你 帶你離開這個迷宮
所以在迷宮裡面呢,在一級魔法師考試進入迷宮之後,有一個考生叫艾戴爾,他就遇到了染折的複製體,所以他就受了重傷,所以他就決定放棄這張考試,他就打破了這個瓶子,那這個哥列姆呢就出來,一開始出來的時候他是坐著的,感覺正在做一些運算
然後這個時候呢 染折的複製體就對艾黛兒發動攻擊
然後這個哥列姆呢 就幫艾黛兒擋住了染折的攻擊
擋住了染折複製體的攻擊
那艾黛兒還讚嘆了一句
他還能確保使用者的安全啊 真是太方便了
然後哥列姆就把艾黛兒抱起來 逃出了迷宮
那這個哥列姆是怎麼做的呢
你可能以為哥列姆是用魔法做的
我告訴你 在這個動畫裡面有講
這個哥列姆呢 是列努林這幾年才開發出來的
你想魔法的發展都已經有上千年了
為什麼這幾年才開發出可以自動運行的哥列姆呢
那是因為哥列姆背後是用大型語言模型在運作的
他是一個用大型語言模型運作的AI agent
那個列努林就是發現了大型語言模型以後
決定把它加到哥列姆裡面
才能讓哥列姆可以做自主運行
我們來看一下這個哥列姆是怎麼做自主運行的
他是一個AI agent
所以他需要有一個終極目標
他的終極目標是什麼呢
我想他的終極目標可能是
你會自律行動安全的把考生帶出迷宮
那哥列姆剛創造出來的時候
他是沒有任何記憶的
所以他的記憶是空的
現在他看到的狀態是
眼前是遭遇複製體攻擊而受到重傷的艾黛兒
那你可能會想說
可是實際上的狀況看到的是畫面啊
要怎麼把它變成文字輸入給語言模型呢
也許你可以用的是Image Captioning的技術
現在已經有很多技術
可以把圖片轉成描述的文字
所以也許你可以呼叫一個Image Captioning的Model
把現在的狀況翻譯成文字
那大型語言模型就可以讀到現在的狀態
是眼前是受到重傷的艾黛兒
接下來呢
這個語言模型就可以根據他的終極目標、記憶、還有狀態
來制定接下來運行的計畫
現在的GPT-4可以做到類似的事情嗎?
可以做到類似的事情
我就給他一個目標
他的目標是你會自律行動安全的把考生帶出迷宮
然後他的記憶目前是空白的
他的狀態是看到受到重傷的愛戴爾
接下來你只要叫他請產生行動計畫
他就會自動產生一個行動計畫給你
他的行動計畫第一點是
立即評估艾黛萊的傷勢
第二點是提供急救
看起來都非常的合理
現在呢有了這個行動計畫以後
我們就把這個行動計畫給大型圓模型
叫他按照這個行動計畫進行執行
大型語言模型可以按照計畫進行行動嗎
完全可以
你就把剛才的目標、記憶、狀態
還有剛才產生的這個行動計畫呢
給大型語言模型
接下來你跟大型語言模型說
請採取下一步行動
那我這邊有強調說執行一個動作就好
不然他會把所有動作都執行完
我叫他採取下一步的行動
他就說那我下一步的行動
就是評估愛戴爾的傷勢
比如說他會仔細檢查身上的每一處傷口
看有沒有大出血或骨折的跡象
會看看他是否還有意識
是否有震盪或其他休克的跡象等等
那這邊的一個難題是
這邊產生的是文字的行動
這個文字的行動要怎麼轉成具體的行動呢
一個機械人 一個哥列姆 一個泥人
他要怎麼按照這串文字
真的進行行動呢
這就是整個AI agent
我覺得目前最困難的地方
你可以找到很多的論文
在嘗試把這些文字的指令轉成真實在物理世界或在虛擬世界可執行的行動
我這邊引用了一篇今年二月的論文
他就說這個Agent要有兩個
一個叫Slow Agent負責高層次的行動
Slow Agent它產生的指令都是人類可以理解的自然語言
但是我們還需要一個Fast Agent
Fast Agent就是按照Slow Agent的指令
去想辦法真的執行這個行動
舉例來說
他可能可以訓練一個類神經網路
專門執行特定的行動
或者是這個Fast Agent
可以乾脆就產生一段程式碼
運行這段程式碼以後
把自然語言
本來用自然語言描述的行為
轉成在物理世界
或者是虛擬世界真的可執行的行動
那到底做起來怎麼樣
我把論文的連結留在這邊
給大家參考
至少他看起來是可以運作在這個
Minecraft的虛擬世界裡面的
那總之假設我們的現在的行動是可以執行的
真的能夠評估艾黛的傷勢
那在評估傷勢的過程中
外界的環境可能是會改變的
比如說會有什麼樣的改變呢
不要忘了染折的複製體還在那裡
所以呢染折的複製體
可能會襲擊哥列姆
所以哥列姆受到染折的複製體的襲擊
所以怎麼辦呢?現在外在的狀態變了
所以計畫也應該跟著改變
那這個語言模型可以透過反思的方式來改變它的計畫
如果你想要看語言模型怎麼改變計畫
你可以參考一篇叫DEPS的paper
它也是運作在Minecraft裡面的
讓語言模型在Minecraft裡面可以根據環境的變化
改變它原來的計畫
那我們就來試試看當狀態改變的時候
之後計畫是否會跟著改變
所以我現在就告訴語言模型
除了剛才有的那些資訊以外
告訴他現在執行的動作
正在評估愛戴爾的傷勢
然後有新的狀態,這個狀態是
在評估傷勢的時候遭遇複製體的襲擊
請產生新的行動計畫
接下來呢
這個語言模型
就可以產生新的行動計畫
發現他新的行動計畫的第一點呢
是要保護愛戴爾
因為評估傷勢現在已經沒有那麼重要了
那麼重要了
所以真正更重要的事情是
他要優先保護愛戴爾
他就可以產生一個新的行動計畫
那除了產生行動計畫以外
這個語言模型能不能夠反思剛才的狀態
然後得到一些未來可以用的經驗呢
也是可以的
如果你想要知道語言模型
如何透過反思取得未來可用的經驗
可以參考 React 跟 Reflection 這兩篇 paper
那我也把論文的連結留在投影片上
我們就來實際試試看語言模型能不能夠產生有用的經驗
語言模型如何產生經驗呢
你就下一句指令告訴他
簡短總結值得記下來的經驗
他就反思之後列了幾點他覺得應該記下來的經驗
包括要對環境有警覺性等等
而有了這些經驗以後
如果再遇到同樣的狀態
模型就可以根據過去所得的經驗產生不同的行動
這些經驗會影響模型採取的行動
那我們就來看看剛才實際的例子
看看哥列姆的例子
看看模型是不是真的會因為經驗而改變它的行動
這個是模型的目標
它現在有記憶了
剛才記憶是空的
現在是有記憶的
在有了記憶的情況下叫它產生行動計畫的時候
它現在行動計畫的第一點
第一點仍然是評估愛戴爾的傷勢
但是第二點呢
是要保持高度的環境警覺性
剛才在沒有記憶的時候
他的第二點是立即施予急救
但現在他的第二點呢
變成要保持高度的警覺性
然後接下來根據這個計畫呢
我們來產生下一步的行動
如果在沒有任何記憶的情況下
這個大型語言模型
他下一步的行動
是會立即評估愛戴爾的傷勢
那如果有記憶的話
他的行動就會略有不同
你看他這邊多加了一句話
就在評估愛戴爾傷勢的過程中
我也會保持高度警覺
注意四周是否有複製體
或其他潛在的威脅
所以有了記憶會影響
有了記憶有了經驗
會影響這個模型的行為
讓他採取更好的行為
他可以透過他的經驗
改變他未來所會採取的行動
所以這個就是一個
想像中的未來AI agent
可以做到的事情
依據今天大型語言的能力
我相信其實AI agent
很有可能在一兩年後
你就會看到滿坑滿谷的AI agent
生活在我們的周遭
而如果你想要知道更多有關AI agent的事情的話
我這邊放了一篇overview paper
然後這一個圖是從overview paper裡面截取出來的
就是作者想像了一個未來
充斥了AI agent的虛擬世界
好 那這個部分呢 我其實就講到這邊
那我們還有大概五分鐘的時間
我就可以來回答一下同學們的問題
