[Music]
hi my name is Tao I'm from National
Taiwan University and come to the
supervision of Professor Ian Lee and
professor
Connie Lee I'm here to introduce our
work towards analyzed speech recognition
and synthesis with quantized speech for
the station learning this work is
completed by me with Alexander's
collaboration first let's take a look at
the speech signal it is continuous in
time and can change dramatically during
the whole sentence however the
underlying phonetic content of speech
signals changes smoothly across time
this it is easier to learn the underline
phonetic content by model our goal is to
learn discrete speech from station which
captures the phonetic information and
perform phony recognition and speech
synthesis according to the discrete
speech from station one way to achieve
this is performing the following two
steps first for each audio frame we
categorize it into a finite number of
recognizable clusters according to how
it sounds we use different colors here
to resend different clusters here each
cluster represents a phoneme and we call
this phonetic clustering second
according to the clustering results the
furthest point the audio into smaller
segments by merging frames which are
from the same cluster we code this
temporal segmentation we can use a
single vector to represent a cluster
because each audio segment has made TF
clustered a speech signal can be
represented by second sub factors which
we coat as speech one station auto
encoder is a model good at linear of
stations the typical auto encoder has
two components encoder and a decoder
given an Emporium the encoder encourage
audio frame into your hidden factor
afterwards the decoder texts a sequence
of
factors and reconstruct the interview
this hidden vectors surely contains
information about audio since the
decoder reconstructs the audio with only
information from those hidden vectors
however there are two more things to do
to change I'll go first
the encoded factor could be any array of
value vectors
this gives infinite number of choices of
the such factors so we need to limit the
diversity of the encoded vector space
take a finite number of recognizable
clusters and this could be done by
phonetic clustering second like we said
we want to represent an audio segment
from the same cluster by a vector
however the 90 photo encoder turns each
audio frame into a vector to fix this
temporal segmentation may need to be
performed so to make things clearer
given an audio the typical auto encoder
in cows each audio frame into a hidden
vector the hidden vector has infinite
choices and a sequence death of the
hidden vectors will be the same as the
input audio frames if we apply the
proposed phonetic clustering and
temporal skew mutation the hidden vector
well has finite choices and the
resulting hidden factors will have
roughly the same sequence death as the
underlying phone in circles now let's
see how the proposed framework works
first we maintain a codebook where each
entry a pet is a real value vector we go
an entry a code this vectors are
learnable that is it could be updated by
gradient descent then give an audio
input we first used an encoder to encode
each audio frame into a hidden vector
like no more auto encoder then for each
hidden vector we can peel the distance
between ed and edge code of the codeword
after this protect the code with the
smallest distance out by repeating this
for each hidden vectors we obtained the
code sequence now we
Oh sequence of finite possible choices
we merge consecutive same codes and we
got a show to recall sequence who stands
as roughly equal to events of lanta wife
on the seconds this is the temporal
segmentation after this pass the shorter
consequence to a decoder the decoder
will try to reconstruct the impure do
now the problem is how to make sure each
code is correspondent to a phoneme to
make codes to phonemes we do the
following things first we make the size
of the codebook equal to the number of
phonemes and we assign each code of
funding second for each hidden vector we
define probability it belongs to a code
as follows we first compute the distance
between the hidden vector and each code
then the sub main operation is performed
to convert distance into a probability
distribution the lower the distance the
higher the probability he gives third we
maximize the probability that the output
code sequence being the true Anto - in
circles and we kill the - of this
probability recognition loss now having
this model how do we recognize our
synthesizes page let's first simplify
the model a little bit we use the block
here to resent the phonetic clustering
operation and we remove the temporal
inpatient illustration feature so let's
see how to perform recognition the
recognition capping done as this we
encode the audio into some hidden
vectors and phonetic clustering is
performed which gives us the code
sequence of finite choices after this
temporary computation is performed to
obtain the recognition result this is
the recognition process
how about synthesis given a phone in
seconds we look up the code book and the
corresponding codes are selected the
synaptic
tells seconds
to decode it to synthesize this page
this is the text to speech process the
model training includes three parts the
first part is the reconstruction part
which needs no label data and we have a
reconstruction there's just like no more
auto encoder in this part the decoder
learns how to reconstruct audio given
this great representation selected
according to the distance between
encoder outputs and cows in the codebook
the encoder and the codebook are also
updated to improve the reconstruction
quality the second part is the
recognition part and we have a
recognition those mentioned earlier to
be more specific we use CTC those here
to maximize the likelihood of Lamar doe
uppity truthful in sequence this part
meets the smile amount of lipo data to
make codes to phonemes the third part is
the synthesis part and we have a Texas
Beach loves this part oscillates the
small amount of label data to make the
decoder knows how to synthesize odile
given discrete representation of
Truphone in circles so in total there
are three doses to train the proposed
model and only on labels page and few
labels page with transcriptions are
needed to train the model okay so that's
move on to experiment section the data
set we used in this book is a rigid page
it is English data set from single
female speaker
it has about 24 hours data and we used
about 22 hours data from add as the
unlabeled data the part of the remaining
data I use as the label data the encoder
of our framework is composed of seven
collusion layers
plus two others TMS the decoder is the
turquoise from tomorrow which is the
seconds two seconds model with attention
mechanism the first experiment has
representation analysis in this part we
take a deep look on the representations
of powerful names
the left part is t-sne visualization of
representations learned by our model the
right part is the IPA vowel chart to
find it by linguists the x-axis of this
chart means how close the tongue is to
the front teeth and Y axis of this chart
means how close the tongue is turret
roof of the mouth we can see that the
closed vowels group at the upper left
region of the TSN plot in contrast the
open house located at lower right region
the representation visualization is
parallel to the IPA vowel chart this
demonstrates the learn representations
contain a certain degree of a phonetic
information next we show the phoneme
recognition can benefit from cher Vince
patient's baseline model is a normal DC
model having the same architecture as
our encoder with that additional
projection later to predict probability
of a phony set table shows the phony
array and different amount of label data
we can see that our model fits the
baseline model for any amount of label
data in our model the speech to speech
reconstruction bridges encoder and
decoder with shared representations
however this is not the case for the
baseline model we suggest it as the
share of stations that make our model a
perform the baseline for the
text-to-speech experiment we can pay our
model at 7:22 and speech chain speech
chan is a new learning framework for
speech recognition and Texas Beach
well the two modules do not share of
stations at the table we show the
modders MO as a different amount of the
label data but highly the mo has the
highest speech quality of LaMotta output
the ground truth audio gets a very high
MLS which can be seen as an upper bound
but they are 20 minute demo data
available our model outperforms our
mother's tackle drum to model can hardly
generate intelligible speech in this
city
this indicates the share of station can
also improve the speech synthesis
finally that's still a quick demo of our
motto trenwith only 20 minute
labor data for phoneme recognition we
pass an audio of the sentence including
a few of major importance to the model
the Truphone in sequence is those the
model output is very similar to the true
one so the text to speech synthesis
which also passes the same sentence
including a few of major importance to
the model and the output of our motto
sounds like liz including a few of major
importance for more details and demo
Odo's please refer to out paper and the
demo page thank you for listening
