好 我們就講下一個主題
我們接下來要講的主題呢
叫做Self-Supervised Learning
在講self-supervised learning之前呢
就不能不介紹一下芝麻街
為什麼呢
因為不知道為什麼
self-supervised learning的模型
都是以芝麻街的人物命名
我今天特別穿了芝麻街的T-shirt
給大家看一下
芝麻街的T shirt
看不清楚的同學呢
或線上的同學呢
這個照片在這邊是同一件
這個是我本人
然後這個這些角色就是芝麻街的角色
這些芝麻街的角色
都是些什麼樣的模型呢
我們先來看看他們的名字
讓我們實際了解他們做的事情之前
先來認識他們的名字
這一個紅色的怪物叫做ELMO
self-supervised learning裡面
有一個模型叫做
Embeddings from Language Modeling
它是最早的self-supervised learning的model
它的縮寫就叫做ELMO
那有了ELMO以後
後來又有另外一個動物叫做Bert
這個是大家最耳熟能詳的Self-Supervised Model
Bert它是Bidirectional Encoder
Representation from Transformers
的縮寫
ELMO跟Bert都是芝麻街的人物
有了兩個芝麻街的人物以後
Bert他最好的朋友就是這一隻
他是誰呢 他叫做ERNIE
那事實上有了Bert以後
馬上就出現了兩個不同的模型
都叫做ERNIE
其中的模型縮寫是這樣啦
Enhanced Representation
from Knowledge Integration
然後它的縮寫是ERNIE
我就問說現在縮寫這樣都可以了嗎
你根本就只是想要叫做ERNIE而已啊
你可能以為這個已經很荒唐了
但是後來這一隻動物
他叫做大鳥 Big Bird
就有一個模型叫做Big Bird
Transformers for Longer Sequences
現在完全要放棄要湊字了
已經要放棄要湊出一樣的名字了
直接就把它叫做Big Bird就結束了
所以我們現在在這個
self-supervised learning的model裡面
都有一堆芝麻街的人物
Cookie Monster還沒有人去湊他啦
就等著你來湊Cookie Monster就是了
那講到Bert又要提到進擊的巨人
以下會提到進擊的巨人的劇情
我個人是覺得沒有雷
但是如果你怕被雷到的話
你就把眼睛閉起來 耳朵摀起來這樣
我個人是覺得沒有雷
好 接下來呢 要提一下進擊的巨人
為什麼要提到進擊的巨人呢
因為Bert是一個非常巨大的模型
有多大 它有340個million的參數
也許我直接給你
一個340 million的數字
你沒有什麼感覺
那我告訴你作業四的模型有多大
作業四的模型它也是一個Transformer
它只有0.1個million而已
如果你覺得作業四的baseline很大
它只有0.1個million
Bert是它的三千倍那麼大
所以它確實是一個非常巨大的模型
所以當我們想到Bert的時候
我們就想到超巨大的巨人
一腳可以踢破瑪莉亞之牆
的那一種超巨大的巨人
但是我有一個發現
我發現超巨大巨人的控制者是誰
你們知道嗎
是貝爾托特對不對
貝爾托特的名字裡面是有Bert的啦
這個絕對不是巧合
我相信這個絕對不是巧合
那你可能會覺得很困惑
就是這個貝爾托特他出現的時間
應該在Bert這個模型被提出來之前啊
Bert模型什麼時候提出來的
2018年的年底
你知道進擊的巨人10年前就有了
所以貝爾托特出現是在Bert之前
但為什麼超巨大巨人
會由貝爾托特所控制呢
那就是因為諫三創
他其實也有進擊的巨人的能力
所以他能夠知道未來事情
所以他就把超巨大巨人的控制者
命名成貝爾托特就這樣結束了
好 就這樣子
所以這個就是Bert
但是其實你可能覺得Bert已經很大了
但是還有更多更巨大的模型
這個時代就像是被發動的地名一樣
有很多超巨大的巨人從地底湧出
有哪些超巨大的巨人呢
最早的是ELMO
ELMO也有94個million
這邊用這些角色的身高
來代表它的參數量
Bert大了一點 340個million
遠比你在作業裡面做的
大了一千倍以上
但是它還不算是特別大的
GPT-2它有一千五百個million的參數
但就算是GPT-2
它也不算是太大的
這個Megatron有8個billion的參數
GPT-2的8倍左右
後來又有T5
T5就是有一款福特汽車叫T5
雖然T5是Google做的
跟車子也沒有什麼關係
那這邊就放一個福特汽車
T5有11個billion
但這也不算什麼
Turing NLG有17個billion
那這也不算什麼
GPT-3有Turing NLG的10倍那麼大
它有10倍那麼大
到底GPT-3有多大呢
如果我們把它具象化的話
它有這麼大
我們先把這些模型的大小轉換成身高
Bert 340個million
我們就當作它有1公尺高
所以Bert是這個 這個是我本人
那GPT-3有多大呢
它就是背後的台北101那麼大
所以從Bert到GPT-3
模型從一個人高變成台北101那麼高
其實GPT-3還不是最大的模型
我現在看到最大的模型
是Switch Transformer
它有1.6個T那麼多啊
還有上兆個參數啊
它比GTP-3又再大了10倍
這邊放了一個Switch
但是它跟任天堂其實沒有什麼關係
你也可以想像說
其實這也是Google做的
我們把它的論文放在這邊給大家參考
如果你有看到更大的模型在跟我講
所以現在已經有上兆個參數的模型
我聽說人腦是有一千億個神經元
如果我們把神經元對應到參數的話
這個Switch Transformer裡面的參數量
竟然是比人腦的神經元的數目
還要多了
那這些巨大的模型
他們都是在做些什麼呢
那在等一下的課程裡面
我們會講兩個東西
我們會講Bert跟GPT
我們會跟大家介紹這兩個模型
告訴你說
這些Self-Supervised Learning的model
它們做的事情是什麼
那我想我們在這邊
還是正好告一個段落
還是休息一下就好了
我們十分鐘以後在回來
