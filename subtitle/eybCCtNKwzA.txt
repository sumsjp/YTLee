臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw/
好那各位同學我們就開始講今天的補充的內容
那各位聽得到我的聲音嗎
好那我就開始講了，那以後從這個禮拜開始，每個禮拜四都會是由助教來講新的內容
那這禮拜我要講的是 graph neural network
那因為這個是第一個禮拜遠距教學所以有些東西還在試，那如果有不方便就請同學就是多多包涵
那一開始就是各位等下如果有問題的話
因為我如果要看 google meet 的那個聊天室會有點麻煩，所以大家可以直接用 slido 然後加進去
然後我會隨時看 slido 有沒有問題，然後有問題的話我就會直接回答
那我們就開始講今天的課程
那我們今天會進行的東西大概就是我們會先講一下
簡單介紹一下 graph neuron network 他在做的事情是什麼東西
然後簡單介紹完之後
我們會來看一下我們今天講的內容具體而言有哪些東西
然後介紹一些比較重要的 task
還有 dataset 還有 benchmark
今天的投影片是會錄影的，我不確定有沒有錄到
對我現在是有在錄影所以應該是沒有問題
然後
接下來我們講完這些 task 之後我們會講
最重要的就是兩個方法
我們是怎麼使用區間端這個方法
區間端有兩種方法，講完後我們會講區間端的應用
好那我們就開始今天的介紹
那 graph neural network 這個詞它包含了兩個關鍵字，一個就是 graph
一個就是 neural network
那我相信既然各位是修 machine learning
然後也寫過幾次作業，各位對 neural network 一定有一定的認識了嘛
這就是一個非常簡單的 multilayer perceptron
他有一個 input layer output layer 然後還有中間的很多 hidden layer 然後 hidden layer 是由一些 weight 還有 bias 組成的一個 model
那除了這種最間單最陽春的 neural network 之外
那各位在寫作業的時候應該也有寫到
用到 CNN
或者是 RNN 那甚至是如果你有想用的話你可以用更潮一點點的 transformer
那這些都是 neural network 的例子
好那如果各位不知道那些是什麼的話，沒關係就是
不重要今天也不會用到這些東西
那問題是大家知道 neural network
我相信大家也知道 graph 是什麼東西
如果你有修過資料結果與演算法的話，那你一定對這個東西非常的熟悉
這是一個 rb tree，那 rb tree 他就是一種特殊的 graph
那我們知道 graph 他是一種資料結構，這個資料結構他是定義在兩種東西上面
一種是 node 一種是 edge
所以在這棵 rb tree 上面，他的 node 就是這些紅色黑色的東西
那我們可以看到 node 他會有不同的特性
有紅色的節點也有黑色的，紅色的節點也有黑色的
然後我們也知道 edge 他也有不同的性質
他的 edge 的性質就是這些方向，這些從 root 往下指的這些東西
那所以 node 有他的性質，edge 有他的性質
那各位如果說你沒有修過資料結構與演算法就是說不知道這個是什麼東西
沒關係那我相信各位應該有修過高中化學，高中化學你一定有畫過長得像這樣的東西
你不一定認識他是什麼分子，但是你就知道
他應該是個分子，他就是某個分子
那這個分子你也可以把它看成 graph
那這些 graph 上面的節點就是這些你沒有畫出來的碳原子還有氫原子
然後他這個 graph 上面的 edge 就是
原子跟原子之間的鍵結
好那我們在這個 graph 上面 ，如果我們把它當成 graph，我們可以看到他的 node
也是有不同性質
他有不同的原子，那這個不同的原子就是不同的性質
那他的鍵結，他的 edge 他也有不同的性質
他有一級鍵，或是一又二分之一級鍵
那就算他全部都是一級鍵，碳氫鍵的一級鍵跟碳碳鍵的一級鍵他也是不一樣的一級鍵
好那就算他是一樣的，他都是碳碳鍵
那也會因為他的碳碳鍵旁邊他的碳是接著什麼東西而有所不同
所以我們就可以看到 graph 他，他的 edge
不只是會因為他兩邊連結的節點不同
他還會因為這個節點他連結到其他的節點的不同而有所不同
所以 graph 他是一種非常有一種連在一起的結構
他就是一個連在一起的東西
好所以這是另外一個 graph 的例子
那大家可能會覺得這兩個 graph 他都非常的小
那其實我們在處理的 graph 他的大小可以是更大的
這是一個 protein protein interaction 的 graph
那每一個，上面每一個節點代表的都是身體某一個組織他的某一種蛋白質
那兩個蛋白質他有連在一起的話，他如果有一個 edge 就表示說他們兩個是有 interaction 的
那這個 protein protein interaction 的 graph
也是我們會處理到的 graph 之一
好那如果各位說這三個東西你都不熟
你覺得這個好像脫離你的日常生活太遠
那其實日常生活中也有一個大家一定看過的 graph 就是
台北捷運路線圖
那台北捷運路線圖他的節點就是這些捷運站嘛
捷運站有不同的性質，他可能是高架
他可能是地面的
他可能是側式月台可能是倒式月台
那這些 edge 上面的 edge 就是捷運站跟捷運站之間的連線，他也有不同的性質
他可以是高運量的，他可以是中運量的，他可以是輕軌
所以說 graph 他最重要的我們要強調的就是
他是由節點跟邊組成的，節點有節點的性質，邊有邊的性質
好那接下來我們要問的是說
好那我們為什麼會需要用到 graph neural network 呢
然後另外我們要怎麼把一個 graph 塞到一個 neural network 裡面
今天 neural network 長這個樣子
那我要把他的 input 原本我們這邊放的可能是一個 sequence
可能是一個文字的 sequence，可能是一張照片
那今天如果我想要把剛剛講的那些 graph 放進來這個 neural network 裡面
然後我要把他餵到這個 model 裡面，那我要怎麼樣讓 model
會同時知道他是這個 graph 的 structure
而且我也要讓他知道每個 node 跟 edge 的 feature 是什麼
所以這就是 graph neural network 他要處理最大的一個問題
好那我們先跳過這個方法的部分，我們先來看 graph neural network 可以做什麼樣的事情
那第一個假設說我有一個，有一大堆不同的分子
然後我想要訓練出一個 classifier，然後告訴我說這個分子會不會導致突變
所以他就是一個很簡單的 supervise 的 classification 的問題
那我的 dataset 就是我有很多 label 好的 data
告訴我說這個分子他是會導致突變的
有些分子是不會導致突變的
然後我希望最後 train 出一個 classifier 然後這個 classifier 可以讓我知道說
我隨便今天丟進去一個他沒有看過的分子
然後我想要問他說這個東西會不會導致突變
好那這就是一個 classification 的問題
那除了 classification 的問題，graph neural network 他可以處理的
還有其他問題像是 generation
好那現在大家知道這個新型冠狀病毒非常的嚴重
然後大家都很努力的在開發新藥
那開發新藥的過程我們就需要投入大量的金錢還有
一些資源
那如果我可以把開發新藥的過程
因為開發新藥我們要做的事情就是要找出一種化學的分子
然後這個分子可以有效的對抗病毒
那如果我可以用一個 neural network 然後
我 train 出一個 generator
然後這個 generator 他可以生成出我想要的分子
那我們就可以省下很多很多開發的資源
那所以想法就是這個樣子，我可不可以想辦法 train 出 generator
這個 generator 或者說他是一個 decoder ，他只要吃一些 random node
然後再加上我要求他生成出某些特定
有某些特定條件的一些分子
然後他就可以給我這些分子出來
那這個 y 這個 commission 他可以是說，我希望他是一個好合成，因為我最後
就是生成出來的這個分子，我希望他是一個可以被我合成出來的藥物
所以我可能希望 y 他的 commission 他是一個很好合成的藥物
或者我希望他是一個成本很低的藥物，或者我希望他是一個
他是一個新藥
我不希望他這個 model train 出來最後吐出來全部都是 rcp，這是沒有任何意義的
所以我希望他是一個新藥
所以我可以加很多不同的 commission
所以我只要 train 出來一個這樣的 model
那我就可以做到這件事情，那當然這個 model 他不一定生成出來都是合理的化學分子
像是這些畫紅色的線就是
他生成出來是違反就是違反一些化學原理的
好那這個我們等下也會講這個東西可以用什麼方法來做到
好那這些東西我們剛才前面講的兩個例子
我們的 input 或是我們的 output 大家都很清楚他就是一個 graph
所以你可以很輕易地理解為什麼我們需要用 graph neural network
但有些東西你看起來他好像不是一個 graph
你也完全不會覺得他是 graph
但是我們其實是可以用 graph neural network 來處理他的
像是什麼呢？像是下面這個例子
下面這個例子是最近日本很紅的一部日劇
他是叫做特修斯之船
那他裡面就是有很多角色，劇情的部分我就不多講
反正他要做的事情就是他想要找到，這個是主角
然後他想要找到陷害他爸爸的兇手是誰
好那就是找兇手這個過程呢
我們可以把它看成一個 classification 的問題
他有這麼多角色
那每個角色他都有他的一些
每個角色我們叫做他是一個 entity
那 entity 他有他的 attribute
所以他的 attribute 就包括他的名字
可能包括他的年齡，可能包括他的職業
可能包括他的在哪裡出生等等
好那這些東西我就可以拿來當作是一個人的 feature
然後 train 出一個 classifier
那這個 classifier 我們 train 的方法很簡單 我們就是丟進這個人他的名字還有剛剛講的那些 feature
然後我們就會告訴這個 classifier，假設我們已經有 label
就是主角一定會知道哪些人一定不是兇手嘛
然後他就可以訓練出一個 classifier
告訴他說這個人不是兇手，這個人不是兇手
這個人不是兇手，然後最後 train 完
一個 classifier 後，然後最後問他這個人會不會是兇手
所以我在 testing 的時候我就可以做這件事情
好這件事情當然是一個，你當然是可以這樣做
因為你有這些 feature
你就可以直接 train 出一個 classifier
但是有一個問題我們是不是有忽略了什麼東西
或是有沒有什麼我們可以利用的資訊但是我們沒有完全利用到
好那各位在看日劇或韓劇的時候一定會
或是什麼小說嘛
有一個很重要的東西就是角色的關係
角色跟角色之間是有很複雜的關係的
他是一部穿越劇，所以他角色看起來很奇怪
好他們倆是姐弟，他們倆是夫妻
他們倆是師生關係，他們倆是同僚關係
他們是鄰居的關係
還有很多很多不同複雜的關係
那這些複雜的關係是不是可以讓我們在做 classification 的時候
有一些額外的資訊
可以幫助我們做更好的 model
所以這樣的 underline structure 還有 relationship
就是我們用 GNN 的另外一個原因
我們要怎麼樣同時考慮這個 entity 他的 feature 並且考慮他跟其他 entity 之間的關係
而且我們不只要考慮這兩個 entity 之間的關係
我們要同時考慮所有 entity 之間的關係
所以這個時候我們就必須要用 Graph Neural Network
好那問題是我們剛才講到 graph neural network 他都是非常非常小的
小的 graph
那我們今天，可能我們的 data set 非常的大
他可能是好幾千個好幾萬個節點
那在這樣的狀況下面，我們要怎麼來做訓練呢
好還有另外一個問題，當我們今天的 data set 非常非常大的時候
我們不可能可以把所有的 data 都做 labeling
那當我們沒有辦法把所有 data 都做 labeling 的時候
我們要用什麼樣的方法來同時利用我們有限的 label
以及整個 graph 他的 structure
這樣的狀況下面我們要利用這兩個僅有的資訊然後
train 出一個好的 model
那我們來看下面這個例子
誒好像有人麥克風沒有關是不是呀
大家就是..好謝謝
好那現在這個例子呢
這是一個很簡單的 graph
那他上面有兩種不同的 node
一種是 labeled node，一種是 unlabeled node
那在這個例子裡面
我們可以看到 labeled node 跟 unlabeled node
他的數量是差不多的
但是我們在某些應用上面，我們的 unlabeled node 他可能是遠大於 labeled node
那在這種狀況下面，我們如果想要學到一個好的 node representation
也就是說我想要知道說這個 node
我想要抽出一個比較好的 node feature
那我要怎麼利用我僅有的這些 labeled data
以及他跟他旁邊這些的鄰居這些 structure 的關係
然後來訓練出一個好的 model 呢
那我們可以很直觀地想，近朱者赤近墨者黑
一個節點跟他所連接的這個節點
他應該會有某種相似的關係
或是他會有某一些特定的關係
那我們就可以利用這些特定的關係然後來幫助我們的 model
學習一個好的 node representation
問題是我們要怎麼做到這件事情
我們要怎麼把它鄰居的這些資訊
aggregate 或我們說先不要 aggregate
我們說我們要怎麼用他旁邊的鄰居跟他相鄰的鄰居
然後來幫助我們的 model 做訓練呢
好這就是我們要問的問題
當然我們可以用一個，我們先回想一下
我們在 convolution 裡面做的事情
我們在一個 convolution neural network 做的事情呢
假設我要做 convolution
那我要對這個
對這個綠色的框框我要用一個 3*3 的 kernel
做 convolution
那我就會放一個 kernel 在上面嘛
然後跟他要做相乘相加
然後再來如果我想要算跟這個黃色的做 convolution 的話，那我一樣
把這個 kernel 移到旁邊，然後一樣做相乘相加
好那做完了之後
我就可以得到他在下一層的 feature map 長什麼樣子
可是這件事情我們要怎麼 generalize 到 graph 上面呢
graph 我們有沒有辦法放一個 kernel 在上面
然後給它做相乘相加，weighted sum 的方式來做 convolution
這好像就不是這麼的容易
好像不是那麼容易
好所以我們要怎麼樣把我們的 node feature 放到一個 feature space 裡面
然後我們想要用 convolution 的方式
好那第一種解法，第一種解法就是說我們可以 generalize
我們在 CNN 裡面用的方法
也就是我們在 CNN 裡面做的 convolution 是
我們其實是做什麼事情呢
我們是看他的鄰居，然後我們用他的鄰居來 update
這個 feature map
那第二種方法他比較神奇，他比較玄妙
他用了傳統上我們在做 signal processing 的一套理論
我們先把這個信號轉到 fourier domain 裡面
轉到 fourier domain 以後，我們再把轉到 fourier domain 裡的信號
跟轉到 fourier domain 的 filter response 做相乘
相乘之後我們再做 inverse fourier transform
我們就可以得到我們最後經過 filter 的信號
第一堂課我們會 focus 在第一個 solution
然後我們第二堂課會比較深入地講一下我們要怎麼做
signal processing
好所以我們來看一下我們今天會講到的內容有哪些
第一個我們會講到的內容就是
convolution 也就是我們要怎麼在 graph
這樣的 data structure 上面做 convolution
好那有兩種，一種是 spatial-based
一種叫做 sp ectral-based
也就是我們剛才講的第一個解法跟第二個解法
那第一個解法，我先把我們等下會講到的方法列出來
大概有這幾個方法
那現在大家看不懂 aggregation 是什麼沒關係
我們等下會詳細解釋
然後 spectral-based 我也會講幾個比較重要的代表的 model
然後講完這些東西後我們會來看一些
比較偏理論分析的東西
很少，非常少的理論，因為我也知道大家不會想聽
太多理論，所以我只會講兩個很簡單的 model
好那講完理論的東西那
這麼多的 model 裡面大家最後其實最喜歡用的還是 GAT 跟 GCN 這兩個 model
好我們講完 model 的部分我們就會來看一下
我們要怎麼用這些 GNN 的 model 然後用在一些常見的 task 上面
包括 supervised classification，像是我們剛才前面舉到的兩個例子
或是 semi-supervised learning 還有 representation learning
所謂的 representation learning 就是我想要學到一個比較好的
feature，不管是 node feature 還是 graph feature
再來我們可以，有時間的話我們可能可以講一下
我們要怎麼做 graph generation，就像我們剛才提到的我們要怎麼去合成
我們要怎麼讓 graph neural network 去合成一個新的分子出來
好最後最後如果還有時間的話
我們會講一些 graph neural network 在 natural language processing 上面的應用
聽起來 graph neural network 跟 natural language processing
聽起來像是八竿子打不著的東西
但是就是有辦法把他扯在一起
好那到這個地方看各位有沒有要問什麼問題
那如果沒有問題的話我們就繼續講
好我們現在簡單講一下
graph neural network 上面會用到的 tasks
然後還有 dataset，然後還有 benchmark，那這個東西是
應該是這個月初才出來的，有一組人就是因為
大家在提出一個新的 model 的時候你必須要說明說你的 model 有多好
那我們就是做實驗來衡量說你的 model 他的 performance 有多好
但今天假設說
A 這組人他提出了 A 這個 model，然後他用了 A 這個 dataset
B 這組人用了 B 這個 dataset，C 這組人用了 C 這個 dataset
那在這種狀況下
我們很難去衡量說這個 model，我們很難客觀的衡量不同的 model 之間的比較
所以我們要用一些 benchmark tasks
然後來衡量 model 的 performance
然後還有一個很重要的是這些 benchmark tasks
你在切 training set 跟 validation set 的時候
必須要是一樣的你才能比較
那各位可能在做作業一或作業二的時候已經有發現一件事
就是當你的 dataset 很小的時候你就很容易 overfit
你可能會看到你的分數在 public boat 上面是第一名
可是你在 private boat 上面掉了一百名
這就是因為 homework 1 他的 dataset 非常小
所以如果你的 validation set 切的跟我們的
public data 太像的話
那你就會 overfit 很嚴重
然後你就會以為說你做的很好
所以在 GNN 上面當然你會有這個問題
所以必須要有一些比較好的 dataset
比較大的 dataset，比較公平的比較方式
然後來衡量說今天提出一個新的 model 他到底有多好
好那衡量的方式，他的 tasks 包括了以下這五個
那以下這五個我想等我們介紹完所有的 model 之後我們再一起回來看這個部分
我們可以一起比較會比較清楚
所以我們今天先跳過，我們先看
spatial-based 的 graph neural network
那我們等下這節課就是
這節課剩下半個小時我們就
介紹 spatial-based 的 graph neural network
好那 spatial-based graph neural network
我們先回憶一下 convolution neural network 是怎麼做
所謂的 convolution neural network 我們做的事情假設有一個 6*6 的 image
然後我們用了 3*3 的 kernal 來做 convolution
那假設他在 layer i 他的 feature map 長這個樣子
我們想要用 convolution 這個 operation 然後
來算出他在 layer i+1 的這個 feature map
那我們用的是 3*3 的 kernel 假設這個 kernel 長這個樣子
如果我們對綠色的這個框框做 convolution 或是 correlation 那他就是
把對應的位置相乘相加，一個做內積的動作
做完內積的動作我們就可以算出
綠色的這個格子是多少
然後我們也可以
接下來我們就把這個 kernel 往右邊移
我們就可以算出黃色他對應的格子他有多少
好那問題是接下來我們要把這個東西 generalize 到 graph 上面
那假設我有一個 graph input graph 他長這個樣子
然後這個 graph 他在 layer i 的時候他的 feature map 是
長這樣子，也就是他的 node 裡面每個都有一個 hidden feature
那我希望透過一個 operational 的 convolution
然後算出來他在 layer i+1 的這個 feature map
那根據我們剛才的在 CNN 裡面的方法
假設我們現在關心的是我們怎麼把這個 h30
這個 3 代表的就是他是第三個 node
然後 0 是他在第 0 層的 feature
然後這個 1 就是他在第一層的 feature
我現在想要看這個東西我要怎麼 update 成這個東西
那最後一個方法很簡單就像我們剛才講的那個方式是一樣的
就是我們要看他的
就是我們剛剛在 CNN 裡面我們就看他結果
如果是一個 3*3 的 kernel 我們看的就是他九宮格裡面那個鄰居嘛
好所以現在在這個 graph 裡面
這個節點他的鄰居有誰
他的鄰居有這個這個這個
所以我們就用這三個節點
他的這個 hidden feature
然後來 update 這個東西
好那這件事情我們在 GNN 裡面叫做 aggregation
所謂的 aggregation 就是我們用的 neighbor
用 neighbor feature 也就是這些 neighbor feature 來 update 下一層 hidden state
也就是說我們用這幾個東西然後來 update 他
大部分我們在做 aggregation 的時候，除了用 neighbor 之外還會用他自己的 feature
好接下來
假設我做完了很多層的 aggregation 之後
我今天要做的東西不只是我想知道
graph 上面每個節點他的 hidden feature
我想要知道的還有就是
這個 graph 整個 graph 的 representation
那什麼時候我們會需要做這件事情呢？像是我們剛才在做
就是那個綠色會不會突變的那個問題
我要預測的並不是每一個
如果他是一個化學分子的話，我要預測不是每個原子他的 feature
我要預測的是整個化學分子的 feature
那這時候我就必須要有辦法把最後一層的每一個節點的 feature
合起來變成代表他的 graph 的 feature
那這件事情我們叫做 readout 也就是把所有的節點
集合起來然後代表整個 graph
好所以這個東西叫做 readout
所以我就可以拿最後這個東西來做整個 graph label 的 classification 或是 prediction
好那我們來看一下實際大部分的 model 是怎麼做的
好那第一個我們介紹的是 NN4G
好 NN4G 這個 model 呢
他做的方法是這樣，假設說我有一個 input graph
input graph 長這個樣子，然後他有
他是一個有五個節點、五個邊的很簡單的 graph
那 input layer 他每個節點他都會有他自己的 input feature
那假設說你這是一個化學分子的話
那每個原子代表的都是一個 node，那不同的
原子他會有不同的 feature，那你可以用任何
跟原子有關的化學性質來當他的 node feature
那這個 node feature 我們必須要先經過一個 embedding layer
然後來得到一個 embedding
所以我們會先把他做類似 embedding 的事情
那embedding 的方式當然我們會直接用一個 embedding matrix 然後
得到了他的 feature
那對每一個節點都做一樣的事情
好現在重點來了，重點就是我們要怎麼做 aggregation
那我們假設說我們在下層裡面看
這個節點他要怎麼，他的 hidden feature 要怎麼被 update
那他 update 的方式根據我們剛才講就是做 aggregation
aggregation 就是看他旁邊的這幾個節點
他這個節點旁邊有誰呢，旁邊有這三個節點
所以我們要把他們三個東西 aggregate 到這個新的 feature map 上面
那 aggregate 的方式，這個 NN4G 他這個 model 用的是
把這三個東西全部加起來
再經過一個 transform 之後
得到的東西再加上原本的這個 input feature
好所以我們就得到了他的這個
他在第一層這個節點他對應的 hidden feature
好所以我們在講一次他做的方法是什麼，他做的方法是
我們把旁邊的鄰居的東西先加起來
所以他用的是 sum
他加起來之後再加上原本 input 的 feature
好那
接下來我們講完 aggregation 之後我們看他怎麼做 readout 這件事情
那 readout 他做的方法是這樣，假設說我疊了很多層之後
就是 GNN 他也是可以疊很多層的
那疊很多層之後每一層都做一樣的事情
疊了很多層之後，假設我們疊了三層
那我就把每一層的 node feature 全部加起來
各自先相加
所以第一層我把他加起來變這樣，第二層我把他加起來變這樣
第三層我把他加起來變這樣
全部加起來之後各自經過一個 transform
再加起來變成一個 feature
就代表他整個 graph 的一個 feature
所以他 readout 他是這樣做的
好所以 NN4G 的 model 他是這樣做的
接下來我們來看
另外一個叫做 DCNN 的 model
那 DCNN 他的名字叫做 Diffusion-Convolution Neural Network
好有人問說為什麼要直接加起來這樣會比較好嗎
好那為什麼要做相加呢
好如果說我用相加的話
如果我不用相加，你可能也會問我說為什麼不用相加
所以就是你要用什麼方法都可以
那等下還會有更多奇怪的方法
那對我們等下可以看他還有更多奇怪的方法
所以 aggregation 的方法大部分都是用相加
那我覺得用相加有一個很重要的原因是因為
你的 graph 上面他的那個每個節點他的鄰居數量不一樣
所以如果你不用相加的話可能會
很難去處理就是那個...好，很難去處理鄰居數量不同的這件事情
好那有人問說為什麼不要像 convolution 這樣每一個鄰居把他權重加起來
這個是可以這樣做沒有錯
那等下我們也可以看到有人是這樣做的
然後我現在電腦好像有點當掉所以我等他一下
好那還有沒有其他問題
好那我們來看 DCNN 這個 model 他是怎麼做的
DCNN 一樣，假設我有一個 input model 長這個樣子
那他做的方式，他再加一層 update 的方式很簡單
就是我把，我們先看一下這個式子他在寫什麼
這個式子告訴我們說我把
跟這個節點，跟 3 這個節點距離是 1 的這個節點全部加起來
所以跟 3 這個節點距離是 1 的
就是這 123 個節點
他跟他們倆，他跟 v3 距離都是 1、1、1
所以我們把它全部加起來然後取平均，他這邊用的是平均
好取平均後再做一個 weight transform
所以每個節點我都可以做一樣的事情
好在第二層的時候，第二層他要做的事情比較特別
他並不是用這層的 feature 來 update，他是用這層的 feature 來 update
那我們來看他怎麼做
他做的方式是我把跟他距離是 2 的
feature 加起來
好那跟這個節點距離是 2 的有誰
就是距離是 2 的是這個
這個還是一樣是他，然後還有他自己回來距離也是 2
所以我就會把這兩個節點他 feature 加起來
但他加的不是這層的 feauture，他加的是這層的 feature
好
所以在這樣的狀況下面假設我疊 k 層，他就可以看到他的 k neighborhood 裡面的東西
所以我每個節點都做一樣的事情
做完了之後呢
我可以把這些 feature 排起來，就是
這是 h0
這是第 0 個節點，第 1 個節點
第 2 個節點然後第 4 個節點的 feature
然後排成一個矩陣
下一層也可以，然後假設我疊了很多層，那我就會有很多個矩陣把他全部疊在一起
疊在一起，那假設我最後要做 node classification
所以我要算出一個 node feature
那我們剛才有講過，這是一個節點，這是一個節點
所以同樣這個地方他代表的是某一個節點的 feature
那我們就把這個東西拉出來
然後經過一個 transform 以後，我們就把代表這個節點他的 feature
好那另外一個很像的 model 叫做 diffusion graph convolution
那這個 model 他做的事情，剛剛這邊是用什麼，這邊是
那他這邊就把他全部加起來，全部加起來之後
就做完了
好那接下來我們來看另外一個 model 叫 mixture model networks
好那這個東西就是剛才有人問到說
為什麼要直接相加我不能直接取 weighted sum 嗎
就是每個鄰居他應該要有不同 weight，因為每個鄰居他的
重要性應該是不一樣的，如果你直接把他相加起來
沒有考慮到一些鄰居跟鄰居之間的 transform
好那這個 model 他主要就是要處理這個問題
好那他做得方式很簡單就是
一樣我們還是拿剛才那個圖然後假設我們要 update 的是這個東西
那他的鄰居有這三個，那我今天在
做 aggregation 的時候
我不要只是用直接加起來的方式
我定義某個 transform 的方式，來代表他們節點跟節點的距離
好那他定義的方式是這樣
3 跟 0、 3 跟 2 還有3 跟 4 的距離他把他定義成是 degree of 這兩個節點的倒數
也就是說這個節點他的 degree 是 2 所以
然後這個節點他的 degree 是 3，所以這個 u30 就會是 sqrt(1/3)sqrt(1/2）
所以對於每一條邊你都可以定義出他的 u30
然後你再用這個 u30
這個 u30 就代表某一個距離的概念
所以你用這個距離然後經過一個 transform
然後來做 weighted sum
所以你就不要直接相加你用距離來做 weighted
那當然這個距離你可以選任何你喜歡的東西
那接下來這個距離是你一開始就訂出來的
所以你可能會覺得就是一開始就訂出來
還不夠強，應該要讓 model 自己學
那後面會有讓 model 自己學的東西
那接下來講一個蠻有趣的、比較有趣的、也是蠻有趣的 model
叫做 GraphSAGE
那這頁我沒有要講，我只是要講說他的 aggregation
除了用 mean pooling 就是把鄰居全部加起來或是 max pooling 對鄰居的 feature 做 max pooling
他還用了一種是 LSTM，也就是說
他用一個 LSTM，LSTM 就是我們平常在處理 sequential data 的
就是 long short-term memory 的那個 model 然後我們來
把他的鄰居的 feature 餵到一個 LSTM 裡面然後
把最後他的 hidden state 當作是他最後的一個 output
然後拿這個東西來做 update
好那你可能會說 LSTM 他處理的是 sequential data
可是我們拿了一個，對一個節點來說他的鄰居不該有任何的順序可言
所以說用 LSTM 來做的時候
他就會隨便亂 sample 出一個順序
然後每次 update 的時候他都會 sample 出不同的順序
所以他最後可能就可以學到說
去忽略這個順序的影響
然後學到一個比較好的 representation
然後但是用 LSTM 他的 performance 不一定會特別好
那最後我們會比較一下所有 model 的 performance
所以我這邊都會先暫時不講實驗的部分
好再來我們來講 Graph Attention Networks
那 Graph Attention Networks 他重點就是
我不只要做 weighted sum，這個 weighted sum 的 weight 我要讓他自己去學
那他做的方法就是我對鄰居做 attention
那什麼叫對鄰居做 attention 呢
做的方法是這樣
假設我有一個 input graph 長這個樣子
然後這個 input graph 假設我們想要算的是
這個節點他在下一層的 hidden representation 應該要長什麼樣子
那我就對他的鄰居做 attention
做 attention 的方式就是去看他的鄰居
不同的鄰居他的 weight 要怎麼給
那給的方式就是我會去算
用這個跟這個的 feature 然後去算出一個
我們叫做 energy 的東西
然後對這兩個他可以算一個 energy
對這兩個他也可以算一個 energy
然後這個 energy 就代表了對於中間這個節點來說
h0 他有多重要
對於他來說 h20 他有多重要
對於他來說 h40 他有多重要
所以我們透過這些 energy 或是透過這些 weight
我們用這些 weight 來做 weighted sum
然後得到下一層的 hidden representation
所以這個東西叫做 Graph Attention Networks
好所以每個節點我們都可以做一樣的事情
就是對他的鄰居做 attention
好那 Graph Attention Networks 就是我剛才前面有講說大家
最喜歡用的其中一個就是他
好接下來我們來看一個稍微比較理論的東西叫做
Graph Isomorphism Network
那他是最近幾年，因為前面那幾篇
前面那幾個方式大家就是直接用，大家也沒有管說為什麼他會 work
那 Graph Isomorphism Network 他就直接告訴我們說有些方法是會 work
有些方法是不會 work
所以他有提供些理論的證明
那理論的證明我相信大家不會有興趣，那我就講一下他的結論
他的結論告訴我們說，我們在 update 的時候
最好可以用這樣的方式來做 update
那這個方式是什麼？這個方式就是說
對於某個節點 v 他在第 k 層的 hidden representation
他要 update 的方式應該要先把他的鄰居全部加起來之後
然後再加上某一個 concept
乘與他自己的 hidden representation
好所以重點就是我們這邊要用 sum 我們不要用 mean
mean pooling 我們也不要用 max pooling
好為什麼不要用 mean pooling 呢
為什麼不要用 max pooling 呢？我們先看下面這個例子
假設有這兩個不同的圖
那我如果對，我要對這個東西做 aggregation
我要對 v 做 aggregation
那我對 v 做 aggregation 的時候
如果我用的是 mean pooling 那你看
這兩個東西是一樣的，所以他取 mean 跟這三個取 mean 會是一樣的
所以 mean pooling 在這樣的狀況下他就沒有辦法分辨這兩個圖是不同的圖
好那同樣的如果我做 max pooling
他們兩個是一樣，他們三個是一樣的
所以我做 max pooling，max pooling的結果也沒辦法分辨出這兩個圖
那另外
假設像是這樣的圖，他的 max pooling
會有問題嗎？大家可以看就知道為什麼會有問題
所以 mean pooling 跟 max pooling 是會有問題的
所以他就是告訴我們說要用 sum，就是你一定要用 sum，不可以用 mean pooling 也不可以用 max pooling
然後另外再加上中間這個節點的 feature
那這個 epsilon 他是說可以讓 model 自己去學，但是他
他在自己的 implementation 裡面他告訴我們說
用 0 就是 epsilon = 0 的時候其實是沒有什麼差的所以
你可以把它看成是，你在做 aggregation 的時候你就是要把
包括自己還有所有鄰居的東西全部加起來
然後再過一個 multi layer perceptron
然後就可以達到很好的效果
好那大概是這個樣子
到這個地方不知道各位有沒有什麼問題
那如果沒有問題的話那我想我們在這邊休息個十分鐘好了
那等下十分鐘過後我在繼續講後面的東西
