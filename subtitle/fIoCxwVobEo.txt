uh I will introduce Professor W Wong
Professor W Wong is a professor at
University of three he has already
co-authored over 300 papers in machine
learning and the signal processing uh he
is the author of audio ldm semantic
codec and the P which are very very
famous paper in the recent uh codak
domain and uh his presentation title is
neuro audio codec recent progress and
the case study with semantic codec let's
welcome Professor
Wong thank
you um thank you thanks for the
introduction and also thanks for the
invitation um so today I'm going to uh
share with you um some of our recent
work on neuro audio codc in particular
um semantic codc which is uh developed
by our excellent student H new um
together with other phds and
colleagues uh here is the outline uh so
I start with the background about audio
codec and neuro audio codc you know just
to provide some information on you know
how this is done historically and why we
need neuro audio codc and then we
discussed the limitations of a current
neuro audio codex and propose our uh
method to address these challenges and
finally we going to show the result and
also conclude the talk with discussion
on future work uh audio codec um so what
is audio codc of course in general there
are also other data like image inov
video so we also have image and the
video codex but here we are focusing on
audio codc so codec is the term for
compressing or decompressing compressor
or decompressor right um so how this is
done
historically uh
historically the this is done using
techniques for example linear prediction
you can directly play with the wave form
you try to use a simple mathematical
model and based on the uh previous time
samples you predict the samples in the
future right so you use this such simple
models or you could use other techniques
by converting the data into the time
frequency domain using techniques like
uh discrete cosign transform or um
modified discrete cosine transform or
subband analysis you try to convert data
from time domain to the frequency domain
or time frequency domain and then you
figure out which time frequency point of
the data can be discarded based on the
ideas like cyc acoustic masking you know
because humans auditory system is not so
sensitive to some of the frequency
components in the data so that's the
basic idea how we can use the perceptual
redundancy to remove the Redundant data
right so well know codex for example mp3
mp3 is is the codec uh under so-called
impact standard right um so what's the
key idea here so you use um you know
this uh so-called uh um cytic masking
auditory masking right you try to figure
out the energies at each frequency
component in the signal and then you um
figure out which uh frequency component
is masked by the laboring frequency
component or other frequency component
in the signal H so that you can remove
that particular frequency component so
this is type of lity coding where
essentially you you know the signal
cannot be perfectly reconstructed it's
different from uh L this coding where
you can reconstruct the signals for
example one of the wellknown um L coding
method is used in this ZIP file right we
we all use you know compress the data
zip file you can perfectly reconstruct
the data but for you know Nory coding
you cannot actually reconstruct the data
so this is where widen used um in uh
audio coding for uh transmission for
storage for playback uh but one of the
key uh challenges here is that we want
to compress the data to as small as
possible but at the same time to
reconstruct data you know to to as close
as possible to the original signal
that's the key challenges right so how
to this is actually is a dilemma because
this two objectives are conflicting so
if you throw away data you are likely to
introduce artifacts into the signal um
so you cannot actually uh get both right
so this is also the design objective you
can see a lot of text books talking
about how to compress the data how to
reconstruct the data so that's the um
main effort in many textbooks um so with
the conventional audio codex we get the
advantage for example we can explain um
where which component in the signal
which frequenc component in the signal
we throw away we understand very well on
that so this is one of the main
advantage we call explainability right
but uh uh the disadvantage of this
conventional audio codex is that the
compression ratio is already reaching to
a certain limit and uh so that's why
this uh neural audio Cod comes in uh NE
Auto CCT you know uh as already
discussed in the previous talks you use
this uh machine Learning System to
design the encoder and decoder or in
other words compressor or decompressor
right and then you can actually then now
the design choice is uh what type of
encoder and decoder should be used right
a lot of talks talking about that you
know some people are using um gun
Network as in this uh Soundstream
they're using gun Network right to do
the uh compression to compression and
they couped with this Vector
chronization this is the key means
techniques to uh throw away the
information redundant information from
the encoded Vector which is z here uh
and then this yodc or disc I actually
built and also H codec actually I built
on Sound
Stream um and speech toiz you know for
for speech for example so the key uh
research question here you know U what
type of uh encoder and decoder should be
used right uh so this is a lot of work
around that but again back to the
original objectiv is the key here is
that we try to compress the data to as
small as possible but to maintain the
quality as high as possible so that's
the always the objective um so apart
from the compression and decompression
which is very popular for or is one of
the key component in P
Communications um there's also uh other
reason for developing uh neural audio
CCH so one of the reason as explained in
the previous talks is that we can
actually uh toonize that audio data so
that we can use large language models uh
you know we can borrow some kind of
Knowledge from L language models to do a
lot of Downstream tasks so that's that's
actually one of the advantage for
converting uh for for developing this
new audio CeX another Advantage is that
we can actually combine this with text
by using text as condition so that we
can build models for text to music text
to audio or speech models so the third
Advantage would be for example we can
actually uh develop this uh joint
finition models for compressing or
understanding like this uh uh recent
work on audio pal right we didn't
mention here but let's work on that so
there's a lot of advantage of doing that
um by building this neuro a uh but the
limitations of the current neuro codex
is that um the one of the limitation is
that uh usually the Tok rate is pretty
high um we explained earlier that you
know neuro aor has Advantage working
with large language models yeah because
we tokenize the data we can then use n
language models to process the tokens
but the current uh or previous um neural
audio codex often you know produce very
high rate of tokens which means the the
number of tokens per second is pretty
high for example here we have example
for the D a which is uh uh given 600
tokens per second uh so why why this
cause problem uh because when we
actually use this uh combined with neity
model to process tokens then the
inference complexity becomes High the
computational comple be high and also
the speed right so that's cause problem
and another things is that uh uh the
current neural a CeX the Reconstruction
quality is often uh is pool especially
for low bit rate uh low bit rate means
bit rate less than 1 kilobits per second
and often the performance was not
satisfactory so that's why uh these two
big challenges um another uh challenges
actually if you look at um the current
neural audio CeX that the semantic
information um given by these tokens
from this previous audio codex uh new
audio codex is is not very good we
compared uh several your a that uh the
semantic information in the tokens
actually have been lost um so the
motivation of our work is that we want
to actually develop audio codex new
audio codex which has a you know lower
uh token rates and also a better
reconstruction quality at a no uh no uh
bit rates especially below one kilobits
per second and of course improved
semantics information Within These
tokens so that's the our
motivation uh this comes the uh semantic
codc and as I discussed earlier so the
key here is about the design of uh you
know the encoder and the decoder right
the decoder you know people use the can
Network like this uh the previous um um
uh AIO uh new audio codex but here the
the there several contributions is here
one is we actually try to
Leverage The excellent performance of
this uh audio
M as um encoder because this audio M
captures these semantic information
whereare later on where I show you
example why this one could be better
than other uh
encoders um so this actually uh leverage
this so-called uh uh semantic learning
you know there are different techniques
out there you know one is uh contrastive
learning for example and contrastive
learning could be either um Lage this uh
positive or or netive samples you try to
distinguish them right but it could also
be so-called Mas language modeling where
you master the data and then you learn
uh the labels um this is a
discriminative method but there's also
method about reconstructing the missing
or the masked uh component within this
signal so here we actually using this uh
audio M because it provides or it
retains a good sematic information from
the uh from the audio signals so that's
the reason another things that it's is a
model which is used to reconstruct the
data so the Reconstruction performance
is also very good so that's why we are
using this uh audio me um and of course
to to create these code Works uh which
you know we try to to to to represent
the original audio data so we use this
vector chronization k means algorithm
right and you could also use other uh
Vector chronization techniques um but
here we actually use this uh to uh
cluster the uh embeddings from the audio
Mee and here we have this So-Cal two uh
encoder uh structure um is uh con
calulated this these two audio encoders
one is a uh you know this pans based we
create this sematic Cod work another one
is uh we call acoustic encoder so it's a
compilation of semantic encoder and
acoustic encoder the sematic encoder is
trying to capture the sematic
information the acoustic encoder is
trying to capture the acoustic
information or the uh residues or the
detailed information from the audio data
so that's the encod side the decod side
we actually using this uh diffusion
model and when we were writing this
paper this is probably the first work
that is using the division model to do
the uh as the decoder so that's the
difference right and uh yeah there's lot
of details here but you can read the
original paper about just a little bit
more about the um audio M so audio m is
a self suppli pre-trained model
essentially is like uh Ling this Whit
which is very well known in image
processing the we wishing uh uh
Transformer actually what they are doing
is that you take this uh for examp our
case the spectrogram you mask some part
of the uh spectrogram and then you learn
a model to reconstruct the Mask Part
yeah so here the learning objective is
we want to reconstruct it but it turns
out actually the embeds learned by uh
audio M it contains this uh um sematic
information uh we compare this audio
M uh against
V embeddings you could use VA
variational audio encoder we found
actually that this is the scatter PL for
um the
embeddings um from way on the left and
on the right is audio me you can see
that the different colors means that the
EMB bedance from different sound classes
uh you can see that the audio
M uh distinguish where in terms of the
uh sound classes you know you can
directly use this edance to do
classification for example so that's why
the sematic information you know about
sound classes in this case is retained
well well and we also compare with other
uh latent um edance you know this one
seems to be doing better right so that's
about uh the encoder the decoder we are
using is a diffusion model um
most of you probably are very familiar
with diffusion model is essentially
contains forward and reverse diffusion
process you know the forward process you
try to turn uh data which is of unknown
distribution so you don't know what
exactly is the distribution of the data
but you can actually turn that one into
uh um distribution which you know in our
case is um noise right we know the
distribution of noise
we actually turn this unknown
distribution into non distribution which
is ging in this case and then we train a
neuron Network model to try to predict
uh or to reconstruct the data and
basically we predict the noise uh added
in each step so you try to optimize the
model parameters um in our case we're
using this uh unet or Transformer unet
this type of neural network uh uh
architecture
uh and then we to try to use that to
predict the the noise added in each step
right and then we can actually
reconstruct the original um audio data
so so we're actually working this in the
latent space instead of directly working
on like audio samples we are working
with um you know Nan vectors which means
that that one of the main advantages is
that um computationally is much more
efficient
right um so this is uh uh and also in
terms of the generation quality is also
very good here is a figure we can we
borrowed from a tutorial paper in Europe
and so basically uh we are working in
this lat space um so the in the paper
there a lot of details I won't go into
details in terms of you know which um
those function we're using and uh and
how we set up the system
and now I come to the evaluations we are
using uh the performance metrics to do
with reconstruction quality for example
me this is measure the difference
between the reconstructed spectrogram as
compared with the original spectrogram
we also use the shorttime for transform
distance thus also compare the two
spectrograms uh and then we also have
this uh objective uh perceptual quality
measures which is we as
uh Q This essentially measures the uh
MOS mean opinion score um so we also
actually perform this uh World error
rates that for speech okay uh and uh
accuracy you know when we evaluate the
semantic information we're using this
audio uh classification task to measure
how good it is in terms of using the uh
audio edance produced by the neuro audio
codex for classification and we also
perform this uh listening test um the UR
this Masa test this is very well known
in U perceptual evaluation the data set
we used for training uh you can see here
including both speech like uh uh Jer
speeech uh Min song data set that's for
music and also met D DB and Muse DB
that's all for music right audio set is
for General sound and W sound is also
for General sound weave caps uh also for
General
sound and on the Bas line We compare
including in codec uh descript codc and
Hyper codec which already discussed
earlier uh in terms of the size you can
see that the motor size and the motor
size of uh codc we developed this about
uh uh 75 million uh
parameters and uh here we performed
these uh evaluations in terms of
reconstruction quality uh semantic
information and subjective evaluations
right and uh this are the data set we
used for uh uh performance
evaluation and uh for subjective
evaluation we used the 10% of the total
eval ation data so in here we have about
for example 25 music tracks 30 speech
recordings 50 General sound samples we
have uh 10 rers 10 um human iners we uh
recruited and now I show you some result
this is the Reconstruction quality um
here you can see that on with semantic
codec when the bit rate um you know
usually the bit rate is much lower here
we can achieve the bit rate as low as
0.36 um kilobits per second you can see
here the uh if you look at the V score
uh which is about
3.17 uh if you compare with DAC for
example um so the the you know there the
different dics here um if you look at
the DIC
at uh uh for example
1.41 uh the score you get is 3.13 so
which means that even though that the
bit rate of our semantic codc is very
low we actually still get very good
reconstruction quality you know is even
better than uh other codex with a higher
um bit rate okay that's the main message
so it does very very good job in terms
of uh reconstruction H in terms of
semantic information uh now we can see
that the because we performed on
different audio classification task ESC
for General sound we also performed on
Peach uh here yeah Peach recognition and
some other tasks for example speaker
counting you know this is are different
The Voice uh imitation for example you
can see the performance here again uh
semantic codc uh does much better in
terms of uh maintaining the sematic
information um about the sound classes
other codex you can see that the average
ACC here for
DAC uh you know the performance is very
poor actually and you know our uh codex
maintains U very good quality in terms
of uh uh capturing the semantic
information about some classes so okay
you can see the result here so that's
the key message
um so we also performed appalation
studies in terms of the design choice
and here we
compared um you know variable um
vocabulary sizes opposed to fixed
vocabulary sizes and in our training we
have uh I didn't see the details here
but actually we have multiple code Works
um for speech music and general sound
and uh each codebook has a different
size in terms of the number of code
words um for example we have 1,000
roughly 1,000 code words we also have
like 16,000 code code words um so we
have five different uh code works for
speech five different for um music as
well as five for um General sound so we
actually when we do the training we
randomly pick up the uh the code works
and we compared this training strategy
against you know the training strategy
using the fixed uh code codebook size
yeah and we also compared between using
the uh Frozen sematic could work and the
n-able ones so that's also the result
which shows that uh uh you know by
Frozen the uh the code work especially
the um the one we already uh obtain is
better uh in this case um there are some
other operation studies for example in
our um when we train this uh latent
diffusion model the the decoder we
actually used this uh classifier
free um strateg which some of you might
be very familiar with basically the idea
is that um you actually uh um add the
conditions right because when you use
the diffusion model to Recon data
essentially you're learning this
posterior probability you add the
condition into uh the condition is the
embedding extracted from the audio data
but you can actually throw away the the
condition here we are using the
parameter for example here there um um
uh guidance scale which tells you how
much condition you added into uh the
model training so in this case we found
that uh by actually uh
adding uh condition together with
unconditional generation you know is the
best uh uh strateg especially the
guidance scale roughly at uh two or
three you know that's that's a good
choice and we have also performed some
other um appliation study I won't go to
the details but if you look at this uh
um spectrogram plot you can see the
quality of the Reconstruction using the
different uh codex here and different
kiles uh per per seconds in terms of the
bit rate uh SC um is a semantic codc we
developed you can see that it does a
better job as compared with
DAC and uh in codc EC and and so on and
here we have a bit more information
about uh um the Reconstruction quality
which is the horizontal nine and the
vertical is actually the um semantic uh
information
uh in the audio codec um in terms of the
classification accuracy from uh you know
from the code word we used right for
performing the classification you can
see that the red one corresponds to
semantic codc which um you know provide
the best information in in terms of
semantic information at the same time in
terms of reconstruction quality is also
on the top end um and the bit rate uh is
NOA so this is a very good uh audio CCT
you know some of you might not be
familiar with you should actually try
this and also compare with some of the
other models to see how good it is right
the here we also have the Masa scores
given by humans the blue one corresponds
to the semantic CC we proposed and uh
the green one corresponds to the
DAC uh you can see that that the mar
score is much lower than uh you know the
semantic codec and of course here we
didn't compare with some of the other uh
neural codex is because most of the
neural codex they actually the bit R is
much higher we are actually focusing on
if the bit rate is much lower can you
still get very high quality
reconstruction and also retain the
semantic information yeah sorry um
there are a lot of more uh demos on the
website I will suggest you to go there
and have a listen there are many other
different type of uh um sound files uh
quite different from this one I just
played uh including music like a vocal
or uh drum sound um piano sound you know
many different type of sound there um
okay
uh time is running out maybe uh so I
want to uh conclude and uh and discuss
fish work so we have presented the
semantic codc which produces uh you know
audio tokens of more semantic than noia
token can rate more sematic information
and and also works for any lens audio
because we have a you know a mechanism
that is able to process any L of audio
and so this provide very good uh outra
no bit rates audio codc and the future
work we haven't actually
evaluated
um the relationship between the semantic
of the
tokens uh with respect to the language
modeling performance so that that's
quite interesting to study actually um
another things is that we haven't
studied the shorten uh audio token
sequences how this uh Elevate this
language modeling robustness issue right
and we could also specialize this
because we developed for General sound
but you can actually specialize
to the task you have um or the data you
have uh so I would like to thank H to
provide most of the slide here and also
the funding here so this is a takeaway
if you are interested just read the
paper and uh source code out there out
there more demos out there and uh thank
you very
much thank you Professor Wong for the
very good talk and a very good work any
question from the
audience
yeah uh Dear Professor thanks for your
excellent presentations and I have
several questions um the first one is
that I wonder to know uh what is the
generation effici
uh of this
codc generation efficiency um means that
uh generate one second speech using how
many
seconds um and in terms of my
understanding is uh if you use gpus is
real time which means that Generation 1
second is less than one second okay okay
yeah uh but for CPU
no we only used one one so in this uh uh
audio we used two but the the uh because
we use the diffusion model the diffusion
audio ldm we use the audio ldm
too uh so I think uh two maximum enough
actually so you could also use just one
yeah okay thank you uh another question
is that I wanted to know what uh what is
the model size of the
audio codc actually already mentioned
it's a in this case is 75 million
parameters 75 milon okay okay thank so
is I think it's comparable with the one
of the other I mentioned earlier um
there's one is uh here NE DAC is 74 me
oh okay you can see that uh yeah in
codex is 23 meaning uh parameters oh
okay yeah uh the last question is that
uh this this codc um consider the
latency
latency uh do you consider the latency
in this codec for Real Time
Communications yeah I mean um I guess
again is depends on what type of uh um
devices or uh
Computing um facility you have if it's a
GPU as I said processing one second is
less than one second actually processing
one second data is is less than one
second so it's a it's real time uh um
but we didn't really measure the precise
latency um it was trying to see how
exactly it is but in terms of my
understanding that uh so it's it's real
time uh if you know your uh
machine it's a GPU type of uh machine so
it's it's it's a it's a real timee but
if if you are using other machine then
it's uh um it won't be real oh okay
thank we be getting higher yeah okay
okay oh thank you Professor thank
you any other
questions no
uh thanks for great talk Prof W I have a
question about using uh codec for
emotional recognition so recently we
benchmarked uh various audio encoders
for music emotional recognition on both
regression and classification and among
the audio includers we used uh two codex
one is the in codc and one is the DAC
and we uh surprisingly found that
uh the two models significantly did a a
worse job than other audio includers
like a self suffice model and uh
contrastive learning model and also the
uh Vish so we we we think that it might
be because of uh some emotion related
music details got lost during uh
compression and decompression so I'd
like to know do you agree with this
explanation or do we have uh any other
explanation or do we have uh work or
example to uh demonstrate that could
actually work well in Emotion
recognition thank you very much uh I
suspect uh you right actually when we if
you look at one of the uh result um this
table
uh here you can see that U for d for
example uh we
tested 6 KOB per second 1.4 Wat 1
kilobits per second also
0.36 kilobit per second and that's the
average classification accuracy we
achieved using the um you know the the
varus data sets and if you look at there
are also uh music data set there um so
it seems
like um you know the codec or the uh the
code word they produ used I I didn't go
to the details of this um uh codex but
uh there there is a strategy used for
very low bit Rage which might be harmful
for the performance so I think it worth
uh going back to that to see what
happens because that's um you know we we
briefly mentioned in the paper about
that um that's why uh you know there's
you know there's drop in terms of the
performance there
um but actually semantic cc is doing a
good job maybe you should try yeah yeah
yeah we should try that yeah thank you
very much
yeah any other
questions okay we will take the final
question because the time is
limited
yeah hi professor one thanks for the
great talk and I want to ask actually
and I just want to ask a very probably
stupid question and if we're using for
example hoopus or webm because they they
actually don't have any assumptions or
or supervision data so if we simply
using this webm to do transitional cing
and then to extract this um ID and this
this um how to say and this discrete ID
and apply a work code on top of this um
discrete ID whether we could still
reconstruct the the web form um from the
audio data because we know that it's it
actually works for speech data
right uh you mean directly perform K
means yeah uh yes using K means to do
conization to cize the and the hidden
vectors extracted from audio data then
trying to reconstruct it using a
coder I think it still works um that's
actually
the uh convention how this is done yeah
yeah um but the but the problem is that
um some of the details we have be
missing depending on how good is um you
know the
acoustic um in out case you know there's
semantic C but also acoustic the reason
we are doing that is uh you know we want
to capture the symmetric information as
well as the detailed uh information
so this is also one one of the main
objectives in majority of the um audio
compression algorithms how to you know
best uh retain the information from the
data while compress the uh data to the
smallest size as
possible um this the the techniques you
mentioning is still should work but uh I
doubt this will give you the best
reconstruction
quality because that's the uh objective
in telecomunications you know we want to
achieve the best uh Recon qualtity but
of course if it's not for uh
communication it's only for
classification then the semantic
information becomes more important uh
for example the early uh audio codex
like RPC linear prediction coding where
it does a very good job in terms of uh
um capture the key information there you
know if you use for classification it's
fine but if you want to use this to you
know in for communication purpose then
they want people need to hear the sound
the sound quality will not be so good
actually so there's always kind of a
treat off there and you want to maintain
the uh the symetric information at the
same time you want to achieve the best
reconstuction
quality yeah the reason we ask is that
because um we actually looking for a
kind of this discre token could applied
both for various and audios for sound
music speech and then it could be easy
to compatible with na language model so
and as we know yeah I I agree with you
that um there's also difference I didn't
explain uh clearly here so if you look
at this three plots on the bottom right
um this is for uh different type of uh
audio data um so
uh not not this one let me check
whether oh I didn't show here so we have
a a comparison uh in terms of uh Speech
music and general sound and we found
that uh this codec uh does better on
music and speech not as good on General
sound the main reason might be uh for
General sound the uh dynamic range is
quite different also the uh frequency
range are much also wide you know you
know the especially the content the
frequency content the spectral content
are quite different that's probably the
the the codebook we designed is not able
to fully capture all this information
and so that's why why it does not do as
good job for as compared with speech and
and music when we apply to uh General
sound yeah okay thank
you let's thanks uh Professor one
again thank you thank you very much
