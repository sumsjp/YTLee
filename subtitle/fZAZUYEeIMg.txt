臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心
好，我們要來上課囉！
剩下的時間，我們要來進入新的主題
我們要來講分類這件事情
在分類這件事情呢，我們要找的是一個 function
它的 input 是一個 object x
它的 output 是這個 object 屬於哪一個 class
屬於 n 個 class 的哪一個
那這樣的 task 有很多的 application
信手拈來就一大堆
比如說，在金融上
他們可以用 classification 的 model
來決定要不要貸款給某一個人
你就找一個 function，它的 input 是某一個人的 income
他的 saving、工作啊、他的年紀阿
還有他過去 financial 的 record
他過去有沒有欠債阿，等等
那 output 就是要借錢給他，或是不借錢給他
這是 binary classification 的 problem， 也就是 accept 或是 refuse
那或者是拿來做醫療的診斷，比如說
input 就是某一個人的症狀，還有他的年紀、性別
過去就醫的歷史阿，等等
或者是來做，手寫的是數字、文字的辨識
那 output 就是，他生的是哪一種病
自動來做醫療的診斷
比如說，你就手寫一個字給機器看
看到這張圖，這是 "金" 這樣子
你知道，鄉民都叫我 "大金"，這是金這樣子
然後，output 就是
這個字、這個 image，它是屬於哪一個 class
如果你是做中文的手寫辨識的話
那中文有至少 8000 個 character
那就是一個 8000 個 class 的 classification
你的 model 要從這 8000 個 class 裡面
選一個 class 當作 output
或者是做人臉辨識，input 一個人臉
然後告訴他說，這個人臉是誰的
好，那我們要用的是怎麼樣的 Example Application 呢？
其實這個也是寶可夢的例子啦
你可能以為說，我只有前面 predict CP 值那裡
其實，我還有很多其他的例子這樣
好，我又做了一些有關寶可夢的研究
我這個研究是這樣子的
我們知道說寶可夢有不同的屬性
有幾種呢？有 18 種屬性
包括水、火、電阿，還有草、冰......等等
總共有 18 種屬性，到第六代為止，總共有 18 種屬性
我們現在要做的是一個分類的問題
這個分類的問題就是，要找一個 function
這個 function 的 input 就是某一隻寶可夢
然後，它的 output 就是要告訴你說，這一隻寶可夢
它是屬於哪一種 type
比如說，input 給它一隻皮卡丘
它 output 就是雷
input 給它一個傑尼龜 ，它 output 就是水
input 給它一個妙蛙種子，output 就是草
所以是一個 classification 的問題
那要怎麼樣做這個問題呢？
你現在的第一個問題，就是怎麼把一隻寶可夢當作
function 的 input
你要把一個東西當作 function 的 input，它得數值化
你要用數字來表示一隻寶可夢， 你才能夠把它放到一個 function 裡面
那我們要怎麼把一隻寶可夢用數字來表示呢？
一隻寶可夢，其實它有很多的特性
這些特性是可以數值化的
比如說，它整體的強度
我先說一下，這並不是那個 Pokemon go 的那個東西
這個是那個 Pokemon 的電玩
你聽不懂我講這個，就算了
就假設沒聽到這句話
那一隻寶可夢，它其實可以用一組數字來描述它的特性
這組數字代表什麼呢？
比如說，這個寶可夢，它 total 的 strong
就是它有多強這樣子，你可以用一組數字來表示它
它的生命值，你可以用數字來表示它
它的攻擊力，你可以用數字來表示它
它的防禦力，你可以用數字來表示它
它的特殊攻擊力，就是它用特殊攻擊的時候
這我也不知道怎麼解釋， 反正就是另外一個特殊攻擊的攻擊力
特殊攻擊的防禦力
還有它的速度，速度可以決定說
就是兩隻寶可夢相遇的時候，誰可以先攻擊
比如說，皮卡丘
它整體強的程度是 320
它 HP 是 35，攻擊力是 55
防禦力是40，特殊攻擊力是 50
然後，它的特殊防禦力是 50，速度是 90
所以一隻皮卡丘，我們就可以用一個 vector 來描述它
這個 vector 裡面總共有 7 個數值
所以，1 隻寶可夢，它就是 7 個數字所組成的一個 vector
所以 1 隻皮卡丘，可以用這 7 個數字來描述它
我們現在要問的問題就是
我們能不能把 7 個數字，輸進一個 function
這個 function 就告訴我們說
它的 output 是哪一種種類的寶可夢
那你可能會問這樣的問題
這件事情的重要性，到底在哪裡？
這件事情，是非常重要的
因為當兩隻寶可夢相遇，在決鬥的時候
他們之間是有屬性相剋的關係
這個是 18*18 的屬性相剋表
因為，你知道說，總共有 18 隻寶可夢
總共有 18 種 type，所以是 18*18 的屬性相剋表
比如說，今天這個格鬥系遇到
左邊，這個是攻擊方
然後，上面這個是防禦方
所以，格鬥系遇到一般的時候，他的攻擊力就 *2
這樣，你看得懂這個圖了嗎？所以，這個是很重要的
那你可能會問說
這個寶可夢屬性，不是寶可夢圖鑑上都有了嗎？
綠色寶可夢屬性，有什麼意義呢？
這件事情是有很大的意義的
因為，你有可能在決鬥中的時候，遇到
對方出的是圖鑑上沒有的
你沒有見過的寶可夢
如果你現在有這個預測的 model 的話
你就可以預測說，它出的寶可夢是哪一種屬性的
你就可以用正確的屬性來對付它
所以，這個是有非常廣泛地運用的
而且我發現說，寶可夢圖鑑，其實是有影像辨識的功能
它影像辨識的功能很強
照一張圖，它就可以告訴你說，它是哪一種寶可夢
我們應該要把這個 prediction 的 model 加到寶可夢的圖鑑裡面
它就可以幫我們 predict 新的寶可夢
那，怎麼完成這個任務
首先，我們要先收集 data
比如說，你就說，我把寶可夢編號 400 以下的， 當作 training data
把編號 400 以上的，當作 testing data
你就假設說，因為它的寶可夢其實是越來越多的
我記得我小時候只有 150 隻
後來發現，越來越多寶可夢，現在已經有 800 隻了
所以寶可夢是不斷增加的
所以編號比較前面的，是比較早發現的那些寶可夢
所以，模擬說，我們已經發現那些寶可的情況下
如果看到新的寶可夢的時候
所以，你要收集 data，比如說
我們能不能夠預測說，它是屬於哪一種屬性的
我們的 data 就是一些 pair
告訴我們說 function 的 input, output 是甚麼
比如說，input 皮卡丘，就要 output 電
input 傑尼龜，就要 output 水
input 妙蛙種子，就要 output 草
那怎麼解這個 classification 的問題？
有人會這麼想
假設有人沒有學過 classification
他學過 Regression，他就說
Classification 就當作 Regression 的問題來硬解
怎麼硬解呢？
我們用 binary 的 classification 來當作例子
假設我們現在只要 output： input 的 x 屬於 class 1 或 class 2
那你就把他當作一個 Regression 的問題
task 1 就代表說它的 target，也就是 y\head 是 1
task 2 就代表說，它的 target 是 -1
就這樣，你就當作 Regression 的 problem， 一直 train 下去
然後，train 完這個 model 以後呢
在 testing 的時候
因為 Regression 的 output 不會正好是 0 或是 1 阿
它是一個 number，它是一個數值
如果這個數值比較接近 1 的話，就說是 class 1
如果這個數值比較接近 -1 的話，就說是 class 2
所以，你可以想成說，現在就是以 0 為分界
如果你的 Regression model output 是大於 0 的話
就比較接近 1
你的 model 就會說是 class 1
如果小於 0 的話，就比較接近 -1， 所以 model 會說是 class 2
如果你這麼做的話
會遇到什麼樣的問題呢？
你會遇到這樣的問題
假設說，我們現在的 model 是一個
我們現在的 model，input 和 output 的關係
y = b + w1*x1 + w2*x2
所以，input 這兩個 feature，也就是 x1 跟 x2
我們現在有兩個 class，紅色是 class 1、藍色是 class 2
那如果你用 Regression 來想的話
藍色的那些 object，藍色的那些東西
input 到這個 Regression 的 model
我們都希望它越接近 1 越好
紅色這些東西，input 到 Regression 的 model
我們都希望它越接近 -1 越好
這件事，可能是做得到的
如果你把這些 data
真的找出這個 b, w1, w2 的話
那你會發現說呢
b + w1*x1 + w2*x2 (投影片上寫錯)
這個式子等於 0 的線，是綠色這條
也就是 class 1 和 class 2 的分界點
是在綠色的這條線上
這看起來、聽起來滿好的
但是，你可能會遇到這樣的問題
假設，你今天 class 1 的分佈不是這樣子
假設你 class 1 的分佈是這樣子，你就麻煩了
為什麼？
因為，如果你用綠色的這條線，所代表的 model 的話
注意一下，這個是二維的
綠色這條線，只是代表說這個 model 的值是 0
就是這個 y 的值，是 0
你的 Regression 的 output 是 0
左上角這邊，代表這個 Regression 的 output 是小於 0
右邊上角這邊，代表這個 Regression 的 output 是大於 0
那綠色這條線，會有什麼問題呢？
綠色這條線的問題就是
左上角 < 0，右上角 > 0
越偏右下，它的值就越大
所以，如果今天是考慮右下角這些點的話
它用這個綠色的 model，它做 Regression 的時候
它的 output 可能會是遠大於 1 的
但是，如果你用 Regression 的話
你會希望，藍色的點都越接近 1 越好
太大也不好，它要越接近 1 越好
太小不行、太大也不行
所以變成說，這些遠大於 1 的點
它其實對 Regression 來說，是 error，是錯的
這些點是不好的
所以，你今天如果拿這樣兩群藍色的點跟紅色的點
去做 Regression 的時候
你得到的線，不會是綠色這條
雖然綠色這條，你用直覺看、你用眼睛一看就會知道說
它是一個比較好的 boundary
但是，如果你用 Regression 刃下去的話
不會是綠色這條，它會是紫色這條
因為，它會覺得說，我把綠色這條線，往右偏一點
這樣的好處就是，這邊這些藍色的點
它的值就沒有那麼大
它的值就會壓小
就讓他們，比較接近 1
結果，這樣子的 function 反而對 Regression 來說
是一個比較好的 function
也就是說，Regression 那個定義 function 好壞的方式
就 classification 來說，不適用
今天這個 problem，對 Regression 來說
紫色的，是一個好的 function
但是，顯然對 classification 來說， 綠色的才是一個好的 function
但是，如果你當作 Regression 的 problem 來做
套用到 Regression，一樣的作法的時候
你得到的結果會是不好的
因為 Regression 對 model 好壞的定義， 是不適合用在這個地方的
所以，如果你用 Regression 的話
你的 Regression model 它會懲罰那些
太正確、那些 output 值太大的那些點
這樣，反而你得到的結果是不好的
那，還有另外一個問題
其實，這種硬把 Classification 當作 Regression 來做
我還看過有人會真的這麼做
不果我勸你不要這麼做
比如說，你今天有 Multiple class 的話
那你可能說，我把 class 1 當作 target 是 1
class 2 當作 target 是 2
class 3 當作 target 是 3
這樣子做事會有問題的
因為當你這樣子做的時候，你就假設說
class 3 和 class 2 是比較近的
它們有某種關係
class 2 和 class 1 是比較近的
它們有某種關係
但是，實際上，如果這種關係不存在的話
class 1, class 2, class 3 他們中間並沒有某種特殊的 relation
並沒有誰應該跟誰比較有關係的話
你這樣子把它當作一個 Regression 的問題來處理
你就沒有辦法得到一個好的結果
那我們這邊，應該要怎麼做呢？
理想上的做法，是這個樣子
找一個 function，這個 function 裡面呢
這在做 Regression 的時候，它的 output 是 real number
對不對？但是在 classification 的時候
它的 output 是 discrete
它是某一個 class，它是 discrete
那我要想辦法讓 model 做到這件事情
你當然可以有不同的想法
那一個可能的想法是
假如是二元分類的問題，是 binary classification 的話
我就說，我們要找的 function f 裡面
內建另外一個 function g
希望它是自動，當然我們的 g 也是根據 training data 被找出來的
如果 g 代進 x，x 代進去的值大於 0 的話
那就說是 class 1
否則呢，就說是 class 2
那在 training 的時候
我們的 loss 應該怎麼定義才好呢？
我們的 loss 應該定義成，我們可以把 loss 定義成呢
如果我選了某一個 function f
它在我們的 training data 上面
predict 錯誤的次數
我們當然希望說
我們找出來的 function
它在 data 上的錯誤次數越小，代表它的 loss 越小
你就可以把這個式子寫成這樣
summation over 所有的 training example
δ (f(x^n) ≠ (y\head)^n)
就是如果 f(x^n) 的 output 跟 正確答案 (y\head)^n 不一樣的話
這個 δ 就是 1，否則就是 0
如果你把它全部 sum 起來的話
就是你用這個 function f，在 training data 上面
它會分類錯誤的次數
那當然希望這個值，越小越好
但是，如果要你解這個 function，你現在八成不會
因為，我們學過的是 Gradient Descent
你可能會用 Gradient Descent 解
但是，這個沒辦法微分阿
這個就沒辦法微分，這個也沒辦法微分，通通不能微分
不知道怎麼做
其實，這個是有方法，比如說
Perceptron 就是一個方法，SVM 就是一個方法
那我們今天先不講這個方法
我們今天先來講另外一個 solution
這個 solution，我們先用機率的觀點來看待它
之後我們會說，這樣的 solution
也是跟 machine learning 的 3 個 step，其實是一樣的
我們先這樣看，就是有兩個盒子
我們來回憶一下，我們國中的時候，機率會問的問題
有兩個盒子，盒子一裡面有藍球和綠球
盒子二裡面也有藍球跟綠球
假設我不告訴你說，我從哪一個盒子裡面， 挑一顆球出來
但是，我告訴你說，我從這兩個盒子的某一個盒子裡面
隨機抽取一顆球出來，它是藍色的
那這顆藍色的球，它從盒子 1
跟從盒子 2 抽出來的機率，分別是多少？
我相信這個，小學生就可以回答我
假如說，你告訴我說
你從盒子 1 裡面，抽一顆球的機率是 2/3
你從盒子 2 裡面，抽一顆球的機率是 1/3
這告訴我說，在盒子 1 裡面，藍球占 4/5
綠球占 4/5
盒子 2 裡面，藍球占 2/5，綠球占 3/5
那你就可以輕易計算說
如果我今天得到一顆藍球
他從盒子 1 裡面，抽出來的機率
這個機率就是，這個國小就應該有教過了
至少國中有教過吧，對不對？
大雞應該都會，就是
Given 一顆藍球，他從 B1 裡面 sample 出來的機率
就是這個樣子，這個沒什麼好解釋的
我相信大家都秒懂，這樣
那這個跟分類，有什麼關係呢？
如果我們把盒子換成分類的話
把盒子 1 跟盒子 2，換成類別 1 跟類別 2
換成類別 1 跟類別 2
這個時候呢，給我一個 x
就是，我們要分類的那個對象
比如說，今天我們的例子就是，分類一隻寶可夢
給我一隻寶可夢，他從某一個 class 裡面
sample 出來的機率是多少呢？
那我們需要知到哪些值？
我們需要知道 class 1
我從 class 1 裡面，抽一個 x 出來的機率
我們要知道從 class 2 裡面，抽一個 x 出來的機率
我們要知道說，從 class 1 裡面，抽一個 x 出來
從 class 1 裡面，抽出我們現在考慮的這個 x 的機率
我們要知道從這個 class 2 裡面
抽出我現在考慮的這個 x 的機率
如果有這些，當給我們一個 x 的時候
有了這 4 個數值
我們就可以計算，這個 x 是屬於 x1 的機率
怎麼算呢？x 屬於 x1 的機率
就是 case 1 本身的機率
乘上 case 1 sample 一個 object 出來，是 x 的機率
再除掉 case 1 本身的機率
乘上 case 1 sample 一個 object 出來，是 x 的機率
加上 case 2 本身的機率
乘上從 case 2 裡面 sample 出一個 object ，是 x 的機率
所以，我們現在的問題就是
如果我們知道這個機率的話
問題就解決了，因為給我一個寶可夢 x
我就可以看說，它從哪一個 case 來的機率最大
那機率最大那個 case，就是正確答案
那現在的問題是
我們如果要算這個值
那我們就要算這 4 個值
假設我們是考慮一個二元分類問題的話
我們就需要算這 4 個值
好，那這 4 個值怎麼來呢？
我們就希望從我們的 training data，去把這些值估測出來
那這一整套想法，叫做 Generative model
為甚麼它叫做 Generative model 呢？
因為有這個 model 的話，你可以拿它來 generate 一個 x
什麼意思呢？
你可以計算，某一個 x 出現的機率
如果你可以計算每一個 x 出現的機率
你就知道 x 的 distribution
你就可以用這個 distribution 來產生 x、sample x 出來
這一個機率是甚麼呢？
這個機率很簡單，它就是你從 c1 裡面
挑一個 x 出來的機率
乘上 c1 挑出 x 的機率
加上你從 case 2 裡面，挑一個 x 的機率
乘上 case 2 產生 x 的機率
你有這些機率
你就可以算這個
你就可以算某一個 x 出現的機率
你就可以自己產生 x
所以，這個東西，叫做 Generative model
那我們先來看一下 P(C1) 跟 P(C2)
我們先來算一下 P(C1) 跟 P(C2) 它出現的機率
那這個機率呢，叫做 Prior
他們呢，是比較好算的
假設我們今天考慮的這兩個 class 呢
分別是水系跟一般系
class 1 就是指水系的神奇寶貝
class 2 就是指一般系的神奇寶貝
另外 16 隻寶可夢，我們就無視它
先考慮一個二元分類的問題
那我們現在呢，把編號 ID，再圖鑑裡面編號 < 400 的
水系的和一般系的，就當作是 training data
剩下的當作是 testing data
如果你想要做的更嚴謹一點的話
你可以再把 training data 裡面 切一份 validation data 出來
好，那 training data 裡面呢，總共有 79 隻是水系的
總共有 61 隻一般系
你可能會問說，為什麼選水系 跟一般系當作二元分類問題
因為我其實統計了一下 18 種寶可夢，每個種類的數目
水系跟一般系是最多的
所以，我們就先選水系跟一般系
不過，我試了一下，如果你要把 18 種 都分類正確，好像是做不太出來的
因為它們中間，做不太出來這樣子
你就先考慮，分兩個 class 就好
如果我們現在知道說，training data 裡面
79 隻水系、61 隻一般系
那從這個第一類裡面
從 class 1 裡面，sample 出一隻寶可夢的機率是多少呢
是不是就是 79 / (79+61)，算出來是 0.56
那從 class 2，sample 出一隻寶可夢的機率
就是 61 / (79+61)，就是 0.44
這個是比較容易、比較簡單可以理解的
好，那再來我們的問題是這樣子
怎麼計算說，如果給我某一個 class
某一隻寶可夢，是從這個 class sample 出來的機率
比如說，如果給你一個海龜
我也不知道這個海龜應該叫什麼名字
給一隻海龜，它是從
就是從水系的神奇寶貝裡面
挑一隻神奇寶貝出來
它是海龜的機率，到底應該有多大呢？
那我們現在的 training data 是長這個樣子
屬於水系的神奇寶貝有 79 隻
所以有傑尼龜、可達鴨、蚊香蝌蚪之類的
這些我是認識的啦，我小時候在卡通有看到
這個是編號後面的
我小時候卡通沒有看到，所以我也不知道它叫什麼名字
它也不在這 79 隻裡面
那我到底要怎麼算說
從水系的神奇寶貝裡面挑一隻挑出來，是海龜的機率呢
你可能會想說
這 79 隻神奇寶貝又沒海龜
所以挑一隻出來，是海龜的機率根本是 0 阿
可是這海龜是水系的
我一看它的臉，我就知道它是水系的 XD
那它就是水系的，所以
你說，它從水系的裡面挑出來是 0 也不對阿
所以怎麼辦？
首先就是，每一隻神奇寶貝
每一隻寶可夢，我們剛才講過說
它都用一個向量來描述
這個向量裡面的值，就是它的各種的特徵值
所以，這個 vector，我們又稱之為一個 feature
所以，每一個寶可夢，都是用一堆 feature 來描述它
然後呢，我們就真的把那些水系的神奇寶貝
水系的寶可夢，它們的防禦力和特殊防禦力畫出來
其實，每一隻寶可夢有 7 個不同的數值阿
不過 7 個沒辦法畫，我就先畫防禦力跟特殊防禦力就好
值得強調的是，這邊的是真正的 data 這樣
如果你想要載完整的寶可夢的 data 的話
這個投影片最後也有附一個連結
你是可以載到完整的 data
那我們就把 79 隻寶可夢
它的防禦力跟特殊防禦力，都先畫在這張圖
二維的平面上
所以這二維的平面上，每一個點，就代表了一隻寶可夢
比如說，這個點是可達鴨
它的防禦力是 48，特殊防禦力是 50
這個點是傑尼龜
它的防禦力是 65，特殊防禦力是 64
可是，現在的問就是
如果給我們一個新的點
這個點是代表一隻 沒有在我們的 training data 裡面的寶可夢
是我們沒有看過的寶可夢，比如這隻海龜
它的防禦力是 103，特殊防禦力是 45
它的位置大概在這個地方
從水系裡面，挑到這隻神奇寶貝，它是水系的機率
到底應該是多少
那你不可以說它是 0 ，你不能說
這個 training data 裡面從來沒有 出現這隻海龜，所以它的機率就是 0
這樣顯然是不對的
你要想個辦法估測說
從這些我們已經有的 神奇寶貝裡面，估測說
如果從水系的神奇寶貝裡面
挑一隻出來，它是這個海龜的機率，到底有多少？
好，那怎麼辦呢？
你可以想像說
這 79 隻神奇寶貝
其實只是冰山的一角
就是水系的神奇寶貝，是從一個機率的分布裡面
水系神奇寶貝它的防禦力跟特殊防禦力
是從一個 Gaussian 的 distribution 裡面，sample 出來的
我們只是 sample 了 79 個點以後
得到的分佈長這個樣子
但是，從 Gaussian 的 distribution 裡面
sample 出這個點的機率，不是 0
我們假設說，這 79 個點是從一個 Gaussian 的 distribution 裡面 sample 出來的
再來，我們要做的事情就是
如果給我這 79 個點
我們怎麼找到那個 Gaussian 的 distribution
那我這邊還做了幾頁投影片要說 Gaussian distribution
那我覺得這應該，不用講吧
假設你不知道 Gaussian distribution 是什麼的話
你就想成它是一個 function
這個 function 的 input 就是一個 vector x
在這邊呢，代表某一隻寶可夢的數值
它的 output 就是這一隻寶可夢， 從這一個 distribution 裡面
這一個 x 從這一個 distribution 裡面， 被sample 出來的機率
其實嚴格說起來，這個東西並不是機率
它是 probability 的 density
它跟機率是成正比的，但它並不 exactly 就是機率
但是這邊，為了讓大家不要太混亂， 我們就假設它是機率
好，那這個機率的分佈呢
它是由兩個東西決定
一個東西，叫做 mean，這邊寫成 μ
另外一個東西，叫做 variance，寫做 Σ，它是一個 matrix
mean 是一個 vector，Σ 是一個 matrix
所以你把 μ 跟 Σ 代入這個看起來有點複雜的 function
那它就會有不同的形狀
同樣的 x，如果有不同的 μ 跟 Σ
那你代進同樣的 x，它 output 的 機率分布呢，就會是不一樣的
下面呢，就舉幾個例子
比如說，同樣的 Σ、不同的 μ
代表說他們機率分布最高點的地方，是不一樣的
比如說，同樣的 μ、不同的 Σ
代表說機率分布的最高點是一樣的
但是，它們的分布，散的程度是不一樣的
那接下來的問題就是
我們假設有一個 Gaussian 存在
從這個 Gaussian 裡面 ，
sample 出這 79 個點
那，到底這個 Gaussian長什麼樣子呢？
如果我們可以找到這個 Gaussian 的話
假設我們可以根據這 79 個點
估測出這個 Gaussian 的 μ (mean)
應該在這個位置，是 [75, 71.3]
它的 Σ 應該是這樣的分布
它的 x 跟 y 是有一些 correlation 的
但是沒有 x^2 跟 y^2 這邊這麼大
大概是長這樣子
那給我們一個新的點 x
它是我們過去從來沒有看過的點
它不在這個 sampling 裡面， 它不在這 79 個 sampling 裡面
但是，如果我們知道 μ 跟 x 的話
我們就可以把 Gaussian distribution 的 function 寫出來
知道 μ 跟 x，我們就可以把這個function 寫出來
這個 function 是 depend on μ 跟 x 的
所以，我們把它寫成 f (下標 μ 跟 Σ(x))
你把這個 x 代進去
經過一串複雜的運算以後
那你就可以算出呢
某一個 x 從這個 Gaussian 裡面
從這個 mean 是 μ、它的 covariance matrix 是 Σ 的 Gaussian 裡面
被 sample 出來的機率
那如果你對這個 function 沒什麼概念的話呢
你就可以想像說
如果 x 越接近中心點，越接近 μ 這個地方
它 sample 出來的機率當然是比較大的
像這個 x 在這麼遠的地方， 它 sample 出來的機率就是比較小的
那再來有一個問題就是
怎麼找這個 μ 跟怎麼找這個 Σ ？
這邊用的這個概念呢，叫做 Maximum Likelihood
你可以想像說，這 79 個點，其實可以從
任何一個 Gaussian 裡面，被 sample 出來
對不對？
任何一個 Gaussian 都有可能sample 出這 79 個點
不管你是 μ 在這個位置
然後，它的 covariance matrix 長這個樣子
還是 μ 在這個位置
它的 covariance 長這個樣子
它都有可能sample 出這 79 個點
對不對？因為你從 Gaussian 裡面 sample 出一個 point
它可以是整個空間上的任何一個點
只是有些地方機率很低，有些地方機率很高
但沒有一個地方的機率是 exactly 等於 0 的
所以，雖然說，右上角這個 Gaussian
右上角這個 Gaussian，它 sample 出 左下角這個點的機率很低
但是，並不代表說，這個機率是 0
但是，雖然說每一個 Gaussian 都 有可能 sample 出這 79 個點
但是，他們 sample 出這 79 個點的可能性是不一樣的
他們 sample 出這 79 個點的 Likelihood 是不一樣的
顯然說，如果你的 Gaussian 是這個的話
他 sample 出這 79 個點的 Likelihood 就比較高
如果你的 Gaussian 是這個的話
他 sample 出這 79 個點的機率是比較低的
所以說，今天給我們某一個 Gaussian 的 μ 跟 Σ
我們就可以算這個 Gaussian 的 likelihood
也就是說，給我一個 Gaussian 的 μ 跟 Σ
我們就可以算這個 Gaussian
sample 出這 79 個點的機率
那這個 likelihood、這個可能性呢
我們可以把它寫成這樣一個式子
這個可能性呢
這邊呢，我們也用了 L，因為我想不到更好的 notation
可能會跟 loss function 有點混淆
但是，Likelihood 用別的 notation 又很怪
還是用 L
那這個 L，它的 input 就是
Gaussian 的 mean(μ) 跟 covariance (Σ)
L 做的事就是把這個 μ 跟 Σ， 代到這個 likelihood 的 function 裡面
那它會告訴我們說
這個 μ 跟 Σ，它 sample 出這 79 個點的機率
到底有多大？
它的可能性到底有多大？
這個東西怎麼算？
這個點，這個東西就是這樣算
因為所有的 79 個點是獨立被 sample 出來的
所以，今天這個 Gaussian，它 sample 出這 79 個點的機率
就是，這個 Gaussian sample 出第 1 個點的機率
乘上 sample 出第 2 個點的機率
乘上 sample 出第 3 個點的機率
一直到 sample 出第 79 個點的機率
那所以我們有 79 隻水系的神奇寶貝
我們知道，它是從某一個 Gaussian，被 sample 出來的
我們接下來要做的事情，就是
找到那一個 Gaussian
找一個 Gaussian
那個 Gaussian，它 sample 出這 79 個點的機率
是最大的
它 sample 出這 79 個點的 Likelihood 是最大的
那這個 Gaussian，我們就當作是
sample 出這 79 個點的 Gaussian
那這個 Likelihood 最大的 Gaussian 呢
我們寫作 (μ*, Σ*)
所以，我們現在要做的事情是這樣
Likelihood 的 function 寫做這樣子
那這每一個 f，如果你想知道的話
它很複雜，是寫成這個樣子
你就把這個 x 代進去，然後再算出它的 x^2 代進去
然後你就算出它
然後呢，我們要窮舉所有的 μ
窮舉所有的 Σ，看哪一個可以讓上面的 likelihood 的式子最大
它就是我們要找的 μ* 跟 Σ*
它就是我們認為最有可能產生這 79 個點的 μ* 跟 Σ*
我們就當作這 79 個點，是從這個 μ*, Σ* sample 出來的
這個東西，怎麼做呢？
其實這樣子講，如果你爽的話，你就用微分解一下
找那個極值的地方這樣
秒解這樣，你也可以背個公式解
怎麼秒解，就是
哪一個 μ* 可以讓這個最大呢？
這個結果是很直覺的
就是平均值可以讓它最大
所以，你就把 79 個 x 平均起來
你就把 79 個 x 當作是 vector 加起來，除 79，就得到 μ*
平均就是 μ*
如果你不爽的話，你就把這個式子取個微分阿
對 μ 取個微分，然後找它微分是 0 的點
解出來就是你的 μ*
Σ* 是甚麼呢？你先把 μ* 算出來
然後對所有的 x^n，你都算 (x^n - μ*)
乘 (x^n - μ*)^T (的 transpose)
你就算說，假設 x^n 的 mean 是 μ* 的話
的 covariance，那你算出來呢，就是 Σ*
那如果你不爽的話，就把這些值對 Σ* 做微分
對 Σ* 做微分，然後解它微分是 0 的點
你就解出來這個
有了這些以後
我們就真的去算一下
這個是真正的結果
79 隻水系的神奇寶貝，79 隻水系的寶可夢
算出來的 μ 是這樣子
算出來的 Σ 是這樣子
也就是說呢
假設這 79 隻水系的神奇寶貝
是從這個 Gaussian sample 出來的話
那最有可能 sample 出這 79 個點的 Gaussian
它的 mean 是 μ1，它的 covariance 是 Σ1
那如果你看 class 2 的話
class 2 是一般系的神奇寶貝
有幾隻呢？有 61 隻
那我們一樣算它的 mean 跟 variance
這 61 隻一般系的神奇寶貝
最有可能 sample 出它的 Gaussian
它的 mean 是長這樣，它的 variance 是長這樣
有了這些以後
就結束了，我們就可以做分類的問題了
怎麼做呢？
我們說要做分類的問題
我們只要算出 P(C1|x)
給我一個 x，它是從 C1 來的機率
那這整項可以寫成這樣子
只要我們最後算出來這一項，大於 0.5 的話
那 x 就屬於 class 1
那 P(C1) 很容易算，就是這麼回事
那 P(C2) 我們算過，就是這麼回事
P(x|C1) 怎麼算呢？
我們已經找出
我們已經假設說這個東西
它就是一個 Gaussian distribution
這個 Gaussian distribution 的 mean 跟 variance
分別就是 μ1 跟 Σ1
這我們剛才已經算出來過了
因為我們剛才已經根據
class 1 所有的、那 79 隻寶可夢的分布
知道說，他們是從一個
mean 是 μ1，covariance 是 Σ1 的 distribution 裡面 sample 出來的
那如果是這一項呢？
P(x|C2)，那我們也知道說， 它的這個 Gaussian distribution
它是從 mean 是 μ^2, covariance 是 Σ^2
的 distribution 裡面 sample 出來的
有了這些以後，問題就解決了
那結果怎麼樣呢？
我是真的有做的
藍色的點是
這個橫軸跟縱軸
分別就是防禦力跟特殊防禦力
藍色的點，是水系的神奇寶貝的分布
紅色的點，是一般系的神奇寶貝的分布
看到這個結果，我有點緊張
因為覺得分不出來，我用人眼看就知道不太 ok
那我們就真的計算一下
在這個二維平面上
每一個點，我都當作一個 x
進去我都可以算一個，它是 C1 的機率對不對？
這個圖上的每一個點，我都可以算它是 C1 的機率
那這個機率呢，用顏色來表示
紅色就代表說，在這個區域呢
是 class 1，是水系神奇寶貝的機率是比較大的
在這個地方呢，水系神奇寶貝的機率是比較小的
你看這很合理嘛
因為，水系神奇寶貝在這邊的分布還是比較多
在這邊比較多，所以這個地方機率是比較大的
那現在，因為我們處理的是分類的問題
我們算這個機率，我們是要 output 說是哪一類
所以我們說，機率大於 0.5，就是類別一
也就是紅色這個區間，就是類別一
機率小於 0.5，就是藍色這個區間
他們就是類別二，這個是類別一
你會發現說
如果你看藍色的點的話
是比較多藍色的點，在這個紅色的區間
那紅色的點，是比較多在這個藍色的區間
那也有一些到紅色的區間
有點難搞，因為他們中間沒有一個明確的 boundary
那把它 apply 到 testing set 上
現在，testing set 就是編號大於 400 那些寶可夢
把他們整個 class 1 跟 class 2 的寶可夢 畫在這個二維平面上
那 boundary 是一樣的
這個 boundary 是一樣的
你會發現說，分的不甚太好
正確率是 47 %
那你有一點擔心
會不會是這題有可能分不出來，這也是有可能的
但是我想說我們現在只看了二維的空間，對不對
機器學習厲害的地方就是
因為我們讓機器處理這個問題
所以高維空間也可以處理，不是只處理二維的空間而已
所以我們看一下高維的空間
事實上，每一隻神奇寶貝(寶可夢呢)
它是分布在一個七維的空間裡面
如果只用二維的空間分不出來， 可是搞不好七維分的出來阿
就是這個紅色跟藍色搞不好在高維的空間上看到
一個是這樣，一個是這樣
現在從上面往下看，就覺得疊在一起
高維空間上，搞不好是分開的
搞不好在七維空間上，是分開的
所以，每一個寶可夢都是用七個數值來表示
所以，每一個寶可夢都是存在七維空間中的一個點
我們一樣可以算 class 跟 class 2 在七維空間中
sample 出那些點的 μ1 跟 μ2
μ1 跟 μ2 都是七維
Σ1 跟 Σ2 都是 7*7 的 matrix
然後你就做一發，正確率就 50%，很糟
這樣就跟你 random 猜，大概也是這個樣子
然後就 so sad 這樣，然後我們下周再看看要怎麼改進它
謝謝
各位同學大家好，我們就開始上課吧
上次我們講到哪裡呢？我們講到說
如果我們想要做，寶可夢的屬性的分類的話
我們可以假設一個機率模型
那我們把這個機率模型裡面呢
拆成有 required probability
跟每一個 class 自己的 distribution
那每一個 class 自己的機率呢，就用 Gaussian 來假設它
那經過一番運算
我們算出每一個 class 的 required
跟每一個 class 的 Gaussian distribution 以後呢
做一下
講到這邊，做一下之後發現呢
結果壞掉了
就算是我用了全部寶可夢的 7 個 feature
還是壞掉了
那怎麼辦呢？
那其實呢，當你用這樣子的 probability generated 的 model 的時候
像我在上一堂課裡面用的那種模型
是比較少見的
其實你不常看到
給每一個 Gaussian 都有自己的 mean 跟自己的 variance
每一個 Gaussian 都有自己的 mean 跟自己的 variance
class 1 有一個 μ1，有一個 Σ1
class 2 有一個 μ2、 Σ2
比較常見的做法是，不同的 class
可以 share 同一個 covariance 的 matrix
首先。你想想看，covariance matrix
它其實是跟你 input 的 feature size
是跟它的平方成正比的
所以，covariance matrix 當你的 feature size 很大的時候
它的增長呢，其實是可以非常快的
所以在這個情況下呢
如果你把兩個不同的 Gaussian 都給它不同的 covariance matrix
那你的 model 參數可能就太多了，model 參數多
variance 就大，也就是容易 overfitting
所以，如果我們要有效減少參數的話
我們可以給這兩個 class
就是屬於水系的神奇寶貝和屬於一般系的神奇寶貝
它們的描述這兩個 class 的 feature 分布的 Gaussian
故意給他們同樣的 covariance matrix
強迫他們共用 covariance matrix
這樣子呢，你就只需要比較少的 parameter
就可以來 model 這一個模型了
這甚麼意思呢？
也就是說，現在我們有 79 隻水系的寶可夢
我們假設它是從一個 mean 是 μ1
covariance 是 Σ 的 Gaussian 所 generate 出來的
那另外，這邊是多少隻呢？
這邊應該是 61 隻
我們這邊給它編號從 80 到 140
有另外 61 隻是屬於一般系的寶可夢
我們假設這些寶可夢他們的屬性的這個分布呢
是從另外一個 Gaussian 所 generate 出來的
另外一個 Gaussian，它的 mean 是 μ2
但是它的 covariance matrix
跟 generate 前一個
跟 generate class 1，跟 generate 水屬性的寶可夢
他們用的 covariance matrix 是同一個
這兩個 class 他們 share 同一個 covariance matrix
如果這樣子的話，你怎麼計算 Likelihood 呢？
如果你現在要計算，某一組 μ1, μ2 和 Σ
generate 這總共兩個 case 合起來
140 筆 data 的可能性的話
你就像下面這樣計算
這個計算方法就是
計算如果你今天用 μ1 跟 Σ1 產生 x^1 的機率
乘上用 μ1 跟 Σ1 產生 x^2 的機率
剛才都一直唸 Σ1 不好意思
這邊不是 Σ1，這個是只有 Σ 而已
因為這兩個 class，是共用同一個 covariance matrix
現在呢，用 μ1 跟 Σ 產生 x^1
到用 μ1 跟 Σ 產生 x^79
那如果是第一個 class 的 x 用這個方法來產生
如果是第一個 class 的 x 呢
你就用 μ2 跟 Σ 產生 x^80
用 μ2 跟 Σ 產生 x^81
到用 μ2 跟 Σ 產生 x^140
那在這個式子裡面呢 μ1, μ2
你要怎麼算 μ1, μ2 呢？
你要怎麼找一個 μ1, μ2 跟 Σ 讓 這個 Likelihood 的 function 最大呢？
那 μ1, μ2 的算法
跟我們之前沒有把 class 1 跟 class 2 的 covariance
tight 在一起的時候，那個算式是一模一樣的
你就只要把 class 1 裡面的 x 平均起來就變 μ1
class 2 裡面的 x 平均起來就變 μ2
唯一不一樣的是
(右下角)我們要把它按接受這樣子
那唯一不一樣的是
Σ 嗯？
唯一不一樣的是 Σ
因為我們現在 Σ 要同時考慮這兩個 class
所以它當然是不一樣的
那這個 Σ 的式子應該長甚麼樣子呢？
這個 Σ 的式子，這個結果非常的直觀
如果你想要看它的推導的話
我這邊引用的就是 Bishop 這本教科書
以後如果要引用的話，盡量引 Bishop，為甚麼呢？
因為它在網路上，有 available 的版本
這個 Σ 應該長甚麼樣子呢？
這個結果非常的直觀
你就把原來我們根據這些 data
所算出來的 covariance matrix (Σ^1)
跟根據這些 data
所算出來的 covariance matrix，Σ^2
weighted by 他們 element 的數目
你這個 class 1 有 79 個
所以你就把 Σ^1 * 79
class 1 有 61 個
所以你就把 Σ^2 * 61，再取平均
你就把原來這兩個 Gaussian，各自算的 covariance matrix
加權平均，就會得到
如果你要求他們用共同的 Gaussian 的時候
所得到的 covariance matrix
那我們來看一下結果
假設我們仍然是用兩個 feature
用 Defense 跟 SP Defense 的話
後來我發現，我這兩個 example 選的不是很好
因為如果你看 Defense 跟 SP Defense
你是沒有辦法把水系跟一般系的神奇寶貝分開
後來我研究了一下，我覺得
好像用一般攻擊力和一般防禦力
合起來呢，就可以分的滿開的這樣子
但是，我怎麼會事先知道這件事呢？
我又不是大木博士，對不對？
那如果我們今天共用 covariance matrix 會發生甚麼事？
在沒有共用之前，class 1 跟 class 2 的 boundary
是這條，是這個曲線
如果我們今天共用同一個 covariance matrix 的話
你會發現說
他們的 boundary，變成是一個直線
假設你把這兩個不同的 class
強迫他們的 covariance matrix 必須共用同一個的話
那你今天在分類的時候，你的 boundary 就會變成是
一條直線
所以，像這樣子的 model
我們也稱之它為 linear 的 model
你可能會想說，Gaussian 甚麼的不是 linear 的阿
但是，它分兩個 class 的 boundary 是 linear 的
所以，這樣的 model，我們也稱它為 linear 的 model
如果今天兩個 class， 你用不同的 covariance matrix 的話呢
它們就不是 linear 的 model
如果，我們考慮所有的 feature 會怎麼樣呢？
如果我們考慮所有的 feature 的話
原來我們只得到 50% 正確率
但是，神奇的是，當我們共用 covariance matrix 的時候
我們就得到 79% 的正確率了
顯然是有分對東西
那你說，為甚麼會做到這樣子呢，那這就很難分析了
因為，這個是在高維空間中發生的事情
是在 7 維空間中發生的事情
我們很難知道說，這個 boundary 是怎麼切的
但是，這個就是 machine learning fancy 的地方
就是，人沒有辦法知道怎麼做
但是，machine 可以幫我們做出來
如果今天 feature 很少，人一看就知道怎麼做
那其實可以不用用上 machine learning，對不對？
所以，現在可以得到 73% 的正確率
我們來回顧一下，我們講得這個
機率的模型
那我們講說 machine learning 就是 3 個 step
那這個機率模型呢，它其實也是 3 個 step
首先，你有一個 model， 這個 model 就是你的 function set
這個 function set 裡面的 function 都長甚麼樣子呢？
這個 function set 裡面的 function 都長下面這個樣子
input 一個 x
我們有 class 1 的 required probability
class 2 的 required probability
class 1 產生 x 的 probability distribution
class 2 產生 x 的 probability distribution
這些 required probability 和 probability distribution
就是 model 的參數
你選擇不同的 probability distribution
你就得到不同的 function
那你把這些不同的 probability distribution
就像 Gaussian 你選不同的 mean 跟不同的 covariance matrix
你就得到不同的 probability distribution
你把這些不同的 probability distribution 積分起來
就是一個 model，就是一個 function set
那怎麼決定是哪一個 class 呢？
如果 P(x|C1) 這個 posterior probability > 0.5 的話呢
就 output class 1，反之呢，就 output class 2
這個是 function 的樣子
接下來呢，我們要找
evaluate function set 裡面每一個 function 的好壞
那怎麼 evaluate 呢？
在這個機率模型裡面
假設我們今天使用 Gaussian 的話
那我們要 evaluate 的對象， 其實就是 Gaussian 裡面的參數
也就是 mean 跟 covariance matrix
那今天呢，我們就是說
如果一個 mean 跟一個 covariance matrix
你用這些參數來定義你的 probability distribution
而它可以產生我們的 training data 的 likelihood
就是這組參數的好壞
所以，我們要做的事情就是
找一個 probability distribution
它可以最大化產生這些 data 的 likelihood
這個是定義 function 的好壞
定義一組參數的好壞
最後，怎麼找出一組最好的參數呢？
你就看看前面的投影片，它的結果是很 trivial 的
那有人就會問說
為甚麼要用 Gaussian，為甚麼不選別的這樣子？
簡單的答案就是
如果我選了別的機率模型，你也會問我同樣的問題
其實你永遠可以選一個，你自己喜歡的
這個 probability distribution
這個是你自己決定的
這個不是人工智慧
是你人的智慧，去決定說你要選哪一個人的模型
是比較適合的
那你選擇比較簡單的機率模型，參數比較少的
那你的 bias 就大、variance 就小
那你選擇複雜的，你 bias 就小、variance 就大
那你可能就要用 data set 決定一下
你要用怎麼樣的機率模型，是比較好的
那我們有另外一種常見的假設是這樣
假設我們的這個 x
我們知道 x 是由一組 feature 來描述它的
那剛才在寶可夢的例子裡面，x 可以有 7 個數值
7 個參數
那我們假設，每一個 dimension
它從機率模型，產生出來的機率是 independent 的
所以，這個 x 產生的機率
可以拆解成，x1 產生的機率
乘上 x2 產生的機率，乘上 xk 產生的機率
一直到乘上 xK 產生的機率
如果我們假設，這些機率分布是 independent 的話
每一個 dimension 分布是 independent 的話
我們可以做這樣子的假設
那今天你可以說，每一個機率
就是 x1 產生的機率、x2 產生的機率， xk 產生的機率
他們分別都是一維的 Gaussian
一維的 Gaussian，大家知道意思嗎？
如果你這樣假設的話，等於是說
我們之前討論的都是 multi-variable 的 Gaussian 嘛
都是多維度的 Gaussian
如果你假設說，每一個 dimension 分開的 model
他們都是一維的 Gaussian 的話，意思就是說
原來那個高維度的 Gaussian，它的 covariance matrix
它的 covariance matrix，變成是 diagonal
在不是對角線的地方，值都是 0
只有對角線的地方，有值
這樣你就可以更減少你的參數量
你就可以得到一個更簡單的模型
那如果試一下這個，試一下這個結果是壞的
所以看來這個模型太簡單了
model 不同的 feature 間的 covariance
我看也是必要的
我覺得，比如說，像是戰鬥跟防禦力是有正相關的
他們這個 model 之間的 covariance，看來還是必要的
那你也不一定要用 Gaussian
有很多時候你憑直覺就知道應該用 Gaussian
比如說，今天假設你有某個 feature，它是 binary 的
有某個 feature，它代表的是：是或不是
或是它的 output 就是 0 跟 1 這樣
比如說，有一隻寶可夢，它是神獸還是不是神獸
之類的，這個就是 binary 的 feature
如果是 binary 的 feature 的話
你說它是用 Gaussian distribution 產生的
就太自欺欺人了
所以，它應該不太可能是用 Gaussian 所產生的
這個時候，你就會假設別的 distribution
比如說，假設你的 feature 是 binary 的
它 output，要馬是 0，要馬是 1
這個時候，你可能就會選擇說， 它是一個 Bernoulli distribution
而不是一個 Gaussian distribution
如果我們今天假設所有的 feature
它都是 independent 產生的
我們不 model feature 和 feature 間 covariance 的關係
那我們用這種方法做分類的話
我們叫做用 Naive Bayes Classifier
它前面有一個 Naive，因為它真的很 naive
它真的很簡單，這樣
那你可能會常聽到，有人說 Naive Bayes Classifier 很強
其實它強不強是 depend on 你的假設是不是精準的
如果你今天假設不同的 dimension 之間是 independent
這件事情是很切合實際的
那 Naive Bayes Classifier 確實可以 給你提供很好的 performance
那如果這個假設是很不成立的話
那 Naive Bayes Classifier 它的 bias 就太大了
它就不是一個好的 Classifier
接下來呢，我們要做的分析是
我們要分析這項 Posterior Probability
我們在做一些整理以後，我們會發現一些有趣的現象
這一項呢，大家應該都沒有甚麼問題
把他們上下同除分子
上下同除分子，我們把他們上下
都同除 P(x|C1)*P(C1)
所以，分子的地方就變成 1
分母的地方就變成 1 加
[P(x|C2) * P(C2)] / [P(x|C1) * P(C1)]
那我們假設，這一項阿
這一項取 natural log 以後阿
它等於 z
我們假設這一項取 natural log 以後，它等於 z
那我們就可以把這個 Posterior Probability
1 / (1 + exp(-z))
這個 z 是這一項
那你把這一項放進去
乘負號，就是上下顛倒，再取 exponential
把 exponential 跟 natural log 抵銷，你就得到這一項
然後你就得到 Posterior Probability
相信這個，大家應該沒有甚麼問題
這個 function，它的 input 是 z
這個 function，叫做 sigmoid function
如果你把它 output 對 z 的關係作圖的話
你就會發現是這個樣子
也就是 z 趨近無窮大的時候
它的 output 就趨近於 1
z 趨近負無窮大的時候
它的 output 就趨近於 0
接下來，我們要做的事情是
我們要把這個 z 算一下
它到底應該長甚麼樣子
我們來算一下，這個 z 應該長甚麼樣子
接下來這邊呢，是數學比較多
如果你覺得這很無聊的話
你就睡一下，聽一下結論就好
那這個 z 應該長甚麼樣子呢？
我們已經知道這個 Posterior probability
它是一個 z 的 sigmoid function
z 長甚麼樣子？
我們把相乘的部分，取 ln，所以就變成相加
那 P(C1) / P(C2) 是甚麼呢？
我們都知道說，這邊 N1 代表 Class 1
它在 training data 裡面出現的數目
N2 代表 Class 2，它在 training data 裡面出現的次數
所以，P(C1) 就是 N1 / (N1 + N2)
P(C2) 就是 N2 / (N1 + N2)
分母的地方消掉，所以得到 N1/N2
這個是小學生的數學
那 P(x|C1) 是甚麼呢？
我們說它是一個 probability distribution
這個 Gaussian 的 distribution
P(x|C1) 是另一個 Gaussian 的 distribution
如果我們把它相除
我們把這兩個 Gaussian probability 相除，再取 ln
會得到甚麼式子呢？
就得到這樣子，把這個放在上面，把這個放在下面
那這一項跟 distribution 是沒關係的
就把它消掉
然後，這一項把它提出來
相乘變相加，把這一項提出來
然後這個 exp 的部分呢
相除等於 exp 裡面的相減
這個，也沒什麼特別的
把他們分開相乘，變相加
相乘變相加，得到這樣
那接下來呢？接下來你就
做一些運算，把它展開
你可能想要知道說 (x - μ1)^T
乘上 (Σ1)^(-1)，再乘上 (x - μ1)
它應該長甚麼樣子
把它展開
所以，這個乘這個乘這個，就得到它
這個乘這個乘這個，就得到它
這個乘這個乘這個，就得到它，這樣
那中間這兩項呢，是可以合併的
是可以合併的，他們其實是一樣的
那你就得到這樣子式子
所以，這一項展開，就變成這樣
那這一項展開呢？
這一項展開呢？因為這個跟這個
只差了一個，把 1 換成 2 嘛
所以你就把，下面這個式子的 1 都換成 2
就行了
所以，z 這一項呢，它寫成這樣
前面是跟 Σ1 和 Σ2 有關
前面是 Σ1 和 Σ2 的 determinant 相除
後面呢，把 -1/2 乘進去
把 -1/2 乘這項，你得到這一項
把 -1/2 乘進去，把 -1/2 乘這項，你得到這一項
最後呢，再加上這個機率
你就知道 z 是多少了
那如果你剛才沒有聽得很懂，沒有關係
那其實沒有特別重要
反正就是經過一番運算以後，我們知道 z
哇！長得很複雜，長這個樣子
但是，我們剛才有說過呢
一般我們會假設，covariance matrix 是共用的
所以，Σ1 = Σ2 = Σ
在這個情況下，我們就可以簡化上面這個式子
我們就可以簡化成
Σ2 的 determinant 除以 Σ1的 determinant
如果 Σ1 = Σ2 的話，它就可以被消掉
這邊有一項， -1/2 * x^T * (Σ1)^-(1) * x
這邊有一項， 1/2 * x^T * (Σ2)^-(1) * x
如果 Σ1 跟 Σ2 是一樣的話呢，它們也可以被消掉
所以，我們最後得到的結果呢，就只剩下
1, 2, 3, 4, 5，5 項
然後，你會發現說
只有這一項跟這一項，是跟 x 有關的
是跟 x 有關的
這三項，是跟 x 無關的
這兩項，最後都有乘上 x
所以，先把這兩項集合起來 這兩項集合起來把它的 x 提出來
所以這兩項集合起來，就變成呢
因為 Σ1 = Σ2 ，都有乘上 x
所以 Σ 跟 x 可以提出來，變成 Σ^(-1) * x
這邊有 μ1 的 transpose，跟 μ2 的 transpose
這邊是相減，所以
這項跟這項合起來， 就變成 (μ1 - μ2) 的 transpose * Σ^(-1) * x
剩下這三項，就把它原封不動地擺在後面
接下來呢，我們假設說
這個東西，(μ1 - μ2)^T * Σ^(-1)
它合起來就是一個 vector
假設你把 μ1 算出來，把 μ2 算出來，把 Σ 算出來
那你再代到這個式子裡面，把 Σ 做 inverse
把 (μ1 - μ2) 做 transpose，那你就得到一個 vector
把那個 vector 叫做 W^T
後面這項，你可覺得看起來很可怕
但它其實很簡單
因為，我們從這邊開始看
這是一個 vector，這是一個 matrix
這是一個 vector，把他們三個乘起來以後
你得到的其實就是一個 scalar
那它其實不是甚麼複雜的東西，它是一個數字
你把這一項乘這一項乘這一項
你把 vector 的 transpose 乘上 matrix 的 inverse
再乘上一個 vector，它也是一個 scalar
那這個 ln (N1/N2)，它也是一個 scalar
所以你只是把這 3 個數字加起來而已，它就是個數字
所以我們就拿 b 來代表這個看起來很複雜的數字
假設你知道 μ1, μ2 跟 Σ，那這一項其實就是個 vector
這一項其實就是個 scalar
它就是一個數字而已
所以呢，我們知道說
我們可以把 posterior probability
這項機率呢，寫成 σ(z)
z 呢，又可以寫成這樣子
所以，我們其實可以把這個 posterior probability
就簡單寫成 σ(w * x + b)
w 跟 x 的 inner product，再加上一個常數 b
我們可以把 z 寫成 w 跟 x 的 inner product
再加上一個常數 b
其實這個 posterior probability
它根本就沒有這麼複雜
它寫起來呢，就是這個樣子
所以，從這個式子，你就可以看出來說
為甚麼我們今天把 Σ1 跟 Σ2 共用的時候
假設 Σ1 必須等於 Σ2 的時候
你的 class 1 跟 class 2 的 boundary 會是 linear
你從這個式子呢，就可以很明顯地看出這件事
那再 generative model 裡面
我們做的事情是，我們用某些方法
去找出上面這個式子裡面的
N1, N2, μ1, μ2, Σ
找出這些以後，你就算出 w，你就算出 b
你把它代進這個式子，你就可以算機率
但是，如果你看到這個式子的話
你可能就可以有一個直覺的想法
為甚麼要這麼麻煩呢？
假設我們最終的目標，都是要找一個 vector w
都是要找一個 constant b
我們何必先去搞個機率
算出一些 μ, Σ 甚麼的
然後再把它搞起來，再得到 w 跟 b
這不是捨近求遠嗎？
做一件你根本就不需要做的事
最後你只需要 w 跟 b 嘛
所以，我們能不能夠直接把 w 跟 b 找出來
這個呢，就是我們下一份投影片要講的東西
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
