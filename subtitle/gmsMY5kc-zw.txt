那剛剛好 我們來講一下
怎麼從這一排 vector 得到 b2
其實從這一排 vector 得到 b1
跟從這一排 vector 得到 b2
它的操作是一模一樣的
好 所以我們再講一次
怎麼從 input vector 得到 b2
等於是幫大家複習上上週說過的東西
好 那當然這邊要強調一點是
這邊的 b1 到 b4
它們並不需要依序產生
你並不需要算完 b1 再算 b2
再算 b3 再算 b4
b1 到 b4
它們是一次同時被計算出來的
好 那怎麼計算這個 b2 呢
那我們現在的主角
就變成 a2
a2 會乘上一個 transform
就是你把 a2 乘上一個 matrix
變成 q2
然後接下來你會根據 q2
去對 a1 到 a4 這四個位置
都去計算 attention 的 score
那怎麼計算 attention 的 score 呢
你就把 q2 跟 k1 做個這個 dot product
你把 q2 跟 k2 也做個 dot product
你把 q2 跟 k3 也做 dot product
你把 q2 跟 k4 也做 dot product
得到四個分數
你得到這四個分數以後
你可能還會做一個 normalization
比如說 softmax
然後得到最後的 attention 的 score
那我們這邊用 α'
來表示經過 normalization 以後的
attention score
那得到這個 attention 的這個分數以後
得到這個 α2,1 α2,2 α2,3 α2,4
這四個數值以後
接下來我們拿這四個數值來做什麼呢
我們把這些數值
分別乘上 v1 v2 v3 v4
我們把 α2,1
乘上 v1
把 α2,2 乘上 v2
把 α2,3 乘上 v3
把 α2,4 乘上 v4
然後全部加起來就是 b2
或者是我們把 b2 的式子列在這邊
b2 是怎麼來的呢
b2 就是把 vi
vi 就是這邊的 v1 到 v4
把 vi 乘上 α
這邊每一個 v1 到 v4 都有一個對應的 α
α2,1 到 α2,4
把每個 vi 都乘上 α
全部再加起來
就得到 b2
所以我們這邊就再跟大家操作了一下
b2 是怎麼被計算出來的
同理你就可以
由 a3 乘一個 transform 得到 q3
然後就計算 b3
從 a4 乘一個 transform 得到 q4
就計算 b4
所以現在你就知道說
怎麼從 a1 到 a4 計算出 b1 到 b4
好 那剛才講的是 Self
是這個 Self-attention 它運作的過程
那接下來我們從矩陣乘法的角度
再重新講一次我們剛才講的
Self-attention 是怎麼運作的
好 那 Self-attention 是怎麼運作的呢
我們現在已經知道說 a1 到 a4
它們每一個 a 都要分別產生 q k v
所以 a1 要產生 q1 k1 v1
a2 要產生 q2 k2 v2
以此類推
每一個 a 都產生 q k v
如果要用矩陣運算表示這個操作的話
是什麼樣子呢
我們每一個 a
ai 都乘上 q 這個
都乘上一個矩陣
我們這邊用 wq 來表示它
得到 qi
每一個 a 都要乘上 wq
得到 qi
那這些不同的 a 你可以把它合起來
當作一個矩陣來看待
什麼意思呢
a1 乘上 wq 得到 q1
a2 也乘上 wq 得到 q2
一樣 a3 a4 也都乘上 wq 得到 q3 跟 q4
那你可以把 a1 到 a4 拼起來
看作是一個矩陣
這個矩陣我們用 I 來表示
這邊這個 I
這個大寫的字母 I
它代表的是一個矩陣
那這個矩陣有四個 column
它的 column 就是 a1 到 a4
那 I 乘上 wq 就得到另外一個矩陣
我們用 Q 來表示它
這個 Q 就是 q1 到 q4
你把 q1 到 q4 這四個 vector 拼起來
就是 Q 的四個 column
所以我們從 a1 到 a4
得到 q1 到 q4 這一件事情 這個操作
其實就是把一個叫做 I 的矩陣
I 這個矩陣
它裡面的 column
就是我們 Self-attention 的 input
a1 到 a4 把 I 這個矩陣
乘上另外一個矩陣 wq
那 wq 其實是 network 的參數
它是等一下會被認出來的
把 I 乘上 wq 得到 Q
這個 Q 的四個 column
就是 q1 到 q4
那接下來怎麼產生 k 跟 v 呢
它的操作跟 q 是一模一樣的
我們就不要再細講
免得你覺得很無聊
所以 a 乘上 wk 就會得到這個 key
就會得到這個 k 這個 vector
那我們把這個 input 的四個 vector
a1 到 a4 拼起來
當作是一個矩陣 叫做 I
把 I 乘上一個矩陣 wk
就得到另外一個矩陣 K
K 的四個 column
就是這四個 key
k1 到 k4
那同理 v 也是一模一樣的操作
我們已經知道說 ai 乘上 wv
會得到 vi
也就是說把 a1 到 a4 拼起來得到 I
I 乘上矩陣 wv
會得到 V 這個矩陣
V 矩陣的四個 column
就是這邊四個
代表 value 的 vector
好 所以每一個 a 怎麼得到 q k v 呢
其實就是把輸入的這個
vector sequence 乘上三個不同的矩陣
你就得到了 q
得到了 k
跟得到了 v
好 這個是怎麼從 a 得到 q k v
好 那接下來呢
我們說下一步是什麼
下一步是
每一個 q 都會去跟每一個 k
去計算這個 inner product
去得到這個 attention 的分數
那得到 attention 分數這一件事情
如果從矩陣操作的角度來看
它在做什麼樣的事情呢
你就是把 q1 跟 k1 做 inner product
得到 α1,1
所以 α1,1 就是 q1 跟 k1 的 inner product
那這邊我就把這個
k1 它背後的這個向量
把它畫這個倒下來的
把它畫成比較寬一點
代表說它是 transpose
希望你可以了解我的意思
所以 q1 乘上 k1 的 transpose
也就是 q1 跟 k1 做 inner product
得到 α1,1
同理 α1,2 就是 q1 跟 k2
做 inner product
α1,3 就是 q1 跟 k3 做 inner product
這個 α1,4 就是 q1 跟 k4 做 inner product
那這個四個步驟的操作
你其實可以把它拼起來
看作是
矩陣跟向量相乘
什麼意思
這邊 q1 乘 k1 q1 乘 k2 q1 乘 k3 q1 乘 k4
這四個動作
你可以看作是我們把 k1 到 k4 拼起來
當作是一個矩陣的四個 row
那把這個矩陣乘上 q1
得到另外一個向量
你把 q1 乘上由 k 所組成的這一個矩陣
就得到另外一個向量
這個向量裡面的值
就是 attention 的 score
α1,1 到 α1,4
所以你把 q1 乘上這個矩陣
你就得到 α1,1 到 α1,4
好 那我們剛才講過說
我們不只是 q1
要對 k1 到 k4 計算 attention
q2 也要對 k1 到 k4
計算 attention
那怎麼把 q2 對 k1 到 k4
計算 attention 呢
你就把 q2 放在這邊
本來是只有 q1 要乘上 k1 到 k4
我們現在把 q2 也乘上 k1 到 k4
得到 α2,1 到 α2,4
那怎麼拿 q3 去做attention
怎麼拿 q4 去做 attention 呢
你現在的操作是一模一樣的
你就把 q3 乘 k1 到 k4
得到了 attention
你把 q4 乘上 k1 到 k4
得到 attention 的分數
所以這些 attention 的分數是怎麼來的
你可以看作是兩個矩陣的相乘
一個矩陣它的 row
就是 k
k1 到 k4
另外一個矩陣它的 column 就是 q
就是 q1 到 q4
把這個 k 所形成的矩陣
乘上 q 所形成的矩陣
就得到這些 attention 的分數
所以這邊就寫說
k 的 transpose
我們假設這個 k 它的 column
就是 k1 到 k4
所以在這邊相乘的時候
我們要對 K 這個矩陣做一下 transpose
K 的 transpose 乘上 Q
就得到一個矩陣叫做 A
那 A 裡面存的就是 Q 跟
這個 K 之間的 attention 的分數
好 那我們說我們會在 attention 的分數
做一下 normalization
比如說你會做 softmax
你會對這邊的每一個 column
每一個 column 做 softmax
讓每一個 column 裡面的值相加是 1
那我們之前有講過說 其實這邊做 softmax
不是唯一的選項
你完全可以選擇其他的操作
比如說 ReLU 之類的
那其實得到的結果也不會比較差
好 反正我們這邊就是把 a
通過了 softmax 以後
它得到的值有點不一樣了
所以我們用 A'
來表示通過 softmax 以後的結果
好 那接下來呢
我們已經計算出 A' 以後
接下來呢
接下來下一個步驟
我發現我這個投影片上有一個小小的 bug
這個 bug 是什麼呢
這個 bug 就是這邊應該是 prime
而不是 hat
這邊應該是 prime 不是 hat
本來最早是寫成hat
那我才會把它改成 prime
但這個地方沒有改過來
好 那我們把這個 v
v1 到 v4
乘上這邊的 α 以後
就可以得到 b
得到 b
好 那個 b 是怎麼被計算出來的呢
你就把 v1 到 v4 拼起來
你把 v1 到 v 4當成是
V 這個矩陣的四個 column
把它拼起來
然後接下來你把 v 乘上
A' 的第一個 column 以後
你得到的結果就是 b1
如果你熟悉線性代數的話
你知道說把這個 A' 乘上 v
這個你就是把 A'的第一個 column
乘上 V 這一個矩陣
你會得到你 output 矩陣的第一個 column
而把 A 的第一個 column
乘上 V 這個矩陣做的事情
其實就是把 V 這個矩陣裡面的
每一個 column
根據第 A' 這個矩陣裡面的每一個
這個 column 裡面每一個 element
做 weighted sum
那就得到 b1
那就是這邊的操作
把 v1 乘上 weight
把 v2 乘上 weight
把 v3 乘上 weight
把 v4 乘上 weight
全部加起來得到 b1
如果你是用矩陣操作的角度來看它
就是把 A' 的第一個 column 乘上 V
就得到 b1
然後接下來就是以此類推
接下來就不需要再跟你詳加說明
免得你覺得這一段非常地冗
就是以此類推
把 A' 的第二個 column 乘上 V
就得到 b2
A' 的第三個 column 乘上 V 就得到 b3
A' 的最後一個 column 乘上 V
就得到 b4
好 所以我們等於就是把 A' 這個矩陣
乘上 V 這個矩陣
得到 O 這個矩陣
O 這個矩陣裡面的每一個 column
就是 Self-attention 的輸出
也就是 b1 到 b4
所以其實整個 Self-attention
我們在講操作的時候
我們在最開始的時候 跟你講的時候我們講說
我們先產生了 q k v
然後再根據這個 q 去找出相關的位置
然後再對 v 做 weighted sum
其實這一串操作
就是一連串矩陣的乘法而已
怎麼說呢
我們再複習一下我們剛才看到的矩陣乘法
I 是什麼
I 是我們的 input
是 Self-attention 的 input
Self-attention 的 input
是一串的 vector 是一排的vector
這排 vector 拼起來當作矩陣的 column
就是 I
所以 I 是 Self-attention 的 input
那這個 input 分別乘上三個矩陣
Wq Wk 跟 Wv
得到 Q K V 這三個矩陣
接下來 Q 乘上 K 的 transpose
Q K V 都算出來了
你把 Q 乘上 K 的 transpose
得到 A 這個矩陣
A 的矩陣你可能會做一些處理
得到 A'
那有時候我們會把這個 A'
叫做 Attention Matrix
然後接下來你把 A' 再乘上 V
就得到 O
O 就是 Self-attention 這個 layer 的輸出
所以 Self-attention 輸入是 I
輸出是 O
那你會發現說雖然是叫 attention
這邊做了一個很複雜的操作
但是其實 Self-attention layer 裡面
唯一需要學的參數
就只有 Wq Wk 跟 Wv 而已
只有 Wq Wk Wv 是未知的
是需要透過我們的訓練資料把它找出來的
所以 Wq Wk Wv 是未知的
是需要被找出來的
但是其他的操作都沒有未知的參數
都是我們人為設定好的
都不需要透過 training data 找出來
只有 Wq Wk Wv
是透過 training data 找出來的
好 那這整個就是 Self-attention 的操作
從 I 到 O 就是做了 Self-attention
而那 Self-attention 有一個進階的版本
叫做 Multi-head Self-attention
那 Multi-head Self-attention
其實今天的使用是非常地廣泛的
那在作業 4 裡面
助教原來的 code 4 有
Multi-head Self-attention
它的 head 的數目是設成 2
那剛才助教有給你提示說
把 head 的數目改少一點 改成 1
其實就可以過 （00：14：55）
但並不代表所有的任務
都適合用比較少的 head
有一些任務
比如說翻譯
比如說語音辨識
其實用比較多的 head
你反而可以得到比較好的結果
那至於需要用多少的 head
這個又是另外一個 hyperparameter
也是你需要調的
那為什麼我們會需要比較多的 head 呢
你可以想成說相關這件事情
我們說
我們在做這個 Self-attention 的時候
我們就是用 q 去找相關的 k
但是相關這件事情有很多種不同的形式
有很多種不同的定義
所以也許我們不能只有一個 q
我們應該要有多個 q
不同的 q 負責不同的
不同種類的相關性
所以假設你要做 Multi-head Self-attention 的話
你會怎麼操作呢
你可能會這麼操作
你先把 a 乘上一個矩陣得到 q
接下來
你再把 q 乘上另外兩個矩陣
分別得到 q1 跟 q2
那這邊還有 這邊是用兩個上標
i 代表的是位置
然後這個 1 跟 2 代表是
這個位置的第幾個 q
所以這邊有 qi,1 跟 qi,2
代表說我們有兩個 head
我們認為這個問題
裡面有兩種不同的相關性
是我們需要產生兩種不同的 head
來找兩種不同的相關性
那既然 q 有兩個
那 k 也就要有兩個
那 v 也就要有兩個
那怎麼從 q 得到 q1 q2
怎麼從 k 得到 k1 k2
怎麼從 v 得到 v1 v2
那其實就是把 q 把 k 把 v
分別乘上兩個矩陣 兩個矩陣 兩個矩陣
得到不同的這個
得到這個不同的 head
就這樣子而已
好 那所以對另外一個位置
也做一樣的事情
另外一個位置 a 在輸入以後
它也會得到兩個 q 兩個 k 兩個 v
那接下來怎麼做 Self-attention 呢
跟我們之前講的操作是一模一樣的
只是現在
1 那一類的一起做
2 那一類的一起做
也就是這個 q1
它在算這個 attention 的分數的時候
它就不要管那個 k2 了
不要管 k2
它就只管 k1 就好
只管 k1 就好
所以 qi,1 就跟 ki,1 算 attention
qi,1 就跟 kj,1 算 attention
也就是算這個 dot product
然後得到這個 attention 的分數
然後今天在做 weighted sum 的時候
也不要管 v2 了
看 Vi,1 跟 vj,1 就好
所以你把 attention 的分數乘 vi,1
把 attention 的分數乘 vj,1
然後接下來就得到 bi,1
好 那你得到 bi,1 以後
這邊只用了其中一個 head
那你會用另外一個 head
也做一模一樣的事情
所以 q2 只對 k2 做 attention
q2 只對 k2 做 attention
它們在做 weighted sum 的時候
只對 v2 做 weighted sum
然後接下來你就得到 bi,2
如果你有多個 head
有 8 個 head 有 16 個 head
那也是一樣的操作
那這邊是用兩個 head 來當作例子
來給你看看有兩個 head 的時候
是怎麼操作的
現在得到 bi,1 跟 bi,2
然後接下來你可能會把 bi,1 跟 bi,2
把它接起來
然後再通過一個 transform
也就是再乘上一個矩陣
然後得到 bi
然後再送到下一層去
那這個就是 Multi-head attention
一個這個 Self-attention 的變形
好 那講到目前為止
你會發現說 Self-attention 的這個 layer
它少了一個也許很重要的資訊
這個資訊是什麼呢
這個資訊是位置的資訊
你想想看對一個 Self-attention 而言
對一個 Self-attention layer 而言
每一個 input
它是出現在 sequence 的最前面
還是最後面
它是完全沒有這個資訊的 對不對
你可能會說 欸 剛才不是說
input 就是有位置 1 2 3 4 嗎
但是那個 1 2 3 4 是我們畫在投影片上的時候
為了幫助大家理解所標上的一個編號
你想想看對 Self-attention 而言
那個位置 1 跟位置 2 跟位置 3 跟位置 4
有任何差別嗎
完全沒有任何差別 對不對
這四個位置的操作其實是一模一樣
對它來說 q1 到跟 q4 的距離
並沒有特別遠
1 跟 4 的距離並沒有特別遠
2 跟 3 的距離也沒有特別近
對它來說就是天涯若比鄰
所有的位置之間的距離都是一樣的
沒有任何一個位置距離比較遠
也沒有任何位置距離比較近
也沒有誰在整個 sequence 的最前面
也沒有誰在整個 sequence 的最後面
好 但是這樣子設計可能會有一些問題
因為有時候位置的資訊也許很重要
舉例來說
我們在做這個 POS tagging
就是詞性標記的時候
也許你知道說動詞比較不容易出現在句首
所以如果我們知道說
某一個詞彙它是放在句首的
那它是動詞的可能性可能就比較低
會不會這樣子的位置的資訊往往也是有用的
可是在我們到目前為止
講的 Self-attention 的操作裡面
它根本就沒有位置的資訊
所以怎麼辦呢
所以你做 Self-attention 的時候
如果你覺得位置的資訊是一個重要的事情
那你可以把位置的資訊把它塞進去
怎麼把位置的資訊塞進去呢
這邊就要用到一個叫做
positional encoding 的技術
這個技術是這樣子的
你為每一個位置設定一個 vector
叫做 positional vector
這邊用 ei 來表示
上標 i 代表是位置
每一個不同的位置
就有不同的 vector
就是 e1 是一個 vector
e2 是一個vector
e128 是一個vector
不同的位置都有一個它專屬的 e
然後把這個 e 加到 ai 上面
就結束了
你等於就是告訴你的 Self-attention
位置的資訊
如果它看到說 ai 好像有被加上 ei
它就知道說現在出現的位置
應該是在 i 這個位置
那這個 ei 長什麼樣子呢
最早的這個 transformer
就 Attention Is All You Need 那篇 paper 裡面
它用的 ei 長的是這個樣子
這邊這個圖上面
每一個 column 就代表一個 e
第一個位置就是 e1
第二個位置就是 e2
第三個位置就是 e3
以此類推
所以它就是把這邊這個向量
放在第一個位置
把這個向量加到第二個位置的 a上
把這個向量加到第三個位置的 a 上
以此類推
每一個位置都有一個專屬的 e
希望透過給每一個位置不同的 e
你的 model 在處理這個 input 的時候
它可以知道現在的 input
它的位置的資訊是什麼樣子
那這樣子的 positional vector
它是 handcrafted 的
也就是它是人設的
那人設的這個 vector 有很多問題
就假設我現在在定這個 vector 的時候
只定到 128
那我現在 sequence 的長度
如果是 129 怎麼辦呢
不過今天在最早的那個
Attention Is All You Need paper裡面
比較沒有這個問題
它這個 vector 是透過某一個規則所產生的
透過一個很神奇的
sin cos 的 function 所產生的
當然你又會有新的問題
為什麼是 sin 跟 cos 呢
為什麼不是別的東西呢
為什麼一定要這樣產生
handcrafted 的這個 positional vector 呢
其實你不一定要這麼產生
這個 positional encoding
仍然是一個尚待研究的問題
你可以創造自己新的方法
或甚至 positional encoding
是可以根據資料學出來的
好 那有關 positional encoding
你可以再參考一下文獻
這個是一個尚待研究的問題
比如說我這邊引用了一篇
這個是去年放在 arxiv 上的論文
所以可以想見這其實都是很新的論文
裡面就是比較了跟提出了
新的 positional encoding
比如說這個是最早的 positional encoding
它是用一個神奇的 sin function 所產生的
那如果你的 positional encoding
你把 positional encoding 裡面的數值
當作 network 參數的一部分
直接 learn 出來
看起來是這個樣子的
這個圖是那個橫著看的
它是橫著看的
它是每一個 row
代表一個 position
好 所以這個是這個最原始的
用 sin function 產生的
這個是 learn 出來的
它裡面又有神奇的做法
比如說這個
這個是用 RNN 生出來的
positional encording 是用 RNN 出來的
這篇 paper 提出來的叫做 FLOATER
是用個神奇的 network 生出來的
總之你有各式各樣不同的方法
來產生 positional encoding
那目前我們還不知道哪一種方法最好
這是一個尚待研究中的問題
所以你不用糾結說
為什麼 Sinusoidal 最好
你永遠可以提出新的做法
好 那這個 Self-attention 當然是用得很廣
我們已經提過很多次 transformer 這個東西
那我們大家也都知道說
在 NLP 的領域有一個東西叫做 BERT
BERT 裡面也用到 Self-attention
所以 Self-attention 在 NLP 上面的應用
是大家都耳熟能詳的
但 Self-attention
不是只能用在 NLP 相關的應用上
它還可以用在很多其他的問題上
比如說在做語音的時候
你也可以用 Self-attention
不過在做語音的時候啊
用 Self-attention
你可能會希望對 Self-attention
做一些小小的改動
舉例來說
因為一般語音的
如果你要把一段聲音訊號
表示成一排向量的話
這排向量可能會非常地長
因為我們之前有說過說
在做語音辨識的時候
你要把聲音訊號表示成一排向量
而每一個向量
其實只代表了 10 millisecond 的長度而已
所以如果今天是 1 秒鐘的聲音訊號
它就有 100 個向量了
5 秒鐘的聲音訊號
就 500 個向量了
你隨便講一句話
都是上千個向量了
所以一段聲音訊號
你要描述它的時候
那個像這個 vector 的 sequence 它的長度
是非常可觀的
那可觀的 sequence
可觀的長度
會造成什麼問題呢
你想想看
我們今天在計算這個 attention matrix 的時候
它的 complexity 是長度的平方
你要計算這個 attention matrix A′
你需要做 L 乘以 L 次的 inner product
那如果這個 L 的值很大的話
它的計算量就很可觀
如果 L 很大的話
你也需要很大的這個 memory
才能夠把這個矩陣存下來
所以今天如果在做語音辨識的時候
你講一句話
那一句話所產生的這個 attention matrix
可能會太大
大到你根本就不容易處理
不容易訓練
所以怎麼辦呢
在做語音的時候
有一招叫做 Truncated Self-attention
Truncated Self-attention 做的事情就是
我們今天在做 Self-attention 的時候
不要看一整句話
就我們就只看一個小的範圍就好
那至於這個範圍應該要多大
那個是人設定的
那為什麼我們知道說
今天在做語音辨識的時候
也許只需要看一個小的範圍就好
那就是取決於你對這個問題的理解
也許我們要辨識這個位置有什麼樣的（00：26：58）
這個位置有什麼樣的內容
我們並不需要看整句話
只要看這句話
跟它前後一定範圍之內的資訊
其實就可以判斷
所以如果在做 Self-attention 的時候
也許沒有必要看過一整個句子
也許沒有必要讓 Self-attention 考慮一整個句子
也許只需要考慮一個小範圍就好
這樣就可以加快運算的速度
好 這個是 Truncated Self-attention
那其實 Self-attention 啊
還可以被用在影像上
Self-attention 怎麼被用在影像上呢
那到目前為止
我們在講 Self-attention 的時候
我們都說 Self-attention 適用的範圍
是輸入是一排向量的時候
輸入是一個 vector set 的時候
它適合使用 Self-attention
那我們之前在講 CNN 的時候
我們是跟大家說影像啊
一張圖片啊
我們把它看作是一個很長的向量
那其實一張圖片
我們也可以換一個觀點
把它看作是一個 vector 的 set
怎麼把一張圖片看作是一個 vector 的 set 呢
這個是一個解析度 5 乘以 10 的圖片
那這一張圖片呢
可以看作是一個 tensor
這個 tensor 的大小是 5 乘以 10 乘以 3
3 代表 RGB 這 3 個 channel
那你可以把每一個位置的 pixel
看作是一個三維的向量
所以每一個 pixel
其實就是一個三維的向量
那整張圖片
其實就是 5 乘以 10 個向量
所以我們其實可以換一個角度
來看影像這個東西
影像這個東西
其實也是一個 vector set
它既然也是一個 vector set 的話
你完全可以用 Self-attention 來處理一張圖片
那有沒有人用 Self-attention 來處理一張圖片呢
是有的
那這邊就舉了兩個例子
來給大家參考
那現在把 Self-attention 用在影像處理上
也不算是一個非常石破天驚的事情
好 那我們可以來比較一下
Self-attention 跟 CNN 之間
有什麼樣的差異或者是關聯性
如果我們今天
是用 Self-attention 來處理一張圖片
代表說
假設這個是你要考慮的 pixel
那它產生 query
其他 pixel 產生這個
其他 pixel 產生 key
它產生 query
其他 pixel 產生 key
你今天在做 inner product 的時候
你考慮的不是一個小的（00：29：29）
而是整張影像的資訊
所以
但是今天在做 CNN 的時候
記不記得我們上上週講的 CNN 呢
會畫出一個 receptive field
每一個 filter
每一個 neural
只考慮 receptive field 範圍裡面的資訊
所以如果我們比較 CNN 跟 Self-attention 的話
我們可以說
CNN 是什麼
CNN 可以看作是一種簡化版的 Self-attention
因為在做CNN的時候
我們只考慮 receptive field 裡面的資訊
而在做 Self-attention 的時候
我們是考慮整張圖片的資訊
所以 CNN
是簡化版的 Self-attention
或者是你可以反過來說
Self-attention 是一個複雜化的 CNN
在 CNN 裡面
我們要劃定 receptive field
每一個 neural
只考慮 receptive field 裡面的資訊
而 receptive field 的範圍跟大小
是人決定的
我記得我們上上週
還花了一些時間講一下說 receptive field
有什麼樣可能的設計
而對 Self-attention 而言
我們用 attention
去找出相關的 pixel
就好像是 receptive field 是自動被學出來的
network 自己決定說
receptive field 的形狀長什麼樣子
network 自己決定說
以這個 pixel 為中心
哪些 pixel 是我們真正需要考慮的
那些 pixel 是相關的
所以 receptive field 的範圍
不再是人工劃定
而是讓機器自己學出來
好 這邊講的是 Self-attention 跟 CNN 的關係
其實你可以讀一篇 paper
叫做 On the Relationship
between Self-attention and Convolutional Layers
在這篇 paper 裡面
會用數學的方式嚴謹的告訴你說
其實這個 CNN
就是 Self-attention 的特例
Self-attention 只要設定合適的參數
它可以做到跟 CNN 一模一樣的事情
所以 CNN 它的 function set 長這個樣子
而 Self-attention 它的 function set 長這個樣子
所以 self attention
是更 flexible 的 CNN
而 CNN 是有受限制的 Self-attention
Self-attention 只要透過某些設計
某些限制
它就會變成 CNN
那這也不是很舊的 paper
你發現它放到網路上的時間呢
是 19 年的 11 月
所以你知道這些
我們今天上課裡面講的東西
其實都是很新的資訊
好 既然 CNN 是 Self-attention 的一個 subset
Self-attention 比較 flexible
那記不記得我們在講 overfitting 的時候
我們講說
比較 flexible 的 model
比較需要更多的 data
如果你 data 不夠
就有可能 overfitting
而小的 model
而比較有限制的 model
它適合在 data 小的
少的時候
它可能比較不會 overfitting
那如果你這個限制設的好
也會有不錯的結果
如果你今天呢
用不同的 data 量
來訓練 CNN 跟 Self-attention
你確實可以看到我剛才講的現象
那這個實驗結果啊
來自於 An image is worth
16 乘以 16 的 words
這個是 Google 的 paper
它就是把這個 Self-attention
apply 在影像上面
那其實把一張影像呢
拆成 16 乘以 16 個 patch
它把每一個 patch 呢
就想像成是一個 worth
因為一般我們這個 Self-attention
比較常用在 NLP 上面嘛
所以他就說
想像每一個 patch 其實就是一個 word
所以他就取了一個很 fancy 的 title
叫做一張圖呢
值 16 乘以 16 個文字
那這個橫軸是什麼
橫軸是訓練的影像的量
那你發現說
對 Google 來說 用的
所謂的資料量比較少
也是你沒有辦法用的資料量啦
這邊有 10 個 million 就是
1000 萬張圖
是資料量比較小的 setting
然後資料量比較大的 setting 呢
有 3 億張圖片
在這個實驗裡面呢
比較了 Self-attention 是淺藍色的這一條線
跟 CNN 是深灰色的這條線
那你就會發現說
隨著資料量越來越多
那 Self-attention 的結果呢
就越來越好
最終在資料量最多的時候
Self-attention 可以超過 CNN
但在資料量少的時候
CNN 它是可以比 Self-attention
得到更好的結果的
那為什麼會這樣
你就可以從 CNN 跟 Self-attention
它們的彈性來加以解釋
這個 Self-attention 它彈性比較大
所以需要比較多的訓練資料
訓練資料少的時候
就會 overfitting
而 CNN 它彈性比較小
在訓練資料少的時候
結果比較好
但訓練資料多的時候
它沒有辦法從更大量的訓練資料得到好處
所以這個就是 Self-attention 跟 CNN 的比較
那你可能問說
那 Self-attention 跟 CNN
誰比較好呢
我應該選哪一個呢
事實上你也可以都用 對不對
在我們作業四裡面
如果你要做 strong baseline 的話
就特別給你一個提示
就是用 conformer
裡面就是有用到 Self-attention
也有用到 CNN
好 那我們來比較一下
Self-attention 跟 RNN
RNN 呢
就是 recurrent neural network
那其實在這門課裡面呢
我們現在就不會講到 recurrent neural network
因為 recurrent neural network 的角色
很大一部分都可以用 Self-attention 來取代了
所以在這門課裡面
我們就不會再特別把 RNN 拿出來講
但是 RNN 是什麼呢
假設你想知道的話
那這邊很快地三言兩語把它帶過去
RNN 跟 Self-attention 一樣
都是要處理 input 是一個 sequence 的狀況
那在 RNN 裡面呢
這是你的 input sequence
你有一個 memory 的 vector
然後你有一個 RNN 的 block
這個 RNN 的 block 呢
它吃 memory 的 vector
吃第一個 input 的 vector
然後 output 一個東西
然後根據這個 output 的東西
我們通常叫做這個 hidden
這個 hidden 的 layer 的 output
然後通過這個 fully connected network
然後再去做你想要的 prediction
那 RNN 這個 module 呢
接下來當第二個
這個 sequence 裡面
第二個 vector 作為 input 的時候
你會把這個 vector
第二個 vector 當做 input
也會把前一個時間點吐出來的東西
當做下一個時間點的輸入
再丟進 RNN 裡面
然後再產生新的 vector
再拿去給 fully connected network
做你想要做的事情
然後第三個 vector 進來的時候
你把第三個 vector 跟前一個時間點的輸出
一起丟進 RNN
再產生新的輸出
然後在第四個時間點
第四個 vector 輸入的時候
把第四個 vector 跟前一個時間點
產生出來的輸出
再一起做處理
得到新的輸出
再通過 fully connected network 的 layer
這個就是 RNN
Recurrent Neural Network
那你會發現說
它跟 Self-attention 做的事情其實也非常像
它們的 input 都是一個 vector sequence
Self-attention output 是
另外一個 vector sequence
這裡面的每一個 vector
都考慮了整個 input sequence 以後
再給 fully connected network 去做處理
那 RNN 呢
它也會 output 另外一群 vector
這另外一排 vector 也會給
fully connected network 做進一步的處理
那 Self-attention 跟 RNN 有什麼不同呢
當然一個非常顯而易見的不同
你可能會說
啊 這邊的每一個 vector
它都考慮了整個 input 的 sequence
而 RNN 每一個 vector
只考慮了左邊已經輸入的 vector
它沒有考慮右邊的 vector
那這是一個很好的觀察
但是 RNN 其實也可以是雙向的
所以如果你 RNN 用雙向的 RNN 的話
其實這邊的每一個 hidden 的 output
每一個 memory 的 output
其實也可以看作是考慮了
整個 input 的 sequence
但是假設我們把 RNN 的 output
跟 Self-attention 的 output 拿來做對比的話
就算你用 bidirectional 的 RNN
還是有一些差別的
對 RNN 來說
假設最右邊這個黃色的 vector
要考慮最左邊的這個輸入
那它必須要把最左邊的輸入呢
存在 memory 裡面
然後接下來都不能夠忘掉
一路帶到最右邊
才能夠在最後一個時間點被考慮
但對 Self-attention 來說沒有這個問題
它只要這邊輸出一個 query
這邊輸出一個 key
只要它們 match 得起來
天涯若比鄰
你可以從非常遠的 vector
在整個 sequence 上非常遠的 vector
輕易地抽取資訊
所以這是 RNN 跟 Self-attention
一個不一樣的地方
還有另外一個更主要的不同是
RNN 今天在處理的時候啊
你 input 一排 sequence
output 一排 sequence 的時候
RNN 是沒有辦法平行化的
你要先產生這個向量
接下來才能產生這個向量
才能產生這個向量
所以 RNN 它今天 input 一排是 vector
output 另外一排 vector 的時候
它沒有辦法一次處理
沒有辦法平行處理所有的 output
但 Self-attention 有一個優勢
是它可以平行處理所有的輸出
你今天 input 一排 vector
再 output 這四個 vector 的時候
這四個 vector 是平行產生的
並不需要等誰先運算完才把其他運算出來
output 的這個 vector
裡面的 output 這個 vector sequence 裡面
每一個 vector 都是同時產生出來的
所以在運算速度化
運算速度上
Self-attention 會比 RNN 更有效率
那你今天發現說
很多的應用都往往把 RNN 的架構
逐漸改成 Self-attention 的架構了
好 如果你想要更進一步了解
RNN 跟 Self-attention 的關係的話
你可以看下面這篇文章
Transformers are RNNs
裡面會告訴你說
Self-attention 你加上了什麼東西以後
其實它就變成了 RNN
發現說這也不是很舊的 paper
這個是去年的六月放到 arXiv 上
所以今天講的都是一些很新的研究成果
好 那 RNN 的部分呢
我們這門課就不會提到
假設你對 RNN 有興趣的話
以下是這一門課之前的上課錄影
那 RNN 的部分
因為這一次不會講到
所以特別有做了英文的版本
RNN 呢 是中文英文版本
都同時有放在 YouTube 上面
好 最後呢
Self-attention 也可以被用在 Graph 上面
記得我們在這一堂 這一份投影片
一開始的時候就有跟你說過說
Graph 也可以看作是一堆 vector
那如果是一堆 vector
就可以用 Self-attention 來處理
所以 Self-attention 也可以用在 Graph 上面
但是當我們把 Self-attention
用在Graph 上面的時候
有什麼樣特別的地方呢
在 Graph 上面呢
我們不只有每一個 node
我們每一個 node 可以表示成一個向量
但我們不只有 node 的資訊
我們還有 edge 的資訊
我們知道哪些 node 之間是有相連的
也就是哪些 node 是有關聯的
我們知道哪些向量間是有關聯
那之前我們在做 Self-attention 的時候
所謂的關聯性是 network 自己找出來的
但是現在既然有了 Graph 的資訊
有了 edge 的資訊
那關聯性也許就不需要透過機器自動找出來
這個圖上面的 edge 已經暗示了我們
node 跟 node 之間的關聯性
所以今天當你把 Self-attention
用在 Graph 上面的時候
你有一個選擇是你在做這個
Attention Matrix 計算的時候
你可以只計算有 edge 相連的 node 就好
舉例來說在這個圖上
node 1 跟 node 8 有相連
那我們只需要計算 node 1 跟 node 8
這兩個向量之間的 attention 的分數
那 1 跟 6 相連
所以只有 1 跟 6 之間
需要計算 attention 的分數
1 跟 5 有相連
所以只有 1 跟 5 需要計算 attention 的分數
2 跟 3 有相連
所以只有 2 跟 3 需要計算 attention 的分數
以此類推
那如果兩個 node 之間沒有相連
那其實很有可能就暗示我們
這兩個 node 之間沒有關係
既然沒有關係
我們就不需要再去計算它的 attention score
直接把它設為 0 就好了
因為這個 Graph 往往
比如說是人為根據
某些 domain knowledge 建出來的
那 domain knowledge 告訴我們說
這兩個向量彼此之間沒有關聯
我們就沒有必要再用機器去學習這件事情
那其實當我們把 Self-attention
按照我們這邊講的這種限制
用在 Graph 上面的時候
其實就是一種 Graph Neural Network
也就是一種 GNN
那我知道 GNN
現在也是一個很 fancy 的題目
那我不會說 Self-attention 就要囊括了
所有 GNN 的各種變形了
但把 Self-attention 用在 Graph 上面
是某一種類型的 Graph Neural Network
那這邊呢
一樣我們也沒有辦法細講了
GNN 這邊坑也是很深啊
這邊水是很深
那就放一下助教之前上課的連結
大概花了快三個小時
在講 Graph Neural Network
而且其實還沒有講完
就告訴你說這個 Graph Neural Network
也是有非常深的技術
這邊水也是很深
那這不是我們今天這一堂課可以講的內容
好 那其實這個 Self-attention 啊
它有非常非常多的變形
你可以看一篇 paper 叫做
Long Range Arena
裡面比較了各種不同的 Self-attention 的變形
因為 Self-attention 它最大的問題就是
它的運算量非常地大
所以怎麼樣減少 Self-attention 的運算量
是一個未來的重點
可以看到這邊有
各種各式各樣 Self-attention 的變形
那 Self-attention 最早是
用在 Transformer 上面
所以很多人講 Transformer 的時候
其實它指的就是這個 Self-attention
有人說廣義的 Transformer
指的就是 Self-attention
那所以後來各式各樣的
Self-attention 的變形都這樣做
都叫做是什麼 former
比如說 Linformer Performer Reformer 等等
所以 Self-attention 的變形
現在都叫做 xxformer
那可以看到說呢
往右代表它運算的速度
所以有很多各式各樣新的 xxformer
它們的速度會比原來的 Transformer 快
但是快的速度帶來的就是 performance 變差
這個縱軸代表是 performance
所以它們往往比原來的 Transformer
performance 差一點
但是速度會比較快
那到底什麼樣的 Self-attention
才能夠真的又快又好
這仍然是一個尚待研究的問題
如果你對 Self-attention
想要進一步研究的話
你還可以看一下
Efficient Transformers: A Survey 這篇 paper
裡面會跟你介紹
各式各樣 Self-attention 的變形
那這個就不是我們這一門課可以講
