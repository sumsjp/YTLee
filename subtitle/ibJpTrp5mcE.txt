臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
各位同學，大家好
我們開始上課吧
我們今天要講的是 Backpropagation
也就是實際上，如果你要用 Gradient Descent 的方法
來 train 一個 neural network 的時候呢
你應該要怎麼做？
那我們上次其實已經講過了 neural network 的基本架構
那我就發現，在作業二裡面
好多人都已經 implement neural network 的 approach
那或許你已經對這個方法非常地清楚了
但是我不知道說，你是不是清楚說
到底實際上，在 train neural network 的時候
Backpropagation 這個 algorithm 是怎麼運作的
我們就來講一下 Backpropagation 這個 algorithm
是怎麼讓 neural network 的 training 變得比較有效率
那在 Gradient Descent 裡面
我們知道說 Gradient Descent 的方法就是
假設你的 network 有一大堆的參數
一堆 w，一堆 b
那你先選一個初始的參數
你先選一個初始的參數在哪裡
然後計算這個 θ^0 對你的 loss function 的 Gradient
也就是計算每一個
network 裡面的參數，w1, w2, b1, b2 ......等等
對你的 L(θ) 的偏微分
計算出這個東西以後，這個 gradient 其實是一個 vector
計算出這個 vector 以後，你就可以去更新你的參數
你就把 θ^0 減掉 learning rate
乘上 gradient，然後得到 θ^1
那這個 process 就持續繼續下去
再算一遍，在 θ^1 的 gradient
然後，再把 θ^1 減掉 gradient
update 成 θ^2，這個 process 就一直持續下去
所以，在 neural network 裡面呢
當你用 Gradient Descent 方法的時候， 跟我們在做 Logistic Regression
還有 Linear Regression 等等，是沒有太大的差別的
但是，最大的差別就是
最大的問題是，在 neural network 裡面
我們有非常非常多的參數
那現在，如果你要做語音辨識系統的話呢
你的 neural network 通常會有，7, 8 層
每層有 1000 個 neuron，它有上百萬個參數
所以，這個 vector 呢
它是非常非常長的
這是一個上百萬維的 vector
所以，現在最大的困難就是
你要如何有效地去把這一個百萬維的 vector
有效地把它計算出來
那這個就是 Backpropagation 在做的事情
所以，Backpropagation 並不是一個
和 Gradient Descent 不同的
training 的方法，它就是 Gradient Descent
它只是一個比較有效率的演算法
讓你在計算這個 gradient，這個 vector 的時候
是可以比較有效率地把這個 vector 計算出來
其實，Backpropagation 呢，裡面沒有
特別高深的數學
你唯一需要記得的就只有 Chain Rule
那我們用一張投影片，迅速地幫大家複習一下， 什麼是 Chain Rule
假設你現在有兩個 function，h 跟 g
g input x 就得到 y
h input y 就得到 z
所以，如果你今天給 x 一個小小的變化的話
給 x 一個變化的話，它會影響到它的 output y
所以，y 會跟這有變化
y 有變化以後，又會影響到 z，所以 z 會跟著有變化
如果我們今天要計算 dz/dx 的話
我們要計算 x 對 z 的微分的話
要怎麼算呢？你可以把它拆成兩項
x 對 z 的微分
它就等於 y 對 z 的微分
乘上 x 對 y 的微分
那這個怎麼來的，你就問一下微積分老師
你可以就把這個 y 消掉
所以左邊就等於右邊， 但不要讓你的微積分老師知道這件事情
那第二個 case
我們來看 Chain Rule 第二個 case
假設現在有 3 個 function，g, h, k
g input s 就得到 x
h input s 就得到 y
k input x, y 就得到 z
所以，今天假如你改變了 s
改變了 x，改變了 s
你會透過 g 和 h 這兩個 function 改變 x 跟 y
改變了 s，就改變了 x 跟 y
那改變了 x 跟 y 以後，透過 k 這個 function
k 這個 function，input x 跟 y，output 是 z
改變了 x 跟 y 以後，你就改變了 z
所以，今天如果你要計算 s 對 z 的微分的話
那這個 s，是透過兩條路徑去影響 z
它可以透過 x 去影響 z，也可以透過 y 去影響 z
所以，s 對 z 的微分
就可以寫成，就拆成兩項
拆成兩項，根據這兩條 path 拆成兩項
這個 s 對 z 的微分
就可以寫成 x 對 z 的微分
乘上 s 對 x 的微分，就是上面這條路徑
加上 y 對 z 的微分乘上 s 對 y 的微分，也就是
下面這條路徑
所以，你大一微積分有好好學的話呢
這個就是我們都學過的 Chain Rule
那我們等一下呢，會需要用到這個東西
那再回到這個 neural network 的 training
我們知道說，我們會定一個 loss function
那這個 loss function 是甚麼呢？
這個 loss function 是 summation over
所有 training data 的
某一個 loss 值，C^n
我們說，假設給定我們一組 neural network 的參數 θ
我們把一個 training data x^n
代到這個 neural network 裡面，它會 output 一個 y^n
那同時呢，我們會有一個
我們希望這個 neural network 的 output：y^n\head
它希望它如果 output y^n\head 的話
就是最正確的，那我們會
定義一個 y^n 跟 y^n\head 之間距離的 function
這邊寫作 C^n
C^n 代表 y^n 跟 y^n\head 之間的距離
如果 C^n 大的話，代表 y^n 跟 y^n\head 之間的距離很遠
所以，這個 network 的 parameter 的 loss
是比較大的，是比較不好的
那如果這個 C^n 很小的話，代表
這個 parameter 是好的
那我們 summation over 所有 training data 的 C^n
summation over 所有 training data
根據這個參數 θ
它的 output 跟它的目標
它的 output y^n 跟它的目標 y^n\head 之間的距離
就是得到我們的 total loss，L
那你把這個式子
左右兩邊都對某一個參數 w 做偏微分的話呢
你就得到右邊這個式子
你就得到 ∂L/∂w 等於
summation over 所有的 training data
n = 1 到 N
∂C^n (θ)/∂w
這個應該是沒有甚麼問題
之所以寫這個式子，只是要講說
接下來，我們就不用
計算 ∂L/∂w
我們就只考慮，我們如何去計算
對某一筆 data 的 ∂C^n (θ)/∂w
你只要能夠把一筆 data 的
∂C^n (θ)/∂w 算出來
再 summation over 所有的 training data
你就可以把 total loss 對某一個參數的
偏微分算出來了
所以，我們等一下就只 focus 在怎麼計算
對某一筆 data，它的 cost C^n 對 w 的偏微分
我們就只 focus 在怎麼計算這一項上面
怎麼做呢？
我們先考慮某一個 neuron
我們先從底下這個 neural network 裡面
拿一個 neuron 出來，考慮它
那這一個 neuron，它是在第一個 layer 的 neuron
所以，它前面的 input
就是外界給它的 input，x1, x2
假設它只有兩個 input，x1 跟 x2
分別乘上 weight w1, w2，再加上 b
會得到 z
這個我想大家應該都非常熟悉，這個 z 呢
就是 x1w1 + x2w2 + b
那得到這個 z 以後，通過了 activation function
再經過了非常非常多的事情以後
你會得到最終的 output，y1, y2
那現在的問題是這樣
假設我們從這邊拿一個 w 出來
等一下，我們就拿 w 當作 z
但是 b 也是一樣，就拿 weight 當作 z
來看怎麼計算，某一個 weight 對 cost
對 example 的某一個 cost 的偏微分
那 b 的話，想必你可以以此類推，就把它算出來
那 ∂C/∂w 怎麼算？
這個 ∂C/∂w 阿
按照 Chain Rule，你就可以把它拆成兩項
∂z/∂w * ∂C/∂z
這個 z 可以把它消掉，沒有問題
所以 ∂C/∂w 可以根據 Chain Rule 拆成兩項
那這兩項，我們就分別去把它計算出來
前面這項是很簡單的
後面這項，是比較複雜的
那計算前面這一項
計算 ∂z/∂w 的這個 process 呢
我們稱之為 Forward pass
那你等下會知道說為什麼叫 Forward pass
那計算前面這一項 ∂C/∂z 的 process
我們稱之為 Backward pass
那我們等一下會講說，為什麼叫做 Backward pass
那我們就先看一下，怎麼來計算這個 ∂z/∂w
怎麼來計算 ∂z/∂w
好，那我們先看這個 w1
那你怎麼計算 ∂z/∂w1 呢？
就是秒算
就是秒算，因為 z 就長這個樣子嘛
然後，w1 在這邊
所以，一眼就可以知道說，它是 x1
那 ∂z/∂w2 呢？
所以，你一眼就可以看出說
它就是 x2，這個都是秒算這樣子
那它的規律是這樣，它的規律就是
∂z/∂w，就是看這個 w 前面接的東西是什麼
那微分以後就是什麼
這個 w1 前面，它的 input 是接 x1
它的 input 是 x1，所以微分以後就是 x1
w2 呢，它前面的 input 是 x2
所以微分以後就是 x2
那它的規律就是這個樣子
所以，今天假如給你一個 neural network
那它裡面有一大推的參數
一大堆的參數，但是你要
計算這裡面每一個參數
對 z 的偏微分
你要計算這裡面每一個參數的 ∂w 跟 ∂z
這件事情呢，非常非常的容易
因為我們剛才知道，它的規律就是
∂z/∂w 就是看你這個 w 的 input 是什麼，它就是甚麼
所以，如果有人問你說，現在 input 是 1 跟 -1
那這個 1，它對
它的 activation function 的 input z 的偏微分是什麼呢？
你就可以瞬間回答它說，就是 -1
因為這個 1，前面接 -1
所以，這個參數對 z 的偏微分就是 -1
同理，比如說這個 -1，它對 z 的偏微分就是 1
這個 -2，它對 z 的偏微分就是 1
這個 1，它對 z 的偏微分就是 1，以此類推
接下來呢
接下來假如有人問你說，這個 w
對它的 activation function 的 input z 的偏微分是什麼呢？
你其實也可以瞬間就回答它
你只要知道說，這個 w 前面接的 weight
前面接的 input 是什麼
那這個 w 前面接的 input 是
某一個 neuron 的 output，對不對？
這個 w 前面接的 input
是第一個 hidden layer 的 neuron 的 output
那這個 hidden layer 的 neuron 的 output 要怎麼算呢？
這個大家都知道，對不對？就是把
1 跟 -1 丟進去，然後根據我們熟悉的 neuron 的運算
然後看看它的 output 是什麼，就是什麼
在這個例子裡面呢
假如這個 function 是 sigmoid function
算出來是 0.98, 0.12
如果你可以算出這兩個 neuron 的 output 是
0.98 跟 0.12 的話
那這個 weight
它做完偏微分以後
這個 weight 對
它的 activation function 的 input z 做完偏微分以後
顯然就是 0.12，因為它前面接的 weight 就是 0.12
這個 -1 也是 0.12
這個 -2 是 0.98，這個 2 是 0.98
這個也很直覺
所以，同樣的 process 你就反覆的在做
你可以得到這兩個紅色 neuron 的 output 是 0.86, 0.11
那你就可以秒反應說，這個
4 對 z 的偏微分就是 0.11
所以，你要算出這個 neural network 裡面的
每一個 weight
對它的 activation function 的 input z 的偏微分
你就把你的 input 丟進去
然後，計算每一個 neuron 的 output
就結束了
所以，這個步驟叫做 Forward pass
它是非常容易理解的
再來，我們要講的是 Backward pass
也就是怎麼算 ∂C/∂z
這個你就不會覺得很困難了
因為，這個 z 阿
它通過 activation function 以後得到 output
後面還有非常非常複雜的 process
它才得到這個 C
要經過非常複雜的 process 才能得到 C
這個 ∂C/∂z 顯然是很複雜的
不過我們可以用 Chain rule 再把這一項做一下拆解
假設這個 activation function 是 sigmoid function
我這邊就寫一個 σ(z)
z 通過 sigmoid function 得到 a
這個 neuron 的 output 是 a
那接下來會發生甚麼事呢？
這個 a 會通過某一個 weight，乘上某一個 weight
再加其他一大堆的 value，得到 z'
它是下一個 neuron activation function 的 input
這個 a 會再乘上另一個 weight，這邊寫成 w4
再加上其他一大堆東西，得到 z"
後面這個 z' 跟 z"
之後可能還會發生很多很多的事情
不過我們就先只考慮下一步會發生什麼事情
所以呢
我們知道說 ∂C/∂z
你可以寫成 ∂a/∂z * ∂C/∂a
那這個就沒有什麼問題，∂a 可以消掉
那 ∂a/∂z 是什麼呢？
我們知道說，a = σ(z)
那這個 ∂a/∂z 呢
其實就是這個 sigmoid function 的微分
那 sigmoid function 長這個樣子
長這個樣子，綠色這條線
那它的微分，你就算一下，長這個樣子
長這樣子，我們之前已經看過很多次了
所以，∂a/∂z 也沒有問題
接下來的問題就是，∂C/∂a
應該長甚麼樣子呢？
它應該長甚麼樣子呢？
那我們就接下來看說，這個 a
∂a 跟 C 的關係是怎樣
你知道 a 它會影響 z'
然後，z' 會影響 z
a 會影響 z"，z" 會影響 C
a 透過 z' 跟 z" 去影響 C
所以，∂C/∂a 你可以寫成
∂z'/∂a * ∂C/∂z'
加上 ∂z"/∂a * ∂C/∂z"
我們假設 a 後面，就是這個藍色 neuron 的下一個 layer
就是紅色的 neuron 只有兩個
所以，這邊就只有兩項
這個 a 只會影響 z' 跟 z"
如果這邊有 1000 個 neuron 的話
那這邊這個 Chain rule
你的 summation 就是 summation over 1000 項
這樣大家了解我的意思嗎？
這邊呢，經過前面我們簡化上課的說明
我們假設只有兩個 neuron，這邊只有兩項
只有兩項，a 只會影響 z' 跟 z"
接下來，∂z"/∂a
會算嗎？這個就是秒算，對不對？這個就是 w3
z' 等於 a 乘上 w3，再加上一些有的沒的東西
所以，這個 z' 對 a 做偏微分
根據這個式子，顯然就是 w3
所以，同理，這個 z" 對 a 做偏微分
得到的結果顯然就是 w4
所以，這兩項算起來，也不是個問題
最後的問題就是，z" 對 C 的偏微分怎麼算呢？
這個 z" 對 C 的偏微分怎麼算呢？
因為我們不知道
z' 對 C 有什麼關係
z" 對 C 有什麼關係
這後面還有發生很多很多的事情
是很複雜的，所以我們搞不清楚後面會發生什麼事情
所以，我們一下子不知道這兩項怎麼算
不過沒關係，我們就假設我們知道
假設這兩項的值，我們已經 somehow 透過
某種方法把它算出來
我們透過一個等一下會講，但你還不知道怎麼做的方法
就已經把這兩項做出來了
那把這兩項算出來以後
把這兩項算出來以後呢
我們就可以把 ∂C/∂z
輕易的算出來
把這兩項算出來以後
我們就可以算 ∂C/∂z
你會算 ∂C/∂z' 跟 ∂C/∂z"
你就會算 ∂C/∂z
然後，再把這些值
代到我們剛才看到的 ∂C/∂z 的式子裡面
就得到這樣一個式子，σ'(z) * [w3 * ∂C/∂z' + w4 * ∂C/∂z"]
你就算出這樣一個式子，那這個式子
還滿簡單的
但是，我們會從另外一個觀點，來看待這個式子
你可以想像說
現在有另外一個 neuron
這個 neuron 並不在我們原來的 network 裡面
有另外一個 neuron
我把它簡化成這個三角形
把它畫成三角形
那這個 neuron 的 input，就是 ∂C/∂z' 跟 ∂C/∂z"
那第一個 input ∂C/∂z'，就乘上 w3
∂C/∂z" 它就乘上 w4
再乘上 activation function， σ'(z)
得到 output，就是 ∂C/∂z
上面這個 neuron 所做的運算
跟下面這個式子，是一模一樣的
我們只是把下面這個式子
把它畫出來，讓它看起來像是一個 neuron 一樣
那這個 σ'(z) 阿
這個 σ'(z) 其實是一個常數
對不對，它不是一個 function，它是一個constant
因為，z 其實在計算 Forward pass 的時候
就被決定好了
z 是一個已經固定的值
z 我們已經知道它是多少，所以
在給定 z 的情況下
這個 σ'(z)，它就是一個常數
所以，這個 neuron
跟我們之前看到的 sigmoid function 是不一樣的
它並不是把 input 通過一個 non-linear 的轉換
而是直接把 input 乘上一個 constant，σ'(z)
就得到一個 output 這樣
所以，我把這個 neuron 畫成三角形的
代表它跟我們之前看到的圓形 neuron
的運作方式是不一樣的，它是直接乘上一個 constant
那你可能會問說，為甚麼是三角形呢
因為我是電機系的，我覺得這是一個 op-amp 這樣子
op-amp 就會乘上一個 constant，它是一個放大器這樣
聽不懂就算了，這不太重要
然後，這樣問題都解決了
都解決了，對不對，我們現在最後的問題就只有
怎麼算這兩項而已
假設能夠算這兩項，問題也就都解決了
那現在怎麼算這兩項呢？
怎麼算這兩項呢？
我們現在假設兩個不同的 case
第一個 case 是，我們假設現在紅色的這兩個 neuron
就已經是 output layer
這兩個紅色 neuron 是在 output layer 裡面
它們的 output 就已經是整個 network 的 output 了
這邊寫成 y1, y2
它的 output 就已經是整個 network 的 output 了
所以，今天你要算 ∂C/∂z'
就很簡單，根據 Chain rule 算 ∂(y1)/∂z' * ∂C/∂(y1)
∂(y1)/∂z' 沒什麼問題
你只要知道這個 activation function 長甚麼樣子
這項就輕而易舉地算出來了
∂C/∂(y1)，y1 對 C 有什麼影響，depend on 你的
你的 cost function 是怎麼定義的
你的 output 跟 target 間是怎麼 evaluate 的
你可以用 cross entropy，你可以用 mean square error
你用不同的定義，這邊這項就不一樣，但總之
它是一個比較簡單的東西，你可以把它算出來
那 ∂C/∂z"，這你也可以算，沒有甚麼問題
就是 ∂(y2)/∂z" * ∂C/∂(y2)
這兩項一樣都是可以秒算
所以，今天假設這個藍色的 neuron 後面
它的下一個 layer 就已經是 output layer 了
這個藍色的 neuron，它在最後一個 hidden layer 裡面
它後面就已經是 output layer 了
那根據我們剛才所學的東西
你就結束了，你就可以把 w1 跟 w2 對 C 的偏微分算出來
所以，這個沒有甚麼問題
那我們真正煩惱的問題是 case 2
假設現在紅色的 neuron 它並不是整個 network 的 output
它後面還有其他東西的話，怎麼辦呢？
那它後面的其他東西，可能長甚麼樣子呢
它可能長這樣，就是 z' 再通過 activation function 得到 a'
再乘上另外一個 weight，w5
再加上一些其他的東西，得到 za
然後，你再把 a' 乘上
w6 再加上其他一大堆東西
得到 zb，然後再丟進另外兩個
activation function 裡面
那現在的問題是這樣
我們想要求 ∂C/∂z'
如果我們知道 ∂C/∂(za) 跟 ∂C/∂(zb)
我們就可以計算 ∂C/∂z'
對嗎？
我們剛才已經有講過說，假設我們知道
∂C/∂z' 跟 ∂C/∂z"
我們就可以算前面一個 layer 的 ∂C/∂z
所以，按照一模一樣的式子
如果知道 ∂C/∂(za) 跟 ∂C/∂(zb)
我們就可以算 ∂C/∂z'
按照一模一樣的式子
就是我們剛才算看到那個 op-amp 的式子
所以，你就把 ∂C/∂(za) 乘上 w5
∂C/∂(zb) 乘上 w6，加起來再通過 op-amp
乘上 σ'(z')
再乘上 op-amp 就得到這個 ∂C/∂z' 的 output
那現在這個問題，就反覆地繼續下去
我們剛才說知道 z' 跟 z" 的偏微分就可以算 z
現在知道 za 跟 zb 的偏微分就可以算 z'
但是，我們又不知道 za 跟 zb 的偏微分
怎麼算，對不對？
你不知道這兩項怎麼算
如果你會這兩項的話，你就把這一項算出來
但問題就是，你不知道
那怎麼辦呢？
我們就再往下一層去看
了解嗎？就是這個綠色的 neuron
如果它是 output layer 的話
如果這個綠色的 neuron，它是 output layer 的話
要計算這兩個東西，就是秒算，沒有問題
假設它不是 output layer 的話
你就繼續走下去
再看下一個 layer
它的 activation function input 對 C 的偏微分
那你就可以把這一項算出來
你就可以把這兩項算出來，再把這一項算出來
你如果沒辦法算這兩項，他們不是 output layer 的話
你就再去推，下一個 layer 它的偏微分會是甚 麼樣子
把這兩個東西推出來，然後再往前把這兩個東西推出來
那你可能會想說，這個方法聽起來還頗崩潰
就是你每次要算一個微分的值
你要一直往後走，走到 network 的 output
如果 network 有 10 層，那你從第一層開始往後展開
感覺應該是一個很可怕的式子
但是實際上，並不是這樣子做的
實際上，你只要換一個方向
從 output layer 的 ∂C/∂z 開始算
你就會發現說，它的運算量
跟原來的 network 的 Feedforward path
其實是一樣的
假設我們現在有 6 個 neuron
每一個 neuron，它的 activation function
input 分別是 z1, z2, z3 一直到 z6
我們現在要計算這些 z 對 C 的偏微分
那本來呢，我們應該是想要知道 z1 的偏微分
你就要算 z3, z4 的偏微分
假如想知道 z3 的偏微分，你就要算 z5 跟 z6 的偏微分
想要知道 z4 的偏微分，你就要算 z5 跟 z6 的偏微分
那你要先算出 z5, z6 的偏微分，你才能算出
z3 的偏微分，z4 的偏微分
你才能夠算出 z1 的偏微分，z2 的偏微分
那如果我們今天是從
z1, z2 的偏微分開始算
那就沒有效率
但是，如果你反過來先算 z5, z6 的偏微分的話
這個 process，就突然之間變得有效率起來了
我們就先算 z5, z6 對 C 的偏微分
然後，算出這兩項以後
你就可以算出 z3, z4 對 C 的偏微分
然後，你就可以算出 z1, z2 對 C 的偏微分
這樣大家懂嗎？講到這邊大家有問題嗎？
沒有哦，那這整件事情可以說
這兩個東西怎麼得到它的偏微分
這兩個東西怎麼得到它的偏微分呢？
我們剛才已經看過了，就是用一個 op-amp 來算
這每一個 op-amp 它放大的倍率呢
就是 σ'(z1), σ'(z2), σ'(z3), σ'(z4)
所以，你算出了
你就先很快地計算 ∂C/∂(z5), ∂C/∂(z6)
然後，再把這一個偏微分的值，跟這個偏微分的值
乘上一些 weight
再通過 op-amp
你就得到這兩個偏微分的值
再乘上一些 weight
再通過一個 op-amp，就得到一些偏微分的值
就這樣，就計算完了
就計算完了，所以，你再算 Backpropagation 的時候
你在這個步驟，叫做 Backward pass
你在做這個 Backward pass 的時候
你實際上的做法就是，建另外一個 neural network
我們本來有一個正向的 neural network，裡面的
activation function 都是 sigmoid function
那現在，你在算 Backward pass 的時候
你就是建一個反向的 neural network
反向的 neural network
從右邊到左邊，反向的 neural network
這個反向的 neural network，它的 activation function 呢
你要先算完 Forward pass 以後
你才算得出來，然後，接下來呢
這個反向的 neural network，它的 input 就是這兩項
這兩項
然後，其他部分就跟一般的 neural network 運算一樣
你就做一個 Backward pass
但是，其實就是做一個 neural network 的運算
你就可以把每一個 z 對 C 的偏微分
就都算出來了，這個就是 Backward pass
所以，我們就 summarize 一下
Backpropagation 是怎麼做的
首先，你做一個 Forward pass
在做 Forward pass 的時候，你可以算出
只要你知道每一個 activation function 的 output
那 activation function 的 output 就是
它所連接的 weight 的 ∂z/∂w
那在 Backward pass 裡面，你要把
原來的 neural network 的方向呢
倒過來，你把原來的 neural network 的方向倒過來
那在這個倒過來的 neural network
它的每一個三角形的 output 呢
就是 ∂C/∂z
然後，把它們乘起來
你就知道某一個 weight
對 w 的偏微分是什麼了
就結束，這樣
講到這邊，大家有沒有甚麼問題呢？
就算你沒有聽懂這個東西
其實，也沒有什麼關係啦
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
