臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
我把 Dimension Reduction 分成兩種
一種做的事情，叫做化繁為簡
它可以分成兩大類，一種是做 Clustering
一種是做 Dimension Reduction
所謂的化繁為簡的意思是說
你現在有很多種不同的 input
比如說，你現在找一個 function
然後它可以 input 看起來像樹的東西
output 都是抽象的樹
也就是把本來比較複雜的 input
變成比較簡單的 output
那在做 Unsupervised Learning 的時候
你通常只會有 function 的其中一邊
比如說，我們要找一個 function，它可以把
所有的樹都變成抽象的樹
但是，你所能擁有的 training data
就只有一大堆的 image
一大堆的、各種不同的 image
你不知道說它的 output，應該要是長什麼樣子
那另外一個 Unsupervised Learning 會做的事情呢
是 Generation，也就是無中生有
它做的事情是這樣，我們要找一個 function
這個 function，你隨機給它一個 input
我給它一個 random number，比如說輸入一個數字 1
然後，它就會 output 這棵樹
輸入數字 2，就 output 另外一棵樹
輸入數字 3，就 output 另外一棵
你輸入一個，給它一個 random number
它就自動畫一張圖出來
不同的 number 給它，它畫出來的圖就不一樣
在這個 task 裡面，我們要找的這個可以畫圖的 function
你只有這一個 function 的 output
但是你沒有這一個 function 的 input
你就只有一大堆的 image
但是，你不知道輸入怎麼樣的 code
才可以得到這些 image
那在這一份投影片，我們先 focus 在
Dimension Reduction 這一件事情上
而且我們只 focus 在 linear 的 Dimension Reduction
那我們現在說一下什麼是 clustering，什麼是 clustering 呢
clustering 就是說，這個太直覺了
這種地方比較沒有什麼特別好講的
有一大堆的 image，假設我們現在要做 image 的 clustering
就有一大堆 image
然後，你就把它們分成一類、一類、一類的
之後，你就可以說
這一邊所有的 image 都是屬於 cluster 1
這邊都屬於 cluster 2，這邊都屬於 cluster 3
就給它們貼標籤的意思
那這樣你就把本來有些不同的 image
都用同一個 class 來表示
就可以做到化繁為簡這一件事情
那這邊最 critical 的問題就是
到底應該要有幾個 cluster，那這個東西沒有什麼好的方法
就跟 neural network 要幾層一樣，是 empirical 地去決定
你要需要幾個 cluster，這個當然不能太多
比如說，你多到說
這邊有 9 張 image，你就有 9 個 cluster
那你乾脆就不要做 clustering
這樣 image 就自己一個 cluster
那這樣有做跟沒有做是一樣的
或者是你說，我們全部的 image
我們只有一個 cluster
全部的 image 都放在同一個 cluster 裡面
那也是有做跟沒有做一樣
但是，要怎麼樣選擇適當的 cluster
這個你就要 empirical 的來決定它
那在 clustering 的方法裡面
最常用的叫做 K-means
很快地講一下，K-means 是怎麼做的
K-means 就是這樣，我們有一大堆的 data
它們都是 unlabeled
一大堆的 data，x^1 一直到 x^N
那這邊每一個 x，它可能就代表一張 image
那我們要把它做成 K 個 cluster
怎麼做呢，我們要先
找這些 cluster 的 center
假如這一邊每一個
object 都是用一個 vector 來表示的話
這一邊這些 center，也是一樣長度的 vector
那我們每一個 cluster 都要先找一個 center
有 K 個 cluster，我們就需要 c^1 到 c^K 個 center
那這些 center
這個初始的 center 怎麼來的呢
你可以從你的 training data 裡面
隨機的找 K 個 object 出來
就是你的 K個 center
接下來，你要對所有在 training data 裡面的 x
都做以下的事情，都做什麼事情呢
你決定說現在的每一個 object
屬於 1 到 K 的哪一個 cluster
那假設現在你的 object, x^n
跟第 i 個 cluster 的 center 最接近的話
那 x^n 就屬於 c^i
那我們用一個 binary 的 value，b (上標n, 下標 i)
來代表說，第 n 個 object 有沒有屬於第 i 個 class
如果第 n 個 object 屬於第 i 個 class 的話
那這一個 binary 的 value 就是 1
反之就是 0
接下來，你要 update 你的 cluster
怎麼 update 你的 cluster 呢
這個方法也是很直覺
你就把所有屬於
假設你要 update 第 i 個 cluster 的 center
你就把所有屬於第 i 個 cluster 的 object
通通拿出來，做平均
你就得到第 i 個 cluster 的 center
然後，你要反覆的做
那這邊之所以你在做 initialization 的時候
你會想要直接從你的
這個 database 裡面去挑 K 個 example
挑 K 個 object 出來做 center 呢
有一個很重要的原因是，假設你是純粹隨機的
你不是從 data points 裡面去挑
你很有可能在第一次 assign 這個 object 的時候
就沒有任何 example 跟某一個 cluster 很像
又有可能是，有某一個 cluster，它沒有任何 example
你現在 update 的時候
程式就會 segmentation fault
所以，你最好就是從你的 training data 裡面
選 K 筆 example 出來當 initialization
clustering 有另外一個方法叫做
Hierarchical Agglomerative Clustering (HAC)
那這個方法
是先建一個 tree，假設你現在有 5 個 example
你想要把它做 cluster
那你先做成一個 tree structure
怎麼建這個 tree structure 呢
你把這 5 個 example
兩兩、兩兩去算他的相似度
然後挑最相似的那一個 pair 出來
假設現在最相似的 pair，是第一個和第二個 example
你就把第一個 example 和第二個 example merge 起來
比如說，把它們平均起來
得到一個新的 vector
這個 vector 同時代表第一個和第二個 example
接下來，你再算說
現在變成有四個 example 了
現在只剩下 4 筆 data 了
把這 4 筆 data，再兩兩去算它的相似度
然後發現說，第三筆和第四筆
最後這兩筆是最像的
那你再把它們 merge 起來，把它們平均起來
得到另外一筆 data
現在只剩三筆 data，再去兩兩算它的 similarity
然後，你發現說
黃色這一個和中間這一個最像
你就再把它們平均起來
你會發現，最後只剩紅色跟綠色，你只好把它平均起來
那你就得到這個 tree 的 root
你就根據這 5 筆 data，它們之間的相似度
建立出一個 tree structure
接下來，你要決定一個
你沒有做 clustering，你只是建了一個 tree structure
這個 tree structure 可以告訴我們說
哪些 example 是比較像的
比較早分支代表比較不像
比如說 ，這三個跟這兩個
一開始在 root 的地方就分成兩類
所以，它們這三個跟這兩個就比較不像
那接下來你要做 clustering，怎麼做呢
你要決定在這個樹上
在這個 tree structure 上面切一刀
比如說，你決定切在這一邊
如果你切在這個地方的時候
你就把你的五筆 data
變成三個 cluster
根據這一刀
這兩個 example 是一個 cluster
它自己一個 cluster，這兩個 example 是一個 cluster
如果你這一刀切在這個地方
那就變成這 3 筆是一個 cluster
這三筆是一個 cluster，這兩筆是一個 cluster
如果，你這一刀切在這邊，那你就變成
分成總共四個 cluster
這個是 HAC 的做法
那 HAC 跟 K-means，我覺得最大的差別就是
你如何決定你 cluster 的數目
在 K-means 裡面，你要
決定你那個 K 的 value 是多少
有時候你覺得說，到底有多少個 cluster 我不容易想
那你可以換成 HAC
好處就是，你現在把
你現在不直接決定幾個 cluster，而是
決定你要在樹上切
切在這個樹的 structure 的哪裡
那有人會覺得說
有時候你會覺得這樣子是比較容易的話
那學 HAC 對你來說，就有一些 benefit
但是，光只做 cluster
是非常卡的
在做 cluster 的時候
我們就是以偏概全，因為每一個 object
都必須要屬於某一個 cluster
這就好像是說，我們知道說
念能力有分成六大類
然後，每一個人
都會被 assign 成這六個念能力的其中一類
他怎麼決定他是哪一類呢？
你就做水鑑識這樣子
你拿一杯水，然後看看它會有什麼反應
然後，就把他塞成某一類
比如說，水滿出來了
就是強化，所以小傑就是強化系
那我們知道說，這樣子把每一個人
都 assign 成念能力裡面的某一個系
是不夠的
是太過粗糙武斷
比如說，像比司吉就有說
小傑其實是接近放出系的強化系念能力者
所以小傑，如果你只是說他是強化系
其實是 loss 掉很多 information
你應該這樣來表示小傑
你應該說，他強化系是 0.7
他放出系是0.2，那其實強化系跟變化系是比較接近的
所以有強化系，你也會有部分的變化系的能力
所以，變化系是 0.05，像小傑可以用剪刀
那個是變化系的能力，然後其他系是 0
所以，你應該要用
如果你只是把你手上所有的 object
分別 assign 到它屬於哪一個 cluster
這樣是以偏概全，這樣太粗了
你應該要用一個 vector
來表示你的 object
那這個 vector裡面的每一個 dimension
就代表了某一種特質
某一種 attribute
那這件事情
就叫做 Distributed 的 representation
如果你原來的 object 是一個
非常 high dimension 的東西，比如說 image
那你現在把它用它的
attribute，把它用它的特質來描述
它就會從比較高維的空間變成比較低維的空間
這一件事情就叫做 Dimension Reduction
那它們其實是一樣的事情
只是有不同的稱呼而已
那我們從另外一個角度來看一下 為什麼 Dimension Reduction
是可能是有用的
舉例來說，假設你的 data 的分布
是長這個樣子
在 3D 的空間裡面
你現在的 data 的分布是長這個螺旋的樣子
但是，用3D的空間
來描述這些 data
其實是很浪費的
你從直覺上就會知道說
其實你可以把這個
這個類似地毯捲起來的東西
把它攤開、把它攤平
就變成這樣
所以，你其實只需要在 2D 的空間
就可以描述這個 3D 的 information
你根本不需要把這個問題放到 3D 來解
這樣是把問題複雜化
你其實在 2D 就可以做這個 task
或者是，我們舉一個比較具體的例子
比如說，我們考慮 MNIST
在 MNIST 裡面，每一個 input 的 digit
它是一個 image，它都用 28*28 的 dimension
來描述它
但是，實際上
多數 28*28 的 dimension 的 vector
你把它轉成一個 image
看起來都不像是一個數字
你 random sample 一個 28*28 的 vector
轉成 image 看起來可能是這樣
它其實根本就不像數字
所以在這個 28維 * 28維的空間裡面
是 digit 的 vector，其實是很少的
所以，其實你要描述一個 digit
或許根本不需要用到 28*28 維
你要描述一個 digit
你要的 dimension 可能是遠比
28維 * 28維少
所以，如果我們舉一個很極端的例子
比如這邊有一堆 3，然後這一堆 3
如果你是從 pixel 來看待它的話
你要用 28維 * 28維
來描述每一張 image
然而，實際上這一些 3，只需要用一個維度
就可以來表示
為甚麼呢？因為這些 3 就只是說
把原來的 3 放正
是中間這張 image
右轉 10 度轉就變成它
右轉20度就變它，左轉10度變它，左轉20度就變它
所以，你唯一需要記錄的事情只有
今天這張 image，它是左轉多少度、右轉多少度
你就可以知道說，它在 28 維的空間裡面
應該長什麼樣子
你只需要抓住這一個重點
你只要抓住這個角度的變化
你就可以知道 28 維空間中的變化
所以，只需要用一維就可以描述這些 image
所以，這就像我剛才舉的例子
這個是樊一翁，這個就是他的頭這樣子
那怎麼做 Dimension Reduction 呢
在做 Dimension Reduction 的時候
我們要做的事情就是
找一個 function
這個 function 的 input 是一個 vector, x
它的 output 是另外一個 vector, z
但是，因為是 Dimension Reduction，所以
你 output 的這個 vector, z
它的 dimension 要比 input 的 x 還要小
這樣你才是做 Dimension Reduction
在做 Dimension Reduction 裡面
最簡單的方法是 Feature Selection
這個方法就沒有什麼好講的，這個方法是這樣
現在如果你把你的 data 的分布拿出來看一下
本來在二維的平面上
但是，其實你發現都集中在 x2 的 dimension 而已
所以，x1 這個 dimension 沒什麼用，把它拿掉
就只有 x2 這個 dimension
你就等於是做到 Dimension Reduction 這件事
但是這個方法不見得總是有用，因為有很多時候是
你的 case 是，你任何一個 dimension 都
其實都不能拿掉
那另外一個常見的方法叫做
Principle Component Analysis
叫做 PCA
這個 PCA 怎麼做呢？這個 PCA 做的事情是這樣
它說這個 function 是一個很簡單的 linear function
這個 input, x 跟這個 output, z 之間的關係
就是一個 linear 的 transform
也就是你把這個 x 乘上一個 matrix, W
你就得到它的 output, z
那現在要做的事情就是
根據一大堆的 x，我們現在不知道 z 長什麼樣子
只有一大堆的 x
根據一大堆的 x，我們要把這個 W 找出來
那如果你要知道比較細節的東西的話
你可以看一下 Bishop 的第十二章
接下來，我們介紹一下 PCA
我剛才講過 PCA 要做的事情
就是找這一個 W
那這個 W 怎麼找呢？
假設我們現在考慮一個
比較簡單的 case
我們考慮一個 one dimensional 的 case
什麼意思呢
我們現在假設，我們只要把我們的 data project 到
一維的空間上面
也就是我們的 z 呢
它只是一個一維的 vector，所謂一維的 vector
就是一個 scalar，我們的 z 是一個 scalar
或者是說，我們可以這樣寫
就是，如果我們這邊 z 只是一個 scalar 的話
那我們的 W 其實就是一個
一個 row 對不對，就是一個 row 而已
那我們就用 w^1 來表示
w 的第一個 row
我們把 x 跟 w 的第一個 row, w^1
做 inner product，我們就得到一個 scalar, z1
接下來，我們要問的問題是
我們要找的這個 w^1
應該要長什麼樣子
首先，我們先假設 w^1 的長度是 1
w^1 的 2-norm 是 1
這個假設是有必要的，等一下你會看得更清楚為什麼
一定要有這一個假設
如果 w^1 的 2-norm 是 1 的話
那 w^1 跟 x 做 inner product
得到的 z1 意味著什麼
它意謂著說，現在呢
w 跟 x 是高維空間中的一個點
w^1 是高維空間中的一個 vector
那所謂的 z1 就是 x
在 w^1 上面的投影
它這個投影的值
就是 w^1 跟 x 的 inner product
這個就沒有什麼問題了
這個就是大一線代裡面教過的東西
所以，我們現在要做的事情就是
把一堆 x
透過 w^1 把它投影變成 z1
我們就得到一堆 z1，每一個 x 都變成一個 z1
現在的問題就是，w^1 應該長什麼樣子呢
要選哪一個 w^1，舉例來說
假設這個是 x 的分布
這個 x 的分布是什麼呢
它是橫座標，在每一個點代表一隻寶可夢
它的橫座標是攻擊力
它的縱座標是防禦力
那今天如果我要選
我們要把這個二維的，這邊這個 x 都是二維的
我把這個二維投影到一維
我應該要選什麼樣的 w^1？
我可以選這樣的 w^1
我可以選 w^1 指向這個方向
我也可以選 w^1 指向這個方向
我選不同的方向
我最後得到的結果，得到的 projection 的結果
它會是不一樣的
那我們會想要
你總是要給我一個目標，我才知道要選什麼樣的 w^1
現在的目標是這樣
我們希望選一個 w^1
它經過 projection 以後，得到的這些 z1 的分布
是越大越好
也就是說，我們不希望通過這個 projection 以後
所有的點通通擠在一起
就變成把本來 data point 和 data point 之間的奇異度
拿掉了，我們是希望說
經過這個 projection 以後，不同的 data point
它們之間的區別
我們仍然是可以看得出來
所以，我們希望找一個 projection 的方向，它可以讓
projection 後的 variance 越大越好
如果我們看這一個例子的話
你就會覺得說，如果是選這個方向的話
經過 projection 以後，你的點可能是分布在
這個地方，大概分布在這個 range
那如果是 projection 在這個方向的話
那你點的分布可能是這個 range
所以，如果 projection 在這個方向上的話
你會有比較大的 variance
但是在這個方向上的話，你會有一個比較小的 variance
所以你要選 w^1 的時候，你可能
選 w^1 的方向是指向這個方向
其實這個 w^1 代表什麼意思呢
如果從這個圖，你可以看到說
這個 w^1 其實是代表了寶可夢的強度
寶可夢可能會有一個隱藏的 vector 代表它的強度
這個隱藏的 vector，同時影響了它的
防禦力跟攻擊力
所以，防禦力跟攻擊力是會同時上升的
如果我們要用 equation 來表示它的話
你就會說我們現在要去 maximize 的對象是
z1 的 variance
z1 的 variance 就是 summation over 所有的 z1
(z1 - z1\bar) 的平方，z1\bar 就是做 z1 的平均
那這樣子
我們等一下再講怎麼做
假設你知道怎麼做，你解一解
你找到一個 w^1，你就可以讓 z1 最大
那你就找到這個 w^1，就結束了
再來，你可能不只要投影到一維
你想要投影到更多維，比如說，你想要投影到一個
二維的平面
那如果你想要投影到二維的平面的時候
這個時候你就把 x 跟另外一個 w^2
做 inner product，得到 z2
這個 z1 跟 z2 串起來就得到這邊的 z
這個 w^1 跟 w^2 的 transpose 排起來就是
W 的第一個 row 跟第二個 row
好，那這一個 z2
我們要怎麼找這一個 w^2 呢
跟剛才找 z1 一樣
我們希望，首先 w^2 它的 2-norm 是 1
然後，接下來這個 z2 它的分佈也是越大越好
也是越大越好
但是，如果你只是要讓
讓 z2 的 variance 越大越好，讓這個式子越大越好
你找出來就不是 w^1，w^1 剛才已經找過了
對不對，所以你就等於什麼事都沒有做
所以，你要再加一個 constraint
這個 constraint 是我們剛才已經先找過 w^1 了
這個 w^2 要跟 w^1是垂直的
或者是 w^1 跟 w^2 是 orthogonal
w^1 跟 w^2，它們做 inner product 等於 0
藉由這個方法，你就可以
先找 w^1，再找 w^2，再找 w^3
就看你要 project 到幾維
你要 project 到幾維是你要自己決定的
這個就跟我們要幾個 cluster
要幾個 hidden layer 也是自己決定的
你要 project 到 K 維
那你就找 w^1, w^2 到 w^K
你就把你所有找出來的 w^1, w^2 到 w^K
排起來當作 W row
放在這一邊，就結束了
那這邊有一件事情就是，這個找出來的 W
它會是一個 orthogonal的 matrix
為什麼呢？
如果你看它的 row 的話
它的 w^1 跟 w^2 是 orthogonal的
然後 w^1 的 2-norm 跟 w^2 的 2-norm 都是 1
所以，它的 row 是
norm 是 1，而且互相之間都是 orthogonal
所以，它是一個 orthogonal的 matrix
接下來的問題就是
怎麼找 w^1 跟 w^2 呢？
怎麼解這個問題呢？
這邊其實有一個，這邊的這個解法其實是蠻容易的
這個怎麼解呢？
你其實要用 Lagrange multiplier
這邊有一個 warning of math，如果你沒有聽懂的話就算了
這個可以都直接 call 套件
這個其實 call 套件就有了
而且就算是你不會下面這套東西的話
你其實可以用 Gradient Descent 的方法
這我們之後會講，你可以把一個
你可以把 PCA 這件事情，描述成一個 neural network
然後，就用 Gradient Descent 的方法來解它
所以不一定要用 Lagrange multiplier 來做這個 PCA
這個 PCA 可以把它看成是一個 neural network 一樣
但我們現在很快講一下 Lagrange multiplier 這個方法
這個很經典的方法，它是怎麼做的呢？它是這樣子
我們說 z1 等於 w^1 跟 x 的 inner product
那 z1 的平均值是 summation over 所有的 z1
也就是 summation over 所有 w^1 跟 x 的 inner product
這邊是 summation over 所有 data point
跟 w^1 無關，所以可以把 w^1 提出來
變成先 summation over 所有的 x
再跟 w^1 做 inner product
得到 w^1 跟 x 的平均的 inner product
接下來，我們說我們要 maximize 的對象
是 z1 的 variance
那 z1 的 variance 我們可以寫成
(z1 - 它的平均值)的平方，再 summation over 所有的 z1
我們先把這一項整理一下
這項整理一下變成什麼樣子呢
z1 是 w^1 跟 x 的 inner product
z1\bar 是 w^1 跟 x\bar 的 inner product
然後，再取平方
那都有 w^1，所以把 w^1 提出來
變成 summation over [w^1 * (x - x\bar)]^2
那這個平方
你可以把它做一下轉化
怎麼轉化呢？
w^1 是一個 vector，(x - x\bar) 是另外一個 vector
兩個 vector，比如說 w^1 就是 a，(x - x\bar) 就是 b
a 跟 b 的 inner product 的平方
可以寫成 a 的 transpose 乘 b 的平方
可以寫成 a 的 transpose 乘 b， 再乘上 a 的 transpose 乘 b
然後這一項，其實可以寫成
a 的 transpose 乘 b，然後乘上
(a 的 transpose 乘 b) 再 transpose
為甚麼這邊可以直接加 transpose 呢
因為 (a^T)*b 是一個 scalar，所以再 transpose
還是它自己
那 transpose 以後
[(a^T)*b]^T 就是把它們順序對調，再加上 transpose
所以變成 (a^T)*b*(b^T)*a
然後把 a 代回 w^1
b 代回 (x - x\bar)，你就得到這樣的式子
(w^1)^T * (x - x\bar) * (x - x\bar)^T * w^1
接下來呢
你這一邊是 summation over 所有的 data
summation over 所有的 data，所以跟 w 無關
跟 w 無關，所以把 w^1 的 transpose 拿出去
把 w^1 拿出去
注意一下，這邊 summation 是 summation over
(x - x\bar)*(x - x\bar)^T 這一項
w^1 被拿出去了
那 (x - x\bar) 的 transpose
(x - x\bar) * (x - x\bar)^T summation over 所有 data point 是甚麼呢
它是 x 的 covariance
對不對，它是 x 的 covariance matrix，所以這一項
其實就是 (w^1)^T * Cov(x) * w^1
我們用 S 來描述 x 的 covariance matrix
所以，現在我們要解的問題是這樣
找出一個 w^1
它可以 maximize w^1 的 transpose
乘上一個 matrix, x
再乘上 w^1
但這個 optimization 的對象是有 constraint 的
如果沒有 constraint 的話
這個問題它會有無聊的 solution
你把 w^1 的每一個值都變無窮大，就結束了
所以，你要有 constraint
它的 constraint 是說
w^1 的 2-norm 要等於 1
那有了這些以後
我們就要解這一個 optimization 的 problem
這邊這個 S，S 是 covariance matrix， x 的 covariance matrix
它是 symmetric 的
而且因為它是 covariance matrix 的關係
它又是半正定
它是 positive-semidefinite
也就是說它所有的 eigenvalue
都是 non-negative 的
如果你對這件事情有困惑的話
你就回去翻一下線代課本，你其實可以在
我的個人的網頁上找到線代的教學
就在 machine learning 的網頁下面
我們就先講結論，假設你不想聽中間的過程的話
結論就是，這一個 programming 的 problem
這個 problem 它的 solution 就是
w^1 是 covariance matrix 的 eigenvector
它不只是一個 eigenvector
它是對應到最大的 eigenvalue
λ1 的那一個 eigenvector
這個就是結論
那中間的過程是怎麼樣呢？
中間的過程是，首先我們要用 Lagrange multiplier
不過，你對這個東西有碰過的話
這不是我們這一堂課應該講的
你就看一下 Bishop 的 Appendix
在做 Lagrange multiplier 的時候，這邊有個式子
這個式子長這樣子
它是先把這一項拿到這邊
再減掉 α 乘上這個 constraint
這個 constraint
然後接下來，你把這個 g
對所有的 w 做偏微分
那 w 是一個 vector
它裡面有很多的 element
所以你把這個 function
對 w 的第一個 element 做偏微分
對第二個 element 做偏微分
然後，令這些式子通通等於 0
整理一下以後，你會得到一個式子
這個式子，是這樣告訴我們的
這個式子告訴我們說
這邊的這個 solution
它會滿足下面這個式子
S*(w^1) - α*(w^1) 等於 0，再整理一下
變成這樣
那這個 w^1 呢
如果你寫成這樣，就是 S*(w^1) = α*(w^1) 的話
那 w^1 就是一個 eigenvector
w^1 就是 S 的 eigenvector
因為 (w^1)*S 等於自己乘上某一個 scalar
所以，w^1 是 S 的 eigenvector
但是現在，S 的 eigenvector 有一大把
有很多，而且你可以找到一大把 eigenvector
它的 norm 都是 1
所以，接下來你要做的事情是
看哪一個 eigenvector
代到這個式子裡面，可以 maximize
(w^1)^T * S * (w^1)
誰可以 maximize 它呢？
我們把這個整理一下
把 (w^1)^T * S * (w^1) 整理一下，它變成
這個 S*(w^1) 就是 α*(w^1)
這一項就變成 α * (w^1)^T * (w^1)
然後，(w^1)^T * (w^1)，這個是 1
所以，這一邊就得到個 α
然後，接下來就是找
誰可以讓這個 α 最大呢？
既然這個值等於 α 的話，誰可以讓 α 最大呢？
w^1 是對應到最大的 那個 eigenvalue 的 eigenvector 的時候
它可以讓 α 最大，這個 α 就是最大的 eigenvalues, λ1
如果你沒有聽懂的話，你只要記得這個結論就好
第二個，如果要找 w^2 的話
我們要解什麼樣的式子呢？我們如果要找 w^2 的話
我們要解的是
這樣子的一個 equation
我們要解說，我們要 maximize
我們要 maximize 根據 w^2 投影以後的 variance
這個寫成這樣，(w^2)^T * S * (w^2)
同時，w^2 要滿足 norm 等於 1
同時，w^2 跟 w^1 的 inner product
w^1 跟 w^2，它們要是 orthogonal 的
那這個結論是什麼呢？
解完這個問題，你會得到什麼呢？
你會得到說，w^2 也是 covariance matrix 的一個
eigenvector，然後，它對應到
第二大的 eigenvalue, λ2
現在，我們就要用 Lagrange Multiplier 來解它
這個解法就是，你先寫一個 function, g
這個 function, g 裡面包含了你要 maximize 的對象
還有你的兩個 constraint
然後，分別要乘上 α 跟 β
接下來，你對你所有的參數做偏微分
你對 w^2 裡面的每一個參數都做偏微分
你對 w^2 的第一個 element，w^2 的第二個 element
都做偏微分，做完以後
你得到這個值
S*(w^2) - α*(^2) - β*(^1) = 0
接下來，左邊同乘 w^1 的 transpose
乘 w^1 的 transpose
乘 w^1 的 transpose 會發生什麼事呢？
乘 w^1 的 transpose 以後，(w^1)^T * (w^1) 等於 1
這邊 (w^1)^T * (w^2) 等於 0
然後，這邊 (w^1)^T * S * (w^2) 等於甚麼呢？
因為這一項，它是一個 scalar
這是一個 vector，這是一個 matrix
這是一個 vector，所以，乘完以後是一個 scalar
scalar 在做 transpose 以後還是它自己
所以，你可以直接把它 transpose
結果是一樣的
做完 transpose 以後
你得到 (w^2)^T * (S^T) * (w^1)
因為 S 是 symmetric 的
所以，它做 transpose 以後還是它自己
所以，這一項 S^T 變成它自己，所以變這樣
接下來，我們已經知道
w^1是 S 的 eigenvector
而且它對應到最大的 eigenvalue, λ^1
所以，S*(w^1) = (λ^1)*(w^1)
然後，(w^1)*(w^2)^T 又等於 0
所以，這一項第一項是 0
所以，從這邊我們得到什麼結論呢？
我們得到的結論是
β = 0，得到的結論是 β = 0
如果 β 等於 0 的話，這一項就會被拿掉
所以剩下的就是 S*(w^2) - α*(w^2) = 0
然後，它會告訴我們說
S*(w^2) 等於 α*(w^2)
所以，你知道 w^2 是一個 eigenvector
但是，它是哪一個 eigenvector 呢
如果你選它是
我們知道說如果
我們都知道說這一項等於 eigenvalue 的值
等於 eigenvalue 的值
但是你不能夠選 eigenvalue 最大的那一個 eigenvector
因為它會跟，它跟 w^1
不是 orthogonal，但是你可以選第二大
憑甚麼選第二大的就跟第一大的是 orthogonal 的呢？
你就去查一下你的線代課本
因為 S 是 symmetric 的
所以，你可以這麼做
那我們今天就把 map 的部分講完
最後，這個地方要說，在 End of Warning之前
就只剩下一頁投影片
這頁投影片要說甚麼呢？這頁投影片是這樣說的
說 z = W*x
這邊有一個神奇的地方，就是
z 的 covariance 會是一個 diagonal matrix
也就是說，如果我們今天做 PCA
你原來的 data distribution
可能是這個樣子
做完 PCA 以後，你會做 decorrelation
你會讓你的不同的 dimension 間的 covariance 是 0
也就是說，如果你算
z 這個 vector 的 covariance matrix 的話
會發現它是 diagonal
這樣做有什麼好處呢
這樣做，有時候會有幫助的
假設你現在的 PCA 所得到的新的 feature
你這個 z，是一種新的 feature
這個新的 feature 是要給其他的 model 用的
而你的 model 假設，比如說是
一個 generative model
那你用 Gaussian 來描述某一個 class 的 distribution
而你在做這個 Gaussian 的假設的時候
你假設說 input data，它的 covariance 就是 diagonal
你假設不同的 dimension 之間
沒有 correlation
這樣可以減少你的參數量
你做 PCA 的時候，接下來的 model 就可以
你把你原來的 input data 做 PCA 以後
再丟給其他的 model
其他的 model 就可以假設現在的 input data
它 dimension 間沒有 correlation
所以，它就可以用比較簡單的 model
來處理你的 input data
這樣可以避免 overfitting 的情形
這件事情，怎麼說明呢？
這個也是很 trivial 的
z 的 covariance 就是
(z-z的平均) * (z-z的平均)^T
那這一項，你仔細想想看
把它展開，它是 W*S*(W^T)
S 是 x 的 covariance
然後，你就把它展開
這個 W 的
這邊有沒有寫錯，沒有寫錯
這個 W 的 transpose，它的第一個 column 就是 w^1
一直到第 K 個 column 是 w^K
把 x 乘進去
變成這個樣子
把 x 乘進去以後呢
S*(w^1)是什麼呢？
w^1 是 S 的 eigenvector
所以，(w^1)*S 等於 λ1*(w^1)
(w^K)*S 等於 λK*(w^K)
這個 w 是 eigenvector，然後 λ 是 eigenvalue
然後，我們再把 W 乘進去
那 W*(w^1) 會是什麼呢？
想想看 (w^1) 其實是 W 的第一個 row
而 W 是一個 orthogonal 的 matrix
所以，W*(w^1) 會等於 e1，e1 就是
一個 vector，它的第一維是 1，其他都是 0
W*(w^K) 會等於 eK，eK 就是第 K 維是 1，其他都是 0
這一個東西就是一個 diagonal 的 matrix，然後
warning 的部分就講完了
這個部分，或許你覺得沒有太容易理解
那我們下次從另外一個角度來看 PCA
你可能就會更清楚說 PCA 是在做什麼
今天就先講到這邊，謝謝
各位同學大家好，我們來上課吧
我們上次講到 PCA
然後，PCA 有一個很冗長的證明
然後我們說，PCA
每次找出來的 w
第一次找出來的 w^1 是 covariance matrix
對應到最大的 eigenvalue 的 eigenvector
然後，第二個找出來的 w^2 呢
就是對應到第二大的 eigenvalue 的 eigenvector
以此類推，等等
然後，有一個很長的證明告訴你說
這麼做的話
我們每一次投影的時候都可以讓 variance 最大
假如現在這些東西，你聽不懂的話
就算了
我們來看看另外一個，可能是比較直觀的說明
另外一個比較直觀的 PCA 的想法是這樣子
假設我們現在考慮的是手寫數字
那我們知道說，這些數字其實是由一些
basic 的 component 所組成的
這些 basic 的 component 可能就代表筆劃
舉例來說，人所寫的數字
可能是有這些 basic 的 component 所組成的
有斜的直線、橫的直線、比較長的直線
還有小圈、大圈等等，所組成的
這些 basic 的 component 把它加起來以後
就可以得到一個數字
那這些 basic 的 component
我們這邊寫作 u^1, u^2, u^3 等等
那這些 basic 的 component，其實就是
一個一個的 vector，假設我們現在考慮的是 MNIST 的話
MNIST 的一張 image 是 28*28 pixel
也就是 28*28 維的一個 vector
那這些 component，其實也就是 28*28 維的 vector
把這些 vector 加起來以後
你所得到的那個 vector
把這些 vector 所代表的 component 加起來以後
你所得到的 vector
就代表了一個 digit
或者如果 j我們把它寫成 formulation 的話
寫起來像是這個樣子
x 代表了某一張 image 裡面的 pixel
某一個 image 可以用一個 vector 來表示它
那這個 vector，這邊寫作 x
那這個 x 會等於 u^1 這個 component
乘上 c^1 加上 u^2 這個 component 乘上 c2
一直加到 u^K 這個 component 乘上 cK
假設我們現在總共有 K 個 component 的話
然後再加上 x\bar
x\bar 是所有的 image 的平均，是 x\bar
所以，每一張 image
就是有一堆 component 的 linear combination
然後，再加上它的平均所組成的
舉例來說，我們說 7
是這一個 component、這一個 component 和這一個 component 加起來以後的結果
所以，這個
對 7 來說，假設 7 就是 x 的話
c1 就是 1，c2 就是 0
c3就是 1，以此類推
那你可以用 c1, c2 到 cK
來表示一張 image
假設你這個 component 的數目是
遠比 pixel 的數目少的話
你就可以用這些
這個 component 的 weight 來描述一張 image
如果你 component 的數目比 pixel 的數目少的話
那這個描述，是會比較有效的
舉例來說，7 是一倍的 u^1，
一倍的 u^3，一倍的 u^5 所組合而成
所以，7 你就可以說它是一個 vector
它第一維、第三維、第五維是 1
那我們現在知道說
我們現在知道說 x
等於一堆 component 的 linear combination
再加上平均，我們現在把平均移到左邊
所以，x 減掉所有的 image 的平均
等於一堆 component 的 linear combination
那我們說，這一些 linear combination 的結果
我們寫作 x\head
現在假設我們不知道這些 component 是什麼
我們不知道 u^1 到 u^K 的這一些
這 K 個 vector，它們長什麼樣子
那我們要怎麼找這 K 個 vector 出來呢？
我們要做的事情
就是我們去找這 K 個 vector
使得 x\head 跟 (x-x\bar)
越接近越好
我們要找 K 個 vector
讓 (x-x\bar) 跟 x\head 越接近越好
那它們中間的差呢
沒有辦法用這個 component 來描述的部分
叫做 Reconstruction error
那接下來，我們要做的事情就是
找 K 個 component，其實就是 K 個 vector
它可以 minimize 這個 Reconstruction error
這個 Reconstruction error
如果你要把 formulation 寫出來的話呢
就是我有一個 Reconstruction error 寫成 L
我們要找 K 個 vector 去 minimize 它
要 minimize 的對象就是 (x-x\bar)
減掉下面這一項的 x\head
減掉 x\head 的 2-norm
x\head 是一堆 component 的 linear combination
那我們先來回憶一下 PCA
在 PCA 裡面，我們講說
我們要找一個 matrix, W
我們原來的 vector, x 乘上 W 以後
就得到 Dimension Reduction 以後的結果，z
那我們可以把 W 的每一個 row 都寫出來
w1, w2 一直到 wK
那我們說 w1, w2 一直到 wK
都是 covariance matrix 的 eigenvector
事實上，如果你要解這個式子
你要解這個式子，找出 u^1, u^2 到 u^K
這個 w^1 到 w^K，就是由 PCA 找出來的這個解
其實，就是可以讓上面這一個式子
最小化，就可以讓這個 Reconstruction error 最小
的 u^1 到 u^K
這個在 Bishop 裡面是有 prove 的
那我這邊講的跟 Bishop 有點不太一樣而已
用一個比較簡單的方式來說明給大家聽
我們現在在 database 裡面有一大堆的 x
一大堆的 x
現在假設有一個 x^1
這個 x^1 減掉平均，x\bar
等於 u^1 乘上 component 的 weight, c(上標 1, 下標 1)
這邊 c(上標 1, 下標 1)的意思是說
c(下標 1) 代表說它是 u^1 的 weight
上標 1 代表說它是 x^1 的 u1 這個 component 的 weight
我想這個大家應該知道我的意思
所以，(x^1 - x\bar) 就等於 c1*(u^1) + c2*(u^2)
那 (x^1-x\bar)，它是一個 vector
那我們把這個 vector 拿出來
這個 u^1, u^2 到 u^K
它們是一排 vector，就把它們排起來
其實，它排起來就是一個 matrix
這個 column 的數目
是 K 個 column
那前面 c1, c2 呢？
它們就是我們把這個
c1, c2，這邊有個錯誤的動畫
我們把這個 c1, c2 排成一排
排成一排這在這邊
那你現在把這個 c1 乘上它
把這個 c2 乘上它
就會得到這個 vector
也就是說，你把這一個 vector
你把這些 component 的 weight 排成一排
這個是一個 vector，這個 vector 乘以這個 matrix
就會得到這個 vector
那我們這個 data set 裡面不是只有一筆 data
我們還有很多，比如說，這邊有一個 x^2
那 (x^2-x\bar)
它就是這個第二個黃色的 vector
u^1, u^2 就在這邊
第二個 component 的 c1, c2
第二個 component 的 c1, c2 跟第一個 component 的 c1, c2
它們是不一樣的，它們的 notation 這邊是不一樣的
我們把這個值擺在這一邊
那你把這個 vector 乘上這個 matrix
就會得到這個 vector
以此類推
你把這個 vector
x3 的 component 的 weight 乘上
component，就會得到這個 vector
那如果我們把所有的 data
都用這個式子來表示，都把它畫在下面的話
那這一邊我們就得到一個 matrix
在這個 matrix 的橫軸
column 的數目就是你的 data 的數目
你有一萬筆 data，這個橫軸就是一萬
那我們在要做的事情就是
用這一個 matrix 去乘上這一個 matrix
這兩個都是 matrix，把這兩個 matrix
乘上這個 matrix，那希望它
越接近這個 matrix 越好
所以你要 minimize 這兩個相乘以後得到的 matrix
跟左邊這個 matrix 之間的差距
你要 minimize 這個 Reconstruction error
那怎麼解這個問題呢？
假如你有修過大一線代的話
你就知道這個問題是怎麼解的
以下是我教線代的時候的投影片
放在這邊給大家參考
這個在線代裡面是怎麼說的呢？
每一個 matrix, X
這邊這一個 matrix, X，你可以用 SVD
把它拆成一個
matrix, U 乘上一個 matrix, Σ 乘上一個 matrix, V
這個 U 是 m*k 維
這個 Σ 是 k*k 維，這個 V 是 k*n 維
這個 k，它就是 component 的數目
所以，我們把這個 X 分解成 U, Σ 跟 V
這一個 U 就是這一個
那這個 Σ*V 就是這一個
那我們知道說，如果我們今天的
我們用 SVD 的方法
把 X 拆成這三個 matrix 相乘
那右邊這三個 matrix 相乘的結果
跟左邊這一個 matrix 它們之間
的這個 Frobenius 的 norm 呢
是會被 minimize 的，也就是說
我們用 SVD 提供給我們的一個 matrix 的拆解方法
你拆出來的這 3 個 matrix 相乘，它跟
左邊這一個 matrix，是最接近的
那解出來結果是怎麼樣
如果你還記得的話，解出來的結果是這樣子
U 這個 matrix，它的 k 個 column
其實就是一組 orthonormal 的 vector
這一組 orthonormal 的 vector
它們是 x 乘上 x 的 transpose
它們是 X*(X^T) 的 eigenvector
它們這邊總共有 k 個 vector
這邊總共有 k 個 orthonormal 的 vector
這 k 個 orthonormal 的 vector 它們就對應到
X*(X^T) 最大的 k 個 eigenvalue 的 eigenvector
這個大家聽得懂嗎？
講到這邊大家有問題嗎？
那你會發現說
這個 X*(X^T) 是什麼呢？
這個 X*(X^T) 是什麼
這個 X*(X^T) 不就是 covariance matrix 嗎？
那我們說之前 PCA 找出來的那一些
w 就是 covariance matrix 的 eigenvector
而我們在這邊要說，我們做 SVD
你解出來的 U 的每一個 column
就是 covariance matrix 的 eigenvector
所以 U 這個解，其實就是
就是 PCA 得出來的解
所以，我們知道說 PCA 現在在做的事情
你找出來的，你從根據 PCA，你找出來的那些 w
你找出來的 Dimension Reduction 的 transform
其實，就是在 minimize 這個 Reconstruction error
那 Dimension Reduction 的結果
你得到其實就是這些 vector
你得到的就是這些 vector
PCA 裡面你得到的那些 W
其實就是 component
如果大家知道這些的話
我們等一下會看說 PCA 跟 neural network 有什麼樣的關係
那我們現在已經知道說
從用 PCA 找出來的 w^1 到 w^K
就是 K 個 component，u^1, u^2 到 u^K
那我們說我們有一個
根據 component linear combination 的結果叫做 x\head
它是 (w^K)*ck 做 linear combination 的結果
那我們會希望說，這個 x\head
跟 (x - x\bar)
(x - x\bar)，它的平均的是越小越好
你要 minimize 這個 Reconstruction error
那我們現在已經根據 SVD
找出來的 W，W 已經找出來了
W 已經找出來了，那 ck 的值到底應該是多少呢？
這個 ck 是每一個 example
如果是 image recognition 的話，就每一個 image
都有一組自己的 ck
所以，你要找這個 ck 就每一個 image 各自找就好
每一個 image 各自找就好了
那這個問題，其實就是問說
我現在有 K 維的 vector
它們做 span 以後，得到一個 space
如果我現在
要用 c1 到 cK 對它做 linear combination
怎麼樣才能夠最接 (x-x\bar)
怎麼樣才能夠最接 (x-x\bar) 呢
因為現在這 K 個 vector 它們是 orthonormal 的
所以你要得到這個 ck，其實是很簡單的，你只要把
(x-x\bar) 跟 w^k 做 inner product
你要找一組 ck 可以
那這個性質是來自於說
minimize 左邊這個跟右邊這個的 error
你只需要把
這個 (x-x\bar) 跟 w^k 做 inner product 就好了
那這個性質是來自於說
這 K 個 vector 是 orthonormal 的
這個如果你有困惑的話
就回去 check 一下線性代數的課本
那我們現在已經知道了這些事情
我們已經知道說
c^k 就是長成這個樣子
那這件事情呢
這個做 linear combination 的事情
其實你可以想成用 neural network 來表示它
什麼意思呢？
假設我們的 (x-x\bar) 就是一個 vector
這邊寫作一個三維的 vector
那我們假設，現在 K 只有兩個 component
K 等於 2
那我們先算出 c^1 跟 c^2
怎麼算 c^1 呢？
c^1 就是 (x-x\bar) 跟 w^1 的 inner product
所謂的 inner product 就是 element-wise 的相乘
也就是把 (x-x\bar) 的每一個 component
乘上 w^1 的每一個 component
接下來，你就得到 c^1
這件事情就好像是說
這個是 neural network 的 input
這是一個 neuron，這是 neuron 的 weight
這個 neuron 它是 linear 的 neuron
它沒有 activation function，它是 linear 的
那這個 neuron，你把這個東西 input
乘上這個 weight，你就得到 c^1
那 c^2 也是一樣
c^2，我們這邊
這邊是這樣子，那我們接下來要把 c^1
我這邊犯了一個錯
這個應該是下標
如果統一起來的話，這個 K 應該是下標
那我們把這個 c1 乘上 w^1
所謂的 c1*(w^1)是什麼意思呢？
你就把 c1*(w^1)
把 c1 乘上 w^1 的第一維，得到一個 value
乘上 w^1 的第二維，得到一個 value； 乘上 w^1 的第三維，得到一個 value
這一項，就是 c1*(w^1)
接下來，我們再算一下 c^2
c^2 一樣就是這個 input 一樣乘上
跟這個 w^2 做 inner product
得到 c^2，然後再把這個 c^2
w^2 的三個 element
再跟原來 w^1 的三個 element 加起來
得到最後的 output，這一項就是 x\head
這一項就是 x\head
接下來，我們 training 的 criteria
就是 minimize
我們要讓這個 x\head 跟 (x-x\bar) 越接近越好
你所以，我們就是希望這個 neural network 的 output
跟 (x-x\bar) 越接近越好
這是我們的 input, (x-x\bar)
它乘上一組 weight
再 hidden layer 的 output 是 c^1, c^2
再乘上另外一組 weight，得到 x\head
那我們希望 (x-x\bar) 越接近越好
那你就會發現說
其實 PCA 可以表示成一個 neural network
它可以表示成一個 neural network，然後
這個 neural network 它只有一個 hidden layer
然後，這個 hidden layer 是 linear 的 activation function
然後，我們現在 train 這個 neural network 的 criterion
是要讓 input 一個東西，得到 output
結果這個 output 要跟 input 越接近越好
這個 output 要跟 input 越接近越好
這個東西就叫做 Autoencoder
那這邊我們就有一個問題
我們這邊就有一個問題
假設我們現在這個 weight
不是用 PCA 的方法
也就不是用找 eigenvector 的方法
去找出這些 w^1, w^2 ......w^K
而是，兜一個 neural network
直接用我們要 minimize 這個 error 的 criterion
然後用 Gradient Descent 去 train 一發的話
那你覺得你得到的結果
會跟用 PCA 解出來的結果一樣嗎？
給大家一秒鐘的時間想一想
你覺得會一樣的同學舉手
有些同學同學會覺得一樣，好
手放下來 ，你覺得會不一樣的同學舉手
也有一些同學覺得會不一樣的
覺得不一樣的稍微多一點
其實，是會不一樣的
你仔細想看看 PCA 解出來這些 W
它們是 orthonormal 的，它們是 orthogonal 的
它們是垂直的
你今天如果你用 neural network
你就兜一個 neural network，硬 learn 一發
你得到的結果，你沒有辦法保證，會是垂直的阿
你會得到一個 solution
但是你沒有辦法保證說
這組 weight，這個 w^1 跟 w^2 是垂直的
你得到的是另外一組解
這樣大家了解我的意思嗎 ?
我們在前面 SVD 的證明裡面
已經說 PCA 導出來的這組解 w^1 到 w^K
它可以讓我們的 Reconstruction error 被 minimize
那你用這個 neural network 的方法
去用 Gradient Descent 硬解一發
你其實也不可能找出來
你不可能讓你的 Reconstruction error 比
比 PCA 找出來的 Reconstruction error 還要小
所以，如果是在 linear 的情況下
或許你就會想要用
直接用 PCA 來找這個 w，是比較快的
你用 neural network，或許是比較麻煩的
但是，用 neural network 的好處就是
它可以是 deep 的
這邊為什麼，只可以有一個 hidden layer 呢
它的一個 hidden layer，就要改成很多的 hidden layer
所以這個，就是我們等一下會講的
下一堂課會講的 Deep Autoencoder
那 PCA 其實有一些很明顯的弱點
一個就是，因為它是 unsupervised
它是 unsupervised
所以今天假如給它一大堆點，沒有 label
那對 PCA 來說，假設你把它 project 到一維上
PCA 會找一個可以讓 data variance 最大的
那一個 dimension
比如說，在這個case 裡面
或許，它就把它 project 到這一維上
把每一個綠色點，都 project 到這一維上
但是，有一個可能是，或許其實
這兩組 data point，它們分別代表了
兩個 class，代表了兩個 class
如果你用 PCA 這個方法來做 Dimension Reduction 的話
你就會使得這兩個藍色跟橙色的 class
被 merge 在一起
它們在 PCA 找出來的 single dimension 上
完全被混在一起，就無法分別
這個時候怎麼辦呢
你可能會需要引入 labeled data
那 LDA，是考慮這個 labeled data 的一個降維的方法
不過它是 supervised，所以這邊就不是我們要講的對象
這個 LDA 是 linear discriminant analysis 的縮寫
另外一個 PCA 的弱點，就是它是 linear 的
我們剛才在一開始舉例子的時候，我們會說
這一邊有一個 S 形的這個 manifold
這邊有一堆的點，它的分佈像是一個 S 形
那我們期待做 dimension reduction 以後
可以把這個 S 形的曲面
把它拉直，但是這一件事情
對 PCA 來說是做不到的
你要它做這一件事情，其實是強人所難
因為你要把這一個 S 形的曲面拉直
是一個 non-linear 的 transformation
PCA 做不到這件事情
如果你做 PCA 的話
你得到的結果
就是把這個 S 形的曲面做 PCA
你得到的結果就是這樣
你就是把它從藍色這一邊把它打扁這樣
你會發現說藍色跟紅色的點
那就被壓在一起，然後綠色的點在這邊
黃色的點在這一邊
你會把它打扁而不是把它拉開
因為把它拉開這件事情
是 PCA 辦不到的
那我們等一下會講 non-linear 的 transformation
再來，我們就把 PCA 用在一些實際的問題上
比如說，我們用它來分析寶可夢的 data
我們這一門課的例子都是寶可夢的例子
寶可夢，我們都知道說
它總共有 800 種寶可夢
那其實有人說是 721 種，因為在 800 種裡面
有一些是重複的
我也不知道怎麼解釋
反正就是假設有 800 種好了
那每一種寶可夢，你可以用 6 個 feature
來表示它，分別是
生命值、攻擊力、防禦力
特殊攻擊力、特殊防禦力還有它的速度等等
所以，它們就是
每一個寶可夢就是一個六維度的 data point
就是一個六維的 vector
那我們現在，要用 PCA 來分析它
那在用 PCA 的時候，常常會有人問的問題就是
我需要有多少個 component
我到底要把它 project 到一維
還是二維、還是三維
這個資訊量才足夠呢，這個 depend on 你想要做甚麼
假設你想要做 Visualization
因為現在每一個寶可夢都是 6 維
你沒有辦法了解這個寶可夢
它們之間的特性有什麼樣的關係
因為 6 維你沒有辦法看
所以，你可能就會想把它 project 到二維
就比較容易分析
那其實要用多少的 principle component，就好像是
neural network 要有幾個 layer，每個 layer 要有多少個
hidden variable，要有幾個 neuron 一樣
所以，這個是你要自己決定的
那一個常見的方法是這樣
一個常見的方法是說
我們去計算每一個 principle component 的 λ
我們知道每一個 principle component 就是一個
eigenvector，然後這個 eigenvector 又對應到一個
eigenvalue，就是這邊的 λ
那這個 eigenvalue 代表什麼意思呢？
這個 eigenvalue 代表說
我們用這個 principle component
去做 dimension reduction 的時候
在 principle component 的 dimension 上
它的 variance 有多大，那個 variance 就是 λ
今天這個寶可夢的 data 總共有 6 維
所以它 covariance matrix 是六維
所以，你可以找出 6 個 eigenvector
你可以找出 6 個 eigenvalue
我們現在來計算一下
每一個 eigenvalue 的 ratio
我們就把每一個 eigenvalue 除掉 6 個 eigenvalue 的總和
你得到的結果，會是這個樣子
第一個 eigenvalue
它佔全部的 eigenvalue 的 0.45
第二個是 0.18，第三個是 0.13
以此類推
現在看到這個結果
我們可以從這個結果看出來說
這一邊是 0.45, 0.18, 0.13, 0.12，再來是 0.07, 0.04
所以，第 5 個 principle component
和第 6 個 principle component，它們的作用
其實是比較小的
你用這兩個 dimension
來做 projection 的時候
你 project 出來的 variance 是很小的
代表說，現在寶可夢的這這些特性
在第五個和第六個 principle component 上
是沒有太多的 information
它 variance 很小，所以它沒有太多的information
所以，如果我們今天要分析寶可夢的 data 的話
感覺只需要前面 4 個 principle component 就好了
所以，我們就實際來分析一下
實際來分析一下
如果你做 PCA 以後
我們得到的四個 principle component
就是這個樣子
然後，每一個 principle component 就是一個 vector
現在每一個寶可夢，它是有用六維的 vector 來描述它
所以呢，每一個 principle component
就是一個六維的 vector
這 4 個 principle component 就是 4 個六維的 vector
那我們來看一下每一個 principle component
它在做的事情是什麼
如果我們看第一個 principle component
第一個 principle component，它的每一個 dimension
每一個 dimension 都是正的
那這個東西是什麼意思，這個東西其實就
代表寶可夢的強度
知道嗎？就是如果你要產生一隻寶可夢的時候
每一個寶可夢都是
由這 4 個 vector 做 linear combination
每一個寶可夢都可以想成是
它的數值可以想成是，這 4 個 vector
做 linear combination 的結果
combine 的 weight，每一隻寶可夢是不一樣的
所以，如果你在選第一個
principle component 的時候，你給它的 weight 比較大
那這個寶可夢它的六維都是強的
那如果選擇的值小，它的六維都是弱的
所以，在 第一個 principle component
就代表了這一隻寶可夢強度
那第二個 principle component 是什麼呢？
第二個 principle component 它在
它在防禦力的地方是正值
它在速度的地方是負值
也就是說，如果你選
這個防禦力跟速度
是成反比的
你今天選，你今天給這個
第二個 component 一個 weight 的時候
你會增加那隻寶可夢的防禦力
但是，會減低它的速度
所以，寶可夢防禦力提升的時候
它的速度會下降
如果我們把
第一個和第二個 principle component 畫出來的話
你發現是這個樣子
這個圖上，有 800 個點，每一個點
就對應到一隻寶可夢
那你把它畫到二維的平面上
那這樣我們很難知道每一隻寶可夢是甚麼
所以，我就做了一件瘋狂的事
如果我們看第三、第四個 component 的話
會發現說第三個 component
它是 special defense
是正的
special defense 是正的
然後，攻擊力跟 HP 是負的
也就是說，這是用
特殊防禦力來換取
用攻擊力跟 HP 來換取特殊防禦力的寶可夢
最後一個呢？最後一個是
它的 HP 是正的
然後，攻擊力和防禦率是負的
也就是說，它是用攻擊力和防禦來換取生命力的寶可夢
那這些分別是什麼呢
如果我們把第三個和第四個 principle component
畫在圖上的話
其實它們也是橢圓的形狀
其實也是橢圓的形狀
不過基本上考慮的是統計的結果，所以會有一些 outlier
不過，統計的結果最後算出來
它是 decorrelation 的
其實，特殊的防禦力用
攻擊力和生命值換特殊防禦力
其實也是普普
它不只是一個防禦力特別高的寶可夢
它也是一個特殊防禦力特別高的寶可夢
第二名是冰柱機器人這樣子
然後，如果我們看生命力的話
第四個 component 就是生命力特別強的
那這個其實跟我們預期是一樣的，就是
這個是吉利蛋跟幸福蛋，對不對
所以，今天我們至少回答到一個問題
你知道最強的寶可夢其實是超夢，還有
那三隻神獸是特別強的
那如果我們可以拿它來做其他 class
比如說，我們拿它來做
影像的手寫數字的辨識的話
那會怎麼樣呢？我們可以把每一張
數字都拆成
component 的 weight 乘上 component
加上 component 的 weight 乘上另一個 component
其實，每一個 component 都是一張 image，對不對
每一個 component 都是一個 28*28 維的 vector
所以，你可以把它畫在圖上，把它變成一張 image
我們如果畫前 30 個 component 的話
我們畫 PCA 得到前 30 個 component 的話
你得到的結果，其實是這個樣子的
白色的地方代表是有筆劃，所以這個是 1
這個看起來有點像 9
這個看起來是 0，中間接一條線
這個看起來不知道，像是加了勾勾
後面這很複雜，看起來像是馬雅文字一樣複雜
你用這些 component
做 linear combination
你就可以得到所有的 digit，就可以得到 0~9
所以，這些 component 就叫做 Eigen-digit
之所以叫 Eigen-digit 就是說
Eigen 就是說，這些 component 其實都是
covariance matrix 的 eigenvector
所以，叫它 Eigen-digit
所以，Eigen-digit 做 linear combination
就可以得到各種不同的 digit
如果做人臉辨識的話
處理人臉的話，得到的結果大概是這樣
這邊有一大堆的人臉
然後，你就找它的 principle component
你就找它的 principle component，你就找前 30 個
principle component
你就會發現說找出來是這樣，每一個都是哀怨的臉
這叫 Eigen-face
你看每一個都是一個臉，每一張都是一個臉
那你把這些臉做 linear combination 以後
就可以得到所有的臉
但是，這邊你有沒有覺得有些地方
跟你預期的不太一樣呢？
我第一次看到的時候，覺得這跟預期的結果不太一樣
是不是程式有 bug 阿
因為，我們說P CA 找出來的是
是 component，對不對
我們把很多 component linear combine 以後
它會變成一個 face 或一個 digit
但我們現在找出來的不是 component 阿
我們找出來的每一個圖，幾乎都是完整的臉
幾乎都是完整的臉
我們剛才前一個數字
你找出來每一個 Eigen-digit 看起來都是馬雅文字
它們不是 component，不是
一筆劃這種東西，不是圈圈、一豎這種東西
那為甚麼會這樣呢？
如果你仔細想想看 PCA 的特性
你就會發現說，會得到這個結果是可能的
因為在 PCA 裡面，你的這個 weight
你的這個 linear combination 的 weight
它可以是任何值
它可以是正的，也可以是負的
所以，當我們用這些 principal component
組成一張 image 的時候
你可以把這些 component 相加
也可以把這些 component 相減
所以，這會導致說你找出來的 component
不見得是一個圖的 basic 的東西
舉例來說，假設我想要畫一個 9
那我可以先畫一個 8
然後，再把下面的圈圈減掉
再把一槓加上去
這樣大家了解我意思嗎？因為現在我們
不一定是把這些 component 加起來
我們可以把這些 component 相減
所以就變成說，你可以先畫一個很複雜的圖
然後，再把多餘的東西減掉
這就是為甚麼我們剛才會看到一堆馬雅文字的關係
這些 component 其實不見得是類似筆劃這種東西
如果你想要得到類似筆劃的東西的話
怎麼辦呢？你要用另外一個技術叫做
Non-negative matrix 的 factorization (NMF)
那在 NMF 裡面
我們剛才講說，PCA 它可以看成是對 matrix, X
做 SVD，SVD 就是一種
matrix factorization 的技術，就是一種矩陣分解的技術
那它分解出來的兩個 matrix 的值
可以是正的，可以是負的
現在如果你用 NMF 的話
我們沒有打算要講它的細節
我等一下列 reference 給大家參考就好
簡單來說的話，精神就是
如果我們現在用 NMF 的話
我們會強迫所有的 component 都是正的
首先，我會強迫所有 component 的 weight
都是正的
是正的好處就是
現在一張 image 必須由 component 疊加得到
而你不能說，我先畫一個圖
很複雜的東西，再把複雜的東西去掉一些部分
得到一個 digit
現在因為每一個 weight 都一定要是正的，所以
你只能相加，再來就是
所有的 component
它的每一個 dimension，也都必須要是正的
如果你用 NMF 的話，會讓你的每一個 dimension
都是正的
如果你用 PCA 的話
你得到的 dimension 不見得每一維都是正的
你找出來的 principal component 裡面
它會有一些負值
那你知道，你今天如果要畫一張 image 的話
其實，負值你是有點不知道該怎麼處理的
如果我今天把負值都當作是
就沒有筆劃，就是 0 的話
你可能整張圖都看起來黑漆漆
你大部分的圖都黑漆漆，就很怪
我前面那幾張圖的作法是
如果有負值又有正值，會把它 normalize
把它做一個平移，讓負的也都變成正的
你看起來的圖才會比較好看
那這個就比較麻煩，如果你用 NMF 就沒有這個問題了
你找出來的 component 都是正的
所以，那些 component 就自然地會形成一張 image
下面這是 reference 給大家參考
所以，如果在同樣的 test 上
比如在手寫數字的 test 上，一樣 apply NMF 的時候
這個時候，你找出來的那些 principal component
它就會長這樣
它就會清楚很多，你就會發現說，這顯然
都是筆劃，這個是縱的，這個是 0 的兩邊
這個是斜的，這個是一小點
一直線、一撇這樣子
一橫線、一撇這樣子
你會發現，你找出來的每一個東西
就都變成是筆劃了，這跟我們
本來想要找的東西，是比較像的
如果你看臉的話
你會發現說，它長的是這個樣子，它比較像是
臉的一些部分
比如說，這個是人中的地方
這個是眉毛，這個是下巴，這個是嘴唇
這個也是嘴唇等等
如果，你今天是用 NMF
對這個人臉圖片做 NMF 的話
你得到的結果會比較像是
你期待要找的像是 component 一樣的東西
那接下來，剩下來的時間
我們講一下 Matrix Factorization
Matrix Factorization 是這樣子，有時候
你會有兩種東西
你會有兩種 object
它們之間是受到某種共通的 latent factor 去操控的
什麼意思呢，假設我們現在做一下調查
調查每一個人手上有，每一個人有買公仔的數目
然後，A B C D E 代表 5 個人
我們調查一下，每個人手上有的公仔的數目
比如說，調查 A 有 5 個涼宮春日的公仔
B 有 4個，C 有 1 個， D有 1 個
然後這個是御坂美琴，A有 3 個，B有 3 個，C有 1 個
這個是小野寺小咲 (人名)
我要想看看人名到底是什麼
小野寺，對不對
D有4個，E有5個
然後這個，這個是小唯，是平澤唯
然後，C有5個，D有4個，E有4個，
你會發現說
在這個 matrix 上面
每一個 table 裡面的 block
並不是隨機出現
你會發現，如果有買涼宮春日公仔的人
就比較有可能會有御坂美琴的公仔
有這個姐寺公仔的人，就比較有可能有小唯的公仔
那是因為說
這每一個人跟每一個人
跟角色背後是有一些共同的特性
有一些個共同的 factor 來操控
這些事情發生
otaku 這個是御宅族的意思這樣
每一個御宅族和每一個角色後面，是有一些
共同的 factor在操控它們的
什麼意思呢？
其實這個動漫宅，或許可以分成兩種
有一種是萌傲嬌的
有一種是萌天然呆的
每一個人，其實就是在萌傲嬌和萌天然呆
這個平面上的一個點
所以，如果比較偏，A 是比較萌傲嬌的
那每一個角色，他也是
他可能是有傲嬌屬性，或者是有天然呆的屬性
所以每一個角色，也都是平面上的一個點
每一個角色，你也都可以用一個 vector 來描述它
如果某一個人萌的屬性
跟某一個角色，他本身所具有的屬性是 match 的話
所謂的屬性 match 是說
他們背後的這個 vector
他們背後的這個 vector 很像
他們背後的這個 vector
比如說，在做 inner product 的時候，值很大
那這個人
就會買很多涼宮春日的公仔
所以，這個他們有沒有 match 呢
這個 degree，他們這個匹配的程度
就取決於他們背後的這個的 latent factor 是不是匹配的
所以，只可能 ABC 他們背後的這個
他們萌的角色，大概是這個樣子
A 是萌傲嬌的， B也是萌傲嬌的
但沒有 A 那麼強 ，C 是萌天然呆的
然後每一個動漫人物角色的後面
也都有傲嬌和天然呆，這兩種屬性
每一角色都有他不同的屬性
然後，如果他們的屬性
它們背後的屬性 match 的話
那這一個人，就會買這個公仔
這個世界操控的邏輯是這個樣子
但是這些 factor
就一個人到底是萌傲嬌還是萌天然呆，這一件事情
是沒有辦法直接被觀察的
因為，其實沒有人在意一個
阿宅他心裡想什麼
所以這些事情，是沒有人知道的
你也沒有辦法直接知道說
每一個動漫人物他背後的屬性是甚麼
這也是沒有辦法直接觀察到的
所以我們有的東西是什麼呢？
我們有的是，這個動漫人物跟阿宅中間的關係
他們中間的關係，也就是
也就是他手上有的公仔的數目，他們之間的關係
那我們要憑著這個關係
去推論出每一個人，跟每一個動漫人物
他們背後的 latent factor
他們每一個人背後都有一個二維的 vector
分別代表他，就每一個這個阿宅背後都有一個 vector
代表了他萌傲嬌或是萌天然呆的程度
他都是用一個 vector來表示
每一個人物，他背後也都有一個 vector
代表他傲嬌的屬性和天然呆的屬性
而我們知道的每一個阿宅，他手上有的這個公仔的數目
你就可以把他們合起來看作是一個 matrix, X
我們可以把合起來看做是一個 matrix, X
那這一個 matrix, X，它的 row 的數目就是
Otaku 的數目
它的這個 column 的數目
就是動漫角色的數目
那我們現在要做的事情，就是做一個假設
這一個假設是這個樣子
每一個，這個 matrix 裡面的 element
它都來自於兩個 vector 的 inner product
都來自於兩個 vector 的內積
這一個 5，為什麼 A 會有 5 個涼宮春日的公仔呢
是因為 r^A 跟 r^1 的 inner product 很大
它們的 inner product 是 5
所以他就會買，他們就會有 5 個涼宮春日的公仔
如果 r^B 跟 ^r1，它的 inner product 是 4 的話
那 B 就會有 4個涼宮春日的公仔
以此類推
這個世間運作的邏輯就是這個樣子
那這件事情，如果你用數學式來表示它的話
你可以寫成這樣子
我們把 r^A 到 r^E 排成一排
把 r^1 到 r^4
也當作是另外一個 matrix 的 row 把它排起來
那這個 K 是 latent factor 的數目
這個東西，我們是沒有辦法知道的
我們把人分成只有萌傲嬌和萌天然呆，這樣是一個
不精確的分析方式
如果我們有更多的 data 的話
我們應該可以更準確知道應該要有多少 factor
但是，實際上要有多少 factor 這件事情
你必須要試出來，就像 principal component 的數目
或者是這個 neural network 的陳述一樣
這個是你要事先決定好的
我們現在就假設說
latent factor 的數目
latent factor的數目就是K
所以， 這個 r^A 到 r^E
你把它當作是 matrix row
這邊就有 N*K，這是一個 N*K 的 matrix
你把 r^1 到 r^4 當作 column，那就是 K*N 的 matrix
你把這個 N*K matrix、 N*K matrix 乘起來
你就會得到一個
我寫錯了，這應該是 M，不好意思，這應該是 M
這個應該是 M 個 Otaku，所以是有 M 個人
所以，把這個 (M*K) * (K*N)
得到一個 M*N 的 matrix
那它的每一個 dimension 分別是甚麼呢？
我們知道說，最左上角這個 dimension
最左上角的這個 dimension
如果你現在熟的話
它就是 r^A * r^1，對不對
那這第一個 row，第二個 column 的 element
n(下標 A2)，就是 r^A * r^2
n(下標 B1)，就是 r^B * r^1
n(下標 B2)，就是 r^B * r^2
所以這一個 matrix 是什麼呢？
這一個 matrix 其實就是這一個 matrix
其實就是這一個 matrix
假設我們說，這一個 matrix 就是這一個 matrix 的話
那我們要做的事情就是找一組
r^A 到 r^E，找一組 r^A 到 r^E
找一組 r^1 到 r^4
把這兩個 matrix 相乘以後
跟這個 matrix, X 越接近越好
minimize 這兩個 matrix 相乘以後
跟這個 matrix, X 之間的 Reconstruction error
那這個東西，我們剛才講過
你就可以用 SVD 來解
那你可能說 SVD 不是解完以後，有 3 個 matrix 嗎？
中間還會有 Σ 呢
你就看你要把 Σ 併到左邊或是併到右邊都可以
你就把這個 matrix, X 拆成兩個 matrix
它們可以 minimize Reconstruction error，然後就結束了
但有時候你可能會遇到這個問題
就是有一些 information，是 missing，是你不知道的
這個 information，比如說 ABC 他們
你並不知道 ABC 手上有沒有，有沒有小野寺的公仔
有可能就只是在他所在的地區，沒有發行這個公仔而已
所以，不知道如果發行的話，他到底會不會買
所以這邊，其實是個問號，這個是問號
假設這個 table 上有一些問號的話，怎麼辦呢？
這 table 上有一些問號的話
你用剛才那個 SVD 的方法
就會有點卡
你可能說我把這個值
都當作是零的話
做一發，這樣子也可以啦
不過總覺得有些怪怪的
可以描述得更精確一點
所以，如果今天在 matrix 上面
有一些 missing value 的話，怎麼辦呢？
有一些 missing value 的話
我們還是可以做的
我們就用 Gradient Descent 的方法來做它
就我們寫一個 loss function
這一個 loss function 是這樣子
我們要讓 r^i
r^i 指的是每一個 Otaku 背後的 latent factor
r^j 指的是每一個動漫角色背後的 latent factor
你要讓 i 這個人
他的 latent factor 乘上 j 這個角色
他的 latent factor，這個 inner product
跟他購買的數量，n(下標ij) 越接近越好
那重點是說
我今天在 summation over 這些 element 的時候
我們可以避開這一些 missing 的 data
我們可以避開這些 missing data
我們在 summation over 這些 ij 的時候
如果今天這個值是沒有的
我就不算它
我們只算這個值是有定義的部分
那接下來怎麼辦呢？
接下來，你都把 loss function 寫出來了
你要找 r^i 跟 r^j
就用 Gradient Descent，就好了
用 Gradient Descent 運算一發
然後就結束了
所以，根據剛才那個例子
我們就可以實際地用 Gradient Descent 來做一下
那我們假設 latent factor 的數目等於 2
假設 latent factor 的數目等於 2
那每一個這個 A 到 E 呢
它們就都會對應到一個二維的 vector
A 的 vector是這樣， B 是這樣
C 是這樣，以此類推
每一個人都會對應到一個 vector
那每一個這個角色呢
他也都可以對應到一個 vector
我怕大家，如果我只寫這個字的話
會不知道什麼意思
所以特別把編號列出來
這個是春日，這個是炮姐
這個是姐寺，這個是小唯
那你就會找出說
每一個角色，都得到一個 vector
也都得到一個 latent factor，這代表他的屬性
代表他的屬性
這個每一個人的 latent factor 就代表他萌哪一種屬性
所以，如果我們把他在兩個維度裡面
比較大的維度挑出來的話
你就會發現說，A 跟 B 是萌同一組屬性的
C，D，E 是萌同一組屬性的
1 跟 2 他們有同樣的屬性
然後 3 跟 4 他們有同樣的屬性
你沒有辦法真的知道說，每一個屬性
分別代表的是甚麼
你不知道說，這個維度是代表是傲嬌，還是天然呆
你不知道這個維度是代表傲嬌，還是天然呆
你會需要先找出這些 latent factor 以後
再去分析它的結果
你就可以知道說
因為我們事先已經知道說
姐寺跟小唯是有天然呆屬性
所以第一個維度代表的就是天然呆屬性
而第二個維度代表的就是傲嬌屬性
接下來，有這些 data 以後
你就可以，預測你的 missing value
你就可以預測問號的值
怎麼預測問號的值呢
我們如果已經知道 r^3
我們已經知道 r^a
那我們知道說，一個人會購買公仔的數量
其實是他背後的
這個動漫角色背後的 latent factor
那個人背後的 latent factor 做 inner product 的結果
所以，我們只要把 r^3 跟 r^a
做 inner product，你就可以預測說
這個人會買多少的公仔
所以，做完 inner product 以後的結果
就是這樣子
這告訴我們說
這個 C，如果說
r^C 跟 r^3 他們的 latent factor，其實是 match 的
所以 C 這個人呢
如果他可以買的話
預期他會買，他會有 2.2 個姐寺的公仔
所以，你就可以推薦姐寺公仔給他
你就可以推薦他入坑
所以，這個方法
常用在這種推薦系統裡面
大家可能都有聽過 Netflix 的比賽
如果你今天把動漫角色換成電影
你把這個中間的這個數值換成 rating 的話
你就可以預測某一個人
會不會喜歡某一部電影，等等
線上推薦系統，現在很多都會使用這樣的技術
其實，剛才那個 model 可以做更精緻一點
我們剛才說，A 背後的 latent factor
乘上 1 背後的 latent factor
得到的結果，就是 table 上面的數值
但是事實上，可能還有別的因素
會操控他的數值
所以更精確的寫法，或許是我們可以寫成
r^A 跟 的 r^1 inner product
加上某一個跟 A 有關的 scalar
再加上某一個跟 1 有關的 scalar
其實才等於 5
這跟 A 有關的 scalar, b(下標A) 指的是甚麼呢？
它代表的是說，A 他有多喜歡買公仔
有的人，他就只是
其實他也不特別喜歡某一個角色
他就只是想要買公仔而已
所以這個 b(下標A)，代表他本身有多想要買公仔
本身有多想要買公仔
這個 b1 代表說
這個角色，他本質上會有
多想讓人家購買
這件事情是跟屬性無關的
就是他本來就會想要購買這個角色
比如說，最近涼宮春日慶祝動畫十周年
就出現藍光 DVD，甚麼的
所以大家就可能會比較想要買
涼宮春日的公仔，其實也不會啦
所以就會有這個 b1
所以，你今天就改一下 minimize 的式子
就變成說，把 r^i 跟 r^j 的 inner product
加上 bi 再加上 bj
然後，你會希望這個值
跟 n(下標ij) 越接近越好
你會希望說這兩個 latent factor 的 inner product
加上這個代表 i 本身的 scalar
還有代表 j 本身的 scalar
要跟在 table 上面看到的值，越接近越好
那這個怎麼解呢
這沒有什麼好講的
你就用 Gradient Descent 硬解一發就好了
你也可以加 Regularization
如果你覺得應該要 Regularization 的話
那你也可以在這個 r^i，r^j，bi，bj 上面
加上 Regularization，比如說你會希望說
r^i, r^j 是 sparse 的
每個人要麼就是萌傲嬌，要麼就是萌天然呆
不會有模糊的人
你可能就會想要加上 L1 的 Regularization
如果你想要知道更多的話
以下是 Matrix Factorization 精簡的 paper
講的是在這個 Netflix 上面，是怎麼做的
那 Matrix Factorization 還有很多其他的應用
比如說，它可以用在 Topic analysis 上面
這個，應該是大家
等一下，11 點多的時候，助教會回來講一下作業 4
作業 4 要做的是跟文字有關的
裡面你可能會需要用到 Topic analysis 的技術
如果是把剛才所講的 Matrix Factorization 的方式
用在 Topic analysis 的上面的話
就叫做 Latent semantic analysis (LSA)
它的技術跟我們剛才所講的是一模一樣的
就只是換一個名詞而已
就是把剛才的動漫人物，通通換成文章
把剛才的人都換成詞彙
那 table 裡面的值呢
把人物換成文章
把人都換成詞彙
那 table 裡面的值呢
就是 Term frequency
就是說，投資這個 word
在 Doc 1 出現 5 次
股票這個 word 在 Doc1 出現 4 次
以此類推
有時候，我們不只會用 Term frequency
你會把 Term frequency
再乘上一個 weight，代表說
這個 term 本身有多重要
如果你把一個 term 乘上比較大的 weight 的話
今天你在做 matrix factorization 的時候
那一個 term 它就比較會被考慮到
你就會比較想要讓那一個 term 的 Reconstruction error
你就比較想要讓那個 term 的 Reconstruction error
比較小
通常怎麼 evaluate 一個 term 重不重要呢
有很多方法
比如說，常用的就是 inverse document frequency
這個我想等一下，助教會講
或是說你不知道的話
也可以自己 google 一下
所以，inverse document frequency 簡單來講
就是計算某一個詞彙
在整個 corpus 裡面，有多少比例的 document
有涵蓋這個詞彙
假如，某一個詞彙是每個 document 都有的
比如說，"的 " 每一個 document 都有
它的 inverse document frequency 就很小
代表說這個詞彙的重要性是低的
假如某一個詞彙，是只有某一篇 document 有
那它的 inverse document frequency 就很大
就代表這個詞彙的重要性是高的
那在這一個 task 裡面
如果你今天把這個 matrix 做分解的話
你就會找到每一個 document 背後的 latent factor
你就會找到每一個詞彙背後的 latent factor
這邊這個 latent factor 是什麼呢
今天這個 task 裡面，你的 latent factor 可能指的是
topic
可能指的是這個主題
某一個 document，它背後要談的主題
有多少部分是跟財經有關的
有多少部分是跟政治有關的
那某一個詞彙，它跟
它有多少部分是跟財經有關的
它有多少部分是跟政治有關的
比如說，可能
可能今天在這個例子裡面
投資跟股票是跟財經有關的
那 Doc1 跟 Doc2 又都有
比較多的投資跟股票這兩個詞彙
那 Doc1 跟 Doc 2
它就有比較高可能性
它背後的 latent factor 是偏向於財經的
其實 Topic analysis 的方法多如牛毛
所以，它們基本的精神是差不多的
但是有很多各種各樣的變化
常見的是 Probability latent semantic analysis (PLSA)
另外一個 latent Dirichlet allocation (LDA)
那這一邊就把 reference 列在這邊，給大家參考
前面我們有講過另外一個 LDA
在 machine learning 裡面，有兩個 LDA
然後是完全不一樣的東西
就是這麼回事
這邊就是一些 reference 給大家參考
其實這種 Dimension Reduction 的方式
這邊還沒有列
這邊只列一些跟 PCA 比較像、有比較關係的
這個 Dimension Reduction 的方法，多如牛毛
比如說，像 MDS
MDS，它特別的地方是
它不需要把每一個 data 都表示成 feature vector
它要知道的只有
feature vector 跟 feature vector 之間的 distance
你只要有這個 distance，你就可以做 Dimension Reduction
那一般教科書舉的例子，都會舉說
我現在有一堆程式
你不知道怎麼把程式描述成一個 vector
但你知道程式和程式之間的距離
你就有每一筆 data 之間的距離
就可以把它畫在一個二維平面上
那其實 MDS 跟 PCA ，是有一些關係的
你如果用某些
如果用某一些特定的這個 distance
來衡量兩個 data point 之間的距離的話
你又做 MDS 就等於是做 PCA
所以，其實 PCA 有一個特性
是它保留了
原來在高維空間中的距離
如果兩個點在高維空間中的距離是遠的
在低維的空間中，它的距離也是遠的
在高維空間中是接近的
在低維空間中也是接近的
那 PCA 有機率的版本
叫做 Probabilistic PCA
PCA 有非線性的版本叫做 Kernel PCA
另外一個東西叫 CCA
CCA 是說如果你有兩種不同的 source
這個時候你會想要用 CCA
比如說，如果你做語音辨識
你有兩個 source ，一個是聲音訊號
另外一個是
假設你今天其實是有螢幕的
那你可以看到這個人的唇形
你可以讀他的唇語
那你現在有聲音訊號
有那一個人嘴巴的 image
你把這兩種不同的 source
都做 Dimension Reduction
這個是 CCA
還有另外一種東西叫做 ICA
它們的名字聽起來都很像
ICA 比較常用在 source separation 上面
它要做的事情是
原來在 PCA 裡面
我們找出來的 component 是 orthogonal
那 ICA 裡面，我們不見得要找
ICA 說我們不見得要找 orthogonal 的 component
我們找 independent component 就好
然後，它用一套很複雜的方法來定義什麼叫做
independent component
還有剛才有提到的 LDA
它是 supervise 的方式
那以上給大家參考，我們在這邊休息10分鐘，謝謝
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
