臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw/
接下來要跟大家講一下
Actor-Critic，要跟大家講一下 Actor-Critic
那在 Actor-Critic 裡面，最知名的方法就是 A3C
Asynchronous Advantage Actor-Critic
那等一下就要跟大家介紹一下，這個 A3C 是什麼？
如果去掉前面這個 Asynchronous 這個字眼啊
只有 Advantage Actor-Critic，就叫做 A2C
那如果前面加了 Asynchronous 這個字眼呢
變成 Asynchronous Advantage Actor-Critic，就變成 A3C
那我們等一下會來講一下這個東西是什麼
那我們很快複習一下 policy gradient
在 policy gradient 裡面，我們是怎麼說的呢？
在 policy gradient 裡面我們說我們在 update 我們的 policy 的參數 θ的時候
我們是用了以下這個式子，來算出我們的 gradient
那我們說以下這個式子其實是還蠻直覺的
以下這個式子在說什麼呢？
以下這個式子是在說，我們先讓 agent 去跟環境互動一下
然後我們知道我們在某一個 state, s
採取了某一個 action, a
那我們可以計算出在某一個 state , s
採取了某一個 action, a 的機率
接下來，我們去計算說
從這一個 state 採取這個 action, a 之後
accumulated reward 有多大
從這個時間點開始，在某一個 state, s
採取了某一個 action, a 之後，到遊戲結束
到互動結束為止，我們到底 collect 了多少的 reward
那我們把這些 reward
從時間 t 到時間大 T 的 reward 通通加起來
那有時候我們會在前面
乘一個 discount factor
因為我們之前也有講過說
離現在這個時間點比較久遠的 action
它可能是跟現在這個 action 比較沒有關係的
所以我們會給它乘一個 discount 的 factor
可能設 0.9 或 0.99
那我們接下來還說，我們會減掉一個 bias
減掉一個 baseline b，減掉一個值 b
減掉這個值 b 的目的，是希望括號這裡面這一項
是有正有負的，那如果括號裡面這一項是正的
那我們就要增加在這個 state 採取這個 action 的機率
如果括號裡面是負的
我們就要減少在這個 state 採取這個 action 的機率
這個是我們之前都講過的東西
那我們把這個 accumulated reward
從這個時間點採取 action, a
一直到遊戲結束為止會得到的 reward
用 G 來表示它
那這個 b 呢？就是一個 baseline
它是要確保説括號裡面的值有時候是正的
有時候是負的
好那但是問題是 G 這個值啊
它其實是非常的 unstable 的
為什麼會說 G 這個值是非常的 unstable 的呢
因為你想想看，這個互動的 process
其實本身是有隨機性的
所以我們在某一個 state, s，採取某一個 action, a
然後計算 accumulated reward
每次算出來的結果，都是不一樣的，對不對？
所以 G 其實是一個 random variable
給同樣的 state, s，給同樣的 action, a
G 它可能有一個固定的 distribution
但我們是採取 sample 的方式
我們在某一個 state, s，採取某一個 action, a
然後玩到底，我們看看說我們會得到多少的 reward
我們就把這個東西當作 G
所以我們只有對 G
把 G 想成是一個 random variable 的話
我們實際上做的事情是
對這個 G 做一些 sample
然後拿這些 sample 的結果，去 update 我們的參數
但實際上在某一個 state, s 採取某一個 action, a
接下來會發生什麼事，它本身是有隨機性的
雖然說有個固定的 distribution
但它本身是有隨機性的，而這個 random variable
它的 variance，可能會非常的巨大
就你在同一個 state 採取同一個 action
你最後得到的結果，可能會是天差地遠的
那今天假設我們可以 sample 足夠的次數
我們在每次 update 參數之前
我們都可以 sample 足夠的次數
那其實沒有什麼問題，但問題就是
我們每次做 policy gradient
每次 update 參數之前都要做一些 sample
這個 sample 的次數，其實是不可能太多的
我們只能夠做非常少量的 sample
那如果你今天正好 sample 到差的結果
比如說你正好 sample 到 G = 100
正好 sample 到 G = -10
那顯然你的結果會是很差的
所以接下來我們要問的問題是
能不能讓這整個 training process
變得比較 stable 一點，我們能不能夠直接估測
G 這個 random variable 的期望值
我們在 state, s 採取 action, a 的時候
我們直接想辦法用一個 network
去估測在 state s 採取 action a 的時候
你的 G 的期望值，如果這件事情是可行的
那之後 training 的時候
就用期望值來代替 sample 的值
那這樣會讓 training 變得比較 stable
從期望值來代替 sample 的值
那怎麼拿期望值代替 sample 的值呢？
這邊就需要引入 value based 的方法
value based 的方法我們介紹的就是 Q learning
那在講 Q learning 的時候我們說
有兩種 functions，有兩種 critics
第一種 critic 我們寫作 V，它的意思是說
假設我們現在 actor 是 pi
那我們拿 pi 去跟環境做互動
當今天我們看到 state, s 的時候
接下來 accumulated reward 的期望值有多少
還有另外一個 critic，叫做 pi
pi 是吃 s 跟 a 當作 input
他的意思是說，在 state s 採取 action a
接下來都用 actor pi 來跟環境進行互動
那 accumulated reward 的期望值，是多少
那這個都是我們看過的圖啦，V input s
output 一個 scalar
Q inputs，然後它會給每一個 a 呢
都 assign 一個 Q value
那這個 estimate 的時候
你可以用 TD 也可以用 MC
TD 會比較穩， 然後用 MC 比較精確
那接下來我們要做的事情其實，就是你完全可以想見
G 這個 random variable
它的期望值到底是什麼呢？
其實 G 的 random variable 的期望值
正好就是 Q 這樣子，因為這個就是 Q 的定義
Q 的定義就是，在某一個 state, s
採取某一個 action, a
假設我們現在的 policy，就是 pi 的情況下
會得到的 reward 的期望值
accumulated reward 的期望值有多大
而這個東西就是 G 的期望值
這個為什麼會這樣，因為這個就是 Q 的定義
Q function 的定義
其實就是 accumulated reward 的期望值
就是 G 的期望值
所以，我們現在要做的事情就是
這一項假設我們要用期望值來代表的話
然後把 Q function 套在這裡，就結束了
那我們就可以 Actor 跟 Critic 這兩個方法，把它結合起來
講到這裡，大家有問題要問的嗎 ?
這個其實很直覺，就是應該要這麼做
那接下來 baseline 呢，baseline 你就用
當然 baseline 你有很多不同的方法去加啦
但通常一個常見的做法是，你用 value function
來表示 baseline，所謂 value function 的意思就是說
假設現在 policy 是 pi，在某一個 state, s
一直 interact 到遊戲結束
那你 expected 的 reward 有多大
那 V 呢沒有 involve action，然後 Q 有 involve action
那其實 V 它會是 Q 的期望值
所以你今天把 Q，減掉 V
你的括號裡面這一項，就會是有正有負的
所以我們現在很直覺的
我們就把原來在 Actor-Critic
你的原來在 policy gradient 裡面
括號這一項，換成了 Q function 的 value
減掉 V function 的 value，就是這樣，就結束了
那接下來呢，其實你可以就單純的這麼實作
但是如果你這麼實作的話，他有一個缺點是
你要 estimate 2 個 networks，而不是一個 network
你要 estimate Q 這個 network
你也要 estimate V 這個 network
那你現在就有兩倍的風險
你有estimate 估測不準的風險就變成兩倍
所以我們和不只估測一個 network 就好了呢？
事實上在這個 Actor-Critic 方法裡面
你可以只估測 V 這個 network
你可以把 Q 的值，用 V 的值來表示，什麼意思呢？
現在其實 Q of (s, a) 呢，它可以寫成
r + V of s 的期望值
其實 Q of (s, a) 呢，可以寫成，r + V of s 的期望值
那今天我們直接假設說
當然這個 r 這個本身，它是一個 random variable
就是你今天在 state s，你採取了 action a
接下來你會得到什麼樣的 reward
其實是不確定的，這中間其實是有隨機性的
所以小 r 呢，它其實是一個 random variable
所以要把右邊這個式子
取期望值它才會等於 Q function
但是，我們現在把期望值這件事情去掉
就當作左式等於右式
就當作 Q function 等於， r 加上 state value function
然後接下來我們就可以把這個 Q function
用 r + V 取代掉
我們就可以把 Q function，用 r + V 取代掉
所以本來是 Q of (st, at) 減掉 V of st
現在就換成把 Q，換成 r + V，然後這邊是 s(t+1)
因為這邊這個式子的意思是說
你在 state s 採取 action a
接下來你會得到，reward r，然後呢
跳到 state, s(t+1)
但是你會得到什麼樣的 reward r
跟跳到什麼樣的 state, s(t+1)
它本身是有隨機性的
所以我們要在前面取一個期望值
左式才會等於右式
那我們現在把期望值這件事情拿掉
就直接說左式等於右式
然後把這個式子直接套進去這樣
這樣大家可以了解我的意思嗎？
大家可以了解我的意思嗎？
講到這一邊，大家有沒有問題要問的
有沒有問題要問的
如果大家可以接受這個想法
因為這個其實也是很直覺啦
因為我們說 Q function 是什麼意思
Q function 的意思就是在 state s 採取 action a 的時候
接下來會得到 reward 的期望值
那接下來會得到 reward 的期望值怎麼算呢？
在 state, s 採取 action, a
接下來會發生的事情就是
你會得到 reward rt，然後跳到 state, s(t+1)
我們現在在 state, st
然後我們採取 action, at
然後我們想要知道說，接下來會得到多少 reward
那接下來會發生什麼事呢？
接下來你會得到 reward rt，然後跳到 state, s(t+1)
那在 state s 採取 action a 得到的 reward
其實就是等於接下來得到 reward rt
加上從 state, s(t+1) 開始
得到接下來所有 reward 的總和
而從 state, s(t+1) 開始
得到接下來所有 reward 的總和
就是 V of s(t+1)
那在 state, st 採取 action, at 以後得到的 reward, rt
就寫在這個地方
所以這兩項加起來，會等於 Q function
那為什麼前面要取期望值呢？
因為你在 st 採取 action, at 會得到什麼樣的 reward
跳到什麼樣的 state 這件事情
本身是有隨機性的，不見得是你的 model 可以控制的
為了要把這隨機性考慮進去
前面你必須加上期望值
但是我們現在把這個期望值拿掉就說他們兩個是相等的
把這個東西塞進去，把 Q 替換掉
這樣的好處就是，你不需要再 estimate Q 了
你只需要 estimate V 就夠了
你只要 estimate 一個 network 就夠了
你不需要 estimate 2 個 network
你只需要 estimate 一個 network 就夠了
但這樣的壞處是什麼呢？
這樣你引入了一個隨機的東西
就是 r 現在，它是有隨機性的
它是一個 random variable
但是這個 random variable，相較於剛才的 G
accumulated reward 可能還好
因為他是某一個 step 會得到的 reward
而 Q 是所有未來的 step 會得到的 reward 的總和
說錯不是 Q，是 G
G 是所有未來會得到的 reward 的總和
G variance 比較大，r 雖然也有一些 variance
但它的 variance 會比 G 還要小
所以把原來 variance 比較大的 G
換成現在只有 variance 比較小的 r 這件事情也是合理的
那如果你不相信的話
如果你覺得說什麼期望值拿掉不相信的話
那我就告訴你原始的 A3C paper
它試了各式各樣的方法
最後做出來就是這個最好這樣
當然你可能說，搞不好 estimate Q 跟 V 也都 estimate 很好
那我給你的答案就是做實驗的時候，最後結果就是這個最好
所以，後來大家都用這個
所以那這整個流程呢，就是這樣
因為前面這個式子叫做 Advantage function
就是前面這個式子，叫做 advantage function
所以這整個方法就叫 Advantage Actor-Critic
那整個流程是這樣子的
我們現在先有一個 pi
然後pi，有個初始的 actor
那去跟環境做互動，先收集資料
在每一個 policy gradient 收集資料以後
你就要拿去 update 你的 policy
但是在 actor critic 方法裡面呢
你不是直接拿你的那些資料，去 update 你的 policy
你先拿這些資料去 estimate 出
你的 value function，那假設你是用別的方法
你有時候可能也需要 estimate Q function
那我們這邊是 A3C
我們只需要 value function 就好
我們不需要 Q function 只需要 value function 就好
那有了 value function
那 actor value function 過程
你可以用 TD，也可以用 MC
那你 estimate 出 value function 以後
接下來，你再 based on value function
套用下面這個式子去 update 你的 pi
然後你有了新的 pi 以後，再去跟環境互動
再收集新的資料，去 estimate 你的 value function
然後再用新的 value function，去 update 你的 policy
去 update 你的 actor
整個 actor-critic 的 algorithm，就是這麼運作的
那這個 implement Actor-Critic 的時候
有兩個幾乎一定會用的 tip，第一個 tip 是
我們現在說，我們其實要 estimate 的 network 有兩個
一個是 D，就不用 estimate Q 了
我們只要 estimate V function
而另外一個需要 estimate 的 network
是 policy 的 network，也就是你的 actor
那這兩個 network
那個 V 那個 network 它是 input 一個 state
output 一個 scalar
然後 actor 這個 network，它是 input 一個 state
output 就是一個 action 的 distribution
假設你的 action 是 discrete 不是 continuous 的話
如果是 continuous 的話，它也是一樣
如果是 continuous 的話
就只是 output 一個 continuous 的 vector
我想大家應該知道我的意思
那這邊是舉 discrete 的例子
但是 continuous 的 case，其實也是一樣的
input 一個 state
然後他要決定你現在要 take 那一個 action
那這兩個 network，這個 actor 跟你的 critic
跟你的 value function，它們的 input 都是 s
所以他們前面幾個 layer，其實是可以 share 的
尤其是假設你今天是玩 ATARI 遊戲
或者是你玩的是那種什麼 3D 遊戲
那 input 都是 image
那 input 那個 image 都非常複雜
那 image 很大張
通常你前面都會用一些 CNN 來處理
把那些 image 抽成 high level 的 information
把那個 pixel level 到 high level information 這件事情
其實對 actor 跟 critic 來說可能是可以共用的
所以通常你會讓這個 actor 跟 critic 的前面幾個 layer 是 shared
你會讓 actor 跟 critic 的前面幾個 layer 共用同一組參數
那這一組參數可能是 CNN
先把 input 的 pixel，變成比較high level 的資訊
然後再給 actor 去決定說它要採取什麼樣的行為
給這個 critic，給 value function
去計算 expected 的 return
也就是 expected reward
那另外一個事情是
我們一樣需要那個 exploration
我們一樣需要 exploration 的機制
那我們之前已經有講過
在講這個 Q function 的時候呢
在講 Q learning 的時候呢
我們有講過 exploration 這件事，是很重要的
那今天在做 Actor-Critic 的時候呢
有一個常見的 exploration 的方法
是你會對你的 pi 的 output 的這個 distribution
下一個 constrain
這個 constrain 是希望這個 distribution 的 entropy
不要太小
希望這個 distribution 的 entropy 可以大一點
也就是希望不同的 action
它的被採用的機率，平均一點
這樣在 testing 的時候
他才會多嘗試各種不同的 action
才會把這個環境探索的比較好
explorer 的比較好，才會得到比較好的結果
這個是 advantage 的 Actor-Critic
那接下來什麼東西是 A3C 呢？
因為這個 reinforcement learning 它的一個問題
就是它很慢
那怎麼增加訓練的速度呢？
這個可以講到火影忍者
就是有一次鳴人說，他想要在一周之內打敗曉
所以要加快修行的速度
他老師就教他一個方法
這個方法是說你只要用隱分身進行同樣修行
那兩個一起修行的話呢？
經驗值累積的速度就會變成2倍
所以，鳴人就開了 1000 個隱分身，開始修行了
這個其實就是 Asynchronous Advantage Actor-Critic 也就是 A3C 這個方法的精神
所以 A3C 這個方法他的精神就是
同時開很多個 worker
那每一個 worker 其實就是一個隱分身
那最後這些隱分身會把所有的經驗
通通集合在一起
這個 A3C 是怎麼運作的呢？
首先，當然這個你可能自己實作的時候
你如果沒有很多個 CPU，你可能也是不好做啦
反正就是講一講
作業也沒有強制你一定要做 A3C 就是了
你可以 implement A2C 就好
那 A3C 是這樣子，一開始有一個 global network
那我們剛才有講過說
其實 policy network 跟 value network 是 tie 在一起的
他們的前幾個 layer 會被 tie 一起
我們有一個 global network
他們有包含 policy 的部分
有包含 value 的部分
假設他的參數就是 θ1
那接下來每一個 worker 做的事情
你會開很多個 worker
那每一個 worker 就用一張 CPU 去跑
比如你就開 8 個 worker 那你至少 8 張 CPU
那第一個 worker 呢
就去跟 global network 進去把它的參數 copy 過來
每一個 work 要工作前就把他的參數 copy 過來
接下來你就去跟環境做互動
那每一個 actor 去跟環境做互動的時候
為了要 collect 到比較 diverse 的 data
所以舉例來說如果是走迷宮的話
可能每一個 actor 它出生的位置起始的位置都會不一樣
這樣他們才能夠收集到比較多樣性的 data
每一個 actor 就自己跟環境做互動
互動完之後，你就會計算出 gradient
那計算出 gradient 以後
你要拿 gradient 去 update 你的參數
我在想說，這 gradient 的三角形的方向怎麼好像怪怪的
這個應該是倒三角形啦
不過沒有關係，大家知道我的意思就好，這是倒三角形
那你就計算一下你的 gradient
然後你就拿你的 gradient 呢
去 update global network 的參數
就是這個 worker，它算出 gradient 以後
就把 gradient 傳回給中央的控制中心
然後中央的控制中心，就會拿這個 gradient
去 update 原來的參數
但是要注意一下，所有的 actor
都是平行跑的，就每一個 actor 就是各做各的
互相之間就不要管彼此，就是各做各的
所以每個人都是去要了一個參數以後
做完，它就把它的參數傳回去
做完就把參數傳回去
所以，當今天第一個 worker 做完
想要把參數傳回去的時候
本來它要的參數是 θ1
等它要把 gradient 傳回去的時候
可能別人 已經把原來的參數覆蓋掉，變成 θ2了
但是沒有關係，就不要在意這種細節，
它一樣會把這個 gradient 就覆蓋過去就是了
這個 Asynchronous actor-critic 就是這麼做的
那這個就是 A3C 啦，那在講 A3C 之後
我們要講另外一個方法叫做
Pathwise Derivative Policy Gradient，
那這個方法很神奇，它可以想成是 Q learning
解 continuous action 的一種特別的方法
那它也可以想成是一種，特別的 Actor-Critic 的方法
所以如果用棋靈王來比喻的話
你想想看阿光是一個 actor
佐為是一個 critic，阿光落某一子以後呢
佐為會說如果是一般的 Actor-Critic
他會告訴他說這時候不應該下小馬步飛
他會告訴你，你現在採取的這一步
算出來的 value 到底是好還是不好
但這樣就結束了，他只告訴你説好還是不好
因為一般的這個 Actor-Critic 裡面那個 critic
就是 input state 或 input state 跟 action 的 pair
然後給你一個 value，然後就結束了
所以對 actor 來說它只知道說現在，它做的這個行為
到底是好還是不好
但是，如果是在剛才講的這個
Pathwise derivative policy gradient 裡面
這個 critic 會直接告訴 actor 説
採取什麼樣的 action，才是好的
所以今天佐為不只是告訴阿光說
這個時候不要下小馬步飛，同時還告訴阿光說
這個時候應該要下大馬步飛
所以，這個就是
Pathwise Derivative Policy Gradient
要告訴我們的 critic
等一下會講得更清楚一點
critic 會直接引導 actor 做什麼樣的 action
才是可以得到比較大的 value 的
那如果今天從這個 Q learning 的觀點來看
我們之前說，Q learning 的一個問題是
你沒有辦法在用 Q learning 的時候
考慮 continuous vector
其實也不是完全沒辦法，就是不容易比較麻煩
比較沒有 general solution
那今天我們其實可以說
我們怎麼解這個 optimization problem 呢？
我們用一個 actor 來解這個 optimization 的 problem
所以我們本來在 Q learning 裡面
如果是一個 continuous action
我們要解這個 optimization problem
但是現在這個 optimization problem
由 actor 來解，我們假設 actor 就是一個 solver
這個 solver 他的工作就是，給你 state, s
然後他就去解解解告訴我們說，那一個 action
可以給我們最大的 Q value
這是從另外一個觀點來看
Pathwise derivative policy gradient 這件事情
那這個說法，你有沒有覺得非常的熟悉呢？
我們在講 GAN 的時候，不是也講過一個說法
我們說，我們 learn 一個 discriminator
它是要 evaluate 東西好不好
discriminator 要自己生東西，非常的困難，那怎麼辦？
因為要解一個 Arg Max 的 problem
非常的困難，所以怎麼辦
用 generator 來生，所以今天的概念其實是一樣的
Q 就是那個 discriminator
要根據這個 discriminator 決定 action 非常困難
怎麼辦？
另外 learn 一個 network
來解這個 optimization problem
這個東西就是 actor
所以，今天是從兩個不同的觀點
其實是同一件事，從兩個不同的觀點來看
一個觀點是說，原來的 Q learning 我們可以加以改進
怎麼改進呢？我們 learn 一個 actor 來決定 action
以解決 Arg Max 不好解的問題
或換句話說，或是另外一個觀點是
原來的 actor-critic 的問題是
critic 並沒有給 actor 足夠的資訊
它只告訴它好或不好，沒有告訴它說什麼樣叫好
那現在有新的方法可以直接告訴 actor 説，什麼樣叫做好
那我們就實際講一下它的 algorithm
那其實蠻直覺的
就假設我們 learn 了一個 Q function
假設我們 learn 了一個 Q function
Q function 就是 input s 跟 a，output 就是 Q of (s, a)
那接下來呢，我們要 learn 一個 actor
這個 actor 的工作是什麼
這個 actor 的工作就是，解這個 Arg Max 的 problem
這個 actor 的工作，就是 input 一個 state, s
希望可以 output 一個 action a
這個 action a 被丟到 Q function 以後
它可以讓 Q of (s, a) 的值，越大越好
那實際上在 train 的時候
你其實就是把 Q 跟 actor 接起來
變成一個比較大的 network
Q 是一個 network，input s 跟 a，output 一個 value
那 actor 它在 training 的時候
它要做的事情就是 input s，output a
把 a 丟到 Q 裡面
希望 output 的值越大越好
在 train 的時候會把 Q 跟 actor 直接接起來
當作是一個大的 network
然後你會 fix 住 Q 的參數
只去調 actor 的參數
就用 gradient ascent 的方法
去 maximize Q 的 output
這個東西你有沒有覺得很熟悉呢？
這其實就是一個 GAN 對不對？
這就是一個 GAN 這樣子
這就是 conditional GAN
Q 就是 discriminator
但在 reinforcement learning 就是 critic
actor 在 GAN 裡面它就是 generator
其實他們就是同一件事情
那我們來看一下這個
Pathwise derivative policy gradient 的演算法
一開始你會有一個 actor
你會有一個 actor pi，它去跟環境互動
然後，你可能會要它去 estimate Q value
estimate 完 Q value 以後，你就把 Q value 固定
只去那個 actor，actor learning 的方向
就是希望它的 output，希望這個 actor 採取
就我們假設這個 Q 估得是很準的
它真的知道說
今天在某一個 state 採取什麼樣的 action
會真的得到很大的 value
接下來就 learn 這個 actor
actor 在 given s 的時候，它採取了 a
可以讓最後 Q function 算出來的 value 越大越好
你用這個 criteria，去 update 你的 actor pi
然後接下來有新的 pi 再去跟環境做互動
然後再 estimate Q，然後再得到新的 pi
去 maximize Q 的 output
那其實本來在 Q learning 裡面
你用得上的技巧，在這邊也幾乎都用得上
比如說 replay buffer，exploration 等等
這些都用得上
我們再講得更具體一點
這個是原來 Q learning 的 algorithm
這個是我們上週看過的
你有一個 Q function，Q
那你會有另外一個 target 的 Q function
叫做 Q hat
這我們之前有講過說你會有
另外一個 target 的 Q function， 叫做 Q hat
然後在每一次 training
在每一個 episode 裡面
在每一個 episode 的每一個 timestamp 裡面
你會看到一個 state, st
你會 take 某一個 action, at
那至於 take 哪一個 action
是由 Q function 所決定的
因為解一個 Arg Max 的 problem
如果是 discrete 的話沒有問題
你就看說哪一個 a 可以讓 Q 的 value 最大
就 take 那一個 action
那你需要加一些 exploration
這樣 performance 才會好
你會得到 reward, rt
跳到新的 state, s(t+1)
你會把 st, at, rt, s(t+1) 塞到你的 buffer 裡面去
你會從你的 buffer 裡面 sample 一個 batch 的 data
這個 batch data 裡面，可能某一筆是 si, ai, ri, s(i+1)
接下來你會算一個 target
這個 target 叫做 y，y 是什麼？
y 是 ri 加上你拿你的 target Q function 過來
拿你的 Q function 過來
去計算 target 的 Q function
input 那一個 a 的時候，它的 value 會最大
你把這個 target Q function 算出來的 Q value 跟 r 加起來
你就得到你的 target y
然後接下來你怎麼 learn 你的 Q 呢？
你就希望你的 Q function
在帶 si 跟 ai 的時候，跟 y 越接近越好
這是一個 regression 的 problem，最後呢
每 t 個 step，你要把 Q hat 用 Q 替代掉
接下來我們把它改成
Pathwise Derivative Policy Gradient
那怎麼做，這邊就是只要做四個改變就好
第一個改變是，你要把 Q 換成 pi
本來是用 Q 來決定在 state, st
產生那一個 action, at
現在是直接用 pi
我們 learn 了一個 actor
這個 actor，我們不用再解 Arg Max 的 problem 了
我們直接 learn 了一個 actor
這個 actor input st
就會告訴我們應該採取哪一個 at
所以本來 input st，採取哪一個 at，是 Q 決定的
在 Pathwise Derivative Policy Gradient 裡面
我們會直接用 pi 來決定，這是第一個改變
第二個改變是，本來這個地方是要計算在 s(t+1)
根據你的 policy，採取某一個 action a
會得到多少的 Q value
那你會採取的 action a
就是看說哪一個 action a 可以讓 Q hat 最大
你就會採取那個 action a
這就是你為什麼把式子寫成這樣
那現在因為我們其實不好解這個 Arg Max 的 problem
所以 Arg Max problem
其實現在就是由 policy pi 來解了
所以我們就直接把 s(t+1)
帶到 policy pi 裡面
那你就會知道說
現在 given s(t+1)
那一個 action 會給我們最大的 Q value
那你在這邊就會 take 那一個 action
那這邊還有另外一件事情要講一下
我們原來在 Q function 裡面
我們說，有兩個 Q network
一個是真正的 Q network
另外一個是，target Q network
那實際上你在 implement 這個 algorithm 的時候
你也會有兩個 actor
你會有一個真正要 learn 的 actor pi
你會有一個 target actor pi hat
這個原理就跟那個
為什麼要有 target Q network 一樣
我們在算 target value 的時候
我們並不希望它一直的變動
所以我們會有一個 target 的 actor
跟一個 target 的 Q function
那它們平常的參數，就是固定住的
這樣可以讓你的這個 target
它的 value 不會一直的變化
所以本來到底是要用哪一個 action a
你會看說哪一個 action a
可以讓 Q hat 最大，
但是，現在
現在因為那一個 action a 可以讓 Q hat 最大這件事情
已經被直接用那個 policy 取代掉了
所以我們要知道哪一個 action a 可以讓 Q hat 最大
就直接把那個 state 帶到 pi hat 裡面
看它得到哪一個 a，就用那一個 a，
那一個 a 就是會讓 Q hat of (s, a)
他的值最大的那個 a 這樣
講到這邊大家可以接受嗎？
其實跟原來的這個 Q learning 也是沒什麼不同
只是原來你要解 Arg Max 的地方
你如果有一個 Max a 的地方
通通都用 policy 取代掉就是了
那這個是第二個不同
第三個不同就是
之前只要 learn Q，現在你多 learn 一個 pi
那 learn pi 的時候的方向是什麼呢？
learn pi 的目的，就是為了 Maximize Q function
希望你得到的這個 actor
它可以讓你的 Q function output 越大越好
這個跟 learn GAN 裡面的 generator 的概念
其實是一樣的
第四個 step，就跟原來的 Q function 一樣
你要把 target 的 Q network 取代掉
你現在也要把 target policy 取代掉
那其實確實 GAN 跟 Actor-Critic 的方法是非常類似的
那我們這邊就不細講
你可以去找到一篇 paper 叫做
Connecting Generative Adversarial Network and Actor-Critic Method
那知道 GAN 跟 Actor-Critic 非常像有什麼幫助呢？
一個很大的幫助就是
GAN 跟 Actor-Critic 都是以難 train 而聞名的
所以，在文獻上就會收集 develop 的各式各樣的方法
告訴你說怎麼樣可以把 GAN train 起來
怎麼樣可以把 Actor-Critic train 起來
但是因為做 GAN 跟 Actor-Critic 的人
其實是兩群人啊
所以這篇 paper 裡面就列出說在 GAN 上面
有哪些技術是有人做過的
在 Actor-Critic 上面，有哪些技術是有人做過的
但是也許在 GAN 上面有試過的技術
你可以試著 apply 在 Actor-Critic 上
在 Actor-Critic 上面做過的技術
你可以試著 apply 在 GAN 上面
看看 work 不 work
這個就是 Actor-Critic 和 GAN 之間的關係
可以帶給我們的一個好處
那這個其實就是 Actor-Critic
講到這裡大家有沒有問題要問的
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
