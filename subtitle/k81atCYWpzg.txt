hi everyone thank you for watching my
presentation
i'm having the first author today i'm
going to
talk about my inter speech paper
this is the title this work was done by
andy leo
professor hong lee and me
this is today's outline today's
presentation
is divided into five sections
motivation background proposed
method experiment and the conclusion
let's get into the motivation
anti-spoofing models with high
performance
are vulnerable to adversarial attacks
and the effectiveness of self-supervised
learning in defense for the virtual
attacks
on anti-spoofing models of sv
hasn't been started yet so in this work
we explore the robustness of
self-supervised learned high-level
features
by using the in the defense against
the virtual attacks
let's get into the background
the automatic speaker verification
systems
are subject to spoofing audios
this moving audios are generated by
voice conversion audio replay
or test to speech so the anti-spoofing
model is proposed
as a shield for sv systems
the anti-spoofing model will help
detect the spoofing audios enable them
spoofing
and throw them away however
if the input is a piece of user's audio
the anti-spoofing model or label is
non-smoothing
and that it passed a lot of
anti-spoofing models
with high performance are proposed
however previous works show that
anti-spoofing models
are subject to universal attacks
given a piece of spoofing audio the
anti-spoofing model
should label it spoofing and throw it
away
however some attackers can find a
versatile noise and generate a reversal
audio the reversal noise
is carefully designed so from
fulfillment's perception
the universal audio is over 99 percent
similar to the original one
but the prediction of anti-spoofing
model
will become non-spoofing in this
scenario
the anti-spoofing model becomes
transparent and useless
this phenomenon will cause some security
problems
so we want to fix it
then i want to introduce how to
generate a virtual attack
look at what we have we have the
original audio
x and the anti-spoofing model
f parameter by theta we want to
find a reversal noise delta to generate
a reversal
audio x delta in order to make the
prediction
of f x delta and f x as different
as possible then how can we do it
just like training a neural network but
we
optimize the input tags rather than the
model parameter theta specifically
we fix the model parameters and then we
use gradient descent to find a suitable
delta
to fulfill this equation we want the
difference
of f x and f x tilde to be maximized
also we want the l infinity norm of data
to be smaller than a predefined value
epsilon different
certain strategies for delta will result
in different
attack methods in this work
we adopted two gradient-based methods
the projected gradient descent method
under the first gradient send methods
the details of the two methods are not
our focus
so we just give the reference here
then i want to talk about what is black
box
attack here we have two
models the attacking model
and the target model the target model is
the model we want to attack
the attacking model is the model we use
to generate universal examples
in the black box attack scenario
the attackers don't know the internals
about the target model
so the attackers will use another
attacking model
to generate universal examples
and the use base of the virtual examples
to attack the target model
now let's get into the proposed defense
method
self-supervised learning is popular
nowadays
level data is hard to acquire
while unlabeled data is everywhere
self-sufficient learning will allow
models to learn knowledge
from a large amount of unlabeled data
banking j is a self supervised learning
based
model during pre-training
we do masking to the input special frame
then we we use the monkey j
encoder to extract the monkey j
representations then we use the
prediction heads
to reconstruct the unmasked special
grams
the clean special grams finally we use
the l1 loss to trend the entire model
through pre-training the monkey j
is able to learn knowledge from a large
amount of unlabeled speech
after training it will extract
high-level
features to improve the performance of
downstream tasks
however the effectiveness of such
high-level features against the reversal
attacks
haven't been studied yet so in this work
we did it recall the procedure
of a reversal attack the attackers
find a versatile noise and generate
universal examples
to deceive the anti-spoofing model
the reversal noise is also a kind of
noise to some extent so if we can purify
them
then we can successfully do the defense
this is the masking procedure of monkey
j the masking procedure
introduces noise to the input special
grams
the monkey is trying to learn how to
weaken the noise in the inputs
and extract pivotal information
from the contaminated inputs and use
them for reconstruction
so after training the monkey j has the
power
to purify the noise in the inputs
the versatile noise is also a kind of
noise to some extent so based on it
we want to use the monkey j to
weaken the reversal noise
to summarize our work in one sentence
we want to use monkey j as a deep filter
in front of the anti-spoofing model to
purify the contaminated
universal examples specifically we
firstly pre-trained the monkey
j and fix the parameters of monkey
j then during the anti-spoofing model
training
we extract the high-level features by
mounting j
and use the high-level features to trend
the anti-spoofing model
in the black box attack scenario the
attackers
are not aware of the existence of
monkey j and treat the entire model as a
black box
they only know the input to the black
box
is spectrogram so they
find a versatile noise and add it to the
special grants
however in the real inference time
before the contaminated universal audios
are thrown into the anti-spoofing model
the monkey j will first lay and weaken
the superfacial noise
the reversal noise and then avoid
the transferability of a reversal attack
then let's get into the experiments
for the data sets we use the la
partition
of asv spoof 2019 challenge
the dataset contains audios generated by
text to speech or voice conversion
models
two different anti-spoofing models are
adopted
i see net and lc and
we just simply borrowed their model
structure
so if you are interested in more details
you can refer to their papers
the text scenario in this paper is black
box attack
where the attackers don't know the
internals of the target model
here are the experiment results
we use two attack methods pgd
and fgsm to attack two
anti-spoofing models lcn
and ic nets so here we have
four scenarios
let's look at the result of attacking
lcn
with pgd the x-axis
is epsilon the intensity of
attack signal the y-axis is the testing
accuracy
of anti-spoofing model the more higher
the accuracy is
the more effective the defense method is
we use monk to denote the proposed
monkey-based
defender we use smell to denote
the anti-spoofing model trained by male
spectrograms
we use median mean gaussian
to denote the anti-spoofing models
equipped with
medium filter in filter or gaussian
filter
in front of them as we can see
the basic male model are subject to
universal attacks
and the
median mean and the gaussian filter
can counter the reversal example to some
extent
however as we can see the monkey-based
defender
outperforms all the filters
also we show the defense results of a
random parameterized amount
in j denoted as rent
the difference between monk and the rent
is that the monkey chain model for rent
is random parameterized rather than
pre-trends as we can see
the random parameterized monkey j
fails to counter the reversal attacks
as epsilon increases we can see the
random parameterized monkey
j is not only worse than the monk method
but also worse than some handcrafted
filters such as mean filter and the
gaussian filter so we can simply draw
the conclusion that
our success in the right curve
is not simply from the mismatch of model
structures between the target model and
attacking model to further show the
importance of pre-training
we trend the entire model the
anti-spoofing model
with the monkey j from scratch
the result is denoted as scratch
the the black dot one
we can see trending the entire model
from scratch results in
very low accuracy which is barely better
than 50
you know random guess the result further
shows the importance of free training
in all four scenarios we can see the
monkey
j based defender the right curve
always attain high accuracy than other
approaches here are the conclusions
in this work we propose to yourself
supervised learning
for the virtual defense on anti-spoofing
models of
asv and the experiment results
illustrate the representation extracted
by self-supervised learning model
prevents the transferability of
universal examples
that's all i have today thanks for your
time
