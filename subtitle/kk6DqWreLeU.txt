好 那我們就開始上課吧
那等一下的規劃是這樣
我們先把 RL 講到一個段落
然後再請助教公布作業
好 那上一次 RL 的部分
我們講說我們要 Learn 一個 Actor
那這一次啊
我們要 Learn 另外一個東西
這個東西叫做 Critic
那我會先解釋 Critic 是什麼
然後我們再來講說
這個 Critic 對 Learn Actor 這個東西
有什麼樣的幫助
好 我們先看 Critic 是什麼
Critic 是什麼呢
Critic 是它要拿
它是它的工作啊
是要來評估一個 Actor 的好壞
就你現在已經有一個 Actor
它的參數叫 θ
那 Critic 的工作就是
它要評估說如果這個 Actor
它看到某個樣子的 Observation
看到某一個遊戲畫面
接下來它可能會得到多少的 Reward
那 Critic 有好多種不同的變形
有的 Critic 是只看遊戲畫面來判斷
有的 Critic 是說採取某
看到某一個遊戲畫面
接下來又發現 Actor 採取某一個 Action
在這兩者都具備的前提下
那接下來會得到多少 Reward
那這樣講呢
還是有點抽象
所以我們講的更具體一點
我們直接介紹一個
我們等一下會真的被用上
你在作業裡面真的派得上用場的
這個 Critic 叫做 Value Function
那這個 Value Function
我們這邊用大寫的 Vθ(S) 來表示
那它的輸入是什麼
它的輸入是 s
也就是現在遊戲的狀況
比如說遊戲的畫面
那這邊要特別注意一下 V
它是有一個上標 θ 的
這個上標 θ 代表什麼意思呢
代表這個 V 呢
它觀察的對象是 θ 這個 Actor
它觀察的這個 Actor 呢
它的參數是 θ
那這個 V 呢
Vθ 就是一個 Function
它的輸入是 S
那輸出是一個 Scalar
這邊用 Vθ(S) 來表示這一個 Scalar
那這個 Scalar 呢
這個數值有什麼樣的含義呢
這個數值的含義是
這一個 Actor θ
放在上標的這個 Actor θ
它如果看到 Observation S
如果看到輸入的這個 S 的遊戲畫面
接下來它得到的
Discounted Cumulated Reward 是多少
你還記得這個
Discounted Cumulated Reward 是什麼嗎
記不記得我們之前說
在評估某一個 Action 好壞的時候
你不能單看那個 Action 執行完的 Reward
你要把那個 Action 執行完的 Reward
加上接下來所有的 Reward
得到一個 Cumulated Reward 叫 G
這樣才能夠評估
在某一個 State
執行某一個 Action 的好壞的程度
但是我們又說
你把所有的 R
從執行某一個 Action 開始到遊戲結束
所有的 r 直接都加在一起
並不是一個好的想法
你應該乘上一個 Discounted 的 Factor 叫做 γ
就是今天得到的 Reward 如果離執行
這個某一個 Action 的時間越遠的話
那個 Reward 就跟那個 Action 越沒有關係
所以應該乘上一個 Discounted 的 Factor
叫做 γ
那我們得到 G'
這個 G' 呢
就是這邊所謂的
Discounted Cumulated Reward
那這個的 Value Function 它的工作
就是要去估測說
對某一個 Actor 來說
如果現在它已經看到某一個遊戲畫面
那接下來會得到的
Discounted Cumulated Reward 應該是多少
當然 Discounted Cumulated Reward
你可以直接透過把遊戲玩到底
就你看到你已經有了 Actor θ
那假設它看到這個 State s
那最後它到底會得到多少的這個 G' 呢
你就把這個遊戲玩完你就知道了
但是這些這個 Value Function
它的能力就是它要未卜先知
未看先猜
遊戲還沒有玩完
只光看到 S 就要預測這個 Actor
它可以得到什麼樣的表現
那這個就是 Value Function 要做的事情
舉例來說呢
假設你給 Value Function 這一個遊戲畫面
它就要直接預測說
看到這個遊戲畫面
接下來應該會得到很高的 Cumulated Reward
為什麼
因為遊戲
這個遊戲畫面裡面還有很多的外星人
假設你的這個 Actor 它很厲害
它是一個好的 Actor
它是能殺得了外星人的 Actor
那接下來它就會得到很多的 Reward
那像這個畫面
這已經是遊戲的中盤
遊戲的殘局
遊戲快結束了
剩下的外星人沒幾隻了
那可以得到的 Reward 就比較少
那這些數值
你把整場遊戲玩完你也會知道
但是 Value Function 想要做的事情
就是未卜先知
在遊戲沒玩完之前
就先猜應該會得到多少的
Discounted Cumulated Reward
那這邊有一件要跟大家特別強調的事情是
這個 Value Function 是有一個上標 θ 的
這個 Value Function
跟我們觀察的 Actor 是有關係的
同樣的 Observation
同樣的遊戲畫面
不同的 Actor
它應該要得到不同的
Discounted Cumulated Reward
我剛才在舉例子的時候我說
假設我們有一個好的 Actor
看到這個遊戲畫面會有高的 Value
看到這個遊戲畫面會有低的 Value
但是假設你的 Actor 其實很爛
它很容易被外星人殺死
那也許看到這個畫面
它的 Value 也是低的
因為有一堆外星人
它隨便動兩下它就被殺死了
它根本得不到 Reward
這個爛的 Actor 在這個畫面
它可能拿到的 V 也是低的
所以 Value Function 的數值
是跟觀察的對象有關係的
好 這個是 Critic
那在講 Critic 要怎麼被使用
在 Reinforcement Learning 之前呢
我們來講一下 Critic 是怎麼被訓練出來的
那有兩種常用的訓練方法
第一種方法呢
是 Monte Carlo Based 的方法
這邊縮寫成 MC
好 那怎麼訓練出這一個 Value 的 Function 呢
這個 Value 的 Critic V θ 呢
你就如果是用 MC 的方法的話
非常直覺你就這樣做
你就把 Actor 呢
拿去跟環境互動
互動很多輪
那 Actor 跟環境互動以後呢
Actor 去玩這個遊戲以後呢
你就會得到一些遊戲的記錄
你就會發現說
假設 Actor 看到 Sa
那因為你已經把遊戲玩完了嘛
你可能玩了很多場遊戲
那就知道說
看到 State
看到 Observation Sa
接下來的 Cumulated Reward 會是 G'a
我這邊把 Discounted 省略掉了
反正你知道我的意思就好了
好 那這個時候呢
你的 Value Function 就得到一筆訓練資料
這筆訓練資料告訴它說
如果看到 Sa 作為輸入
它的輸出啊
這個 Vθ(Sa) 應該要跟 G'a 越接近越好
那假設你今天 Sample 到另外一個 Observation
看到另外一個遊戲畫面
把遊戲玩完之後發現
得到的 Cumulated Reward 是 G'b
那這個時候呢
你的這個 Value Function 呢
輸入 Sa 它就應該得到 Vθ(Sb)
那這個 Vθ(Sb) 就應該跟 G'b 越接近越好
那這個非常直覺
你就去觀察 Actor
會得到的 Cumulated Reward
那觀察完你就有訓練資料
直接拿這些訓練資料來訓練 Value Function
好 這個 MC 呢
是一個很直覺的作法
接下來我們來看另外一個
沒有那麼直覺的作法
這個作法叫做 Temporal-Difference Approach
縮寫是 TD
那 Temporal-Difference Approach
它希望做到的事情是
不用玩完整場遊戲
才能得到訓練 Value 的資料
你只要在某一個 Observation St 的
看到 St 的時候
你的 Actor 執行了 At 得到 Reward rt
然後接下來再看到 St+1 這樣的遊戲畫面
光看到這樣一筆資料
就能夠訓練 Vπ(S) 了
光看到這樣子的資料
就可以拿來更新 Vπ(S) 的參數了
那如果光看這樣一筆資料
就可以更新 Vπ(S) 的參數有什麼樣的好處呢
它的好處是你想在 MC 裡面
你要玩完整場遊戲
你才能得到一筆訓練資料
那有的遊戲其實很長啊
甚至有的遊戲也許
它根本就沒有不會結束
它永遠它都一直繼續下去
它永遠都不會結束
那像這樣子的遊戲
你用 MC 就非常地不適合
那這個時候
你可能就希望採用 TD 的方法
好 那怎麼只看到這樣子的資料
就拿來訓練 Vπ(S) 呢
這邊舉一個例子啊
我們先來看一下
Vθ(St) 跟 Vθ(St+1) 它們之間的關係
那 Vθ(St) 是什麼
我們說 Vθ(St)
就是看到 St 之後的 Cumulated Reward
所以 Vθ(St) 就是 rt + γrt+1 + γ²rt+2 以此類推
然後 Vθ(St+1) 就是 rt+1 + γrt+2 以此類推
那你發現說這兩個 Vθ
Vθ(St) 跟 Vθ(St+1)
它們之間是有關係的
什麼樣的關係呢
你可以把它寫成這樣一個式子
把 Vθ(St+1) 乘上 γ 再加 rt
把 Vθ(St+1) 每一項都乘 γ 再加上 rt
就會變成 Vθ(St)
所以 Vθ(St) 跟 Vθ(St+1) 中間
有這樣子的關係
我們現在呢
有這樣一筆資料以後
我們就可以拿來訓練我們的 Value Function
希望 Value Function 可以滿足
這邊我們所寫的這個式子
好 那什麼意思呢
就假設我們現在有這樣一筆資料
我們就把 St
代到 Value Function 裡面得到 Vθ(St)
我們有 St+1 代到 Value Function 裡面
得到 Vθ(St+1)
雖然我們不知道 Vθ(St) 是多少
我們也不知道 Vθ(St+1) 應該是多少
我們沒有這兩個東西的標準答案
但我們知道它們相減應該是多少
根據上面這一個式子
我們把 Vθ(St+1) 乘上 γ
然後再去減 Vθ(St)
把 Vθ(St) 減掉 γ 乘 Vθ(St+1)
應該要跟 rt 越接近越好
rt 在這邊
我們是有蒐集到 rt 這一筆資料的
我們又知道 Vθ(St)
跟這個 Vθ(St+1) 之間的關係
所以我們知道 Vθ(St) 減掉 γ 乘上 Vθ(St+1)
應該跟 rt 越接近越好
所以你就有了這樣子訓練資料
輸入 St
輸入 St+1
它們都通過 Vθ
然後把它們相減
然後要跟 rt 越接近越好
那這個就是 TD 的方法
好 那所以我們介紹了兩個
這個訓練 Value Function 的方法
介紹兩個訓練 Critic 的方法
一個叫 MC
一個叫 TD
那這兩個方法
其實你拿來計算同樣的
觀察到的結果
同樣的資料
同樣的 θ
你用 MC 跟 TD 來觀察
你算出來的 Value Function
很有可能會是不一樣的
那這邊呢
就舉一個例子
這個例子是這樣子的
我們觀察某一個 Actor
這個 Actor 呢
跟環境互動玩了某一個遊戲八次
當然這邊為了簡化計算
我們假設這些遊戲都非常簡單
都一個回合
就到兩個回合就結束了
所以那個 Actor 第一次玩遊戲的時候呢
它先看到 Sa 這個畫面
得到 Reware 0
接下來看到 Sb 這個畫面
得到 Reware 0 遊戲結束
接下來呢
這個有連續六場遊戲
都是看到 Sb 這個畫面
得到 Reward 1 就結束了
最後一場遊戲
看到 Sb 這個畫面
得到 Reward 0 就結束了
那我們這邊呢
先無視 Actor
為了簡化起見無視 Actor
我們也假設呢
γ 就等於 1
也就是沒有做 Discount
好 那這個 Sb 應該是多少呢
Vθ(Sb) 應該是多少呢
我們知道這個 Vθ(Sb)
它的意思就是這個看到 Sb 這一個畫面
你會得到的 Reward 的期望值
那 Sb 這個畫面呢
我們在這八次遊戲中
總共看到了八次
每次遊戲都有看到 Sb 這個畫面
看到 Sb 這個畫面之後會得到多少 Reward 呢
八次遊戲裡面呢
有六次得到 1 分
兩次得到 0 分
所以平均是 3/4 分沒有問題
所以 Vθ(Sb) 就是 3/4
妥妥的沒有爭議
那 Vθ(Sa) 應該是多少呢
你覺得看到 Sa
接下來應該要得到多少 Reward 呢
根據這八筆訓練資料
看到 Sa 接下來該得到多少 Reward 呢
好 給大家十秒鐘的時間
你把你的答案寫在留言板上
你把你的答案打在留言區上
我們趁著這個機會來回答一下
同學們的問題
有嗎
好 如果大家有問題的話
你就可以打在聊天室裡面
接下來給大家一點時間想想看
你覺得 Vθ(Sa)
根據我們看到的這八筆訓練資料
它應該是多少呢
把你的答案
寫在聊天室裡面
我看這個聊天室到底能不能用
可以
好 我看到了好多答案
幾乎所有人都說是 0
幾乎所有人都是 0
有別的答案嗎
好 幾乎沒有其他答案
所有人都說是 0
好 多數同學都說是 0
0 是不是一個正確的答案呢
它既對也不對
其實還有另外一個可能的答案是 3/4
我看沒有人寫 3/4
等一下來解釋
為什麼有可能算出 3/4
但 0 也是一個合理的答案
為什麼你會覺得是應該是 0 呢
0 是用 Monte-Carlo 的想法得到的
為什麼是 0
因為我們看到 Sa 只有一次
看到 Sa 以後會得到多少 Reward
這是 0
看到 Sa 以後得到 Reward 0
再看到 Sb 得到 Reward 還是 0
所以 Cumulated Reward 就是 0
所以如果從 Monte-Carlo 的角度來看
我們看到 Sa
接下來算出來的 G 應該是多少
就是 0 啊
所以 Vθ(Sa) 應該就是 0
妥妥的沒問題
幾乎所有同學都得到了正確答案
但如果你用 TD
你算出來的
可會是不一樣的結果哦
怎麼說呢
因為 Vθ(Sa) 跟 Vθ(Sb) 中間
有這樣子的一個關係
這個 Vθ(Sa) 應該要等於 Vθ(Sb) 加上 Reward
就是你在看到 Sa 之後得到 Reward
接下來進入 Sb
那這個 Vθ(Sa)
應該等於 Vθ(Sb) 加上這一個 Reward
這邊 Location（00：16：43）沒有用的非常好啦
這個 r 指的是這邊這個 r
所以按照這個想法
Vθ(Sb) 是多少
3/4
這個 r 是多少
是 0
但 Vθ(Sa) 應該是 3/4 對不對
按照 TD 的想法
Vθ(Sa) 應該是 3/4
你可能會問說
那到底 Monte-Carlo 跟 TD
誰算出來是對的呢
都可以說是對的
它們只是背後的假設是不同的
對 Monte-Carlo 而言
它就是直接看我們觀察到的資料
Sa 之後接 Sb 得到的
Cumulated Reward 就是 0
所以 Vθ(Sa) 當然是 0
但對於 TD 而言
它背後的假設是
這個 Sa 跟 Sb 是沒有關係的
看到 Sa 之後再看到 Sb
並不會影響看到 Sb 的 Reward
你現在看這八筆訓練資料
給你一種錯覺
覺得說 Vθ(Sa) 應該是 0
那只是因為你 Sample 到的資料太少了
看到 Sb
應該可以期望的 Reward 是 3/4
只是因為今天正好運氣不好
看完 Sa 以後再看 Sb
正好 r 是 0
但是期望值應該是 3/4
你只是正好運氣不好看到 r 是 0
你才會覺得 Sa 是 0
但是 Sb
看到 Sb 以後得到的期望 Reward 應該是 3/4
所以看到 Sa 以後你會看到 Sb
那你得到的這個期望的 Reward 也應該是 3/4
所以從 TD 的角度來看
Sb 會得到多少 Reward
跟 Sa 是沒有關係的
所以你應該
所以 Sa 的這個 Cumulated Reward 應該是 3/4
但對於 Monte-Carlo 而言
它並不覺得 Sa 跟 Sb 是沒有關係的
也許 Sa 就是一個帶賽的 Observation
看到 Sa 以後
你就影響了 Sb 會得到的 Reward
也許看到 Sa 以後
Sb 就是會得到 Reward 等於 0
就它們之間互相有影響的
看到 Sa 以後 Sb 就會等於 0
那 Vθ(Sa) 應該等於 0
所以總之用 MC 來計算
用 TD 來計算
會有微妙的差異
好 我來看看有沒有同學有問題
好 目前沒有
助教有統計一下說
只有一個同學回答 1
其他都是 0
好 那這個是 MC 跟 TD
這個是 MC 跟 TD
好 那接下來我們就是要看說
這個 Critic 怎麼被用在訓練 Actor 上面
還記不記得我們上一次
最後我們講到這個 Actor 的方法的時候
我們說怎麼訓練一個 Actor
你就先把 Actor 跟環境互動
得到一些 Reward
然後你得到一堆這個 Observation
跟這個 Action 的 Pair
然後你要說
欸 這個在 s1 執行 A1 的時候多好呢
得到一個分數 A1
那我們說這個 A1 啊
它是 Cumulative 的 Reward
那上週也有同學問到說
難道 Cumulative 的 Reward
不需要做 Normalization 嗎
需要做 Normalization
所以我們說
這個減掉一個 b 當做 Normalization
但這個 b 的值應該設多少呢
就不好說
那我這邊呢 告訴大家說
一個 V 合理的設法
是把它設成 Vθ(S)
假設你現在 Learn 出了一個 Critic
就根據這些訓練資料
其實你也可以去 Learn 一個 Critic 嘛
那你現在 Learn 出這個 Critic 以後
這個 Critic 給它一個 Step
它就會產生一個分數
那你把這個分數啊 當做 B
你把這個分數當做 B
所以 G1' 就是要減掉 Vθ(s1)
G2' 就是減掉 Vθ(s2)
以此類推
好 那再來的問題就是
為什麼減掉 V 是一個合理的選擇呢
那我們在下一頁投影片
來跟大家解釋一下
好 那現在呢
我們已經知道說這個
s a 這個 Pair 它有多好呢
這個 At 代表它有多好呢
我們是用 G' 減掉 Vθ(St)
來定義這個 A
好 那我們先來看一下這個 Vθ(St)
到底代表什麼意思
Vθ(St) 代表什麼意思呢
Vθ(St) 是看到某一個畫面 St 以後
接下來會得到的 Reward
那它其實是一個期望值
因為假設你今天看到同一個畫面
接下來再繼續玩遊戲
遊戲有隨機性
你每次得到的 Reward 都不太一樣的話
那 Vθ(St) 呢 其實是一個期望值
好 那在這個時候啊
在看到 St 的時候啊
你的 Actor 不一定會執行
At 這一個 Action
欸 你說為什麼呢
為什麼它不一定執行 At 這個 Action 呢
不要忘了 Actor 本身是有隨機性的
在訓練的過程中
我們甚至鼓勵 Actor 是有隨機性的
所以同樣的 S
你的 Actor 呢
它會輸出的這個 Action 不一定是一樣的
我們說 Actor 的輸出
其實是一個 Probability 的 Distribution
是一個在這個 Action 的 Sbase 上面的
Probability Distribution
它還給每一個 Action 一個分數
你按照這個分數去做 Sample
有些 Action 被 Sample 到的機率高
有些 Action 被 Sample 到的機率低
但每一次 Sample 出來的 Action
並不保證一定要是一樣的
喔 所以看到 St 之後
接下來有很多的可能 很多的可能
所以你會算出不同的 Cumulative 的 Reward
那當然如果你有 Discount 的話
就是 Discounted 的 Cumulative Reward
那我們這邊呢
是把 Discount 這件事情暫時省略掉
反正大家知道我的意思
我這邊沒有加
我這邊沒有加 ' 啦
不過大家知道 沒有加上標 '
不過大家知道我的意思就好
好 那把這些可能的結果平均起來
就是 Vθ(St)
這是 Vθ(St) 這一項的含義
好 那 Gt' 這一項的含義是什麼呢
Gt' 這一項的含義是
在 St 這個位置 在 St 這個畫面下
執行 At 以後
接下來會得到的 Cumulative Reward
所以你執行 At 以後
接下來呢 再一路玩下去
你會得到一個結果 得到一個 Reward
就是 Gt'
那如果今天 At 大於 0 是什麼意思
如果 At 大於 0 代表說
Gt' 大於 Vθ(St)
這個時候代表說呢
這個 Action 是比
我們 Random Sample 到的 Action 還要好的
在這邊得到 Gt' 的時候
我們確定是執行了 At
那在 St 在算這個 Vθ(St) 的時候
我們不確定我們會執行哪一個 Action
好 所以我們執行 Action At 的時候
得到的 Reward
大於隨便執行一個 Action 得到的 Reward
所以當 At 大於 0 的時候代表說
At 大於隨便執行的一個 Action
那這個時候這個 Action At 它就是好的
所以我們給它一個大於 0 的 At
反之 如果 At 小於 0 什麼意思
At 小於 0 代表說
這個平均的 Reward
大過執行 At 得到的 Reward
你隨機採取的 Action
按照某一個 Distribution
Sample 出來的 Action
得到的這個 Cumulative Reward 的期望值
大過採取 At 這個 Action 所得到的 Reward
那這個時候 At 就是壞的
所要給它負的大 At
所以這樣就非常地直覺
為什麼我們應該把 Gt' 減掉 Vθ(St)
但講到這邊
你有沒有覺得有一些地方有點違和呢
什麼地方有點違和呢
這個 Gt' 它是一個 Sample 的結果
它是執行 At 以後
一直玩玩玩 玩到遊戲結束
某一個 Sample 出來的結果
而 Vθ(St) 是很多條路徑 很多個可能性
平均以後的結果
我們把一個 Sample 去減掉平均
這樣會準嗎
也許這個 Sample 特別好或特別壞啊
我們為什麼不是拿平均去減掉平均呢
所以我們這一門課要講的最後一個版本
就是拿平均去減掉平均
也就是說 我們執行完 At 以後 得到 Reward rt
然後跑到下一個畫面 St+1
把這個 St+1 接下來一直玩下去
有很多不同的可能
每個可能通通會得到一個 Reward
把這些 Reward 平均起來
把這些 Cumulative 的 Reward 平均起來
是多少呢 其實就是 Vθ(St+1)
本來你會需要玩很多場遊戲
才能夠得到這個平均值
但沒關係
假設你訓練出一個好的 Critic
那你直接代 Vθ(St+1)
那你就得到 你直接得到
你直接代 Vθ(St+1) 你就知道說
在 St+1 這個畫面下
接下來會得到的
Cumulative Reward 的期望值應該多少
而接下來呢 你再加上 rt
接下來再加上 rt
代表說在 St 這個位置採取 at
跳到 St+1以後
會得到的 Reward 的期望值
因為我們已經知道說
在 St 這邊採取 at 會得到 Reward rt
再跳到 St+1
然後 St+1 會得到期望值
期望的 Reward 是 Vθ(St+1)
所以我們這邊呢
再給它加上 rt
代表說在 St 這邊執行 At 以後
會得到的 Reward 的期望值
接下來再把這兩個東西相減
再把 rt+Vθ(St+1) 減掉 Vθ(St)
也就是我們把 G' 換成 rt+Vθ(St+1)
再減掉 Vθ(St)
我們就知道說
採取 at 這個 Reward
不是 Reward at不是 Reward
它是 Action
採取 at 這個 Action
接下來得到的期望 Reward
減掉不採取 at
而是隨便 Sample 一個 Action
根據某個 Distribution Sample 一個 Action
會得到的 Reward
兩者的期望值差距有多大
那如果 rt+Vθ(St+1) 比較大
就代表 at 比較好
它比隨便 Sample Reward 好
rt+Vθ(St+1) 小於 Vθ(St)
就代表 at 它是 Lower Than Average
它比從一個 Distribution
Sample 到的 Action 還要差
所以今天啊
這個就是大名鼎鼎的一個常用的方法
叫做 Advantage Actor-Critic
在 Advantage Actor-Critic 裡面
你是怎麼定義 at 的呢
也就是 rt+Vθ(St+1) 減掉
rt+Vθ(St+1) - Vθ(St)
就是我們的 At 了
好 那這邊呢
有一個訓練 Actor-Critic 的小技巧啦
那你在作業裡面也不妨使用這個技巧
這個技巧是什麼呢
Actor 是一個 Network
Critic 也是一個 Network
Actor 這個 Network
是一個遊戲畫面當做輸入
它的輸出是每一個 Action 的分數
Critic 是一個遊戲畫面當做輸入
輸出是一個數值
代表接下來會得到的 Cumulative 的 Reward
這邊有兩個 Network
它們的輸入是一樣的東西
所以這兩個 Network
它們應該有部分的參數可以共用吧
尤其假設你的輸入又是一個非常複雜的東西
比如說遊戲畫面的時候
前面幾層應該都需要是 CNN 吧
要了解這個遊戲畫面需要用的 CNN
也許是差不多的吧
所以 Actor 跟 Critic
它們可以共用前面幾個 Layer
所以你今天在實作的時候往往呢
你會把你的 Actor-Critic 設計成這個樣子
Actor-Critic 呢
它們有共用大部分的 Network
然後只是最後呢
輸出不同的 Action
就是 Actor
輸出一個 Scalar
就是 Critic
好 那這是一個訓練 Actor-Critic 的小技巧
好 那其實今天講的啊
並不是 Reinforcement Learning 的全部
那其實在 Reinforcement Learning 裡面
還有一個犀利的做法
是直接採取 Critic
也就是直接用 Critic
就可以決定要用什麼樣的 Action
那其中最知名的就是
Deep Q Network (DQN)
那不過呢 這邊我們就不細講 DQN 了
如果你真的想知道 DQN 的話
可以參考過去上課的錄影
那 DQN 哇 有非常非常多的變形
這邊呢 就是找一個非常
有一篇非常知名的 Paper 叫做 Rainbow
裡面呢 就是試著去嘗試了各種 DQN 的變形
試了七種 然後再把這七種變形集合起來
因為有七種變形集合起來
所以他說它是一個彩虹
所以他把它的方法叫做 Rainbow
那我也把這個 Paper 留在這邊給你參考
那如果你想知道 Rainbow 裡面的
每一個小技巧是怎麼做的話
你就參見上課錄影
過去的課程
有把 Rainbow 裡面的每一個小技巧
都講過一遍
好 那講到這邊呢
正好告一個段落
我們先來看看有沒有同學有問題
好 那個 我來回答一下同學們的問題
好 有一個同學說
Sa 後面接的不一定是 Sb 吧
這樣怎麼辦
這是一個很好的問題
Sa 後面不一定接 Sb
那這個問題
在剛才我們看到的那個例子裡面
就沒有辦法處理
因為在剛才那個
我們看到那個只有 8 個 Episode 的例子裡面
Sa 後面就只會接 Sb
所以我們觀察 沒有觀察到其它的可能性
所以我們沒辦法處理這個問題
所以這就告訴我們說啊
在做 Reinforcement Learning 的時候
Sample 這件事情是非常重要的
你 Reinforcement Learning
最後 Learn 得好不好
跟你在 Sample 的時候 Sample 得好不好
關係非常大
喔 所以這個 Reinforcement Learning
是一個非常吃人品的方法啦
所以你在作業裡面你可以體驗一下
就你 Sample 到的結果
對你最後 Training 的結果
有非常大的影響啊
然後下一個問題
所以每一個 V
都需要對應到固定的環境發生順序嗎
欸 我先說一下
那個王大華同學其實是助教啦
那這不是他的本名啦
他是 但這個王大華同學是助教 他是助教
然後 欸 我看一下喔
所以 V 每一個 V
都需對應到固定的環境發生順序嗎
我沒有很確定你的問題
但是我試著回答一下
就是每一個 V 它不會固定
它不會對應到固定的環境發生順序
也就是（00：32：44）
如果你的遊戲有隨機性的話
那 V 其實是代表了一個期望值
它想要算的就是
給某一個 Observation
看到某一個遊戲畫面以後
接下來你會得到的 Cumulative Reward 的平均值
它的期望值
如果你的遊戲有隨機性的話
V 代表的是期望值
你看到某一個遊戲畫面以後
然後接下來會發生什麼事情
不見得是一樣的
但把所有的可能性都平均起來
取它的期望值
這個就是 V 所代表的意思
後面出現的 S 應該是不固定的
這樣怎麼代公式
好 那個我想我剛才應該算是有回答到了
後面出現的 看到某一個這個 Observation
後面出現的 Observation 確實是不固定的
那如果有些狀況
某些 Observation 你沒觀察到的話
哇 那你真的就沒辦法訓練
喔 V 當
然後有同學問說
就是拿 V 當一般人的實力
超過它就是猛
沒超過就是爛嗎
對 就是這樣
V 就是平均的實力
超過 V 就是好
想請問這個 Distribution 要從哪裡知道呢
我想你這個 Distribution 問的是那個
Actor 的 Distribution 啦 對不對
我們說
Distribution Action的 Distribution
Action 是從某一個 Distribution
Sample 出來的
那個 Distribution 是誰呢
那個 Distribution 是這樣
就是你的 Actor 不是像是一個 Classifier 嗎
你的 Actor 像是一個 Classifier
然後你把 S 丟進去
每一個 Step
喔 不是每一個 Step
每一個 Action 都會有一個分數
那你把這個分數呢
通過 Soft Mess
就做一個 Normalize
它就變得像機率一樣
然後按照那個機率去做 Sample
那這個就是 Actor
從一個 Distribution Sample 出來的
這句話的意思
