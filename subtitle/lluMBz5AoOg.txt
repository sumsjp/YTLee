臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw/
我們就開始吧
今天的主題叫做 New Architecture
我是今天的助教紀伯翰
New Architecture 意思上就是新架構
在講新架構之前先來複習一下你們作業裡面會用到的舊架構
Review 就分幾個超級超級簡單的模型
就是 Fully Connected Network、Concolutional Network 跟 Recurrent Neural Network
一開始就快速把這些投影片帶過
你可以發現在現在模型裡，隨處都可以發現 Fully connected network 的影子
這個 layer 的寬度都是自己可以調整，自己去 train 的
再來就是在 CV 領域裡面很常使用到的
Convolutional Neural Network
你可以知道說像這樣的架構裡
這個架構到後來都很受廣用
甚至發展出有很多很多很深很深的模型
像是 ResNet
甚至後來來有用 residual 發展出來的 DenseNet
還有 Inception Network
CNN 最好的性質其實是
他可以抓住一些 local 的資訊
這個圖就是想表示說
一個例子在 CNN 是怎麼 process 的
再來就是 Recurrent Neural Network
Recurrent Neural Network 就是你在做序列型 data 的時候會做的模型
簡單來說，舉個例子來說
你希望你的 embedding 可以模擬到句子的句意
你會用一個 LCM 或是一個
或是一個 GRU
你就會讓 GRU 從 What time is it 的 What
一路把 What time is it 這個字的 token 吃完
你希望最後這個問號得到的 embedding 是有含
有包含到 What time is it 的句意
從這個架構衍伸出來的就像是現在非常流行的 ******
或是在做 machine translation 的時候
你會做的 Seq2seq 的這種 method
以及像剛剛講 RNN，其實 RNN 有延伸出很多變形，像 LCM 跟 GRU
還有像 Pointer Network 等等
複習完這些很基本很基本的架構之後
你說新架構是什麼，是不是就是把這些架構全部把它疊起來
看自己的創意
我今天在夢中夢到這個參數，這個 hidden size
我是不是要先疊一個 CNN 再疊一個 RNN 再把 Fully connect 接上去
是不是我把所有的東西都疊起來
我就希望這個模型在我想要的 task 上面可以做得很好
我就把他們，隨我自己的意思把他們亂疊
疊好之後就開始 train
然後就開始 tune 你的參數
就像這個圖片一樣你開始 tune 你的參數，什麼 learning rate
L2 的 weighted decay
dropout 開始調
調完之後你發現說
我的模型怎麼 train 起來都 train 不太起來，怎麼這麼奇怪
然後你就會有一個想法
一個很智障的想法
你就會覺得說是不是我疊的不夠深
就再把他疊深一點
繼續把它疊深之後就開始 tune neural network
然後開始 tune 參數跟前面的 case 一樣
然後發現 tune 完之後你的模型還是一個垃圾
這就結束了
其實我們覺得在做一個新的 task 的時候
可以先去 follow 說這個 task 在過去的人
他們是用什麼樣的架構去做
做這件事的目的可以讓你比較好的
假如你想做 MT 你就會去看機器翻譯有關的 paper 他們的架構大概是什麼
為什麼這個架構在這個任務上會有用
你可能可以先去 follow 一些
SOTA 的模型
為什麼需要新的架構
第一個點是他可以在某個 task 的表現變得很好
第二個點是他可能，因為我們知道 ML 跟 DL 最大的差別就是
DL 還有一個方向是他需要把
data 的 feature 抽得很好
所以會不會說我們新的架構是可以把
data 一些好的性質抽出來
第三個點是 generalization，也就是說
今天我這個 model 抽出來的 representation 或是說 vector
它可以很好的被應用在某個領域的各種 task
代表說這樣的架構是非常 generalization 的
他非常泛用，他可以用在
很多很多相同領域不同的 task
最後一個點是你可以減少 parameter 的
你可以減少你的參數量
或是說現在有一些非常非常巨大的模型
你光存下來就要花很多空間
或是說你需要 32 G 的 GPU 才可以 train
我們要的新架構應該是一個
大家都可以 train 可以跑的模型
希望他可以減少參數量
講這麼多
今天會想要講的新的架構其實是
有一些變種的新的架構他的設計
然後他是從舊的模型得來的
2019 發展出來的
或是一些在應用上非常酷炫的架構
也是在 2019 跟 2020 最近才出來的 model 架構
或是在 2019 年有拿到 SOTA 的 model
今天會講的主題就是大概包含這三個特質
outline 一下今天會講的東西
今天會講的東西主要就是下面這個東西
先把他分成兩類
第一類就是一個 RNN less structure
現在雖然說 NLP 的 task 裡面
在以前 16 年 15 年大家還是會用 GRU 或 LTM 在
sequential 的去 train 我的模型
在近幾年來說已經
像 LTM GRU 這種架構已經不太用了
取而代之的比較像是 transformer 這種類型的 model 一直跑出來
所以今天會講的主題就是跟 transformer 有關的新架構
新架構裡面又有 transformer
因為都是從 transformer 延伸出來的新架構
所以就會在講下面這兩個主題
分別是 Sandwich transformer 跟 Universal transformer
再來就是
跟 Universal transformer 有關的
Residual Shuffle Exchange Network
再來就是講 BERT 跟 ALBERT 還有 Reformer
他的最基本的結構其實就是 transformer
等一下會講一下說 BERT 的底層是什麼
還有他延伸出來的兩個架構，分別是 ALBERT 跟 Reformer
一開始就先簡單的講一下 transformer
transformer 我相信老師應該都有提過了
我就再簡單的帶過一下
在一開始 transformer 提出來的時候是拿來做機器翻譯
你 input 是法文的 token
你希望模型在這個 output 端
去預測英文的字
首先 transformer 有兩個 submodule 第一個 module 是 encoder 第二個 module 是 decoder
一開始
假設我想要做的是法文翻英文
我會先把法文的 token
這其實我不太會念 ，不過我會把這三個字各自變成一個 embedding
把這個 embedding 餵進 transformer 的 encoder 裡面
像這樣一層一層的疊到最後
可以得到最後一層的 representation
我會再把這樣的 representation 在 decoder 的地方會用到
這個圖只是想要秀一下說 encoder 跟 decoder
encoder 的部分是怎麼樣
細看一下這個模型的架構就會是
transformer 裡面有所謂的 encoder 跟 decoder
encoder 裡面又有分兩個很小的模型
很小的 layer 一個是 Self-Attention layer
跟 fully connected layer
以及在 decoder 裡面
他也有一個 Self-Attention layer
跟一個 fully connected layer
但他跟 encoder 不同的地方是
encoder 的 Attention layer
這個 encoder 的 Attention layer 就是
剛剛在上一個動畫裡會看到說
encoder 得到的 embedding
其實還是會被送到 decoder 裡面
被送到 decoder 裡面其實就是把這些 token 變成
剛剛在這個位置裡面的 key 跟 value 的 matrix
做這件事情的目的是我希望我在
翻譯成英文的時候
每一個英文的字需要去根據法文的
這些字的意思去翻譯
所以才會有這邊講的 encoder Attention layer
中間這個迴圈就是 Residual block
這就是很簡單的 transformer 的結構
講完結構之後
來簡單的講一下說
他到底要怎麼 encode 跟 decode
剛剛提到說這裡 encoder
可以把法文的字吃進去
吃進去之後最後一層會得到對應的 representation
就是你用 transformer encoder 把這些 token 都餵進去之後你在最後一層得到 representation
你會把這個 representation 變成一個
變成三個疊在一起，可能就是一個 key 的 vector
跟 value 的 vector
其實是還要再過一個 linear transformer，不過就是
你會有一個
法文最後這個 embedding 的
key 跟 value 的 matrix
然後你在 decoding 的時候
你一開始餵進去的應該是一個 BOS
BOS 就是我要開始 decode 的這個字母
你把它餵進去之後
decoder 就會先過一個 transformer decoder 之後
他裡面有一個結構會去 attain encoder 的這些 representation
做 attention，做完 attention 再 decode 出這個 i
我們有了 i 之後就會把 i 這個字的 embedding 再餵進來 decoder
再讓 decoder 做剛剛的事情
這個 decoder 裡面就是像剛剛這個地方
其實是有很多層 decoder 的 layer
餵進去之後假設第一個階段
等這個動畫一下
第一個 i 餵進去之後
你預測出來是 m
我就會把 m 這個 token 再餵進來
餵進來之後 decoder 再 credit 出 a
再把 a 這個字餵進來變成 student
這個地方的 method 就很像是
在做 RNN 的 Seq2seq
在這裡所有的架構都是 fully connected layer
這邊就是很簡單地把 transformer 的結構還有他的 process 過程講完
不知道大家這邊有沒有什麼問題
講完這個部分就來進入今天要講的新的架構
新的架構就會是 Sandwich Transformers
這個 motivation 其實是他想要 design 一個更好的 transformer
他想要怎麼做
他想說可不可以把
sublayer 的 module
交換一下他的順序
也就是說原本 transformer 裡面的
encoder 跟 decoder 都會先過一個 attention
再過一個 fully connected network
那我是不是可以交換他們的順序
用這樣的順序可能一開始先經過 fully connected layer
最後再經過 Self-Attention layer
用這樣的結構我可以做出
去做一樣的 task 我的表現可以變得更好
他的想法是這樣
highlight 一下這個模型最後做出來的結論是什麼
他的結論其實是說
我們知道說 transformer 裡面
每一個 layer 的結構會先經過一個 Self-Attention
再經過一個 fully connected layer
他發現他最後做出來一個新的 structure 是
他發現說一個 model 有超多層 transformer
假如是 16 層
16 層裡面我都各有一個 Self-Attention 跟 fully connected layer
所以我就是有 16 個 Self-Attention 跟 16 個 fully connected layer
我其實把他們順序稍微改變一下變成說
把很多很多 Self-Attention 放在我整個 model 前面
但他可能是 Self-Attention 過完就直接再過一個 Self-Attention
這樣一直過過過過過，放在比較靠近 input 的那一層
以及我把很多 fully connected layer 接在
比較靠近 model output 那邊
這樣做出來的模型架構是
可以得到比較好的 performance 的
這還有一個 highlight 就是
我不需要有多餘的參數
我也不需要有多餘的 memory
因為模型的參數從頭到尾都是一模一樣的
只是我把順序交換了
這是我剛剛所說的，他所提出來的 Sandwich transformer
Sandwich transformer 你可以看到說原本 Self-Attention 要經過
Self-Attention 是個 S
f 就是 fully connected layer
原本你的 transformer 他的 default 其實是有 16 層的
所以你會經過 16 層的 Self-Attention 跟 fully connected 的
組合起來的一層 transformer layer
假設我今天把它變成下面這樣，就是說
我已經知道有 16 個 Self-Attention layer
我就把前面 6 個 sublayer 都改成 Self-Attention
中間一樣維持原本的 transformer 的架構
就是 Self-Attention 加一個 fully connected layer
但是我在模型最後面，就是 16 層最後面的 6 個 sublayer 把它改成
fully connected layer 一直接，接到底
他發現說他用
他搜尋下模型的那個架構
他搜尋完之後發現這樣的架構是
比原本的架構還可以再得到更好的表現的
這就是所謂的 Sandwich transformer
他其實在這個 paper 裡面有試很多很多不一樣的架構搜尋
舉例來說
他想要知道說我用同樣的參數量，但我的順序不一樣，以及我有的 layer 數量也都不一樣
這裡有一個前提其實是這樣的
Self-Attention layer 有的參數量其實只有 fully connected layer 的 1/2
其實今天我只要把模型所有參數量算出來之後
我就讓 sf 就是 Self-Attention
跟 fully connected 去隨意組合
組出同樣的參數量
他想要試說假如參數量在一樣的狀況下
我的 order 怎麼排會不會有什麼影響
這樣做出來後他發現說做在 language model 的 task 裡面
會發現說他其實
非常地不穩定
你會看到他這個分布非常分散
跟 baseline sf
正常 transformer 結構比起來他表現其實沒有很好
但是也有一些比 baseline 還要好
他還有試一件事情是說
這也是他為什麼會決定出比較好的結構的原因
他試了一下發現說
在 model 比較靠近 input 的那一層
如果我疊很多 Self-Attention layer
他的表現其實是會變得比較好的
所以這是他所統計出來的
假設我在前面幾層 layer 裡面疊比較多 Self-Attention
但比較少的 Feedforward 的話
他其實會比 baseline 還要來的好
還有另外一個結論也是他 search 來的就是說
在比較靠近 output 層的 layer
如果比較多的 feedforward layer 他其實是
表現是也會比較好的
所以他才得出一個結論就是說
是不是前面疊多一點的 Self Attention layer
後面疊多一點的 fully connected layer
中間一樣維持這個架構
就可以得到比較好的 performance
Self-Attention 到底要疊幾層他也是
把組合試完才知道
跟三明治的關係是什麼
這個厚厚的地方就是
三明治的土司
就是這個 s
後面這個厚厚的 f 也是吐司
中間就只是他的夾層
所以說他其實試了超多超多種模型架構的 search
簡單來說就是他先試最原本的
他先假設 Sandwich Coefficient 的定義就是
在正常的 transformer layer 的左邊到底疊了幾個 Self-Attention layer
假設我疊了六個
最後的 fully connected 在尾端我也會疊六個
所以他就把所有的 Sandwich Coefficient 都試過一次
試到最後發現把所有的 Self-Attention 都疊在最前面
然後 fully connected 都疊在最後面會怎麼樣
他試出來的結果其實在
在 language model 的實驗裡他發現
當 Sandwich 的 coefficient 是六的時候，會有一個最低的 perplexity
其他的地方你會發現如果全部前面都是 Self-Attention
後面全部都是 fully connected layer 的話，表現其實非常爛
甚至還會比，這個深色的是指
你用正常的 transformer train 出來最好的 performance
比較粗的虛線
比較淺的虛線是
你一樣用正常的 transformer，但你 seed 一換
然後做出來最爛表現的分布
你可以看到說今天 Sandwich Coefficient 我取六
是可以比更好的 baseline 還要來的強的
他就還有進一步做實驗說
我把 baseline
正常的 transformer 試了五個不同的 seed
做出來的 perplexity 的分布
是在這個位置
以及我用 Sandwich Coefficient
我前面疊六個 Self-Attention
最後面疊六個 fully connected layer 的這個架構
試了五個不同的 seed
但他的 perplexity 都可以壓在 18.0 的這個地方
所以才會說他最後實做出來是
他做出來的 Sandwich transformer 是
假設我 transformer sublayer 數一樣的狀況下
我把他的順序改成前面多一點 Self-Attention
中間跟 transformer 正常的架構維持不變
後面再疊很多 fully connected layer，這樣的結構
做出來的表現會是最好的
講到這裡我們第一個新架構就講完了
第一個新架構就是三明治的 transformer
第二個要講的就是 Universal Transformer
什麼是 Universal Transformer
他想提出這個 paper 的原因其實是說
原本你在做 transformer 的時候
他其實在做翻譯可以做得很好，但他如果拿來做一些演算法的 task
他其實會做得很爛
什麼是演算法的 task
就好比說你希望 model 的 input 是 a b c
你希望 model 的 output 是他複製三次的結果
就是會變成 a b c a b c a b c
像這樣的 task 就可以算一種演算法的 task
當然演算法的 task 有很多很多種像是
你希望你 input 一個 sequence
他的字的順序可能是 a b c d e
你希望他 reverse 這個順序變成
e d c b a
這樣也算一個 algorithm task
他發現說在 transformer 裡面雖然
他的翻譯的 task 做得很好
但是像這種演算法的 task 都做得不太好
像之前的有兩篇 work
一個是 neural GPU 跟 neural turing machine 這兩個東西
他在翻譯上都做得不好
可是他在這種演算法的 task 都可以做得很好
他想要做一個 transformer 是可以同時
包含這兩個任務的
他想要他翻譯可以做得很好，演算法任務也可以做得很好
其實他主要 Universal Transformer 提出來的結構是
一般 RNN 都會是從左到右
左到右連續的吃過去
sequential 的吃過去
他開發一種 Universal Transformer 是說
我每個位置的 transformer layer 都是 share 參數的
這跟原本的 transformer 是一樣的
但是有個不一樣的點是
他的每一層參數都是一樣的
不會像原本的 transformer 假如疊六層
雖然六層架構都一樣，但是參數其實是 random in need
所以六個 layer 參數都不一樣
再來就是它會讓這個
share 參數的 layer co t 次在深度的方向
也就是說今天它的深度
假如是六層，我就是用
同樣一組 weight
同樣一組 layer 的 parameter
讓他重複 co 六次
去做出我想要的 embedding
再來還有一個它有趣的結構就是說
叫做 Dynamic Halting
Dynamic Halting 是什麼
就是說今天 input 雖然是四個字
在原本 transformer 的結構裡他可能是疊十六層
所以十六層他就是深度走完十六次就 ok
在這裡他還要做這個機制叫做 Dynamic Halting
就是說不同的字要的深度其實不一樣
這個深度要怎麼決定是由 model 自己用機率預測決定他要不要停下來
預測深度這件事情就好比你在做 RNN 的時候你會有一個
你會有一個 N(sentence)
就是會讓模型自己決定要不要停下來這件事情
舉個例子來說今天
我們看這個動圖來說，你看到 position 2
他所需要的層數是最多的
其他層數該怎麼辦
其他層數在超過他的層數之後
就是 copy 同一個 embedding 讓
這個 position 2 可以繼續做 Self-Attention 到最底
我可以再提醒一下就是
其實每一層的 layer 要的參數都是同一個
這就是所謂的 Dynamic Halting
他們提出這篇 paper 的時候
他們也有說明說
做這件事情可以很好的 improve 模型的表現
只是這件事情還有一個有趣的點是
他取名叫 Universal Transformer 的前提其實是
如果你有足夠的 GPU memory 我就可以做出
我就可以做很多很多 task 相當 universal
其實因為你看像這樣的結構，假設我有一個 token 要走 130 層
其實這個 memory 是非常非常多的
講到這裡其實我就把 Universal 最重要的兩個變形講完了
第一個是他的每一個 encoder 都是同一個參數
第二個是他在深度的方向是自己決定的
而且是每個位置都有不同的深度
來看一下他的實驗
在他的實驗裡他試了兩個 dataset
其實他有試一些演算法的 task
不過我在這裡就沒有多講
所以第一個我會提到就是一個 Lambda Question Answering Dataset
第二個最簡單的就是英翻德
英翻德的機器翻譯任務
我先簡單講一下什麼是 Lambda Question Answering Dataset
舉例來說他在 input 裡你會得到一個 context
跟一個 topic sentence
topic sentence 會有一個地方挖空
這個挖空的地方的答案會出現在 context 的某一個地方
也就是說你的 model 需要做出什麼事情
他需要吃 context 跟 topic sentence
吃完之後他要在 topic sentence 最後這個地方
他要去預測說最後這個位置到底是 context 的哪一個字
所以這個模型需要具備的能力其實是
他在吃完 context 這麼長的 sequence 資訊後
他要回答問題
他要回答問題的話，他需要把 context 這麼長的 sequence 吃完
知道說答案在這麼長的 context 的哪裡
你可以發現我這裡舉的例子都是
答案都會出現在 context 之中
這就是所謂 Lambda Question Answering Dataset
也就是說你在這個位置需要做
token 的 prediction
去 predict 答案到底是哪一個
哪一個字是答案
那個字通常都會在 context 裡面
大概有我記得是 81 %
有 19 % 的答案是沒有在 context 裡的
好像所有的 performance 都做不到 81 % 以上
所以就暫且不論這件事情
大家在做的 module 都還是
從 context 裡面去抓出答案在哪
這裡秀一下 result 就是
我覺得左邊這裡就先不用看
先看右邊這個 accuracy
你可以看到說
transformer 在做這樣的任務都只能做到 40 上下
看這個 task 就好了
Universal Transformer base 6 steps 就是說
6 steps 就是指深度是六層，他是 fixed
假設我是 fixed path
我其實可以做到 48 跟 54
可能是因為他的每一個 layer 都是 share 同一個參數
所以他有可能做得會比較好，但是這個沒有一定
但是有一個結論可以下的是
今天我的 Universal Transformer 如果有這個 Dynamic Halting
每一個位置我在 encode 的時候
我所要的層數是不一樣的
這點做下去他是可以 improve QA 的 accuracy
再來是看他的 perplexity 的部分
perplexity 的話
你也可以看到說在有這個 Dynamic Halting 的 path 底下
他在 task 的表現上是
perplexity 是最低的
所以他想要下一個 claim Dynamic Haliting 這件事情是
可以很好的幫助 model 做 generalization 這個模型的
結構是會跟著 input 端不太一樣
他還有做完第二個 task 是機器翻譯
機器翻譯裡面
他如果用比較少層的 Universal Transformer small
他的 BLEU scope 只能夠到達 26
small 跟 base 只是差在 layer 數目不同
這個 layer 都是 fix
他想要表示的就是今天這個 Universal Transformer
不管是做在機器翻譯也好
不管是做在 QA data 也好
還是做在演算法的 task 也好
他都可以表現得很好
前提是他需要在演算法那樣的 task 以及
QA task 裡面他要用 Dynamic Halting
每一個位置要決定的 layer 數目是不一樣的
有這件事情可以讓他的表現變得很好
到這裡就把 Universal Transformer 講完了
你可以知道 Universal Transformer 有一個很強很強的前提其實是
在足夠 GPU 的 case 下
他可以做到很好的表現
Dynamic Halting 假設你的 input
有一個 position 他所需要的 layer 層數非常深
相當於是在疊一個非常非常深的 model
你需要的 memory 就會很大
可是有這個 Dynamic Halting 的機制是可以幫助你在做不同的 task improve 表現的
也可以造就他在不同的 task 底下他都可以是一種 new architecture
因為它的深度都不太一樣
他所做出來的 Self-Attention 在 encoder 的變化也是不太一樣
到這裡我們就把 Universal Transformer 跟 Sandwich Transformer 這兩種 transformer 的變形講完了
不知道大家有沒有什麼問題
沒有的話就先往下
第三個要講的架構比較有趣一點，他叫做 Residual Shuffle Exchange Network
這個模型是什麼
這個模型最大的重點其實是
他用了非常非常少的參數
跟其他 model 比起來
他做同樣的 task 他需要的參數非常非常少
還有一個有趣的點是
他在做這種序列的任務
你可能 input 是 100 個 sequences
你只需要用 n log n 的複雜度就可以做完所有 task
做完這種序列的 task
所以假設你今天 input sequence 非常非常長
假如是 3000 這麼長好了
你用我的這個模型可以 process
會比你用 Attention module 做出來的還要來的快
在這裡還有一個有趣的東西就是
他用了兩個 operator
去把遠程的資訊抓到
去取代 attention 這件事情
這兩個 operator 分別就是他名字的這兩個字
就是 Shuffle 跟 Exchange
這是他的模型的類比圖
簡單來說這個模型裡就是會有一個
switch 的 unit
每一個 Switch unit 都會吃兩個 input
這裡是 Switch unit
這邊其實就是你的 Shuffle 的 layer
這裡不小心破梗了，反正就是
你看到這樣的模型你會問我說到底哪裡有 Shuffle 哪裡有 Exchange
他在這裡提到說他的 Shuffle 是使用 perfect shuffle
他還用簡單的文字敘述說什麼叫做 perfect shuffle
perfect shuffle 就是你在洗牌的時候你會把牌的上下分成兩疊
然後做 gif 會做的事情，就是把他
交叉疊起來
這就叫做 perfect shuffle
到底 perfect shuffle 要怎麼表示
簡單來說就是你有一個 sequence 是
1 2 3 4 5 6 7 8
你會像牌組一樣，先把它拆成兩疊
上面是 1 2 3 4，下面是 5 6 7 8
然後你再做交叉重疊的事情就會變成
上面第一個 element 在第一個位置
下面這一堆的第一個 element 在之後洗牌完的序列的第二個位置
交叉做出 perfect shuffle 的 operation
我們演示一下就是
你的 1 會變成新的 sequence 的第一個 element
5 會變成第二個
2 會變成第三個
新的 sequence 的第三個 element
6 會變成新 sequence 的第四個 element
他這裡提到的就是 Shuffle Exchange 的 shuffle 就是用 perfect shuffle 做的
那你會問我說這種 Shuffle Exchange network 跟數字的關聯到底是什麼
你會非常困惑說這數字
他是 perfect shuffle 出來的東西，跟結構到底哪裡有關係
我們如果把這個結構看進去的話
這個順序我有按照結構畫
簡單來說就是 input 1 2 3 4 5 6 7 8
最後出來的順序是 1 5 2 6 3 7 4 8
就是你用這個順序，這個順序是怎樣
你的第一個 elememt，你要進入第二個 unit 的時候
第一個 element 是通過第一個 unit
第二個 element 就要進入第二個 unit
第三個 element 就會進入第三個 unit
在第二層的 case
第四個 element 就會變成要進入第四個 unit
其實講了這麼多
它其實就是在做 perfect shuffle
你會發現你做完的這個順序就是剛剛看到的這個
1 5 2 6 3 7 4 8 這個順序，這是他的 shuffle layer
所以說 shuffle exchange network 的 shuffle 就是來自於這
用這件事情的好處是
等一下我們會講 switch 裡面的 operator
這件事情加上 switch 的 operator 就可以取代 attention 去取到遠層的資訊
先來看一下這個黃色框框裡線會是什麼東西
今天要報的這篇 work 其實是 neural shuffle exchange network 的續作
簡單講一下 neural shuffle exchange network 就是
這是他前作提出來的 switch
你會有兩個 path
第一個 path 是正常做一些 non-activation 的轉換
input 會是兩個 element
兩個 element 有兩個 path
第一個 path 是做 nonlinear 的轉換
下面這條 path 會做一個比較不一樣的操作叫做 swapHalf
來細看一下這個架構是什麼，這個 operation 其實也很簡單
今天 input 有兩個 element
分別是 s1 跟 s2
假設你的 dimension 是八維
前面四維就是在這個式子裡面的 a
後面四維就是 b
s2 前面四維就是 c 後面四維就是 d
做 swapHalf 就是我把
下面一半的 element 進行交換
做交換完就是 exchange 的 operator
所以整體架構是什麼
整體架構其實是說，今天有兩個 input element
一開始第一個 path 是你會做一些 nonlinear 的操作
下面這個 layer 你會進行 exchange 交換部分資訊給另外一個 element
做完這件事情之後
做完 swapHalf 之後你會到 point wise artification 的地方
你會做到這個加法的地方，但這個加法其實是說
還會有一個像是 GRU 的 reset gate
這個 reset gate 要做的事情是什麼
他需要去決定說到底多少成分要進行 element 下半部的交換
到底有多少機率你要做原本的 nonlinear 的交換
做完之後就是一整個 switch unit 的操作
簡單來說 switch unit 在做的事情就是
input 兩個 vector
會有一定的機率交換彼此的資訊
還有進行 nonlinear 的 operation
這是前作的 switch unit
到了後作之後，你看到前作這裡其實有非常非常多的 path 跟非常非常多的操作
到了後作，這篇 work 其實只有做
其實只有做一個 residual 的 exchange network
這邊也是有一個交換的機制在裡面
但這個圖並沒有秀出來這樣，反正意思就是這一條 path
他是跟前作一樣做
nonlinear 的 operation
只是他用了 layer none 跟 galue 來做
下面這條 path 其實是有做 swapping 的
也是有做交換的動作但這個圖沒有秀出來
最後再把它分成兩個 element 出去
簡單來說這個 work 跟前一個 work 共通點就是
他們在經過這個 unit 的時候兩個 element 會有部分的資訊被交換
簡單來說，每一個 unit 都會交換兩個 element 的資訊
兩個 element 在不同資訊之間還會
因為 shuffle 的 operation 還會走不同的 unit
有了這兩個 operator 你就可以像是在做 attention 的功能
其實在這兩篇 work 裡面
現在講的這個 work 裡面都是使用 butterfly 的 network
不是用 shuffle exchange
只是我覺得用 shuffle exchange network 講起來比較好講
這兩個差別是什麼
這兩個差別其實你可以發現就是
butter network 的前面三層
會跟 shuffle network 操作是一樣的
只是在後面兩層的時候操作有把它做 reverse
怎麼 reverse，假設你的這個
第一個 unit 是由第三個 unit 連結過來
他做一個鏡像操作就是
他下一個時間的
要連到的地方也是第三個 unit
去做一個鏡射，所以你每一個
你從第三個 layer 切一半可以發現他們是
有鏡射的操作的
在這兩篇 work 裡面都是使用 butterfly network
這我不知道怎麼唸
不過他的另外一個名字就叫做 butterfly network
這其實才是他的主要架構
主要架構就是你會有一個正常的 shuffle 順序
butterfly 前面的可能是順時針的 shuffle 順序
後面兩層是用反向的 shuffle 順序做的
這一整個就是一個 butterfly block
還有一個有趣的點就是
在前面順時針 shuffle 的兩個 layer 裡面的參數是彼此 share 的
後面兩個反向操作的 shuffle 他其實是
也是 share weight
在這個 case 裡面其實就只有
他只需要兩個 butterfly block 就可以做完所有 task
你會問我說，他說他在做 nlogn
到底 nlogn 在哪裡
nlogn 就是指 input 有 n 個 element
n 是這樣來的
logn 其實是說
假設 input 是八個
八個 element
你需要 shuffle 幾次
才可以交換完這八個 element 的所有資訊
就會是 2 的 k 次方，三次
三次減一次就會是兩次
所以你可以知道說這個
這個深度的決定是由 2 的指數次方來決定的
所以他還有一個 constraint 其實是說他的 input 都會是 2 的 k 次方
所以說你今天的這個深度是
log2 的 n
所以說他的整個 sequence process 就會是一個
nlogn 的 case
講完這裡就把他的 shuffle 跟 exchange 的部分講完了
來看一下實驗
他做了五個實驗，這五個實驗分別是
剛剛跟 universal transformer 一樣有一個 Lambda QA dataset
以及他有拿去做 transgression，就是你去預測聲音的 transgression 的 dataset
還有三個，就是剛剛在 universal 裡面提到的演算法的 task
這邊只有 report Lambda QA dataset 裡面的結果
在這個結果裡面，他想要強調的重點其實是
neural shuffle exchange network
是這篇 work 的前作
他所需要的參數量其實是只有 33 M
他的 task accuracy 是 52.28
可是今天我用了這個 residual shuffle exchange network
他其實只要 11 million 的參數量就足夠了
我們知道說像 BERT 這種模型，後面會提到
BERT 模型至少也都是 100 多 million 的 parameter
你可以知道說今天我用了 residual shuffle exchange network
只要 11 million 的 parameter 就夠
task accuracy 是 54.34
還有剛剛上一篇所提到的 universal transformer
他其實所需要的參數量是
這篇 work 的大概 12 倍 13 倍左右
但是 accuracy 卻是只有進步 2%
所以可以知道說今天這個模型的架構是
非常 efficiency 而且可以在比較小的 GPU memory 上面 train
再來還有一個 model 是一個非常大的怪獸，是 open AI
做一個非常大的 language model 是 GPT-2
他用 GPT-2 去做這樣的 QA task
他所需要的參數量是更大
1542 million 的參數
但他做好的 performance 也只好了 9 %
所以他想要講的事情其實是
這個 model 架構非常小
memory 不需要很多的狀況下就可以使用
表現也不會太差
跟他的 parameter 數量比起來
他做了一個實測就是
到底一個 11 G 的 memory
用這個 model 到底可以 evaluate 多長的 sequence
以及他們所需要的時間
所以就跟剛剛所講的他用了一片 11 g 的 GPU memory
他可以做到多長的 sequence 呢
像他這樣的架構他是可以
input 512k 這麼長的 sequence
他 infrance 的 time 只要八秒就可以做完
相較之下，他有跟他的前作
參數量多他三倍的 model 做
還有 Universal transformer 相比
在一個同樣是 11 GPU memory 的狀況下
Universal transformer 在 evaluate 的時候
他只能容納 4k 長度的 sequence 做處理而已
他沒有辦法負荷到 512k 這麼長
他可以用 128 倍更長的 sequence 在同樣的 GPU memory 下
他可以做處理的 sequence 就越長
以及在同樣的 sequence length 底下
你可以看到說同樣都是 4k 的 sequence length 底下
他所需要處理的時間也是非常不一樣的
在 Universal transformer 裡面他需要 0.25 秒
但是在他們提到的這個 work 裡面
只需要 0.031 秒
所以說今天
他所提到的這個結構參數量是非常非常少的
以及他用 shuffle 跟 exchange 這兩個 operator 去取代傳統的 attention 的機制
去 attain 遠程的 information
講到這邊中堂休息一下好了
現在 11:14
休息 10 分鐘後再回來
下面章節就是在講 BERT
11:24 再回來
我們回來繼續講
我先回答剛剛同學問的問題
同學問說 shuffle 的順序可以變好
他不知道為什麼 shuffle 的順序可以讓模型變好
想要在這裡講的點其實是說
今天你在做 shuffle network 的時候
你其實是在 network 的內層做 shuffle
做 shuffle 這件事情其實是在交換句子裡面不同意思的資訊
他其實不是說我是男生妳是女生，直接對 input 層做 shuffle
而是邊做 nonlinear 的 transformation
以及邊做交換資訊的 shuffle 這件事情
讓每個 vector 可以去學到說這個句子的意思是什麼
還有一個點是在於
他每一個位置 shuffle 的順序是固定的
也就是說他每次都是第一個 element 他是直線的過去
所以他的這個 shuffle 並不是一個 random 的 shuffle
而是叫做 perfect shuffle
所以他交換的順序是一模一樣的
不曉得這樣子有沒有回答到你的第一個問題
第二個問題是在 predict 的時候會不會用 shuffle
其實也是會
因為你可以想像成是說
shuffle 在做的事情其實是在把每一個 element 的位置的資訊 encode 進去
也就是說今天句子的第二個 element 他會走的 path 其實是用紅線標的這一條
意旨就是說
透過 shuffle 這個 operation
你再把這是第幾個字的資訊告訴模型
模型在執行 shuffle 的時候只是把
只是在做一個固定的次序的動作
所以他在 infrance 的時候他也是做一樣的 shuffle
第二個問題是他 predict 的時候也會使用到 shuffle 這件事情
這樣的回答不知道有沒有回答道你的意思
我們就回來講 BERT
BERT 其實就是 18 年 10 月 Google 提出來的模型
這個模型提出來以後就猶如進擊的巨人裡面的高牆一樣
在 NLP 界築起了一個非常非常大的高牆
大家做任何 task 都會用一下這個模型
把這個模型做出來的 representation
拿出來做各種不同的任務
做出來之後發現效果可以 improve 超級多
所以在 NLP 界現在的模型架構會跟
BERT 的模型息息相關
或是用 BERT 的 vector 去做任何的任務
什麼是 BERT
BERT 是一個芝麻街的人物
只是自從之前有一個模型取名叫芝麻街的人物名字之後
大家都會用芝麻街的人物名字去命名自己的模型
舉例來說在他的前作有一個東西叫 Elmo
Elmo 也是芝麻街的一個紅色怪物
BERT 也是一個芝麻街的人的名字
這個老師上課投影片應該也是有的
簡單地 remind 一下他在幹嘛
他的架構長怎樣
跟剛剛的 transformer 差不多
差不多架構就是
他有 12 層
BERT 有 12 層
每一層都是一個 transformer 的 encoder
在 encoder 裡面都會有一個 self-attention layer
跟一個 fully connected layer
舉例來說你有一個 input 是
今天天氣很好
你會把今天天氣很好先換成
對應的 token id
可能是 1 5 6 7 8
1 5 6 7 8 要先去 embedding 的 metrix
找出第一個 row 第五個 row
第六個 row 第七個 row 第八個 row 的 vector
然後把他湊出來就是各是一個 7 6 8 的 vector
所以這個矩陣其實是一個 30000 x 768 的矩陣
你的 input 就會是一堆 token id
id 要去這個矩陣裡面把對應的 row 取出來
取出來之後再讓這些 embedding 去過 transformer 的 encoder layer
過完 12 層之後拿到最後一層的 embedding 去做不同的事情
這就是 BERT
其實在 train BERT 的時候有兩個階段
第一個階段叫做 pretraining
第二個階段叫做 fine-tuning
pretraining 都是大公司在做
他灌了超多超多 data
讓 BERT 最後一層 embedding 有蘊含許多 contectualized 的資訊
就是說他看過超多超多 data 他知道說
今天 input 一個句子，句子的意思要怎麼在 representation 裡面表示出來
講有點抽象，就是說他看過超多超多 data 他知道說
不同字的組合
要產生什麼樣的 embedding
雖然說產生出來的 embedding 都是一樣的
但是他們可以互相學到彼此的資訊
互相學到彼此資訊的意思就是這裡
Self-Attention layer
透過 12 層的 transformer encoder 他可以學到
蘊含這個句子意思的 embedding 在每一個 token 上
BERT 的 training 剛剛提到就是他有兩個 stage
第一個 stage 叫做 pretraining
pretraining 的時候他的 input 可能是這樣
BERT input 都會有一個 CLS token
起手的 token
這裡的例句是 CLS token 劉碩他要簽，再一個 MASK
你希望 BERT 的 embedding 在
最後一層的 embedding
他的 MASK 你希望他預測出，這是正確答案，所以你希望他預測說簽博
博班的博
所以你希望 BERT input 的時候看到 CLS token 劉碩他要簽 MASK
你希望 MASK 的地方他要 predict 博
這是 Pre-training 的其中一個 task
Bert 的 Pre-training 的其中一個 task
Mask Language Model
簡單來說就是你有一個句子
你把句子的隨意的一些 token 把它用
Mask 的字取代掉
你希望說 Bert 在通過最後一層
過了 12 層之後
讓他過一個簡單的 linear classifier
就讓他預測說今天這個 mask 的位置應該要是哪一個字
這是他的其中一個 Pre-training 的 task
第二個 task 是 Next Sentence Prediction 就是說
Bert 的 input 是
你希望他學到有上下文的關係
假設 input 是 CLS 是 這隻手是人民的意志
SEP 就是你可以把兩個句子隔開，用 SEP
隔開就是這是一個句子，這是一個句子
所以這整句話就是
[CLS] 這隻手是人民的意志
[SEP] 人民的法槌
你希望模型在看到這個 input 句子的時候
他要在 [CLS] 的這個地方
這個 embedding 過一個 classifier 要預測這是有上下文關係的句子
所以要預測它是 positive
但是假如是上面這個句子
你看一下就知道他們並沒有上下文的意思
今天他的 input 是 [CLS] 我要當老師
[SEP] 人民的法槌
這樣的句子在 CLS 的 linear classifier 要預測他是 false
做這個任務是希望 Bert
去學到句子跟句子之間的 relation
BERT 的 Pre-training task 就是這兩個
第一個是要學會預測字跟字之間的關係
透過沒有被 mask 的資訊預測出 mask 的字是什麼
以及，input 兩個句子
這兩個句子的關係是什麼
我希望 BERT 在 Pre-training task 的時候可以學到一點
在他的 representation 裡面
簡單來講我已經把 BERT 的 Pre-training task，Pre-training task 跟 BERT 的模型架構講完了
底下我要講的就是，因為這是 18 年的 work
底下我要講的會是 19 年 BERT 的變形
這邊再簡單的 remind 一下就是
雖然說 BERT 的 12 層都是
一個 transformer 的 encoder
但是每個 encoder 的 weight 都是不一樣的
只是它的結構是一樣的
所以你會有 12 層 transformer layer 的參數
參數的值都是不一樣的
ALBERT 就是
有 BERT 這個架構之後在去年的
iclear conference
投上的 paper
他叫做 ALBERT
ALBERT 簡單來說就是 alike BERT
就是一個小版的 BERT
A L 是 like BERT
他是怎麼個小法
簡單來說這是剛剛看到的 BERT
BERT 有 12 層
12 層的 transformer layer encoder 他的參數都是不一樣的
在 ALBERT 裡面
像剛剛的 Universal transformer 一樣
每一個 layer 參數都是一樣的
我用一樣的 layer 過 12 次讓他去
讓他達到跟 BERT 一樣的層數
只用同一個 weight
去做剛剛 BERT 的 task
做這件事情的好處其實是說
他的每一層都是 share 同一個參數
所以就是 ALBERT 裡面只有一個 layer
你會用這個 layer 重複過 12 次得到最後一層的 embedding
再用這個 embedding 去做其他的事情
我待會會講這個地方是在幹嘛
首先，這個地方是他的一個很大的 improvement
即使我每一層都 share 參數我的表現也不會掉太多
這就是他所做出來的圖表
就是說 BERT 有兩個 model，一個是 base ，一個是 large
base 需要的參數是 108M
large 是 334M
BERT 的 base 是 12 層不同的
transformer encoder 的 layer
他都沒有 share 參數所以這裡 parameter-sharing 是 false
你可以看到 ALBERT base 在這裡的 setting 是一樣的
只是這裡 parameter-sharing 是 true
也就是說他 share 了 12 層的參數變成 1 層
用這層做 12 次就可以做到跟 BERT 一樣的事情
所以他的參數就會變少非常非常多
還有其他像是 ALBERT large 就是
我用跟 BERT large 一樣的 hidden size
layer 數也跟 BERT large 一樣
embedding 都是 128，等一下會講這個地方在幹什麼
簡單來說你可以看到 ALBERT 的所有參數都是非常非常少的
每一層都有 share
不過有個有趣的點就是說你看這個 235M
就是指說 ALBERT 的一層參數就可以達到 235M 這麼多
還要疊 12 層
他怎麼減少參數量，第一點就是
第一點是我剛剛沒有提到，就是
剛剛說要細講的部分就是這個地方
原本在 BERT 的 input 裡他是會有一個 30000 乘以 768 的 matrix
這個 matrix 是幹嘛的就是
假設 input token
會有對應的 id
我就會用這個 id 去對應的 row
從這個 30000 x 768 的矩陣裡取出
對應的 id 的 row 變成某一個字的 embedding
所以在 BERT 的 model 會有一個超級大的矩陣
是一個 30000 x 768 的矩陣
你在 input 的時候假如 input 剛剛說的，今天天氣真好
對應到的 id 是 156278
你就會去拿 30000 x 768 的第一個 row 第五個 row 第六個 row 第七個 row 第八個 row 的 embedding 抽出來
就各是一個 768 尾的 vector
讓他變成 model 的 input 再過 BERT 的 12 層
在 ALBERT 這邊他發現說
今天我想要減少參數量我也可以在這個地方做
我們怎麼做
就把 30000 x 768 的矩陣
因為這個 30000 的數字非常非常的大
一開始這個矩陣 size 就把他降小 6 倍
變成 128
你可以看到說光是這樣的參數量就非常非常多
因為左邊要乘的東西是 30000
我先把 128 尾拿出來之後
再讓每個 128 尾過一個 linear transform
把它變成 768 尾
即使是這樣這個參數量也非常非常少
所以這樣加總起來就可以讓參數量變得非常非常少
跟原本 30000 x 768 的矩陣比起來
參數量至少少了 10 倍
7 倍左右
這是他第一個 factorize embedding matrix
就是說讓 embedding size 變少
讓 embedding matrix 先變小
先抽出對應 128 尾的 vector
再把它投射成 768
這件事情比起你直接 30000 x 768 matrix 還要來的少參數
這是他第一個減少參數量的地方
第二個減少參數量就是這個地方
他的每一個 layer 都是同一個
同一個 encoder layer 的參數
這裡是需要 12 倍的 transformer layer encoder 的參數
這裡只要 1 個，但這 1 個會重複 co 12 次
其實這個第二點也是一個影響他 parameter 會這麼少的主因
他是怎麼發現說 share 參數是不會影響的
假設今天我要 share 參數到底要 share 哪些地方
他就去做了以下的實驗，就是說
我們知道 encoder layer 有兩個 layer
一個 layer 是 attention layer 一個 layer 是 fully connected layer
其實這個表要秀的結果比較像是說
即使今天我 share 所有的參數
就是這個 all-shared，他的表現也不會掉太多
你看到這個 average 也不會差太遠
所以後來他所用的模型架構都是
不管是 self-attention layer 的 matrix 跟 fully connected layer 的 matrix
都是 share 12 層之間
再來就是 ALBERT 的 pre-training 跟 BERT 不太一樣
他覺得 mask 這裡是一模一樣的
他把 next sentence prediction 拿掉變成 sentence order prediction
他覺得說原本的任務比較簡單，他就把他改了一個比較難的
希望模型可以知道句子的上下順序是對的還是錯的
所以跟剛剛例子不太一樣就是
這隻手是人民的意志這件事情要在前面
倒過來的 case 就是
人民的法槌，這隻手是人民的意志
我希望模型經過，就是 ALBERT input 到最後一層
[CLS] 的 linear classifier 我希望他預測是 false
但是在這個 case 底下他要預測它是 true
希望他去預測句子的順序
因為他覺得上下文可能太簡單了
到這裡 ALBERT 跟 BERT 差異我就講完了
接下來要講的是 reformer
reformer 也是今年 iclear 提出來的新架構
我們知道這是 BERT layer，剛剛提到的
我們簡單把這個 Attention layer 拿出來看
他在做的事情是什麼
他在做的事情其實是說 input 假如有 4 個 token
每個 token 長度都是 64
我先舉個例子，假設是 64 好了
因為 self-attention 裡面會有多個 head
簡單來說就是一個 head 會負責的 dimension 非常少
所以四個 head 就是把 dimension
分成四份分開來
每一個 attention head 只負責一個部分的 dimension
這裡就是舉例說假設我有四個 head 的 attention 會是怎麼樣
會有一個 input 是 4 x 64 的 matrix
你會讓 4 x 64 的 matrix 先乘以一個 Q
變成 Query 的 vector
再乘以一個 K 的 matrix 變成一個 Key 的 vector
還有再乘以一個 V 的 matrix 變成 Value 的 vector
再去做 attention
要怎麼做 attention
我們用一個 head 的 process 來解釋一下
等一下的投影片就只專注在這個 head
因為我們知道每一個 head 做的事情其實是一樣的，所以就只講一個 head
他會做什麼事
這是剛剛的 query key 跟 value
假設今天是第一個 query
就是第一個 element 他的 query vector 會個別跟
key 的每一個 element 做內積
內積以後得到一個數值，如果這兩個 vector 很像的話數值就會越高
越不像就會很低
你會得到四個分數，所以 query 的第一個位置會跟這四個分別做內積
做完之後得到分數再做 softmax 完就會有機率分布
有了這個機率分布後你會把機率分布乘以對應的 vector
舉例來說，因為第一個位置的 key 跟 Q 乘完內積是 70
他就會把 0.7 乘上第一個位置的 element 的 value vector
乘起來之後再加上 0.1 乘以第二個位置的 value vector
把他們全部加起來就會成為新的 representation 在 1 這個位置
dimension 的 vector
第一個 vector 就要做 n 次
n 次就是 4 次，因為你有 4 個 vector 就要做 4 次
如果你 4 個都要做的話你就會做 4 的平方次
這裡就要 highlight Self-attention attention 的 complexity 是 O(n^2)
他需要對 input sequence 所有的做一次 attention
全部做完之後，每一個 head 也都做完，剛剛只有把上面這塊做完
假如做完之後每一個 head 都要做一樣的事
做完之後再把他們 concatenate 起來
再過一個小的 fully connected layer
就會得到新的 representation
這裡的 FFN 跟剛剛的 transformer encoder 是不一樣的
他是藏在 self-attention layer 裡面一個小的 fully connected layer
這裡就有一個問題就是
剛剛的例子裡是舉 4 個 input，假設今天 input 句子非常非常長
他可能有 8000 個 candidate
self-attention 要做 8000 平方次
他就會有 8000 平方次的 time complexity
意思是說假設今天 input sequence 非常長
它的複雜度就會變得非常非常大
所以說 reformer 是針對這點去做改進的
他要怎麼改進呢，這裡就是在講說剛剛 n = 8k 的 case
假設今天跟例子一樣就是只有 4 個 input
complexity 就是 4 的平方
n 平方
今天是 8k 的話就是 8k 的平方
複雜度非常非常的高
所以我們可不可以有一個方法是
只對 8000 個 candidate 裡面某些
某一個 subset 做 attention 就好
而這個 subset 的 attention 是
跟當下的 element 的 embedding 比較有關的 embedding 做 attention 就好
不需要對 8000 個都做
因為 8000 個裡面不是每一個位置
出來的 attention 數值都非常大
可能只有一個非常非常大
是不是可以先抓出每一個位置的 subset
只對這個 subset 做 attention
reformer 就是針對這點改進的
他怎麼做呢，他用了一個很簡單的方式就是 Hash Function
今天假如 input 的 element 非常非常小
經過 Hash Function 他們會丟到的 bucket 就是同一個
這裡在講的 bucket 就是說
假設有 4 個 bucket
不管我們有哪些 input
經過 Hash Function 就是會被分成 4 類
這 4 類怎麼決定呢，就是用
假設他 input 非常像的話
他就會被丟到同一個 bucket 裡面
做這件事情的意義是什麼
假設今天 input 變 8000 個
可以透過 Hash Function 知道說
可以只對 20 個還是 30 個人做 attention 就好
不需要對整整 8000 個人做
做這件事情就相當於是在找一個 subset 的 candidate 做 attention 就好
不需要對全部的人做
假設你有兩個非常接近的 vector
他會被丟到同一個 bucket 的機率其實非常非常高
不管他 Hash Function 的 rotation 再怎麼轉，他都會被丟到同一個 bucket 裡
這裡的 bucket size 是 4
如果他們非常非常遠的話
他們如果有點距離的話是有機會被分到不同的 bucket 裡的
所以總結一下剛剛做的事情就是
這是原本的 input sequence
透過 Hash Function 我可以知道那些 token 要被歸在同一個 bucket 裡面
知道後我就對整個 input sequence 歸成 4 類
這裡有藍黃白紅四個顏色
分成 4 類之後我就讓這 4 類的 element 只對
他們彼此做 attention
還有一個點是 reformer 不對他自己做 attention
只對他的其他成員做
因為 bucket 有 4 類所以會被拆成 4 份
這 4 份不一定是原本 sequence 之前的
有可能因為像這個位置
attain 到的 element 可能是跳來跳去的
這些跳來跳去的 element 只是因為 embedding 長得比較像
被歸在同一類
他會先 sorting 好
前面這堆是第一個 bucket 裡面的 candidate
第二個 bucket 裡面的 candidate，這是第三個，這是第四個
再對 bucket 內的 element 做 attention
這麼做就可以大幅度的減少運算量
簡單來說你可以先決定 bucket size 是多少
怎麼決定呢，假設 input size 是 1024
我先決定 bucket size 是 32
32 怎麼來，就是 1024 開根號
每個 element 最多只能 attend 32 個 candidate
做這件事情就相較於說你在把整個 attention 的複雜度從 n 平方變成 n log n，因為剛剛取根號
所以到最後 attention 的 mechanism 只有 n log n 的複雜度
這是 reformer 第一個點
第二個點是他有做一個東西叫做 Reversible Layer
什麼是 Reversible Layer
今天在做 model 的 forward 跟 backward 的時候
我們知道底層是有幫你計算圖的
假如說你有 7 層
你在 forward 到最後一層的時候，他其實會幫你把 7 層所有的資訊都記起來
佔一個很大的 memory
Reversible Layer 的概念是，只要記住最後一層 layer 的 embedding
可以透過這個 embedding 去推前面的 embedding 是多少
一樣可以做正常的 gradient update
這件事情的優點就是
可以只存一個 layer 的 memory 就夠了
不需要存整個 12 層的 memory
簡單來說就是今天做運算的時候，我不需要存下 12 層的運算量，我只需要存 1 層
只需要存最後一層，用最後一層的 memory 去推前面的 gradient
這樣一來不需要把 12 層的計算圖存起來
存起來之後，我就只需要存最後一層的，透過原本 residual 的 connection
原本 residual connection 是 self-attention 4 個 F
這個 residual 是原本還沒有過 self-attention 之前的 embedding
跟做完 self-attention 的 embedding，你會把他們加起來
這個 x1 跟 x2 是一樣的，把他加起來
這裡會分開來因為有 residual
residual 是一個有過 fully connected 一個沒有
有過的這個部分再把他加起來
有這個 residual 有什麼用處，因為我知道
他是透過這樣的加法所得到的
可以透過減法去回推 x1 x2 是什麼
用這樣的方式去計算 embedding
就不需要把 12 層的 layer 計算圖全部都存下來
這樣一來我只要存最後一層 layer 的 vector
就可以回算前面所有 layer 的 gradient
就可以達到省記憶體的功用
這就是他提到的第二個東西叫 Reversible Layer
這個東西好處也是在於，假如你的 input candidate 非常多
其實就相當於一個 layer 需要非常非常大記憶體
透過這個機制你只需要存一層，不需要把 12 層計算圖都存起來
再一次 backward 算完，你只要存最後一層
可以用最後一層資訊去回推前面所有層的資訊，也可以做到一樣的 update
講到這裡 reformer 的重點就在於他把 complexity n 平方的東西變成 n log n
n log n 的 log n 就在於說，我可以先決定好 bucket size 是多少
通常決定的方式都會是 input 長度的開根號
開根號這一點就可以讓他是 log n
意思是說假設 input 是 1024 個 candidate
我只要對 1024 開根號個 candidate 做 attention
不需要對全部 1024 個人都做 attention
這麼一來可以大幅減少運算量
講到這裡 reformer 就講完了
BERT 的兩種模型也都講完了
最後這一頁是想要講說 19 年 Style GAN 有一個
它可以做到多好就是
這是一個 The Wife Doesn't Exist
這是一個網站
他讓 Style GAN 去訓練很多動漫圖形
讓他可以把動漫的人生出來
生的很像是畫家畫的
這些圖片都是 Style GAN 自己生的
只是想 highlight 一下說再 19 年也有一個這麼酷的東西
講到這邊其實今天的 New Architecture 就上完了
不知道大家有沒有什麼問題
如果沒有的話今天課程就到這邊結束
大家就可以下課
有問題也可以在這裡問一下
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw/
