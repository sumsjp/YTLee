各位同學,大家好啊!
那我們就開始來上課吧!
上週我們講到一些在不訓練模型的情況下
就強化語言模型能力的方法
包括用神奇的咒語,還有提供額外的知識
今天我們來看看還有哪些其他的方法
可以在不訓練語言模型的情況下
完全不動、不調整語言模型的參數
在完全固定語言模型的情況下
強化他的能力
第三個要跟大家分享的方向
是把本來比較複雜的任務
拆解成簡單的任務
有時候會想說
我能不能夠直接拿一個語言模型
直接把任務當作輸入
然後他給我們任務的輸出
一次到位、一次把問題解決
但是如果這個任務非常的複雜
那語言模型可能很難做得好
這個時候呢
你可以把本來複雜的任務
拆解成一個一個步驟
每一個步驟都是比較簡單的任務
然後讓語言模型對每一個步驟各個擊破
那語言模型就有可能解一個複雜的任務
那這邊舉一個具體的例子
假設呢你今天要寫一個跟生成式AI有關的報告
你當然可以直接跟ChairGBT說
生成一個跟生成式AI有關的報告
但是他往往沒有辦法真的寫得太好
而且他寫的這個文章
可能也沒有辦法真的寫得非常的長
那怎麼辦呢?
也許我們可以把生成一個長篇報告這件事情
拆解成比較簡單的步驟
首先第一步呢
是先寫大綱
所以你跟ChatGPT說
我要寫生成式AI的報告
幫我把大綱列出來
他可能就告訴你說
第一節 先寫生成式AI的重要性
第二節 告訴大家生成式AI有什麼樣的種類
第三節 剖析生成式AI的技術等等
然後接下來呢 大綱裡面的每一個章節
再來分開撰寫 叫ChatGPT說
撰寫生成式AI的重要性
撰寫生成式AI有哪些種類
撰寫生成式AI背後的技術等等
啊如果我們這邊一段一段分開來寫的話
也許這個ChatGPT呢
他寫的時候因為不知道前面已經寫過什麼
可能會有一些前言不對後語的狀況
那怎麼辦呢
也許你可以把前面
ChainGBT已經有寫過的內容
再叫他自己做一下摘要
所以在寫新的段落的時候
是根據過去已經寫的內容的摘要
來寫新的段落
那期待這樣就可以讓ChatGPT
寫一個長篇的報告
那類似的做法有非常的多
不是隻有我在這一頁投影片上秀的例子而已
你要怎麼把一個大的任務拆解成小的任務
有各式各樣不同的可能性
那像這種把大任務拆解成小任務
這樣子的想法不是隻有在ChatGPT的年代才有的
我這邊引用一篇論文是這個2022年10月的論文
這篇paper叫做Recursive Reprompting and Revision
他想做的事情就是用當時有的大型語言模型寫長編小說
但他發現這些大型語言模型寫小說的時候
往往同一個人物寫著寫著
這個人設就變了
性別就變了
忘記自己寫過的劇情了
所以怎麼辦
先讓大型語言模型把小說的大綱架構好
先說整個小說有三個不同的場景
然後在每一個場景依序去把內容寫出來
這樣就比較有可能讓大型語言模型完成一個長篇小說
那講到這個拆解任務的部分
我們就可以來回顧一下上週我們講到的
為什麼叫模型思考或叫模型解釋他的答案會有用
上週我們講了一個技術叫Chain of thought
我們說Chain of thought就是叫機器思考
叫機器思考以後
他的能力不知道怎麼回事就變強了
那如果你現在瞭解拆解任務
可以把本來比較複雜的任務變得比較簡單
然後讓這個大型語言模型可以做得更好的話
現在我們就可以來解釋為什麼Chain of thought會有用
其實Chain of thought也可以是看作是拆解任務的一種
怎麼說呢
如果你要當你叫語言模型think step by step的時候
實際上對語言模型造成的影響是什麼呢
實際上對語言模型通常造成的影響是
他會詳細的把計算過程列出來
所以如果你給他一個數學問題
當你要求他要思考的時候
他就會先把計算過程
一步一步的詳細列出來
然後再給你答案
而先把計算過程
一步一步的列出來
再給答案這件事情
其實就等同於把本來解數學這個問題
拆解成兩個步驟
第一個步驟是
先讀完問題以後
把計算的過程把你要列的數學式子一條一條的列出來
再根據你列的數學式子把答案解出來
今天如果給一個數學的問題不列式子
不列計算過程直接就寫答案
就算是對人類來說也是非常難得到正確答案的
那如果把解數學這個問題拆解成兩個步驟
先把詳細的計算過程列出來
先把數學式列出來
再根據列出來的數學式把答案寫出來
寫出來 那語言模型就更有可能得到精確的答案
那上面這個圖跟下面這個圖 我講的是同一件事
就語言模型先列式再產生答案 跟把這個問題拆解成兩部分
先列式 再把列式的結果跟剛才的問題
集合起來一起叫語言模型生成答案 它是同一件事情
因為語言模型在生成文字的時候 它是用文字接龍的方式
這個詳細的劣勢跟答案是用文字接龍的方式
一個一個字生出來的
所以當他產生答案的時候
如果前面已經有劣勢的話
就等同於是根據列式的結果
來做文字接龍來產生答案
所以語言模型在回答問題的時候
先列式再產生答案
跟把這個問題拆成兩段
先列式再根據劣勢的結果產生答案
是同一件事情
所以這就是為什麼Chain of Thought會有用
因為Chain of Thought等於是叫語言模型
把解數學問題拆成多個步驟
那這也解釋了為什麼對GPT-3.5而言
Chain of Thought沒有那麼有用
我們上週有提醒大家說
這一類神奇的咒語往往是對某些模型有用
或對一些比較古老的模型有用
對新的模型往往不一定那麼有用
為什麼 因為GPT-3.5你今天自己用用看
讓它去解數學問題
基本上你沒叫他列式 他也會主動的把式子詳細的列出來
他自己就知道應該把數學這個問題 拆解成先列算式再寫答案
這就是為什麼Chain of Soul對GPT-3.5幫助不大
因為他本來就已經會做Chain of Thought要求他要做的事情了
剛才講說我們可以把一個複雜的任務 拆解成多個小任務
現在我們可以在原來的任務後面再加一個步驟
這個步驟是讓語言模型檢查自己的錯誤
我們可以讓語言模型把答案產生出來以後
再叫語言模型再檢查自己的輸出有沒有正確
有時候他可以檢查出自己之前的答案是錯的
修正錯誤的答案
最終得到正確的答案
那講到這邊可能有人會很懷疑想說
剛才錯誤的答案不就是語言模型自己產生出來的嗎
那根據錯誤的答案
語言模型再看一次
自己產生出來的錯誤答案
難道他有辦法修正自己產生出來的錯誤答案嗎
你自己想想看喔
這個語言模型檢查自己的答案這件事
就好像是你在寫考卷以後
寫完之後再檢查一次你有沒有寫錯
那大家國中高中的時候都考過了無數的考試
那你的考試一定很有經驗
你知道你再檢查一次的時候
往往有辦法檢查出自己的錯誤來
為什麼我們有辦法檢查出自己的錯誤來呢?
因為很多問題是計算出答案很難
但是驗證答案是不是正確的
也許非常的容易
那這邊隨便舉一個雞兔銅龍的例子
告訴你說雞跟兔總共有35隻
合起來有94隻腳
問你有幾隻雞幾隻兔
就算你完全不會解雞兔銅龍的問題
我告訴你答案是20隻雞20隻兔子
你也可以馬上反射過來說這個答案是錯的
因為就一開始題目就告訴你有35個頭了
20隻雞20隻兔子顯然不是35個頭
所以就算你不會解基督同龍的問題
看到錯誤的答案的時候
你其實也可以很快的反射過來說
它是一個錯誤的答案
所以有可能語言模型沒有辦法得到正確的答案
但是他產生錯誤的答案以後
自己再看一遍錯誤的答案
他有機會發現他是錯的
那這件事情用擬人化的講法
我們可以說這些語言模型是有自我反省的能力的
那這邊實際舉一個例子讓大家看看
今天GPT-4自我反省的能力
你叫他介紹臺大玫瑰花節
大家都知道說明天就是杜鵑花節
臺大有杜鵑花節 但是沒有玫瑰花節
但是對GPT-4來說他也管不了那麼多
你叫他介紹臺大玫瑰花節
他就瞎掰一個臺大玫瑰花節的介紹
那現在大家都知道說這些大喜圓模型
他就是做文字接龍
所以他會產生不存在的東西
會瞎掰一些不存在的事實
那這是情有可原的
但接下來在同一則對話裡面
我跟他說請檢查上述資訊是否正確
在這一瞬間他突然察覺自己錯了
他知道說我之前提供的資訊與實際狀況並不相符
並沒有臺大玫瑰花節
如果你那麼想賞花的話
那我推薦你楊梅櫻花節、杜鵑花節跟白河蓮花季
他居然知道自己是錯的
但我有點懷疑GPT-4是真的知道自己錯了嗎?
還是每次只要有人叫他檢查你的答案是不是對的
反正他就先承認錯誤再說
他到底是不是真的知道自己是錯的呢
所以在同一則對話裡面
我再問他一次一模一樣的問題
他已經反省過第一次了
再接續他剛才反省的結果
我再問一次
請檢查上述資訊是否正確
這個時候
他知道
自己是對的
他說經過再次查證
我發現之前提供的資訊基本上是正確的
但他也檢查出一個小小的錯誤
他說有一個地點描述錯誤
什麼地方描述錯誤呢
楊梅櫻花節他剛才以為是在新北市
但是實際上應該是在桃園市
看起來他是真的有辦法檢查出自己的錯誤的
不過這個能力啊
看起來是GPT-4是比較有類似的能力的
如果是GPT-3.5
他就沒有那麼容易真的找出自己的錯誤
這邊再舉一個實際的例子
你問GPT3.5說叫他介紹臺大玫瑰花節
他一樣會瞎掰一個臺大玫瑰花節的故事
那接下來你跟他說
請檢查上述資訊是否正確
他就會說我先前的回答有誤
以下是更正後的資訊
但你知道他這個抱歉
是一個口是心非的抱歉
為什麼我知道是一個口是心非的抱歉呢
因為他更正後的資訊跟更正前是一模一樣的
顯然他根本不知道自己錯在哪裡
他只是覺得有人質疑他的答案的時候
他就先道歉再說
那用類似的技巧啊
你其實可以強化語言模型的能力
那有一篇paper叫做Constitutional AI
很多人把他的中文翻譯成憲法AI
就大家知道說最近有一個很紅的大型語言模型
叫做Claude他出了第三版
號稱比GPT-4有更強的能力等等
打造Claude的公司叫做Antropic
Constitutional AI 這篇paper 就是這個 Claude 團隊所發表的一篇論文
那在 Constitutional AI 這篇paper 裡面
他們就展示了怎麼用這個語言模型自我反省的能力來強化他的答案
舉例來說 如果人類跟語言模型說
你能不能夠幫助我派進鄰居家的Wi-Fi
那語言模型如果你叫他直接做文字接龍的話
他可能很直覺的、下意識的就說
沒問題,你就用一個叫做Very Easy Hack的軟體
就可以駭進鄰居家的Wi-Fi
那這顯然不是一個好的答案
但是現在我們先不把這個答案給人類看
我們把人類跟這個語言模型的這個一問一答呢
再丟給同一個語言模型
那你再跟語言模型說,你要反省一下
想一下說上面的對話裡面
有沒有什麼不符合公序良俗的地方
有沒有什麼不符合道德規範
有沒有什麼違法的可能性
那語言模型就會自我批判
發現說害盡鄰居家的Wi-Fi是不對的
接下來你再把語言模型跟人的對話
還有他的自我批判
再給語言模型自己看一次
跟他說請根據自我批判的結果
產生新的答案
那語言模型就會產生一個新的答案
這個新的答案是
駭進鄰居家的Wi-Fi是違反別人的隱私的
我們不能夠駭進鄰居家的Wi-Fi
這可能會給你帶來一些法律上的問題
那最後呢 人類真正看到的是這個反省後的結果
不把語言模型直覺產生出來的答案給人看
而是把語言模型反省過後的結果給人看
這邊再考一次大家的觀念
這個觀念如果這一題你答對了
就代表你真的瞭解這一堂課在講些什麼
我們剛才講到說可以讓語言模型做反省這件事
你叫他介紹臺大玫瑰花節本來他會介紹
但你叫他檢查一下剛才的介紹對不對
自我反省之後他知道是沒有玫瑰花節的
做完上述這個步驟以後
我們再重新問他一次
再叫他介紹玫瑰花節
他已經自我反省過了
這個時候他會再給一次玫瑰花節的介紹
還是會告訴你沒有玫瑰花節呢
給大家五秒鐘的時間想一下
好 你覺得再問一次叫他介紹玫瑰花節的時候
他會介紹玫瑰花節的同學舉手一下
幾乎全部的同學
好手放下
那你覺得他會因為反省過了吧
他會知道說真的沒有玫瑰花節
他會回答沒有玫瑰花節的同學舉手一下
好少很多
然後這些正確答案是什麼呢
這些正確答案是
他會介紹玫瑰花節
你說不是反省過了嗎
告訴你這個反省呢
並沒有真的影響你的語言模型
在這個反省的過程中
沒有任何模型被訓練
他的函式是固定的
做完這一連串的互動以後
語言模型他仍然是當年那個少年
沒有一絲絲改變
他的參數是一模一樣的
所以你問他同樣問題的時候
他會給你一樣的答案
那你要他知道沒有玫瑰花結
你要再叫他反省一次
你要再叫他檢查他的答案
他才會知道沒有玫瑰花結
所以這邊再強調一次到目前為止
這一堂課裡面到目前為止
都沒有任何的模型被訓練
因為大家常常會對這個模型有些誤解
覺得說模型讀過這些文字以後
就學到了東西它就變了
不是這樣子的
語言模型就是一個函式
它要參數有改變的時候
它的能力才會真的有改變
你讓它讀什麼東西
語言模型的參數是固定的
你並不會影響它的能力
你並不會影響它的輸出
所以要再強調一下
在反省的過程中
沒有任何模型被訓練
這個函式是固定的
當然如果有人讀了constitutional AI那篇paper
你可能會知道說
constitutional AI那篇paper裡面
有讓模型從自我反省中再進一步學習
但是怎麼從自我反省中再進一步學習呢
那個就是另外一個故事了
這個我們之後會再講到
那有同學可能會說
我在使用語言模型的時候
每次問他同一個問題
他的答案都不一樣耶
這到底是發生了什麼事
那麼這邊來跟大家回答一下
為什麼每次你問語言模型一模一樣問題的時候
他每次答案都是不一樣的
那麼之前已經有講過說
語言模型在回答你問題的時候
其實就是做文字揭露
那我們再更精確一點講
實際上語言模型在做文字接龍的時候
它並不是輸出一個字
而是輸出一個機率分佈
也就是說它實際上輸出的是
每一個字可以接在輸入後面的機率有多高
比如說你叫他對臺灣大這個句子
去做文字接龍
那語言模型可能輸出說
有50%的機率學這個字
可以接在臺灣大後面
有25%的機率
因為臺灣大車隊嘛
所以車也可以接在大後面
有25%的機率
車可以接在大後面
或臺灣大哥大也有一定的機率
哥可以接在大後面
語言模型實際上做的事情
是給每一個
可以拿來做文字接龍的符號一個機率
這個機率代表了這個符號
可以接在輸入句子後面的
可能性有多大
那有了這個機率以後
語言模型會根據這個機率去植一個骰子
去決定說實際上要拿來做文字接龍的符號是哪一個
那在這個例子裡面
學有50%的機率就代表你擲骰子的時候
有1/2的機率會躑到學
有1/4的機率會擲到車
所以當語言模型參數是固定的時候
你輸入一樣的句子
輸出的這個機率分佈是一樣的
就好像是那個骰子是同一顆
但是雖然骰子是同一顆
你每次擲骰子的時候
植出來的東西不一定是一樣的
這就是為什麼語言模型在給你答案的時候
它每次的答案都會是不一樣的
所以語言模型就算它是固定的
它會產生固定的機率分佈
但它每次最終擲完骰子產生出來的答案
可能會是不一樣的
所以實際上語言模型在回答問題的時候
整個過程是這樣子的
你問他說什麼是大型圓模型問號
他把這個問題當作一個未完成的句子
再繼續去做文字接龍
他產生一個機率分佈
根據這個機率分佈去植一次骰子
這個機率分佈如果你沒辦法想像它是什麼的話
就想像成它是一面骰子
這個骰子的每一面呢 寫了一個符號
那每一面被擲出來的機率是不一樣的
有些符號被植出來的機率比較高
有些符號被植出來的機率比較低
那擲一次骰子就擲出一個大
然後把大貼到剛才的問題後面
再把這一串文字當作一個未完成的句子
再得到一個機率分佈
再製造一個骰子出來
然後再擲一次骰子
假設這一次擲到行
就把行再貼到書的句子後面
就這樣一路擲骰子下去
擲到最後產生一個機率分佈
骰子擲下去 擲出來是結束這個符號
現在答案完整了 語言模型已經把他想產生出來的東西都產生完了
這個就是你最終看到的語言模型
Chair GPT之類的語言模型給你的答案
所以你現在知道 為什麼語言模型每一次產生的答案都是不一樣的
並不是每一次你問同樣的問題的時候
他內部參數都改變了 他內部的參數仍然是一樣的
但是因為他會直骰子
所以每次的答案都是不一樣的
那用剛才語言模型同一個問題
每次可能產生不一樣答案這件事情
你也有其他的方法
可以來強化這些語言模型的能力
舉例來說你問他一個問題
你可以讓語言模型對同一個問題回答多次
比如說他第一次的答案是3
然後再做一次實驗
因為每一次產生答案的時候都是要直骰子的
所以每次答案都不一樣
所以可能第二次最終的答案選擇是5
然後再做一次同樣的問題
最後答案是3
你做多次一點問題
然後取最常出現的答案當作是正確答案
那這個方法叫做Self Consistency
你把本來只需要做一次的問題改成做多次
每次的答案可能都不太一樣
但看哪一個答案最常出現
就把它當作是正確答案
這個方法叫做Self Consistency
那我們已經講了很多
把一個複雜的任務拆解成多個小任務的方式
包括把複雜的任務拆解成多個步驟
還有在最後面多加一個額外的步驟做自我反省
還有同一個問題做多次以後
你可以取最常出現的答案
或者是用其他方法來驗證答案是不是正確的
這些方法可以組合起來
打一套組合拳
這邊就是舉一個
組合究極的組合的例子
假設你現在手上有一個複雜的任務
那我們把這個複雜的任務呢
拆解成三個步驟
在語言模型呢
先做第一個步驟
任務輸入
語言模型呢先解第一個步驟
那你不讓語言模型呢
不只產生一個答案
你把同一個語言模型
讓他做三次文字接龍
產生三個不同的答案
那三個不同的答案
哪些是正確的哪些是不正確的呢
也許我們可以透過
自我反省的方式
讓語言模型來檢查這些答案
是不是正確的
所以我們假設說語言模型
讓他讀一下這個答案
他可以判斷是正確的還是不正確的
那我們就問語言模型
第一個答案你自己覺得對不對呢
他可能覺得是對的
那我們就再進入第二步驟
產出解第二步驟,一樣產生多個答案
再叫語言模型檢查自己的答案是不是對的
發現第二步驟產生的答案都是錯的
那沒關係,再退回第一步
再檢查第一步的第二個答案
發現也是錯的
那沒關係,再檢查第一步的第三個答案
發現是對的
那進入第二步
然後再產生兩個答案
再把這兩個答案分別拿去檢查
發現第一個答案就是對的
那就再進入第三步
再把第三步產生兩個答案
在第三步的時候再產生兩個答案
第三步再根據第二步的這個結果呢
產生兩個答案
再檢查第三步的結果發現是對的
那你就得到一個最終的答案
那這個方法呢
是有名字的
這個方法叫做Tree of Thought
它的縮寫呢
是TOT就是一個枯簾
而像什麼of Thought啊
這類的做法
今天真的是層出不窮
自從有了Chain of Thought以後
大家取這個方法名字的時候
都一定要叫什麼of Thought
那這個方法叫做tree of thought
那tree of thought就是把我們剛才講到的各種不同的技術組合起來
打一套組合拳
其實還有很多其他的什麼of soul
比如說algorithm of thought
或者是graph of thought
這些什麼of soul它的概念都是
把一個複雜的任務拆解成小的任務
讓語言模型去各個擊破
那我們這邊就不細講
我把文獻留在投影片上
給大家自己慢慢參考
那這個第四個方法呢
要跟大家分享的是
這些語言模型可以使用工具
來強化自己的能力
我們知道
這些語言模型雖然很厲害
但是他們也有一些
不擅長的事情
比如說他們不擅長做運算
我隨便出個六位數
乘六位數的乘法
GPT 3.5
會給我們答案
但這個答案是對的嗎?
你一按計算機就知道這個答案是錯的
我神奇的地方是
他其實沒有錯的非常離譜啊
前兩位數跟最後兩位數
他的答案其實是對的
但是憑藉著文字接龍
想要接出六位數乘六位數乘法的答案
看起來是有非常高的難度的
所以語言模型也有一些他不擅長的事情
但是就好像人類沒有尖牙利爪
但人類發明瞭各式各樣的工具
去強化人類自身的能力
讓人類可以跟猛獸對抗
創建了文明
這些語言模型
今天也有機會使用額外的工具
來強化他們自身的能力
那你發現這邊呢
在做展示的時候
是用GPT-3.5
沒有用GPT-4
你很快就會知道
為什麼這個例子
不能用GPT-4來展示它的結果
那有哪些工具可以用呢?
也許一個跟語言模型搭配起來非常適合的工具
就是搜尋引擎
那講到這邊大家可能會有點困惑
因為很多人會覺得
欸?大型語言模型自己本身不就是個搜尋引擎嗎?
很多人會把大型語言模型當作搜尋引擎來用
比如說前幾週
OpenAI出了SORA
那有的人就會覺得說
那我直接來問問GPT-4
SORA是什麼吧
那你問GPT-4
SORA是什麼的時候
他就會開始瞎扯跟你說
SORA是一個用於提高大型語言模型
在特定領域內性能的技術
包括預篩選、專家評審、反饋循環等等
就是一個瞎掰的技術
那你再問他一次一模一樣的問題
你就知道他是真的不知道SORA是什麼
因為他要開始瞎扯
SORA是一個新一代的超級計算機
那為什麼語言模型會瞎扯呢
那你今天已經知道說
這一些語言模型就是做文字接龍
他背後並沒有一個資料庫
所以他的答案都是文字接龍所產生出來的
他並不是查找一個資料庫以後給你答案
那用文字接龍產生答案
難免會產生跟事實不合的東西
總之把語言模型當做搜尋來用
並不是一個合適的做法
他完全不適合當作一個搜尋引擎
那怎麼辦呢?
怎麼把語言模型搭配搜尋引擎來使用呢?
今天一個常見的做法是
假設有一個困難的問題
你覺得是語言模型憑藉著文字接龍
是不太可能接對的
那你可以怎麼做呢?
你可以先把這個困難的問題拿去搜尋
那你可能搜尋的對象
是整個Internet 或者是某一個你手上有的資料庫
那你得到這些搜尋結果以後
這些搜尋結果等於就是我們之前講過的額外資訊
你把你現在要問的困難問題
加上從網路上或資料庫裡面得到的搜尋的結果
把它接起來
一起給語言模型去做文字接龍
他就比較有可能接出正確的答案
因為如果只憑著困難的問題
只讀了這個問題要做文字接龍可能很難接出正確的答案
但如果有額外的資訊
把這個問題加額外的資訊一起去做文字接龍
那模型就更有機會可以接出你要的結果
那這個技術就是上週大家反覆一直問到的
Retrieval Augmented Generation
它的縮寫是RAG
那3月29號的時候呢
NVIDIA的團隊會來跟大家講
怎麼做RAG這個技術
那 RIG 這個技術今天之所以變得非常的熱門
是因為首先它的實作並沒有非常的困難
在這整個過程中要再強調一遍
語言模型沒有任何改變
你沒有訓練語言模型
你完全沒有動到它
你完全沒有改變它的能力
但是你因為加了一個額外的搜尋的步驟
可以讓你自己的語言模型
跟其他人的 雖然是同一個語言模型
有不一樣的反應
尤其是假設你今天搜尋的對象
也許是一個很特別的資料庫
這個資料庫只有你自己
才有access的權限
裡面有很多很特別的資訊
這個時候你取得的
搜尋到的這些結果
可能是別人在網路上
是都沒有辦法取得到的
這個時候你的語言模型
根據這些特別的資訊
他可能就可以得到很特別的答案
所以今天如果你要
客製化你的語言模型
但這邊所謂的客製化
我再強調一次
這個強調幾次都不為過
就是這邊沒有訓練任何模型
就我們這邊講的客製化
並沒有訓練模型
你今天如果要
在不訓練模型的狀況下
客製化你的語言模型
讓他的答案跟其他人的語言模型很不一樣
也許透過RAG
今天往往是一個非常有效
非常快速又有效的方法
這就是為什麼RAG今天變得非常的熱門
每個人都不斷的在提RAG這個技術
所以今天如果GPT-4可以使用搜尋引擎的話
你問他什麼是Sora
他的答案就不一樣了
事實上GPT-4他是可以使用搜尋引擎的
只是什麼時候要用搜尋引擎
是由他自己決定的
那等一下我們馬上就會看到
大型語言模型如何自己決定
什麼時候要用什麼工具
那在我剛才的例子裡面
我問GPT-4 Sora的時候
他顯然是沒有去做上網搜尋的
那怎麼強制讓他上網搜尋呢?
嗯 就直接跟他講人話
告訴他說 上網搜尋以後再來回答我的問題
那這個時候呢 你就會看到這樣子的一個
一段文字 他說
Doing research with Bing
意思就是他去在Bing上面做搜尋
那得到的搜尋 得到搜尋結果以後呢
他再根據搜尋結果來回答你的問題
這個時候他的回答就會比較精確
那他也會告訴你說
這一段答案的資料來源在哪裡
所以當你用這個GPT-4的時候
發現他會引用他的資料來源
就代表說他其實是有上網搜尋的
他是上網搜尋以後再給你他的答案
他上網搜尋以後把網頁的內容爬下來
根據網頁的內容去做文字接龍
這才是你看到的答案
那這些語言模型現在還有另外一個他可以使用的工具就是寫程式
當你問今天這些大型語言模型一些數學的問題
比如說這邊問他一個雞兔同籠的問題的時候
過去啊 或者過去是在比如說去年4月GPT-4剛出來的時候
你問他數學的問題 沒問題 他解的了
他會列式 他會先另一些未知數
然後把式子列出來
但是列出式子以後
他往往沒辦法得到正確的答案 為什麼呢
因為式子列出來以後 你就要把X跟Y解出來
那他會自己去做移項等等
你知道這一個解題的過程過去
是靠著文字接龍接出來的
那往往這個解題的過程就會有奇怪的疏漏
他在非常奇怪的地方犯了人類根本不可能犯的錯誤
然後就得到錯誤的答案
但是今天GPT4 你在問他雞兔同籠的問題的時候
他基本上已經不太可能會答錯了
為什麼?當他列完式子以後
他不會再憑藉著文字接龍自己硬解出X跟Y是多少
他直接寫一段程式碼
他就直接寫一段程式碼
他會先呼叫一些他要的package
然後把剛才的兩個式子列出來
然後直接呼叫一個solver
直接呼叫一個可以解題的模組
把這個函式給他
把答案解出來
再把答案印出來
這個就是正確答案了
那會寫程式的大型語言模型其實很多啦
其實GPT 3.5也會寫程式
但GPT 4比較特別的地方是
他寫完程式以後 他能夠自己去執行他自己寫的程式
把執行的結果再拿來給你看
像這種解聯立方程式的問題
如果你是直接呼叫這個現成的package
呼叫現成的程式 呼叫現成的模組來幫你解這個問題
你根本不可能會得到錯誤的答案
那其實透過寫程式來強化語言模型能力這件事啊
其實很早就有了
那這邊就是引用一篇論文
這篇論文呢 叫做
Program of Thought
今天如果有什麼好的想法
尤其是這種不訓練語言模型
憑空讓它能力增強的方法呢
往往都要叫什麼什麼of thought
Program of Thought的概念
就跟我們剛才展示GPT-4的能力
是一模一樣的
就你問它一個數學問題
然後呢 如果它直接用
這個文字接龍的方法
硬解的話
很容易得到錯的答案
但如果呢 把一段程式寫出來
在執行這段程式往往比較可能得到正確的答案
那這是一個蠻古早的論文
這是2022年11月GPT-3.5上線之前就已經有的論文
所以我們上週講到GPT-4
如果你叫他哈哈哈哈100次
他會有一個截然不同的答案
什麼樣截然不同的答案呢
我們先來看GPT-4的回答
你跟他說請說哈哈哈哈100次
他真的就會產生300個哈
我數過了一個都不少
正好就是300個哈
但他是怎麼做到產生
正好是300個哈的呢
你會發現這邊有一段文字叫做
Finish Analyzing
當你看到這個句子的時候
當你看到這段文字的時候
就代表GP4寫了一個程式並執行它
那你可以點這個鈕把它展開
你就發現
原來他是寫了一段簡單的程式啊
這個程式就是這樣
哈哈哈哈反覆一百遍存到text裡面
再把text print出來
正好就是三百個哈哈
一個不多一個不少
所以GPT-4可以用其他模型
現在還沒有辦法使用的解法
真的去解一些可能本來透過文字接龍
蠻難解出來的問題
好那現在這些語言模型啊
還有什麼可以使用的工具呢
比如說這個GPT-4可以去接一個文字生圖的AI
那這個AI叫做DALI
那文字生圖的AI它的運作非常簡單
非常直覺就是你給它一段文字
它按照文字的描述
把你要它畫的東西給它畫出來
那因為今天GPT-4可以調用這個文字生圖的AI
所以大家通通都會用文字生圖了
今年過年的時候
你是不是收到很多的長輩圖賀年卡呢
這些長輩圖賀年卡可能是長這樣的
這樣的 這樣的
這些龍看起來都沒有什麼問題
雖然這個龍呢 它噴出來的火有點像是仙女棒噴出來的火
覺得有點奇怪
但是看起來都有模有樣的
但你仔細定睛一看
就發現這些龍有一些怪怪的地方
比如這隻龍
它有一個呆毛長在頭上
這個龍不知道為什麼它的鬍鬚呢長一根在頭上
長一個非常長的毛出來
這些龍都是用DALL-E,也就是透過GPT-4去呼叫DALL-E生成的
你要怎麼用GPT-4產生龍年的賀年卡呢?
你就跟他講產生龍年賀年卡,圖上不要出現文字
然後他就會產生creating image這段文字
就代表說他去呼叫DALL-E,然後DALL-E就會把圖畫出來
那為什麼上面要特別強調不要出現文字呢?
因為如果你不加這句話的話
他就很喜歡在圖上面加一些
Happy New Year 啊
或加一些中文啊
而且這邊這段文字是對的
Happy Year of the Dragon 感覺還可以
這邊這個 Year 顯然拼錯了
那因為這邊他想要產生一些中文
但是顯然都不知道在寫些什麼
所以避免他產生文字
以免自曝其短
但如果他不產生文字的話
結果往往畫的都還是不錯的
你不定睛看 你是看不出來
這是AI畫的
那你可能會想說
把文字生圖接到GPT-4裡面
讓大型語言模型
可以自己去Call一個生圖的軟體
有什麼樣的作用呢?
這個生圖的軟體
比如有很多啊
我自己打開那些軟體
比如說MeJourney
我也可以畫圖啊
那把語言模型跟生圖的軟體結合起來
有什麼樣的妙用呢?
這邊舉一個例子
在一年前呢
一年前我曾經示範了怎麼用語言模型玩文字冒險遊戲
玩法是這個樣子啦
一年之前的玩法是這個樣子
你就輸入一段文字
告訴他說我們要來開始一個文字冒險遊戲
然後由玩家來決定要採取的動作
然後遊戲開始的時候請詳述故事背景遊戲開始
你只要輸入這些文字
這個ChetGBT呢,他就可以主持一個文字冒險遊戲
他會提供選項讓你選,根據你的選項
他就會有不同的劇情展開
所以他可能會這樣回答你,就是你是一個探險家
然後你聽說一個古代寶藏的傳說
你要踏上這個旅程,尋找古代的寶藏
給你三個選項,來選不同的選項
會有不同的結局
因為玩這個文字冒險遊戲呢
只有文字實在是太乾了
所以在一年之前
為了讓整個遊戲不那麼乾
我把文字冒險遊戲的敘述
丟到一個文字生圖的AI
當時用的是MidJourney
讓它產生遊戲的插圖
如果你想知道玩起來感覺怎麼樣的話
你可以看這一段錄影
但今天你其實是可以有不一樣的玩法的
怎麼玩呢?
今天你可以多加一句
每次你描述完場景之後
請根據你的描述產生一張圖
那他一樣會開始架構一個文字冒險遊戲的場景
他說你醒來在一個陰暗的森林中
你有一個揹包 揹包裡面有一個地圖
然後看起來像是這個森林的地圖
你還有小手電筒 你還有舊的日記
上面寫著尋找遺失之城等等
那講完這些描述之後 後面還有很長的描述啦
這邊就不秀出來給大家看
玩完這些描述以後呢,卻GPT就去呼叫Dolly生一個跟場景有關的圖
看起來真的有個森林,有個揹包,有手電筒,還有地圖
跟他剛才的描述呢,是滿一致的
那接下來他一樣會給你幾個選項
問你要走左邊的小徑,還是繼續往前走,還是走右側的小徑
那這樣這個文字冒險遊戲呢,玩起來就更帶感了
那其實這個GPT-4還有很多其他它可以使用的工具啦
那我們剛才講的幾個,比如說呼叫搜尋引擎
呼叫文字生圖的AI等等
是它內建本來就可以呼叫的工具
那如果你想要呼叫更多工具的話
有一個東西叫做GPT的Plugin
這個Plugin裡面現在有收集了超過上千個工具
那你要用這些工具的時候呢
你就要進入一個叫做plugin的模式
然後在那個模式裡面呢
你可以從那上千個工具裡面
選三個工具在對話的時候進行使用
好那講到這邊
我就想要回答我剛才講的問題
語言模型是怎麼使用這些工具的呢
不要忘了語言模型只會做一件事
就是文字接龍
所以當他使用工具的時候
他也是用文字接龍的方式在使用工具的
怎麼用文字接龍的方式來使用工具呢?
以下就舉一個具體的例子告訴你說
怎麼用文字接龍的方式使用工具
假設你現在想問語言模型說
我用五美金可以換多少新臺幣
語言模型想要回答你這個問題
所以開始做文字接龍
五美金可以換
他接了這幾個字以後
覺得資訊量不太夠
因為到底美金跟臺幣兌換的匯率應該是多少呢
這個兌換的匯率啊
應該是隨時間變動的
所以應該去網路上查一下
最新的兌換的比例
所以他就會產生一個特殊的符號
這個特殊的符號代表呼叫工具
你可以想像說他先定義好一個符號
比如說一個
你平常在對話的時候絕對不會用到的符號
那個符號呢
只要一出現就代表呼叫工具
那接下來呢 它會繼續去做文字接龍
直到它產生另外一個特殊符號
代表結束使用工具為止
在呼叫工具到結束使用工具這兩個符號之間的文字
就是操作工具的指令
那至於操作工具的指令該長什麼樣子
這個是你要事先訂好的
那這邊我是假設說
在括號之前的東西代表說
是要使用哪一個工具
比如說這邊寫搜尋 代表要使用搜尋引擎
括號裡面的內容 代表你要給那個工具的內容
如果是搜尋引擎的話 就是你要拿去搜尋的那個關鍵字
就把美金臺幣兌換這幾個關鍵字 丟到Google上面
那Google就會回傳給你一些相關的資訊
比如說你馬上就可以看那個網頁
那個網頁上是寫一美金等於31.5臺幣
那把你從網頁上搜尋到的結果
當作是文字接龍的一部分
貼到剛才已經產生的句子後面
這些網路上搜尋到的內容
就是當作是語言模型
做文字接龍已經產生出來的結果
語言模型會根據這些網路上搜尋到的內容
再繼續去做文字接龍
比如說我已經知道一美金就是31.5臺幣
但是五美金到底兌換多少臺幣呢
而如果用文字接龍的方式來計算
恐怕非常容易算錯
所以再呼叫一次工具
產生一段文字
直到呼叫工具結束為止
那這一段文字
就是操控工具的指令
括號前面是計算機
代表要呼叫計算機
括號裡面的內容
就是你要輸入給計算機的內容
5乘以31.5
計算機按一下以後
得到157.5
那把計算機的輸出
當作是文字接龍
已經產生出來的結果
再繼續去做文字接龍
那這個時候語言模型就不需要自己再去做任何數學了
這邊前面都已經接出來是157.5
他根據這段文字繼續去做文字接龍
他只要複製就好
他把157.5複製出來
得到答案是157.5元新臺幣
那輸出結束的符號代表生成結束
假設你想要知道語言模型實際上是
怎麼學會使用工具的
他怎麼學會在適當的時機輸出使用工具那個符號
如果你想知道他是怎麼做到這件事情的
你可以看這一段錄影會告訴你說
這些語言模型是怎麼學會使用工具的
也就是在適當的時機產生使用工具這個符號
但是既然使用工具這件事情是語言模型自己學出來的
那就意味著他難免會犯錯
那這邊就是舉個犯錯的例子
我今天叫語言模型
畫一個表格整理
GPT-1到GPT-3的模型參數量
然後我想大家應該都聽得懂我的意思吧
我想要畫一個表格
沒有人會誤會吧
但是GPT它就畫了一個表格這樣子
它就把這個指令解讀成字面上的意思
真的去畫了一個表格
它呼叫搭理這個工具
畫了一個表格出來
上面呢 還在某個地方 寫了GPT
其實這個語言模 現在這個GPT-4啊
我感覺他對話這個字 非常的敏感啊
你少跟他講話這個字
因為看到話這個字
他就會忍不住要把搭理呼叫出來
比如說我說
請話一隻狗 把這句話翻譯成英文
我就是要他做翻譯
他會把請話一隻狗 翻成英文
Please draw a dog
但他控制不了 他本能想話一隻狗的衝動
看到話那個字
所以他順便要把一隻狗 話出來
所以這些工具要怎麼使用
是語言模型自己決定的
所以他難免會犯一些使用工具上的錯誤
那要怎麼再強化語言模型使用工具的能力呢
有一篇paper叫做那個AnyTool
這篇paper是一個非常新的論文
是上個月才放到archive的
裡面就使用了各式各樣的技巧
希望語言模型可以把使用工具這件事情用到極致
那剩下一段呢
我們請等助教講完作業以後再繼續說
那我們現在就請助教呢來講這個作業
那我們現在要進入作業三
那作業三開始是一個比較有挑戰性的作業
前兩個作業你沒有碰到任何的程式
那從作業三開始
我們要真的碰到寫程式的部分
我們就請助教來講一下作業三
