臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw/
Q-learning 是什麼呢？我們等一下，會簡單的介紹一下 Q-learning
那其實在 machine learning 那一門課裡面， 其實也簡單的介紹過 Q-learning 了
那不過今天就算是再很快地複習一下，接下來呢
會講一些 Q-learning 的 tips
是之前沒有講過的
然後會講說 Q-learning 怎麼樣用在 continuous 的 action 上
我們就先從 Q-learning 的簡介開始說，那我們說
Q-learning 這種方法，它是 value-based 的方法， 在value based 的方法裡面
我們 learn 的並不是 policy，我們並不是直接 learn policy
我們要 learn 的是一個 critic
critic 並不直接採取行為，它想要做的事情是
評價現在的行為有多好或者是有多不好
這邊說的是 critic 並不直接採取行為
他是說我們假設有一個 actor pi
那 critic 的工作就是來評價這個 actor pi 他做得有多好，或者是有多不好
舉例來說，有一種 actor 叫做 state value 的 function
這個 state value function 的意思就是說
假設現在的 actor 叫做 pi，拿這個 pi 跟環境去做互動
拿 pi 去跟環境做互動的時候
現在假設 pi 這個 actor，他看到了某一個 state s
那如果在玩 Atari 遊戲的話，state s 是某一個畫面
看到某一個畫面，某一個 state s 的時候
接下來一直玩到遊戲結束
累積的 reward 的期望值有多大
accumulated reward 的 expectation 有多大
所以 V(pi) 它是一個 function
這個 function 它是吃一個 state
當作 input，然後它會 output 一個 scalar， 這個 scalar 代表說
現在 pi 這個 actor 它看到 state s 的時候
接下來預期到遊戲結束的時候，它可以得到多大的 value
我這邊就舉一個例子，假設你是玩 space invader 的話
也許這個 state 這個 s，這一個遊戲畫面
你的 V(pi) of s 會很大
因為接下來還有很多的怪可以殺， 所以你會得到很大的分數
一直到遊戲結束的時候，你仍然有很多的分數可以吃
那在這個 case，也許你得到的 V(pi)
就很小
因為一方面，剩下的怪也不多了
那再來就是現在因為那個防護罩 這個紅色的東西防護罩已經消失了
所以可能很快就會死掉
所以接下來得到預期的 reward，就不會太大
那這邊需要強調的一個點是說
當你在講這一個 critic 的時候， 你一定要注意，critic 都是綁一個 actor 的
就 critic 它並沒有辦法去憑空去 evaluate 一個 state 的好壞
而是它所 evaluate 的東西是
在 given 某一個 state 的時候， 假設我接下來互動的 actor 是 pi
那我會得到多少 reward，因為就算是給同樣的 state
你接下來的 pi 不一樣，你得到的 reward 也是不一樣的
舉例來說，在這個 case
雖然假設是一個正常的 pi
它可以殺很多怪，那假設他是一個很弱的 pi
他就站在原地不動，然後馬上就被射死了
那你得到的 V 還是很小
所以今天這個 critic output 值有多大，其實是取決於兩件事
一個是 state，另外一個其實是 actor
所以今天你的 critic 其實都要綁一個 actor
他是在衡量某一個 actor 的好壞
而不是 generally 衡量一個 state 的好壞
這邊有強調一下，你這個 critic output 是跟 actor 有關的
你的 state value 其實是 depend on 你的 actor
當你的 actor 變的時候
你的 state value function 的 output 其實也是會跟著改變的
再來問題就是，怎麼衡量這一個 state value function 呢？
怎麼衡量這一個 V(pi) of s 呢？
有兩種不同的作法
那等一下會講說，像這種 critic
它是怎麼演變成可以真的拿來採取 action，這是等一下要講
我們現在要先問的是怎麼 estimate 這些 critic
那怎麼 estimate V(pi) of s 呢
有兩個方向，一個適用 Monte-Carlo MC based 的方法
如果是 MC based 的方法，他非常的直覺
他怎麼直覺呢，他就是說
你就讓 actor 去跟環境做互動
你要量 actor 好不好， 你就讓 actor 去跟環境做互動，給 critic 看
然後，接下來 critic 就統計說， 這個 actor 如果看到 state sa
他接下來 accumulated reward，會有多大
如果它看到 state sb，他接下來 accumulated reward
會有多大
但是因為實際上， 你當然不可能把所有的 state 通通都掃過
不要忘了如果你是玩 Atari 遊戲的話，你的 state 可是 image 喔
你可是沒有辦法把所有的 state 通通掃過
所以實際上我們的 V(pi) of s，它是一個 network
對一個 network 來說，就算是 input state 是從來都沒有看過的
它也可以想辦法估測一個 value 的值
怎麼訓練這個 network 呢？
因為我們現在已經知道說，如果在 state sa
接下來的 accumulated reward 就是 Ga
也就是說，今天對這 value function 來說
如果 input 是 state sa
正確的 output 應該是 Ga
如果 input state sb
正確的 output 應該是 value Gb
所以在 training 的時候， 其實它就是一個 regression 的 problem
regression problem 大家應該都知道
胡亂 call 一下就有，它是一個 regression problem
你的 network 的 output 就是一個 value
你希望在 input sa 的時候，output value 跟 Ga 越近越好
input sb 的時候，output value 跟 Gb 越近越好
接下來把 network train 下去，就結束了
這是第一個方法，這是 MC based 的方法
那還有第二個方法是 Temporal-difference 的方法， 這個是 TD based 的方法
那 TD based 的方法是什麼意思呢？
在剛才那個 MC based 的方法
每次我們都要算 accumulated reward
也就是從某一個 state Sa，一直玩遊戲玩到遊戲結束的時候
你會得到的所有 reward 的總和，我在前一個投影片裡面
把它寫成 Ga 或 Gb
所以今天你要 apply MC based 的 approach
你必須至少把這個遊戲玩到結束
你才能夠估測 MC based 的 approach
但是有些遊戲非常的長
你要玩到遊戲結束才能夠 update network
你可能根本收集不到太多的資料，花的時間太長了
所以怎麼辦？有另外一種 TD based 的方法
TD based 的方法，不需要把遊戲玩到底
只要在遊戲的某一個情況，某一個 state st 的時候
採取 action at
得到 reward rt，跳到 state s(t+1)
就可以 apply TD 的方法
怎麼 apply TD 的方法呢？
這邊是基於以下這個式子，以下這個式子是說
我們知道說，在 state st
採取某一個 action
就是說我們現在有某一個 policy pi
假設我們現在用的是某一個 policy pi，在 state st
以後在 state st，它會採取 action at
給我們 reward rt
接下來進入 s(t+1)
那就告訴我們說，這個 state s(t+1) 的 value
跟 state st 的 value，他們的中間差了一項 rt
因為你把 s(t+1) 得到的 value
加上這邊得到的 reward rt
就會等於 st 得到的 value，st 得到的 value，就是 rt 的 value
加上 s(t+1) 的 value
st 的 value 就是 rt 加上 s(t+1) 的 value
有了這個式子以後，你在 training 的時候
你要做的事情並不是真的直接去估測 V
而是希望你得到的結果，你得到的這個 V
可以滿足這個式子
也就是說你 training 的時候
會是這樣 train 的，你把 st 丟到 network 裡面
因為 st 丟到 network 裡面會得到 V of st
你把 s(t+1) 丟到你的 value network 裡面
會得到 V of s(t+1)
這個式子告訴我們，V of st 減 V of s(t+1)
它應該值是 rt
V of st 減 V of s(t+1)，它得到的值應該是 rt
然後按照這樣的 loss，希望他們兩個相減跟 rt 越接近越好的 loss
train 下去，update V 的參數
你就可以把 V function learn 出來
這邊是比較一下 MC 跟 TD 之間的差別
那 MC 跟 TD 它們有什麼樣的差別呢？
MC 它最大的問題就是它的 various 很大
因為今天我們在玩遊戲的時候
它本身是有隨機性的
所以 Ga 本身你可以想成它其實是一個 random 的 variable
因為你每次同樣走到 sa 的時候
最後你得到的 Ga，其實是不一樣的
對不對，你看到同樣的 state sa
最後玩到遊戲結束的時候，因為遊戲本身是有隨機性的
你的玩遊戲的 model 本身搞不好也有隨機性
所以你每次得到的 Ga ，是不一樣的
那每一次得到 Ga 的差別，其實會很大
為什麼它會很大呢？ 因為 Ga 其實是很多個不同的 step 的 reward 的和
假設你每一個 step 都會得到一個 reward
Ga 是從 state sa 開始，一直玩到遊戲結束
每一個 timestamp reward 的和
那舉例來說，我在右上角就列一個式子是說， 假設本來只有 X
它的 various 是 var of X
但是你把某一個 variable 乘上 K 倍的時候
它的 various 就會變成原來的 K 平方
所以現在這個 Ga 的 variance
相較於某一個 state 的 reward
它會是比較大的，Ga 的 variance 是比較大的
Ga 的 variance 相較於某一個 state
你會得到的 reward variance 是比較大的
現在，如果說用 TD 的話呢？
用 TD 的話，你是要去 minimize 這樣的一個式子
在這中間會有隨機性的是 r
因為你在 st 就算你採取同一個 action
你得到的 reward 也不見得是一樣的
所以 r 其實也是一個 random variable
但這個 random variable 它的 variance
會比 Ga 還要小，因為 Ga 是很多 r 合起來
這邊只是某一個 r 而已，Ga 的 variance 會比較大
r 的 variance 會比較小
但是這邊你會遇到的一個問題是
你這個 V 不見得估的準
假設你的這個 V 估的是不準的
那你 apply 這個式子 learn 出來的結果，其實也會是不準的
所以今天 MC 跟 TD
它們是各有優劣，那等一下其實會講一個 MC 跟 TD 綜合的版本
今天其實 TD 的方法是比較常見的
MC 的方法其實是比較少用的
那這張圖是想要講一下
TD 跟 MC 的差異
這個圖想要說的是什麼呢？這個圖想要說的是
假設我們現在有某一個 critic
它去觀察某一個 policy pi
跟環境互動搭著 episode 的結果
有一個 actor pi 它去跟環境互動了 8 次 得到了 8 次玩遊戲的結果是這個樣子
接下來我們要這個 critic 去估測 state 的 value
那如果我們看 sb 這個 state 它的 value 是多少
sb 這個 state 在 8場遊戲裡面都有經歷過
然後在這 8 場遊戲裡面，其中有 6 場得到 reward 1
再有兩場得到 reward 0
所以如果你是要算期望值的話
state sb，就看到 state sb 以後
得到的 reward，一直到遊戲結束的時候
得到的 accumulated reward 期望值是 3/4，非常直覺
但是，不直覺的地方是說
sa 期望的 reward 到底應該是多少呢？
這邊其實有兩個可能的答案
一個是 0，一個是 3/4
為什麼有兩個可能的答案呢？
這取決於你用 MC 還是 TD
你用 MC 跟用 TD，你算出來的結果是不一樣的
假如你用 MC 的話
你用 MC 的話，你會發現說
這個 sa 就出現一次，它就出現一次
看到 sa 這個 state
接下來 accumulated reward 是多少，就是 0
所以今天 sa 它的 expected reward 就是 0
但是如果你今天去看 TD 的話
TD 在計算的時候，它是要 update 下面這個式子
下面這個式子想要說的事情是
因為我們在 state sa 得到 reward r=0 以後
跳到 state sb
所以 state sb 的 reward
會等於 state sb 的 reward
加上在 state sa 它跳到 state sb 的時候
可能得到的 reward r
而這個可能得到的 reward r，它的值是多少？它的值是 0
而 sb expected reward 是多少呢？它的 reward 是 3/4
那 sa 呢它的 reward 應該是 3/4
有趣的地方是用 MC 跟TD 你估出來的結果
其實很有可能是不一樣的
就算今天你的 critic observed 到一樣的 training data
它最後估出來的結果，也不見得會是一樣
那為什麼會這樣呢？
你可能問說，那一個比較對呢？
其實就都對，對不對
因為今天在 sa 這邊
今天在第一個 trajectory
sa 它得到 reward 0 以後，再跳到 sb 也得到 reward 0
這邊有兩個可能，一個可能是
sa 它就是一個帶賽的 state
所以只要有看到 sa 以後
sb 就會拿不到 reward
有可能 sa 其實影響了 sb
如果是用 MC 的算法的話
它就會考慮這件事， 它會把 sa 影響 sb 這件事，考慮進去
所以看到 sa 以後，接下來 sb 就得不到 reward
所以看到 sa 以後，期望的 reward 是 0
但是今天看到 sa 以後， sb 的 reward 是 0 這件事有可能只是一個巧合
就並不是 sa 所造成
並不是因為 sa 它是一個帶賽的 reward
而是因為說，sb 有時候就是會得到 reward 0
這只是單純運氣的問題， 其實平常 sb 它會得到 reward 期望值是 3/4
跟 sa 是完全沒有關係的
所以 sa 假設之後會跳到 sb
那其實得到的 reward 按照 TD 來算
應該是 3/4
所以不同的方法，它考慮了不同的假設
最後你其實是會得到不同的運算結果的
那接下來我們要講的是另外一種 critic
這種 critic 叫做 Q function
它又叫做 state-action value function
那我們剛才看到的那一個 state function，它的 input
就是一個 state
它是根據 state 去計算出，看到這個 state 以後的 expected accumulated reward 是多少
那這個 state-action value function 它的 input 不是 state
它是一個 state 跟action 的 pair
它的意思是說，在某一個 state，採取某一個 action
接下來假設我們都使用 actor pi
得到的 accumulated reward 它的期望值有多大
那在講這個 Q-function 的時候
有一個會需要非常注意的問題是
今天這個 actor pi，在看到 state s 的時候
它採取的 action，不一定是 a
大家了解我的意思嗎？
Q function 的假設是說
假設在 state s，強制採取 action a
不管你現在考慮的這個 actor pi， 它會不會採取 action a，這不重要
在 state s，強制採取 action a
接下來，都用 actor pi 繼續玩下去
就只有在 state s，我們才強制一定要採取 action a
接下來就進入自動模式，讓 actor pi 繼續玩下去，得到的 expected reward
才是 Q of sa
那假設你讓 pi 看到這個 state 的時候， 它不見得要去採取 a
我們只是強制手動讓它一定要去採取 action a
那 Q function 有兩種寫法
一種寫法是你 input 就是 state 跟 action
那 output 就是一個 scalar，就跟那個 value function 是一樣
那還有另外一種寫法，也許是比較常見的寫法是這樣
你 input 一個 state s
接下來你會 output 好幾個 value
假設你的 actor 是 discrete 的
假設你 action 是 discrete 的
你 action 就只有 3 個可能，往左往右或是開火
那今天你的這個 Q function output 的 3 個 values
就分別代表假設，a 是向左的時候的 Q value
a 是向右的時候的 Q value
還有 a 是開火的時候的 Q value
那你要注意的事情是
像這樣的 function 只有 discrete action 才能夠使用
如果你的 action 是無法窮舉的
你只能夠用左邊這個式子，不能夠用右邊這個式子
這個是文獻上的結果，但是在作業 4-1 裡面
其實作業 4-2 不是要玩這個，是玩打磚塊
但如果是作業 4-1 碰的遊戲
你去 estimate Q function 的話
看到的結果可能會像是這個樣子
這是什麼意思呢？他說假設現在在這個 state
上面這個畫面就是 state
在這個 state，那我們有 3 個 actions
3 個 action 就是原地不動，向上，向下
那假設是在這個 state
不管是採取這個 action，這個 action，還是這個 action
最後到遊戲結束的時候，得到的 expected reward
其實都差不了多少
因為球在這個地方，就算是你向下
接下來你其實應該還來的急救
所以今天不管是採取哪一個 action，就差不了太多
但假設現在這個球
這個乒乓球它已經反彈到很接近邊緣的地方
這個時候你採取向上
你才能得到 positive 的 reward，才接的到球
如果你是站在原地不動或向下的話
接下來你都會 miss 掉這個球
你得到的 reward 就會是負的
這個 case 也是一樣，球很近了，所以就要向上
接下來，球被反彈回去，這時候採取那個 action
就都都沒有差了，這個是 state-action value 的一個例子
是在文獻上截下來的
大家應該都知道說，deep reinforcement learning 最早的
受到大家重視注意的一篇paper 就是
deep mind 發表在 Nature 上的那個 paper
就是用 DQN 玩 Atari 可以痛電人類
那個是那篇 paper 上的一個圖
接下來要講的是說
雖然表面上我們 learn 一個 Q function
他只能拿來評估某一個 actor pi 的好壞
但是實際上只要有了這個 Q function
我們就可以做 reinforcement learning
其實有這個 Q function
我們就可以決定要採取哪一個 action
他的大原則是這樣
假設你有一個初始的 actor，也許一開始很爛， 隨機的也沒有關係
初始的 actor 叫做 pi
那這個 pi 跟環境互動
會 collect data，接下來你 learn 一個 pi 這個 actor 的 Q value
你去衡量一下 pi 這個 actor
他在某一個 state 強制採取某一個 action
接下來用 pi 這個 policy 會得到的 expected reward
那你可以用 TD 也可以用 MC 都是可以的
你 learn 出一個 Q function 以後
等一下我們接下來會細講的一個神奇的地方就是
只要認得出某一個 policy pi 的 Q function
就保證你可以找到一個新的 policy
這個 policy 就做 pi prime
這一個 policy pi prime
他一定會比原來的 policy pi 還要好
那等一下會定義說，什麼叫做好
所以這邊神奇的地方是
假設你只要有一個 Q function
你有某一個 policy pi
你根據那個 policy pi learn 出那 policy pi 的 Q function
接下來保證你可以找到一個新的 policy 叫做 pi prime
它一定會比 pi 還要好
你今天找到一個新的 pi prime，一定會比 pi 還要好以後
你把原來的 pi 用 pi prime 取代掉
再去找它的 Q，得到新的 Q 以後
再去找一個更好的 policy
然後這個循環一直下去，你的 policy 就會越來越好
今天這邊我們講完這一頁我們就下課
這一頁要講的是什麼呢？ 這一頁就是要講我們剛才講的到底是什麼
首先第一個要定義的是
什麼叫做比較好？我們說 pi prime 一定會比 pi 還要好
什麼叫做好呢？這邊所謂好的意思是說
對所有可能的 state s 而言
對同一個 state s 而言
pi 的 value function 一定會小於 pi prime 的 value function
也就是說我們走到同一個 state s 的時候
如果拿 pi 繼續跟環境互動下去
我們得到的 reward 一定會小於用 pi prime 跟環境互動下去得到的 reward
所以今天不管在哪一個 state
你用 pi prime 去做 interaction
你得到的 expected reward 一定會比較大
所以 pi prime 是比 pi 還要好的一個 policy
那有了這個 Q 以後
怎麼找這個 pi prime 呢？
這邊的構想非常的簡單
事實上這個 pi prime 是什麼？
這個 pi prime 就是， 如果你根據以下的這個式子去決定你的 action
根據以下的這個式子去決定你的 action 的步驟叫做 pi prime 的話
那這個 pi prime 一定會比 pi 還要好，在下一頁會有證明
這個是什麼意思呢，這個意思是說
假設你已經 learn 出 pi 的 Q function
今天在某一個 state s
你把所有可能的 action a，都一一帶入這個 Q function
看看說那一個 a
可以讓 Q function 的 value 最大
那這一個 action，就是 pi prime 會採取的 action
那這邊要注意一下，今天 given 這個 state s
我們剛才有講過 Q function 的定義
given 這個 state s
你的 policy pi，並不一定會採取 action a
今天是 given 某一個 state s
強制採取 action a，用 pi 繼續互動下去
得到的 expected reward，才是這個 Q function 的定義
所以在 state s 裡面
不一定會採取 action a
我們強調一次，在 state s 裡面，不一定會採取 action a
今天假設我們用這一個 pi prime
它在 state s 採取 action a
跟 pi 所謂採取 action
是不一定會一樣的
然後 pi prime 所採取的 action
會讓他得到比較大的 reward
所以實際上，根本就沒有所謂一個 policy 叫做 pi prime
這個 pi prime 其實就是用 Q function 推出來的
所以並沒有另外一個 network 決定 pi prime 怎麼 interaction
我們只要 Q 就好，有 Q 就可以找出 pi prime
但是這邊有另外一個問題是我們等一下會解決的就是
在這邊要解一個 Arg Max 的 problem
所以 a 如果是 continuous 的就會有問題
如果是 discrete 的
a 只有 3 個選項，一個一個帶進去， 看誰的 Q 最大，沒有問題
但如果是 continuous 要解 Arg Max problem
你就會有問題，但這個是之後才會解決的
下一頁投影片想要跟大家講的是說
為什麼用 Q theta 這個 Q function
所決定出來的 pi prime
一定會比 pi 還要好
所以下一頁是要證這件事
那下一頁的證明，假設你覺得你沒有辦法 follow 的話
其實就算了，就只要記得這個結果
下一頁投影片要證的就是這樣，假設現在呢
我們有一個 policy 叫做 pi prime，它是由 Q(pi) 決定的
我們要證說，對所有的 state s 而言
V(pi prime) 一定會比 V(pi) 還要大
這件事怎麼證呢？
這邊的式子是這樣，我們先把 V(pi) 寫出來
那 V(pi) 這個式子，會等於 Q(pi) of (s, pi of s)
假設你在 state s 這個地方，你 follow pi 這個 actor
它會採取的 action，也就是 pi of s
那你算出來的 Q(pi) 會等於 V(pi)
之前 in general 而言，Q(pi) 不見得等於 V(pi) 是因為這一個 action 不見得是 pi of s
但這個 action 如果是 pi of s 的話，Q(pi) 是等於 V(pi) 的
今天 Q(pi) of (s, pi of s) 一定會小於等於 q(pi) of (s, a) a 取最大的那一個
對不對，因為這邊是某一個 action
這邊是所有 action 裡面可以讓 Q 最大的那個 action
所以今天這一項一定會比它大
那我們知道說這一項是什麼，這一項就是 Q(pi) of (s, a)
然後 a 是什麼，a 就是 pi prime of s
因為今天 pi prime of s，它 output 的 a， 就是可以讓 Q(pi) of s 最大的那一個
所以今天這個式子可以寫成 Q(pi) of (s, pi prime of s)
這邊我們就知道 V(pi) of s
它一定小於等於 Q(pi) of (s, pi prime of s)
也就是說你在某一個 state
如果你按照 policy pi，一直做下去
你得到的 reward 一定會小於等於你在現在這個 state s
你故意不按照 pi 所給你指示的方向
你故意按照 pi prime 的方向走一步，但之後
只有第一步是按照 pi prime 的方向走
只有在 state s 這個地方，你才按照 pi prime 的方向走
pi prime 的指示走，但接下來你就按照 pi 的指示走
雖然只有一步之差， 但是我們可以按照上面這個式子知道說
這個時候你得到的 reward，只有一步之差
你得到的 reward 一定會比完全 follow pi 得到的 reward 還要大
那接下來，eventually，你想要證的東西就是
這一個 Q(pi) of (s, pi prime of s)
會小於等於 V(pi prime) of s
也就是說，只有一步之差，你會得到比較大的 reward
但假設每步都是不一樣的， 每步通通都是 follow pi prime 而不是 pi 的話
那你得到的 reward 一定會更大
就這樣，直覺上想起來是這樣子的
如果你要用數學式把它寫出來的話，略嫌麻煩，但也沒有很難
只是比較繁瑣而已，怎麼寫呢？你可以這樣寫
Q pi 這個式子，它的意思就是說
我們在 state st
我們會採取 action at
接下來我們會得到 reward r(t+1)
然後跳到 state s(t+1)
這邊有一個地方我覺得我寫得不太好，我覺得這邊應該寫成 rt 跟我之前的 notation 感覺比較一致
但這邊寫成了 r(t+1)，其實這都是可以的， 在文獻上有時候有人會說
在 state st 採取 action at 得到 reward r(t+1)， 有人會寫成 rt，但意思其實都是一樣的
在 state s，按照 pi prime 採取某一個 action at，得到 reward r(t+1)
然後接下來跳到 state s(t+1)
然後我們這邊是 state s(t+1)
根據 pi 這個 actor 所估出來的 value
上面這個式子，等於下面這個式子
這邊要取一個期望值
因為在同樣的 state 採取同樣的 action
你得到的 reward 還有會跳到 state 不見得是一樣， 所以這邊需要取一個期望值
這一項會小於等於下面這個式子， 為什麼這一項會小於等於下面這個式子呢？
因為我們上面已經講過說 V(pi) of s
一定小於 Q(pi) of (s, pi prime)
也就是這邊 V(pi) of s(t+1)
一定會小於等於 Q(pi) of ( s(t+1), pi(prime) of s(t+1) )
也就是說，現在你一直 follow pi
跟某一步 follow pi prime，接下來都 follow pi，比起來
某一步 follow pi prime 得到的 reward 是比較大的
這一個式子就可以寫成
就可以寫成，下面這個式子
因為 Q(pi) 這個東西可以寫成 r(t+2) + s(t+2) 的 value
然後接下來你再把這個式子
你再把 V(pi) 小於等於 Q(pi) of (s, pi prime of s) 這件事情
再帶進去，然後一直算算算到底
算到 episode 結束
那你就知道說 V(pi) of s 會小於等於 V(pi prime) of s
反正這邊假設你沒有辦法 follow 的話，總是想要告訴你的事情是說
你可以 estimate 某一個 policy 的 Q function
接下來你就一定可以找到另外一個 policy 叫做 pi prime
它一定比原來的 policy 還要更好
我們講一下 Target Network， 我們講一下接下來在 Q learning 裡面
typically 你一定會用到的 tip
有幾個 tip，第一個你會用一個東西叫做 target network
什麼意思呢？我們在 learn Q function 的時候
你也會用到 TD 的概念
那怎麼用 TD 的概念呢？
就是說你現在收集到一個 data， 是說在 state st，你採取 action at 以後
你得到 reward rt，然後跳到 state s(t+1)
然後今天根據這個 Q function 你會知道說
Q(pi) of (st, at) 跟 Q(pi) of ( s(t+1), pi of s(t+1) )， 他們中間差了一項就是 rt
所以你在 learn 的時候，你會說我們有 Q function
input st, at 得到的 value
跟 input s(t+1), pi(s(t+1) 得到的 value 中間
我們希望它差了一個 rt， 這跟剛才講的 TD 的概念是一樣的
但是實際上在 learn 的時候，你會發現說
這樣 in general 而言這樣的一個 function 並不好 learn
為什麼，因為假設你說這是一個 regression 的 problem
這是你 network 的 output，這是你的 target，你會發現你的 target 是會動的
當然你要 implement 這樣的 training 其實也沒有問題
對不對，就是你在做 back propagation 的時候， 這個 model 它的參數要不要 update
這個 model 參數也會被 update，當然是同一個 model
所以你會把兩個 update 的結果加在一起
他們是同一個 model， 所以兩個 update 的結果會加在一起
但是實際上在做的時候，你的 training 會變得不太穩定
因為假設你把這個當作你 model 的 output， 這個當作 target 的話
你會變成說你要去 fit 的 target，它是一直在變的
這種一直在變的 target 的 training 其實是不太好 train 的
所以實際上怎麼做呢？實際上你會把其中一個 Q
通常是你就選擇下面這個 Q
把它固定住
也就是說你在 training 的時候，你並不 update 這個 Q 的參數
你只 update 左邊這個 Q 的參數
而右邊這個 Q 的參數，它會被固定住
我們叫它 target network， 它負責產生 target，所以叫做 target network
因為 target network 是固定的
所以你現在得到的 target，也就是 rt 加上
Q(pi) of ( s(t+1), pi of s(t+1) ) 的值也會是固定的
那我們只調左邊這個 network 的參數
那假設因為 target network 是固定的
我們只調左邊 network 的參數
它就變成是一個 regression 的 problem
我們希望我們 model 的 output，它的值跟你的目標越接近越好
你會 minimize 它的 mean square error
那你會 minimize 它們 L2 的 distance
那這個東西就是 regression
在實作上呢，你會把這個 Q update 好幾次以後
再去把這個 target network 用 update 過的 Q
去把它替換掉
你在 train 的時候，先update 它好幾次
然後再把它替換掉
但它們兩個不要一起動，他們兩個一起動的話， 你的結果會很容易壞掉
今天本來一開始這兩個 network 是一樣的
然後接下來在 train 的時候，你會把它 fix 住
然後你只調這個，你在做 gradient decent 的時候
只調左邊這個 network 的參數，那你可能 update 100 次以後
才把這個參數，複製到右邊去，把它蓋過去
把它蓋過去以後，你這個 target 的 value，就變了
就好像說你今天本來在做一個 regression 的 problem
那你 train... 把這個 regression problem 的 loss 壓下去以後
接下來你把這邊的參數把它 copy 過去以後
你的 target 就變掉了
你 output 的 target 就變掉了， 那你接下來就要重新再 train
它其實不會變成 0，為什麼呢？因為首先它們的 input 是不一樣
同樣的 function，這邊的 input 是 st 跟 at
這邊 input 是 s(t+1) 跟在 s(t+1) 會採取的 action pi of s(t+1)
因為 input 不一樣
所以它 output 的值會不一樣
所以光這一項跟這一項的值
就會不一樣，今天再加上 rt
所以他們的值就會更不一樣
但是你希望說你會把這兩項的值把它拉近
如果大家 ok 的話，這是第一個你會用到的 tip
第二個會用到的 tip 是 Exploration
我們剛才講說，當我們使用 Q function 的時候
我們的 policy 是怎麼樣，我們的 policy 完全 depend on 那個 Q function
看說 given 某一個 state，你就窮舉所有的 a， 看那個 a 可以讓 Q value 最大
它就是你採取的 policy
它就是採取的 action
那其實這個跟 policy gradient 不一樣，在做 policy gradient 的時候
我們的 output 其實是 stochastic 的
對不對，我們 output 一個 action 的 distribution
根據這個 action 的 distribution 去做 sample， 所以在 policy gradient 裡面
你每次採取的 action 是不一樣的，是有隨機性的
那像這種 Q function， 如果你採取的 action 總是固定的會有什麼問題呢？
你會遇到的問題就是，這不是一個好的收集 data 的方式
為什麼這不是一個好的收集 data 的方式呢？
因為假設我們今天真的要估某一個，我這邊應該要
就是說某一個 state，你可以採取 action a1, a2, a3
我這邊應該寫 a1, a2, a3，但我忘了加
今天你要估測在某一個 state 採取某一個 action 會得到的 Q value
你一定要在那一個 state，採取過那一個 action
你才估得出它的 value 對不對
如果你沒有在那個 state 採取過那個 action
你其實估不出那個 value 的
當然如果是用 deep 的 network，就你的 Q function 其實是一個 network，這種情形可能會比較沒有那麼嚴重
但是 in general 而言，假設你 Q function 是一個 table
沒有看過的 state-action pair，它就是估不出值來
當然 network 也是會有一樣的問題就是， 只是沒有那麼嚴重，但也會有一樣的問題
所以今天假設你在某一個 state，action a1, a2, a3 你都沒有採取過
那你估出來的 (s, a1) (s, a2) (s, a3) 的 Q value
可能就都是一樣的，就都是一個初始值，比如說 0
但是今天假設你在 state s
你 sample 過某一個 action a2 了
那 sample 到某一個 action a2
它得到的值是 positive 的 reward
那現在 Q of (s, a2)
就會比其他的 action 都要好
那我們說今天在採取 action 的時候， 就看說誰的 Q value 最大
就採取誰，所以之後你永遠都只會 sample 到 a2
其他的 action 就再也不會被做了，所以今天就會有問題
就好像說你進去一個餐廳吃飯， 餐廳都有一個菜單，那其實你都很難選
你今天點了某一個東西以後，假說點了某一樣東西， 比如說椒麻雞，你覺得還可以
接下來你每次去，就都會點椒麻雞，再也不會點別的東西了，那你就不知道說別的東西是不是會比椒麻雞好吃
這個是一樣的問題
那如果你今天沒有好的 exploration 的話， 你在 training 的時候就會遇到這種問題
舉一個實際的例子， 假設你今天是用 Q learning 來玩比如說slither.io
在玩 slither.io 你會有一個蛇
然後它在環境裡面就走來走去， 然後就吃到星星，它就加分
那今天假設這個遊戲一開始，它採取往上走
然後就吃到那個星星，它就得到分數，它就知道說往上走是 positive
接下來他就再也不會採取往上走以外的 action 了
所以接下來就會變成每次遊戲一開始
它就往上衝，然後就死掉，再也做不了別的事
所以今天需要有 exploration 的機制
需要讓 machine 知道說，雖然 a2
根據之前 sample 的結果，好像是不錯的，但你至少偶爾也試一下 a1 跟 a3，搞不好他們更好也說不定
有兩個方法解這個問題，一個是 Epsilon Greedy
Epsilon Greedy 的意思是說，我們有
1-epsilon 的機率，通常 epsilon 就設一個很小的值， 1-epsilon 可能是 90%
也就是 90% 的機率，完全按照 Q function 來決定 action
但是你有 10% 的機率是隨機的
通常在實作上 epsilon 會隨著時間遞減
也就是在最開始的時候
因為還不知道那個 action 是比較好的
所以你會花比較大的力氣在做 exploration
那接下來隨著 training 的次數越來越多
已經比較確定說哪一個 Q 是比較好的
你就會減少你的 exploration，你會把 epsilon 的值變小
主要根據 Q function 來決定你的 action
比較少做 random，這是 Epsilon Greedy
那還有另外一個方法叫做 Boltzmann Exploration
這個方法就比較像是 policy gradient
在 policy gradient 裡面我們說 network 的 output 是一個
根據 probability，根據 expect(ed) action space 上面的一個 probability distribution
再根據 probability distribution 去做 sample
那其實你也可以根據 Q value 去定一個 probability distribution
你可以說，假設某一個 action，它的 Q value 越大，代表它越好
那我們採取這個 action 的機率就越高
但是某一個 action 它的 Q value 小
不代表我們不能 try try 看它好不好用
所以我們有時候也要 try try 那些 Q value 比較差的 action
那怎麼做呢，因為 Q value 它是有正有負的
所以你要把它弄成一個機率，你可能就先取 exponential
然後再做 normalize
然後把 Q of (s, a) exponential， 再做 normalize 以後的這個機率
就當作是你在決定 action 的時候 sample 的機率
Q 一開始嗎？其實在實作上，你那個 Q 是一個 network
所以你有點難知道說， 今天在一開始的時候 network 的 output
到底會長怎麼樣子，但是其實你可以猜測說， 假設你一開始沒有任何的 training data
你的參數是隨機的，那 given 某一個 state s
你的不同的 a output 的值
可能就是差不多的
所以一開始 Q of (s, a) 應該會傾向於是 uniform
也就是在一開始的時候，你這個 probability distribution 算出來
它可能是比較 uniform 的
假設今天你的值通通都是 1
你的值通通都是 2，你的值通通都是 100
靠這個式子，以後算出來的結果，會是一樣的
那還有第三個你會用的 tip，這個 tip 叫做 replay buffer
replay buffer 的意思是說
現在我們會有某一個 policy pi 去跟環境做互動，然後它會去收集 data
我們會把所有的 data 放到一個 buffer 裡面
那 buffer 裡面就排了很多 data，那你 buffer 設比如說
5 萬，這樣它裡面可以存 5 萬筆資料
每一筆資料是什麼？每一筆資料就是記得說
我們之前在某一個 state st
採取某一個 action at
接下來我們得到的 reward rt
然後接下來跳到 state s(t+1)，某一筆資料，就是這樣
那你用 pi 去跟環境互動很多次
把所有收集到的資料通通都放到這個 replay buffer 裡面
這邊要注意的事情是
這個 replay buffer 它裡面的 experience
可能是來自於不同的 policy
就你每次拿 pi 去跟環境互動的時候，你可能只互動 10,000 次
然後接下來你就更新你的 pi 了
但是你的這個 buffer 裡面可以放 5 萬筆資料
所以那 5 萬筆資料，它們可能是來自於不同的 policy
那這個 buffer 只有在它裝滿的時候
才會把舊的資料丟掉
所以這個 buffer 裡面它其實裝了很多不同的 policy
所計算出來的不同的 policy 的 experiences
接下來你有了這個 buffer 以後，你做的事情
你是怎麼 train 這個 Q 的 model 呢？
你是怎麼估 Q 的 function，你的做法是這樣
你會 iterative 去train 這個 Q function，在每一個 iteration 裡面
你從這個 buffer 裡面，隨機挑一個 batch 出來
就跟一般的 network training 一樣，你從那個 training data set 裡面，去挑一個 batch 出來
你去 sample 一個 batch 出來，裡面有一把的 experiences
根據這把 experiences 去 update 你的 Q function
就跟我們剛才講那個 TD learning 要有一個 target network 是一樣的
你去 sample 一堆batch
sample 一個 batch 的 data，sample 一堆 experiences
然後再去 update 你的 Q function
這邊其實有一個東西你可以稍微想一下，你會發現說
實際上當我們這麼做的時候， 它變成了一個 off policy 的做法
對不對，因為本來我們的 Q 是要觀察 pi 這個 action 它的 value
但實際上存在你的 replay buffer 裡面的這些experiences
不是通通來自於 pi
對不對，有些是過去其他的 pi
所遺留下來的 experience
因為你不會拿某一個 pi 就把整個 buffer 裝滿
然後拿去測 Q function，這個 pi 只是 sample 一些 data
塞到那個 buffer 裡面去，然後接下來就讓 Q 去 train
所以 Q 在 sample 的時候， 它會 sample 到過去的一些資料
但是這麼做到底有什麼好處呢？
這麼做有兩個好處，第一個好處
其實在做 reinforcement learning 的時候， 往往最花時間的 step
是在跟環境做互動
train network 反而是比較快的
因為你用 GPU train 其實很快， 真正花時間的往往是在跟環境做互動
今天用 replay buffer，你可以減少跟環境做互動的次數
因為今天你在做 training 的時候，你的 experience 不需要通通來自於某一個 policy
一些過去的 policy 他所得到的 experience
可以放在 buffer 裡面被使用很多次
被反覆的再利用
這樣讓你的 sample 到 experience 的利用是比較 efficient
那還有另外一個理由是
你記不記得我們說在 train network 的時候
其實我們希望一個 batch 裡面的 data
越 diverse 越好
如果你的 batch 裡面的 data 通通都是同樣性質的
你 train 下去，其實是容易壞掉的
對不對，不知道大家有沒有這樣子的經驗
如果你 batch 裡面都是一樣的 data，你 train 的時候，performance 會比較差
我們希望 batch data 越 diverse 越好
那如果你今天，你的這個 buffer 裡面的那些 experience
它通通來自於不同的 policy 的話
那你得到的結果
你 sample 到的一個 batch 裡面的 data
會是比較 diverse 的
但是接下來你會問的一個問題是
我們明明是要觀察 pi 的 value， 我們要量的明明是 pi 的 value 啊
裡面混雜了一些不是 pi 的 experience
到底有沒有關係？
一個很簡單的解釋，也許這些不同的 pi 也沒差那麼多
所以也沒有關係，但是你仔細想一項
這一件事情其實是沒有關係的
這並不是因為過去的 pi 跟現在的 pi 很像， 就算過去的 pi 沒有很像
其實也是沒有關係的
這個留給大家回去想一下，為什麼會這個樣子
今天主要的原因是因為， 我們並不是去 sample 一個 trajectory
我們只 sample 了一筆 experience
所以跟我們是不是 off policy 這件事是沒有關係的， 就算是 off-policy
就算是這些 experience 不是來自於 pi
我們其實還是可以拿這些experience
來估測 Q(pi) of (s, a)
這件事有點難解釋，不過你就記得說
replay buffer 這招其實是在理論上也是沒有問題的
那這個就是 typical 的一般的正常的 Q learning 演算法
這個演算法是這樣
我們說我們需要一個 target network， 先開始 initialize 的時候
你 initialize 2 個 network，一個 是 Q，一個是 Q hat
那其實 Q hat 就等於 Q，一開始這個 target Q-network，跟你原來的 Q network 是一樣的
那在每一個 episode，就你拿你的 agent
你拿你的 actor 去跟環境做互動
那在每一次互動的過程中
你都會得到一個 state st
一個遊戲的畫面
那你會採取某一個 action at
那怎麼知道採取那一個 action at 呢？ 你就根據你現在的 Q-function
但是記得你要有 exploration 的機制
比如說你用 Boltzmann exploration 或是 Epsilon Greedy的 exploration
也有一點 exploration 的機制
那接下來你得到 reward rt，然後跳到 state s(t+1)
所以現在 collect 到一筆 data，這筆 data 是 st, at ,rt, s(t+1)
結果這筆 data 就塞到你的 buffer 裡面去
那如果 buffer 滿的話， 你就再把一些舊有的資料再把它丟掉
那接下來你就從你的 buffer 裡面去 sample data
那你 sample 到的是 si, ai, ri, s(i+1)
這筆 data 跟你剛放進去的，不見得是同一筆
懂嗎？你把這筆 data 塞到 buffer 裡面
再到 buffer 裡面去抽 data，抽出來並不是同一筆
你只是可能抽到一個舊的，也是有可能的
那這邊另外要注意的是，我這邊notation 不太好寫
其實你 sample 出來不是一筆 data
你 sample 出來的是一個 batch 的 data
你 sample 一個 batch 出來，sample 一把 experiences 出來
你 sample 這一把 experience 以後，接下來你要做的事情就是
計算你的 target
根據你 sample 出來的 data
假設你 sample 出這麼一筆 data
根據這筆 data 去算你的 target
你的 target 是什麼呢？
target 記得要用 target network，也就是 Q hat 來算
我們用 Q hat 來代表 target network
好那 target 是多少呢？ target 就是 ri 加上
Q hat of (s(i+1), a)，a 是什麼？a 就是
看說現在哪一個 a，可以讓 Q hat 的值最大
你就選那一個 a，因為我們在這邊，在 state s(i+1)
會採取的 action a
其實就是那個可以讓 Q value 的值最大的 那一個 a
接下來我們要 update Q 的值
那就把它當作一個 regression 的 problem
希望 Q of (si, ai) 跟你的 target 越接近越好
然後今天假設這個 update 已經 update 了某一個數目的次
比如說 c 次，你就設一個 100 c = 100， 那你就把 Q hat 設成 Q，就這樣
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
