好那我
好那我們接下來要講的是一些
跟 generalization 有關的 indicator
那我等下下一頁會告訴你說我們到底要講什麼
反正我們現在知道這樣子
其實 network 的 capacity 非常的巨大
所以今天在一個 network 它所訂出來的這個function set 裡面
有很多 function 都可以達到 training error 是 0 這件事情
那我們今天要做到 generalization
意思就是說
我們要從這些 training error 都是 0 的function 裡面
挑出一個弄到 testing set 上 apply 到 testing set 上
它的這個正確率會最高
它的 generalization gap 最小的
它最不會 overfitting 的那個 function
現在的問題就是 怎麼挑出那個 function
就好像是說 現在你只有 4 個點
如果你做一個 regression 的話 只有 4 個點
你可以用藍色的線來 fit 這 4 個點
也可以用紅色的線來 fit 這 4 個點
它們的 error 都是 0
但是我們都知道說 我們要挑藍色的線
不要挑紅色的線
紅色的線有可能是 overfitting
藍色的線比較可能會給我們 generalize 的結果
那在 deep learning 上也是一樣
這個 function 也有 zero 的 error
到底要挑哪一個 function
才能夠給我們最好的 generalization 的結果
當然有人可能會問說
就算能夠
發現說有哪些 indicator 給我們最好的 generalization 的結果
那我們搞不好可以 有一個最簡單的可以知道說
這兩個 network 誰比較不會 overfitting 的方法
怎麼做呢 一秒鐘弄個 validation set 結束這樣
就是說 你現在有這個 network 它的 error 是 0
這個 network 也是 error 是 0
那你想要知道說誰的 generalization 能力比較好
你就弄一個 development set 或一個叫做 validation set
是 training 的時候沒有看過的
然後在那個 validation set 上
去測這兩個 network 的正取錯誤率 那你就知道說
誰是 overfitting 誰的 performance 比較好了
所以為什麼我們要找這些 indicator 呢
有另外一個重要的理由是
當我們找出這些 indicator
我們知道說
一個 network 有什麼樣的特性的時候
它比較不容易 overfitting 的話
我們在 training 的時候
就可以把這些 indicator 把它塞進去
舉例來說 我們知道說在做 regularization 的時候
我們把 ** function 裡面再加一個 regularization 的 term
因為我們知道說
今天如果一組參數它的 norm
可能 one norm 或比較小
two norm 比較小的話
它可能比較能夠 generalize
所以我們就可以直接把它塞到 objective function 裡面
讓我們找到一個比較能夠 generalize 的 solution
那今天假設我們找得到 知道說 這個
在 network 上代表 generization 能力的 indicator 是什麼
我們就可以同樣把這件事情塞到 network 裡面
塞到 network train 的過程裡面
讓我們 train 出比較能夠 generalize 的 network
那等下會跟大家分享兩個知名的 indicator
一個叫做 sharpness 一個叫做 sensitivity
好在講之前 我們先跟大家 我們剛剛才在
上一堂課看到一個很驚人的結果就是
給 network random 的 label
它可以硬是全部***
real 的 label 當然它也可以得到正確的答案
不管是給它 real 的 label 還是給它 random 的 label
它都可以 train 出 100% 的正確率
它都可以 train 出 0% 的 error rate
那但是實際上如果你把那個 network 的參數 拿出來分析
會發現說 它們的參數其實看起來是非常不一樣的
舉例來說這是 CIFAR10 上面第一個 hidden layer 的參數
那其中一個 case 是用 real 的 label train
另外一個 case 是用 random 的 label train
好那來猜一猜 哪邊是 real label 哪邊是 random label 這樣
憑著你的常識和直覺
你覺得哪邊是 real label 哪邊是 random label
你覺得左邊這個比較像是 random label train 出來的 network 舉手一下
好手放下
你覺得右邊這個比較像是 random network train 出來的舉手一下
好沒有 其實這個不是計中計就跟大家猜的一樣
就是 這個就是用 random label train 出來的
所以跟我們直覺想起來是一樣的
這個 filter 看起來比較複雜 比較奇怪
這個 filter 看起來比較 make sense
裡面有很多感覺很有用的 pattern 比如說斜線 直線等等
所以左邊這個是用 random label train 出來的
右邊這個是用 real label
看起來和大家的直覺是一樣的
所以今天一個 network
它是在一個非常 overfitting 的狀況
還是沒有 overfitting 的狀況
它看起來可能是不一樣的
我們要找出說到底什麼樣的特徵代表一個 network
它有沒有 overfitting
好這個是另外一個實驗結果 這個實驗結果也是做一下說
random label 然後這個五條不同顏色的線
我們等下先看右邊這張圖
這五條不同顏色的線
代表用了不同程度的不同量的 random label
所以黃色這條線代表說所有的 data 都是對的
所有 data 都是對的
然後 20% random 40% random 60% random 一直到 80% random
好那你會發現說
雖然這 4 個 network 在 training set 上
你 train 到最後 錯誤率都是 0
它們都是一樣好
在 training set 上你看起來是一樣好的
但實際上這五個 network
它們 train 出來的那個 function 是不太一樣
那右邊這張圖就是想要表示說
它們的 function 有什麼樣不一樣的地方
那這個縱軸 這個縱的一個 *** 的一個 major 叫做
這個 ratio of critical samples
那什麼是 critical samples 呢
它的意思是說 現在你 train 好一個 network 以後
你 train 好一個 network 以後
好那你拿出你的 testing set
好你把一筆 data 丟進去
好你把 testing set 裡的一筆 data 丟進去
然後接下來看說在那個 testing set 的附近
的某一個範圍內
有沒有其他 data 它的 label 是不一樣
如果有的話就叫做 critical sample 大家知道我的意思
你現在假設你做的是
不管你做的是 image classification
每一筆 testing data 就是 ***維空間中的一個點
一個點
把你的 testing data 都拿出來
然後接下來你用你的 network 去對每一張 &&& 去做 label
比方 label 說這個叫做 1
這個叫做 1
這個叫做 2 這個叫做 3 這樣子
然後一個正常的 network
假設它沒有 overfit 的話
通常我們會相信說 比較接近的這些點
它們應該就是有 因為所有的 *** 都長得蠻像的
所以它們在*** 空間中比較接近 所以有 %%%%
那如果今天是一個硬 fit 的 network
train 在 random label 上 overfit 的 network
那它的 label 可能就會很怪 這個是 7
這個是 7 這個是 3 這個是 4 這個是 6 等等
所以 critical point 的意思是說
我們把 testing set 的每一筆 data 都拿出來
以那筆 data 當作圓心
畫一個半徑是 r 的圓
然後看在這個半徑是 r 的這個球體的範圍裡面
有沒有其他的 testing set 它的 label 跟圓心的那個點 label 是不一樣的
如果是 它就是一個 critical point
所以如果 critical point 愈多意味著什麼
代表我們的 function 它所代表的這個形狀 愈奇怪
因為它 input 只要有一點點的變化
它的 output 就會是不同的 label
好所以這個圖就告訴我們說
如果今天是在一個很 overfit 的情況 下 你的 sample 設這條線
你 critical point 的 ratio 最多
然後如果是全部都是對的
critical point 的 ratio 最小
那左邊是這五個不同的 case
在 training data 上的正確率
training 和 testing data 上的正確率
這邊它沒有把圖示標出來 實線是
training data 上的正確率 那你會發現 training data 上
不管是哪一個 case 通通跑到 100%
好那虛線是 testing data 的正確率
你會發現說如果所有的 data 都是對的
那 testing 的正確率當然最高
是黃色這條線 如果是錯的 80% 是錯的
那當然 testing 的正確率最低這樣
那這邊還有一個有趣的現象 如果你看
深紅色 最深的這個紅色這條線的話
你發現它 testing 上的正確率是先增加後減少
有沒有這邊有一個 peak
然後先增加後減少
這意味著什麼 這意味著 network 在訓練的初期
它其實是有學到有用的東西的
因為這 80% 這些 data 裡面是 80% random
20% real
所以顯然 network 先從那 20% real 的裡面
先去學到一些有用的東西
所以它在這個 testing
它從 20% real 的 data 裡面
先學到有些有用的東西
所以它在 testing 的正確率
在初期是先上升的 它是有學到東西的
只是到後來 隨著它看的 data 愈來愈多
它想要硬記剩下 80% 不 make sense 的東西 所以結果才爛掉
它先記了有用的東西
才去記沒有用的東西 所以最後的結果才爛掉
這是一個有趣的發現
好那這個是另外一個發現是要講說
如果今天一個 network 它是 train 在 real data
跟 train 在 random data 上它看起來有不一樣
好那這個實驗是這樣子
紅色這條線代表是 random label
綠色這條線代表的是 real label
橫軸代表的是 training data 的量
好那縱軸是什麼
縱軸是去 evaluate 那個參數的 weight 的 norm
weight 的 norm 比如說
l2 norm l1 norm 等等
這個 path norm 是什麼我們就不要講 反正有各種不同的
算那個參數的 norm 的方法 那 l2 norm 就是大家
在做 regularization 的時候非常熟悉的 看 l2 norm 就好
那這告訴我們什麼 這告訴我們說
今天當一個 network 它處在 overfitting 的狀況的時候
它確實 它的 l2 norm 是比較大的
間接告訴我們說 我們加上 l2 的 regularization
可能是 make sense 的
因為一個 network 處在 overfitting 的狀態的時候
它的 l2 norm 是比較大的
當它沒有 overfitting 的時候 它的 l2 norm 是比較小的
好那接下來我們就要講一些現在
人們覺得對偵測一個 network 是不是 overfitting
有用的一些 indicator
那最近提出來的一個呢
叫做 sensitivity
那這其實是發表在 這是一個非常新的 paper
是 google 我忘記是 *** 還是 google&&& 做的 然後它是投稿到
ICLR 2018 然後是有被 accept
好那在講 sensitivity 之前呢
我們要先跟大家講一下什麼是 Jacobian Matrix
之前已經講過 *** Matrix
那有另外一個 Matrix 叫做 Jacobian Matrix
Jacobian Matrix 的定義是什麼呢
我們現在假設有一個 function y=f(x)
那這個 function 的 input x 呢
是一個三維的 factor y 是一維 一個二維的 factor
所以 Jacobian Matrix 的意思
就是說我們拿 x 對 y 去做偏微分
講到這你可能有一點困惑 x 是一個 factor
y 也是一個 factor
拿 factor 對 factor 去做偏微分到底是什麼意思
如果是拿一個 variable 對一個 function output 的一個 loss 做偏微分
這個你知道 factor 對 factor 做偏微分是什麼意思
這邊意思是這個樣子
我們就是把 y 每一個 y
跟每一個 x 的 element 跟每一個 y 的 element
兩兩去做偏微分
我們就是拿 x1 對 y1 還有 y2 去做偏微分
x2 對 y1 y2 去做偏微分
x3 對 y1 y2 做偏微分
總共得到 6 個值
把它放到 matrix 裡面
這個東西就是 partial x partial y
這個東西就是 Jacobian Matrix
當然這個放法也是有講究的
假設你這個 x 是放在分子的地方
那你是把 x 放在 row 把 y 放在 說錯 x 放在 column
y 放在 row 這樣子
所以今天 y1 y2 第一個 row 通通是 分子都是 y1
第二個 row 分子都是 y2
第一個 column 分母都是 x1
第二個 column 分母都是 x2 第三個 column 分母都是 x3
那就是 定義就是這樣排的
好所以今天就舉一個例子說 Jacobian matrix 是什麼
假設你一個 function f input 是 x1 x2 x3
它會 output y1=x1+x2x3 y2=2x3
接下來我們去算 x 對 y 的 Jacobian Matrix
這個是 x 然後假設這個是 y
上面這個是 y1 這個是 y2 我們算 x 對 y 的 Jacobian Matrix
怎麼算呢 我們就先把 x1 對 y1 做偏微分
x1 對 y1 做偏微分 得到多少 得到 1
然後接下來我們再把 因為衡軸是 x1 x2 x3
x1 對 y1 做偏微分是 1
x2 對 y1 做偏微分得到 x3
x2 對 x3 對 y1 做偏微分得到 x2
好那這是第一個 row 第二個 row 呢
把 x1 x2 x3 分別對 y2 做偏微分
x1 對 2x3 做偏微分得到 0
x2 對 2x3 做偏微分也得到 0
x1 對 2x3 做偏微分得到 2
所以 Jacobian 就是這個樣子
好那有了 Jacobian Matrix 以後呢
你可以根據 Jacobian Matrix 定義出一個 network
對某一筆 data 的 sensitivity
但是要注意的事情是這個所謂的 sensitivity
你光拿一個 network 本身沒有辦法算 sensitivity
一定要 network 加上一個 input data
才能夠算 sensitivity 這樣
所以這個 sensitivity 的實用性是比較 limited
就是你要有 data 的時候 就是 data 已經給你了
如果光拿到 network 你沒有辦法算 sensitivity
要有 data 以後才能算 sensitivity
好那這一個 network
對某一筆 data x 的 network 我們就用一個 function f 來表示
某一個 f 對某一個 data x 的 sensitivity 是什麼呢
就是它的 Jacobian Matrix 的 Frobenius norm
好 Jacobian Matrix 大家已經知道了
那 Frobenius norm 是什麼呢
Frobenius norm 就是把這個 matrix 裡面的每一個 element
平方和再開根號就是 Frobenius norm
所以今天假設 f(x) 就是你的 network
它的 output 是一個 factor
network 就是 input 一個 factor x output 一個 factor y
你算出 x 對 y 的 Jacobian
把這個 Jacobian Matrix 裡面每個 element 取平方和
然後再開根號
你就得到這個 network
它對 input 這筆 data x 的 sensitivity
那這個結果其實非常的直覺對不對
這個 sensitivity 的意思就是說
我現在 input 這筆 data 如果有一個小量的變化的話
對我的輸出到底是有多少的變化
那這個 Jacobian Matrix 它想要 evaluate 的就是
x 這個 factor 對 y 這個 factor
它有多大的影響
這個 input 的這個 factor 有少量的變化的時候
y 它對應到底會有多少變化
那這個存起來就是 Jacobian Matrix
這個愈大 代表 x 的某一維對 y 的某一維的影響愈大
所以我們把這些值通通取平方和就代表說
x 整體來說當它有一些變化的時候
它對y 有多大的變化
所以這個 sensitivity 非常直覺
就 input 有一個變化的時候
對那個結果 y 到底有什麼樣的變化
那 sensitivity 對 跟 generalization 之間的關係
雖然沒有理論的證明但是從直覺上
你就知道說這顯然是很有關係的
如果今天某一個 network 對某一筆 data
非常的 sensitive
那可能就意味著這個 network 它不夠 robust
它很有可能今天 input 這個 data
受到noise 一干擾它 output 就不對
所以它就不夠 robust 它的 generalization 能力就不夠好
那其實 regularization 也可以看作是在 minimize sensitivity
有時候在做 regularization 的時候
我們是會希望我們 network 的 weight 值愈接近 0 愈好
那今天如果你 network 的 weight 值愈接近 0 愈好
其實你就會讓它的 output 愈平滑
如果你有上 machine learning 的話
第一堂課我們就講過說加入 regularization 以後
你 learn 出來的那個 function
加入 regularization 以後 你在 train regression 的時候
你 learn 出來的那個 function 它是會比較平滑的
那比較平滑其實就是意味著它的 sensitivity 是比較小的
因為 input 有變化的時候 output 的變化是比較小的
好那這個 sensitivity 它必須要有 data 的時候你才能夠算 sensitivity
好那所以你可以用一個用法就是
假設現在有一筆 testing data x 進來
但是沒有 label 你只有那個 testing data
你可以在有 你可以先去算它的 sensitivity
它就會告訴你說 現在這筆 testing data
你到時候有沒有可能做對 如果 sensitivity 愈高
那你就愈有可能得到的答案是錯誤的
那等於就是 machine 它可以告訴你說
針對這筆 testing data 它的 confidence 有多大
好那這邊這個圖是想要講說
這個 sensitivity 這件事
其實跟你的 training
跟你的那個 data 的 manifold 是非常有關係的
也就是說今天在你的 training data 有出現的那些位置附近
sensitivity 會比較小
在 training data 沒有出現的地方
它的 sensitivity 會比較大
何以見得呢
今天做法就是說
首先在那個高維空間中先隨便畫一個橢圓形
先隨便畫一個橢圓形
那在橢圓形上等距離的 sample 一些點
然後每一個點你都去算它的 sensitivity
然後發現說 如果你是隨機畫一個橢圓形
基本上得到的是黑色這條線
那 sensitivity 在這個橢圓形上晃一圈
差別不大
那接下來我們看說 假設
我們拿三筆 training data
我們得到的是這個
深紅色這條線
拿三筆 training data
那這三筆 training data 代表的是不同的數字
代表是不同的 假設做 MNIST 的話就不同的 digit
那把這三張圖片 這三張圖片其實就是高維空間中的三個點
這三個點你可以根據三個點畫出一個圓
在這個圓上走一圈
你會發現說當你走到這三張圖片的附近的時候
當你走到這三張圖片的附近的時候
你的 sensitivity 是特別低的
那在圖片跟圖片中間
這個地方你取一個點
看起來不像是 image
看起來不像是一個正常會出現的圖片 這個時候呢
它的 sensitivity 就特別高
那如果最後這個紅色這條線也是一樣
只是說它取了三張圖片是三張圖片代表同樣的數字
如果是代表同樣的數字你會發現說
在圖片和圖片之間
它的 sensitivity 是比較低的
如果 visualize 的話 結果也是一樣
這個 visualize 的圖是什麼意思呢
其實在這個圖上有一個小小的東西不知道大家有沒有發現
在這個圖上是有三個數字的 你有看到嗎
你有看到嗎 這是隱藏的
你要有很強的瞳力才能夠看得出來
其實它這邊你看這邊有一個 6
這邊有一個 6
這邊有一個 5 你有看到嗎
這邊有三張圖片
好它說你知道在高維空間中你拿三點
你就可以定義一個平面對不對
它說我們現在用這三張圖片在高維空間中
所得到的那三個點訂一個平面
好接下來因為它 train 的是一個 relu network
relu network 它的這個
它是 piecewise linear 的
它 input output 都要是 piecewise linear
然後你就可以在那三個點
定義出來的平面上把那個 piecewise linear 的 function 畫出來
每一個 piece 都用不同顏色標
就會看到現在這樣一塊一塊的這樣
一塊一塊
非常的複雜萬花筒這樣子
好那這個是 training 之前的時候
的 relu network 長這樣子
它發現 train 之後長這個樣子
你會發現說這些區塊
有些地方變得比較淡
有些地方變得比較小
那三張圖片落在哪裡呢 那三張圖片落在這裡
這裡 跟這裡
然後你就發現說在有出現圖片地方的附近
那個 relu 的那個區間是比較大的
就那個 piecewise linear 的那個 diff 是比較大的
然後在區間和區間之間
這個地方沒有數字的地方 piece 是比較小的
就代表說今天在這個區域
如果你有對 input 進行移動的話
output 變化可能是比較小的
那這邊你 input 有移動的話
你可能一次就越過好幾個不同的 pieces
那你可能在這個地方有移動的時候
它的 output 就會比較大 它的 sensitivity 就會比較大
那所以這個實驗室想要告訴我們說
今天 train 好一個 network
它在你的 training data 的 manifold 上
也就是說 training data 有出現的地方附近
它的 sensitivity 會比較低
但是在 data 沒有出現的地方它的 sensitivity 就比較高
好那有這個東西有什麼用呢
它可以來預測你的 network 能不能夠 generalize
這個是文獻上的結果它就說我們試一些方法
這些方法是我們知道說可以導致 network 比較能夠 generalize
比如說一個很極端的是 現在呢
現在圖上的每一個點代表同樣的一筆
network 然後把它 train 了兩次
一次是用 true label 一次是用 random label
然後圖上的這個值代表的是
generalization 的 gap
也就是 training error 跟 testing error 之間的差
如果你今天是用 true label
你有可能會 overfit 你的 train 跟 test 之間的差很大
但你也有可能不 overfit 你的 train 跟 test 之間的差很小
但是如果你是用 random label
一定 overfit 你的 true 跟 test 之間的差
不管你是用哪一組參數固定 都是這麼大
好那這個是 generalization gap
那如果你今天沒有 label data
你就沒有辦法算 generalization gap
但是你可以算 Jacobian norm
那 Jacobian norm 這邊算出來是這個樣子
好這個圖可能比較不明顯我們看下面這個例子
下面這個例子是有沒有做 data augmentation
我們知道說有做 data augmentation 可以對抗 overfitting
可以減少 generalization gap
那事實上也是如此你會發現說
如果沒有做 data augmentation
不管你是用什麼樣的方法
不管你是用什麼 這邊不同的點就代表你的 network 的
那個***** 比如說 layer 的數目
它的 learning rate 等等是不一樣的
今天如果你沒有 data augmentation
它的 generalization gap 都是這麼大
不管你用什麼 network gap 都這麼大
那如果有 data augmentation 你就有機會
但是不一定 有可能還是很大
你就有機會 它的 generalization gap 是比較小的
那你會發現說 如果我們今天秀的不是 generalization gap
我們的橫軸跟縱軸換成 Jacobian norm
就對 testing data 每一筆 data Jacobian norm 的平均這樣
因為 testing 有很多筆 data
把 testing data 每一筆拿出來算 Jacobian norm 的平均
你會發現說 當然這個點沒有辦法完全一樣
但你會發現說我們確實觀察到
如果是有做 data augmentation 的時候
Jacobian norm 都比較小
沒有做 data augmentation 的時候
Jacobian norm 就比較大
也就是 network 是比較 sensitive 的
這個是其他的結果那你會發現說
我們就不要細講 那這個圖只是要告訴你說
generalization gap 跟 Jacobian norm
它們呈現的結果是蠻一致的
好這個圖也是就是想要直接秀說
sensitivity 跟 Jacobian norm 的關係
然後把它做在不同的實驗上 有 CIFAR10
有 CIFAR100 有 MNIST 有 FASHION_ MNIST
這圖上每一個點就代表一個 network
train 好一個 network 以後
計算它在 testing set 上的 generalization gap
接下來計算它在 testing set 上的 sensitivity
你就會發現說 sensitivity 跟 generalization
它們是成正比的
這意味著說
我們可以用這個 sensitivity 來預測一個 network
它的 generalization 能力好不好 所以以後如果你沒有
testing data label 但有一筆 testing data 進來
你就可以先用 sensitivity 來預測說有沒有可能答對
那你說假設預測你很不可能答對那怎麼辦呢
其實你就可以說那這筆 data 我就丟掉 交給人來做 label
因為它可能是機器沒有辦法答對的
那這個在實作上就會是蠻有用的 application
好這個我們跳過
好接下來下一個東西要講的呢
其實是 下一個東西要講的是 sharpness
好那 sharpness 是什麼呢
其實 sharpness 這個東西它很早以前就有人觀察到了
它是來自一個非常遠古的傳說這樣
真的真的 1997 年的時候就有 paper 討論
sharpness 跟 generalization 之間的關係
兩千年以前 所以那個對我們來說就是很遠古的時代這樣
好所以在遠古的時代 其實人類就知道說
你今天在 training 的時候 你最後找到的 solution
我們假設我們找到的 solution 它就是一個 local minima
這個 local minima 它是在一個很 sharp 的山谷裡面
還是在一個很 flat 的山谷裡面
會影響到最後這一個 solution
一 train 出來的 network
這組參數它的 generalization 能力
好這個假設這是你的 training set 上面的 loss
然後假設你經過 training 以後
假設你經過 training 以後
它有兩個 local minima
那有時候你 training 的時候
可能會找到這一個 local minima
有時候你 training 的時候
可能會找到這一個 local minima
那這個 local minima 它是在一個比較平坦的峽谷裡面
這個 local minima 它是在一個比較尖銳的峽谷裡面
你要不要憑著直覺猜一下 現在
在 deep learning community 的通念裡面
哪一個 case 是比較容易 overfitting 的呢
憑著你的直覺猜一下
你覺得如果是這個 flat 的 minima 它比較容易 overfitting
的同學舉手一下
好沒有 如果你覺得是這個 sharp 的 minimum
它比較容易 overfitting 的同學舉手一下
好幾乎全班
好手放下 大家的直覺跟古聖先賢想的是差不多的這樣
那所以古聖先賢是怎麼想的呢
假設現在 training 和 testing
之所以在 training 跟 testing 的 error 不一樣是因為
training data distribution 跟 testing data distribution
還是有一些差異的
所以你 training loss 的 error surface 長這樣子
就純一個參數的變化
loss 的 error surface 的變化是這個樣子
那 testing 的時候可能是跟 training 的時候
有一個誤差 可能形狀差不多 但是有一個誤差
那如果有這個誤差的時候
你會發現說 如果是一個 flat 的 minima
你從這個點拉上去
在 testing 上的 loss 是比較小的
而在另外一個 case
在 sharp minima 你可能只要偏一點
本來是在山谷谷底就變到山峰了
本來是在谷底 偏一點就跳到山峰去了
所以今天在 sharp 的 minima
只要 training 和 testing 有一點 mismatch
那你的結果就差了 那就是 generalization 的能力就差了
如果是 flat minima
你今天偏一點
你的 loss 不會變太多
training 跟 testing 的 loss 不會變太多
代表它 generalization 的能力是比較好的
那其實從這個角度 也許有可能有機會解釋說
為什麼 network 它自帶 regularization
為什麼 network 通常 train 出來
它就會有好的 generalization 的能力
因為這就是一個想像 想像說
現在這種 local minima 是比較好的 這種是比較差的
這種 local minima 它山谷比較寬
所以它的流域是比較大的
它流域很大 那這種 local minima
它流域比較窄 它流域比較小
然後如果你 random sample 一個初始值
你比較容易 random sample 在這種地方
然後用 gradient discent 你就跑到好的 local minima
你比較 你 random sample 在這種地方的機率
就比較低 所以今天跑到這種地方的機率就比較低
這可以解釋為什麼 network 自帶 regularization
為什麼它 train 出來其實不會 不容易 overfitting
但這就只是一個解釋而已
有可能我們今天講的東西
在幾年後看起來就好像講一些什麼
太陽繞著地球轉一樣的荒謬這樣子
因為 deep learning 的領域其實變化是非常快的
今天覺得是這個樣子 也許明天就不是這樣子了
好那怎麼定義 sharpness 呢
那我們在作業裡面的 bonus 會叫大家算一下 sharpness
其實 sharpness 這個東西並沒有一個固定的定義
你怎麼樣爽就怎麼樣算這樣子
你就用你覺得爽的方法算
那有不同的方法來定義它 舉例來說
假設這個是你的谷底 它是 θ*
我們要怎麼算這個山谷它有多尖銳呢
我們就往上找 往上拉 epsilon 這樣的距離
然後畫一條橫線
然後看這條橫線切過山谷的這個切面有多大
這個切面愈大當然代表愈平坦
切面愈小 當然就代表愈尖銳
那因為現在這個圖是畫在一維的平面上
就假設參數是一維的
所以這個 sharpness 的定義是這個線的長度
如果是二維的有兩個參數
那你 sharpness 的定義就是一個平面的那個面積
如果是三維的參數就變成是體積
如果高維的話就是高維空間中的體積這樣
好沒有的話我們就講一下第二個 definition
好那這個第二個 definition 是什麼呢
其實跟第一個 definition 也沒有太大的差別
其實這個 definition 到時候你在作業裡面你只要
高興就好 等下助教會講一 下說
在文獻上 最常見那個 definiton
長得是什麼樣子 但你可以用自己喜歡的方式來算
好那這個第二個 definition 是什麼呢
第二個 definition 又更直觀 我們在 θ* 當作圓心
然後畫一個球
畫一個球 那個球的半徑它是 epsilon
然後再看這個球畫出來的範圍內
最大的 loss 是多少
那假設最大的 loss 算出來
是 L(θ') 那你就把最大的 loss 減掉谷底的 loss
把 L(θ') 減掉 L(θ*)
它中間的差距就是這個山谷的 sharpness
好接下來我們就要看一下說
這個 sharpness 有什麼用呢
那現在最常見 和 sharpness 最有關係的
就是 batch size
那我們發現說
這件事 這個是老早就有人知道
上古時代大家就知道的事情這樣
上古時代大家就知道說
batch size 對 training 是很重要的 batch size 愈大
你 training 你的那個 network 的 performance 就愈差
舉例來說我把 reference 貼在這邊
橫軸是 batch size 的變化
64 128 256 502 到 64k 你自己沒辦法做了這樣
好那縱軸是 ImageNet
它的 error
testing set 上的 error validation set 上的 error
你會發現說 batch size 愈大 validation error 就愈大
再來的問題是說
現在的 validation error 愈大 到底是什麼原因造成的呢
是 overfitting 嗎
還是 training 根本就沒有 train 好
我們在上 machine learning 的時候已經有講很多次說
你今天要知道你的 network 到底 performance 好還是不好
第一個要先檢查 training set 上的 performance
才知道說它到底是不是 overfitting
所以光看這個圖我們不知道說 比較大的 batch size
到底是很容易 overfitting 還是 training 其實 train 不好
但是我們可以看下面這個結果
這是另外一個文獻的結果
它試了左邊這個應該是 MNIST 右邊這個是 CIFAR10
然後 SB 代表的是小的 batch size
小的 batch size 是 256
LB 是 large batch size
large batch size 是 data set 的 1/10
那 CIFAR10 有五萬筆 data
1/10 那就是五千筆 data
那這個實線 實線是 testing error 我們先看
我們先看虛線
虛線是 training 的 error
training 的正確率
我們看右邊這個圖好了
在 CIFAR10 上
不管是大的 batch size 還是小的 batch size
你 train 夠多 data 最後它們的正確率是一樣的
它們的正確率是一樣的
Etrain 是一樣的
但是你發現如果你看 testing 的正確率
small batch size 就硬是比 large batch size 多了一截
在 training 上是一樣的
在 testing 上 small batch size performance 比較好
這代表什麼
這就代表如果用 small batch size 來 train 的時候
它 generalization 的能力是比較好的
那有人就推測說 small batch size
大的 batch size 或小的 batch size 這件事情
可能跟你最後找到的 optimal
是不是 sharpness 這件事情 是有關係的
那有***的實驗呢 來佐證這件事
而在這個實驗上面就做了各式各樣不同的 model
從 MNIST TIMIT 做到 CIFAR10 CIFAR100
所以我剛剛看到 F2 其實是 TIMIT
我說錯了它不是 MNIST 它是 TIMIT
那有人可能會問說 TIMIT 是甚麼
TIMIT 是語音辨識的一個 **
它不是 image 影像辨識 的*** 它是語音辨識的****
好那這個
這邊就是六個不同的 model
它們在 training data 的 accuracy
還有在 testing data accuracy
不管是 large batch size 還是 small batch size
它們在 training set 上的正確率其實都是差不多的
但是在 testing set 上你會發現說小的 batch size
它的 testing 的正確率就是比 large batch size 大過一截
顯示小的 batch size 所 train 出來的 network
它不知道怎麼回事 它 generalization 的能力
就是比較好這樣子
那這個是什麼樣的原因呢
有一個可能的說法是 這件事情也許就跟我們
找到的 local minima 的 sharpness 有關
我們用 large batch size 的時候
我們比較容易找到那種很 sharp 的 local minima
我們用 small batch size 的時候
比較容易找到那種很寬很 flat 的 local minima
那這件事情在實驗上有觀察到這樣的現象嗎
有觀察到這樣的現象
發現這邊的實驗結果
如果我們看 large batch size
跟 small batch size 用不同的 epsilon
我們剛才說你今天算 batch size
算 sharpness 的時候你要畫一個圓
那這邊就是畫不同的半徑
不管畫什麼樣的半徑得到的結論是一樣的
如果是 large batch size 你走到那些 local minima
它的 sharpness 是比較大的
如果 small batch size 你走到那些 local minima
它都是比較平坦的
對差一個數量集
好那這邊有另外一個結果
那這個結果是跟那個
*** 的 visualization 用的是一樣的 它說
small batch size 是 0 這個點
large batch size 是 1 這個點
然後你畫這兩個 model
它們中間連線的 loss 的變化
好你往左邊多畫一點 你往右邊多畫一點
好這個藍色的實線
是 training set 的 error
那從這個圖上我們就可以很明顯的看出來說
這個 small batch size 跟 large batch size 它們都
走到一個像是 local minima 的地方
而 small batch size 它這個 local minima 是比較寬的
large batch size 它這個 local minima 是比較尖銳的
然後這個虛線是 testing set 上 loss 的變化
好那有同學可能會想說
看 training set 跟 testing set 它們的 local minima
好像是在一樣的位置
剛剛不是說 training 跟 testing 可能會有一些 mismatch
shift 一點點 這樣才能夠看出它們之間的差異對不對
那我覺得其實是這個樣子
今天這個 shift 不見得能夠在這個圖上看出來
因為你有沒有想過這個 shift 這個是一維的空間而已
我們今天的參數是在高維空間中的
所以它的 shift 不一定要在你觀察到的方向 shift
它搞不好是這樣 shift
這我們看不出來 就這樣子
對不對 所以你會發現說如果你看這兩個 loss
如果是看 training 的時候 這個點跟這個點
它的 loss 是一樣低的
但是看 testing 的時候 這個 loss 跟這個 loss
這個 loss 是比較高的 這個 loss 是比較低的
所以有可能是這邊就是在這個方向上
有一個 shift 所以這個點可能已經不是一個 local minima
它是距離 local minima 有一段距離 所以它的 loss 比較高
然後這邊可能離 local minima 比較接近
所以它的 loss 比較低這樣
好這個是另外一個實驗跟剛才結果也差不多
它就說 我們來算一下 training 的 accuracy
說錯 testing 的 accuracy 藍色這條線
橫軸是 batch size batch size 愈來愈大
testing accuracy 當然就愈來愈低了
好那這個紅色的線是那個 sharpness
那隨著 training accuracy 愈來愈低
batch size 愈來愈大 sharpness 就愈來愈大
那這個實驗是想要講說 sharpness 和 batch size 是有一些關係的
好那這個是今天這堂課的結論
就是說 sensitivity 可能跟好的 generalization 有關係
flatness 可能也跟好的 generalization 有關係
這邊打一個問號
等下再告訴你說為甚麼打一個問號
然後這邊是要跟我們講說假設你有一些好的 indicator
知道說什麼樣的 network 比較容易 generalize
那你在 training 的時候可以把這些東西塞進去你的 training process
希望你的 network train 出來可以更 generalize
好這邊是一些 reference 就給大家參考
這個第一篇是講說
第一篇這個是要分析
overfit 跟沒有 overfit network 的差別
因為自從 ICLR2017 有那篇嚇到大家吃手的 paper 以後
就是 network 其實可以硬記 training data 那篇實驗以後
在同年的 workshop 裡面馬上就有另外一篇 paper
是表面上看它的題目好像是要 against
那篇 ICLR 的*** paper 但其實不是
因為它做出來的實驗也是
就是 network 確實可以硬記 training data
但是當你的 training data 是 true label 跟
random label 的時候
它得到的 network function 是不太一樣 特性是不太一樣
後來那篇 workshop paper 就擴寫成 ICML 的 paper
好那這篇是要講說
batch size 跟 generalization
還有是不是 sharp 之間的關係
然後既然知道說 sharpness 是比較容易 overfit
flat 這個東西比較容易導致好的 generalization solution
那這邊就有提出一個新的方法叫 Entropy SGD
那我們就不細講 但它的精神就是想辦法把這條東西
想辦法把找 flat 的 local minima 這件事
把它塞到你的 train process 裡面
希望 network 在 train 的時候
盡量去找 flat 的 local minima
然後這篇是比較各種不同的 indicator
那有一些是我們今天沒有講到的 那這一篇是講
最後一篇是講 sensitivity
重點是倒數第二篇 倒數第二篇
它題目是 sharp minima can generalize for deep network
你看我們本來是說 flat minima 可以 generalize
但它其實告訴我們說 sharp minima 也可以 generalize
所以你可以仔細讀一下這篇 這篇會告訴說
有關 sharpness 那一整套講法
可能都是跟事實不符的這樣子
就是說你會覺得說剛才聽了那麼久都是在浪費時間
我還猶豫說到底要不要講這篇這樣
因為我怕講這篇你等下就生氣了這樣子
你自己再回去 你有興趣的話再回去看一下 這篇告訴你說
也許 sharpness 跟 generalization 的關係
跟我們想像的不太一樣這樣
所以 deep learning 就是這麼神奇
