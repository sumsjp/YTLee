台灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
Transfer learning 指的意思是甚麼？
Transfer learning 指的意思是說
假設你現在手上有一些跟你現在進行的 task 沒有直接相關的 data
能不能用這些沒有直接相關的 data，來幫助我們做一些甚麼事情
比如說你現在要做的是，貓跟狗的 classifier
那所謂的沒有直接相關的 data，是甚麼意思呢？
所謂的沒有直接相關，有很多不同的可能
一個可能是，input 的 distribution 是類似的
比如說，input distribution 一樣是動物的圖片
但是 task 裡的 label 是無關的
比如說現在你的另外一些 data 其實是要分大象跟老虎
input domain 是像的，但是你的 task 是不一樣的
還有另外一個可能是，input domain 不一樣，但 task 是一樣的
一樣是做貓跟狗的分類，但是圖片是招財貓和高飛狗的圖片
跟原來圖片的 distribution 是非常不像的，但要做的task是一樣的
transfer learning 要問的問題就是，我們能不能夠在有一些不相干的 data 的情況下
然後來幫助我們現在要做的 task
為甚麼我們要考慮 transfer learning 這樣的 task 呢？
舉例來說，在 speech recognition 裡面
可能你現在要做的事情，是台語的語音辨識
但是台語的 data 是很少的
但其實，語音的 data 是很容易蒐集的，你隨便去 YouTube 爬一下，就可以爬到一大堆英文、中文、其他語言的 data
那我們能不能夠用其他語言的 data，來 improve 台語的語音辨識這件事情
或者是如果在 image recognition 裡面，或許是現在有興趣的 task 是做 medical 的image
你想要讓機器自動診斷說，有沒有 tumor 之類的，這是現在很流行做的事情
可是這種 medical的image 其實是很少的，不可能手上有太多這種 image
但是實際上 image 的 data 我們永遠都不缺
胡亂網路爬就有一大堆 image
或 download MNIST，裡面有超過一百萬張 image
有這麼多 image，只是不是 medical 的 image
這些其他的 image 對你現在要考慮的 task，有沒有可能有幫助
或者是在文件的分析上，你現在要分析的文件
是某個很 specific 的 domain，比如說你想要分析的是，某種特別的法律的文件
那這種法律的文件或許 data 很少，但是假設你可以從網路上 collect 一大堆的 data
那這些 data，有沒有可能是有幫助的
那其實 transfer learning 這件事情可能嗎？
用不相干的 data、來自其他 domain 的 data 來幫助我們現在的 task 是有可能的嗎
是有可能的，因為我們在現實生活中，我們都是不斷的在做 transfer learning
比如說，你可能是一個研究生，你可能想要知道研究生應該怎麼樣過日子
那怎麼辦呢？你就可以參考"爆漫王"這樣子
那在"爆漫王"裡面呢，其實漫畫家就是研究生 責編就等同於指導教授
漫畫家每周就要畫分鏡去給責編看，跟責編討論
每周跟指導教授進度報告一樣，畫分鏡就是跑實驗
目標就是要在 jump 上連載，在 jump 連載就是投稿期刊
連 word embedding 都知道這件事情
雖然我們沒有研究生的守則，但是從"爆漫王"裡面，我們可以知道身為一個研究生
應該要做什麼樣的事情
你可能會覺得說，拿漫畫來跟神聖的研究做類比，有點不倫不類
我跟你說，漫畫家都是用生命來畫漫畫的，其實比我們做研究認真多了
我不知道大家有沒有看過"爆漫王"，我簡介一下"爆漫王"的劇情，用三十個字
"爆漫王"就是說有兩個高中生，一個叫真城，一個叫高木
他們不知道怎麼回事，就很想當漫畫家，後來就當了漫畫家
有一次，因為每週都要連載，實在太累了，真城就累到要住院、開刀
住院的時候，他手還握著畫筆畫漫畫，大家都阻止他
他女朋友來看他，決定要支持他，他女朋友握著他的手繼續畫下去
這個就好像說，你今天在做研究的時候，你覺得每周都進度報告太累了
譬如說累到生病住院了，其他人都叫你不要再做研究
你女朋友來看你，把你的手壓在鍵盤上，繼續寫 code
比如說裡面有一個天才漫畫家，叫新妻惠一兒
應該在影射尾田榮一郎，新妻惠一兒想要結束他現在的連載
跟總編說要結束現在的連載，他跟總編談一個條件
我從現在開始接下來十周，我都要是 jump 人氣第一名
如果我做到的話你就要讓我結束連載，如果做不到我就繼續畫到死為止
叫你跟指導教授說我做這個題目很膩，我想要畢業
指導教授說你接下來連發十篇 paper 都拿 best paper award 我就讓你畢業
如果做不到就繼續做到死為止，就是這個概念
他們是比我們還要認真很多的，所以我覺得研究生都應該看一下"爆漫王"
transfer learning 是這樣子的，transfer learning 有很多很多的方法
他是很多方法的集合，你以下聽到的 terminology，這個地方有點混亂
不同的文獻，他用的詞彙其實是不一樣的
有些方法有人說算是 transfer learning 有些方法有人又說不算是 transfer learning
這邊是很混亂的
如果你看到我跟別人說的不一樣，並不是我錯，或者是別人錯
這個地方就是很混亂
你只要知道那個方法是甚麼就好了
我們講法是我們現在有一個我們想要做的 task
有一些跟這個 task 有關的 data，這個叫做 target data
我們有一些 data 是跟這個 data 無關的 data，這個 data 叫做 source data
target data 他有可能是有 label，也可能沒有 label source data 也有可能是有 label，也有可能沒有 label
所以現在我們會有四種可能，之後我們會分這四種可能來討論
Transfer learning 我們可以分成四個不同的象限來討論他
我們的 data 分成兩種，一種是 target data，一種是 source data
所謂的 target data 是跟我們現在考慮的 task 是直接相關的
source data 跟我們現在考慮的 task 沒有直接的關係
甚麼叫做"沒有直接的關係"，這個定義是比較模糊的
有很多不同的定義方式
雖然 input 都是 image，但 image 的 distribution 是很不一樣的
比如說一個是真正的實物 image，一個是動畫的 image
他們就差很多，那就可以算是和這個 task 沒有直接相關
我們可以分成我們的 target data 有可能是有 label 的，有可能是沒有 label 的
source data，也可能同時是有 label 的，也可能是沒有 label 的
總共有四種可能，分別對這四種可能的情況，介紹些方法
我們先來講，假設我們現在我們的 target data 跟 source data 同時都有 label 的情況下
我們可以做甚麼事情 最常見的也最簡單的作法就是 fine-tune 你的 model
甚麼意思呢？ 我們先來看一下我們現在的 task
在我們 task 裡面，我們的 target 和 source data 通通都有 label
target data 我們就用上標 t 來表示
target data 裡有我們要找的 function 的 input，x 上標 t，跟他的 output，y 上標 t
source data 裡有 function 的 input，x 上標 s，跟 y 上標 s
通常我們會假設 target data 的量非常少
如果 target data 量很多，就當作一般 machine learning 的 problem
直接拿 target data 來 train 你的 model 就好了
也不需要做 transfer learning
在做 transfer learning 的假設是 target data 的量是非常少的
而 source data 是很多的，雖然 source data 跟我們的 task 沒有關係
但我們想知道說，在 target data 很少的情況下
如果有一大堆不相干的 source data 到底有沒有可能會有幫助
如果今天 target data 的量非常少，少到只有幾個 example
這個叫做 one-shot learning
從名字來看好像只有一個 example，所以叫 one-shot 但有時候也不見得只有一個 example 才能稱之為 one-shot
反正意思就是如我 target data 的量非常非常少，你可以說你在做 one-shot learning
這樣子的 task 有什麼樣的例子呢
語音上最典型的例子呢，就是 speaker 的 adaptation
甚麼意思呢？我現在的 target data 是某一個人的聲音
我們要辨識某一個人的聲音 這個人的聲音不可能太多的label data
不可能對這個人的 audio 去做太多的 transcription 只有他非常少量的 transcription
他可能只有對你的 machine 說三句話 只有這三句話的 label 而已
但 source data 你有一大堆的 audio data 來自於不同人的 這些 audio data 都有 transcription
通常這種 data 你要 collect 上萬小時都是有可能的
當然不可能直接用 target data、某一個 speaker 他的 data 去 train 語音辨識系統
這樣一定會壞掉，你會希望說，好幾萬個小時的 source data 對這個 task 有甚麼幫助
這邊的處理方式可以非常直覺，怎麼做呢
你就拿你的 source data 直接去 train 一個 model
接下來你去 fine-tune 你的 model ，用這個 target data
甚麼叫做用 target data 來 fine-tune 你的 model 呢
這個想法非常直覺，你就把你在 source data 上 train 出來的 model
當作是 training 的 initial value ，當作是初始值
然後再用 target data train 下去就結束了
但是這邊可能會遇到的 challenge 是 source data 真的非常少
今天就算在 target data 上 train 出一個好的 model
當你把它當 model 的 initialization
在 source data 再去做 training 可能 train 下去就壞掉
所以在 train 的時候要很小心 有很多不同的技巧
有一個技巧叫 conservative training
conservative training 是說現在有大量的 source data
在語音辨識裡面，他就是很多不同 speaker 的聲音都有 transcription
可以去 train 一個拿來做語音辨識的 neural network
接下來呢，你有 target data
target data 是某個 speaker 的聲音跟 transcription
你可能只有五句、十句這麼多而已
怎麼辦呢？ 如果你直接拿這些 target data 去 train
Train 下去就壞掉 你可以說我在 training 的時候，下一些 constrain
讓 training 的時候，train 完了新的 model 跟舊的 model 不要差太多
我下一個 constrain，下一個 constrain 其實就是加一個 regularization
我們在 training 的時候，你會加L1、L2的 regularization
在 conservative training 裡面，你會加另外一種不同的 regularization
新的 model 的 output 跟舊的 model 的 output 在看到同一筆 data 的時候
他們的 output 越接近越好
或者是下 constrain 是新的 model 跟舊的 model
他們的 L2 norm 差距越小越好
總之就是做一些事情，讓新的 model 跟舊的 model 差距不要太大
這樣就可以防止 overfitting
防止 target data 只有一點點，train 下去就壞掉的情形
另外一個方法是 layer的transfer
現在有用 source data train 好的一個 model
你把這 model 裡的某幾個 layer 拿出來
直接 copy 到新的 model 裡面去
你把某幾個 layer 直接 copy 到新的 model 裡面去
接下來你用你的 source data ( 應該是target data ) 只去 train 沒有 copy 的 layer
可能只有保留一個 layer 是沒有 copy 的
source data ( 應該是target data ) 就只 train 那個 layer 就好
這樣的好處就是 source data ( 應該是target data ) 只需要考慮非常少的參數
就可以避免 overfitting 的情形
當然如果 source data ( 應該是target data ) 夠多了
我還是要 fine-tune 整個 model，也是可以的
layer transfer 是一個非常常見的小技巧
現在可能問題是；那些 layer 應該被 transfer，那些 layer 不應該被 transfer 呢
有趣的是在不同的 task 上面，需要被 transfer 的 layer 往往是不一樣的
比如說，在語音辨識上，通常是 copy 最後幾層
然後重新 train input 那一層
在語音上你可想成每一個人用同樣的發音方式，但因為口腔結構略有差異
同樣的發音方式，得到的聲音是不一樣的
那 neural network 前幾層是從聲音訊號裡面，得知說話的人的發音方式
再根據發音方式，他就可以得到現在說的是哪一個詞彙，就可以得到辨識的結果
從這個角度來看，從發音方式到辨識結果，neural network 的後面幾層
是跟說話的人沒有關係的，所以是可以被 copy 的
不一樣的是，從聲音訊號，到發音方式這一段
可能是每個人都不一樣的
所以在做語音辨識的時候，常見的作法是把 neural network 的後幾層是 copy 的
只有前面可能第一層用 target data、某一個 speaker 的 data train
但是在 image 的時候，我發現是不一樣的
因為在 image 的時候，通常是 copy 前面幾層，只 train 最後幾層
為甚麼呢？因為在 image 你會發現當你在 source domain 上 learn 了一個 network
你 learn 到的 CNN 通常前幾層他做的
就是 detect 最簡單的 pattern
比如說前幾層，做的事情就是 detect 有沒有直線橫線，或者是有沒有簡單的幾何圖形
所以在 image 上面，neural network 前幾層 learn 的東西
他是可以被 transfer 到其他的 task 上面
而最後幾層 learn 的東西往往比較 abstract
這個比較 abstract 他就沒有辦法 transfer 到其他的 task 上面去
在做影像處理、辨識的時候，反而是 copy 前面幾層
然後後面幾層重 train 到底哪些 layer 要被 transfer
其實是 case by case 特別是運用之妙，存乎一心
這邊是 image 在 layer transfer 上的實驗
這是出自 Bengio 在 NIPS,2014 的 paper
這個實驗做在 ImageNet 上，這個實驗他把 ImageNet 的 corpus
一百二十萬張 image 分成 source 跟 target
這個分法是按照 class 來分的 我們知道 ImageNet 的 image
一個 typical 的 setup 是有一千個 class
把其中五百個 class 歸為 source data
把另外五百個 class 歸為 target data
橫軸是我們在做 transfer learning 的時候，copy 了幾個 layer
Copy 0 個 layer 就代表完全沒有做 transfer learning
這是一個 baseline ，就直接在 target data 上面 train 下去
縱軸是 top-1 accuracy，所以是越高越好
直接 train 下去的結果是白色這個點
沒有做 transfer learning 是白色這個點
如果今天 copy 前面幾個 layer，只有 train 最後幾個 layer 的時候
如果今天 copy 第一個 layer，train 剩下的 layer
Copy 前面兩個、三個 layer，在 source data 上 train 一個 model
然後 copy 第一個 layer，或 copy 第二個，copy 第三個，一直到 copy 前面七個 layer
然後剩下的 layer 再用 target data 去 train，會得到什麼樣的結果
今天它的結果是會變差的
但是如果只有 copy 前面幾個 layer 的時候
只有 copy 第一個 layer 的時候，performance 稍微有點進步
不過 copy 前面兩個 layer，performance 幾乎是持平的
但是 copy 的 layer 太多，結果是會壞掉
這個實驗顯示說在不同的 data 上面，train 出來的 neural network
前面幾個 layer 是可以共用的
後面幾個可能是沒有辦法共用的
上面這條橙色的線是說，如果 copy 完以後，還有 fine-tune 整個 model 的話
把第一個 layer，在 source domain 上 train 一個 model
然後把第一個 layer copy 過去以後，再用 target domain fine-tune 整個 model
包括前面 copy 過的 layer 的話
那得到 performance 是橙色這條線
在所有的 case 上面都是有進步的 其實這個結果滿 surprised
不要忘了，這可是 ImageNet 的 corpus
一般在做 transfer learning 的時候，都是假設 target domain 的 data 非常少
這邊 target domain 可是有六十萬張，你可能都沒有處理過這麼多的 image
這 target domain 的 data 是非常多的
但是就算在這個情況下，再多加了另外六十張 image 做 transfer learning
其實還是有幫助的
這兩條藍色的線跟 transfer learning 比較沒有關係 不過是這篇paper裡面發現一個有趣的現象
假設呢，假設呢 他想要做一個對照組
在 target domain 上面 learn 一個 model
把前幾個 layer copy 起來
再用一次 target domain 的 data train 剩下幾個 layer
前面幾個 layer 就 fix 住，只 train 後面幾個 layer
直覺上這樣做應該跟直接 train 整個 model 沒有太大差別
但最後發現假設你 fix 前面幾個 layer
先 train 好一個 model，fix 前面幾個 layer
接下來只 train 後面幾個 layer
接下來重新 train 後面幾個 layer
結果有些時候是會壞掉的
他的理由是 training 的時候，前面的 layer 跟後面的 layer
他們其實是要互相搭配
所以如果只 copy 前面的 layer，然後只 train 後面的 layer
其實 performance 會是比較差的
後面的 layer 就沒有辦法跟前面的 layer 互相搭配，結果有點差
如果可以 fine-tune 整個 model 的話，performance 就跟有沒有 transfer learning 是一樣的
這是另一個有趣的發現，作者自己對這件事情是滿 surprised
這是另外一個實驗結果
這個實驗結果是 紅色這條線是前一頁看到的紅色的這條線
這邊假設 source 跟 target 是比較沒有關係的
把 ImageNet 的 corpus 分成 source data 跟 target data 的時候
把自然界的東西，通通當作 source
target 通通是人造的東西，桌子、椅子等等
這樣 transfer learning 會有什麼樣的影響
如果 source 跟 target 的 data 是差很多的
在做 transfer learning 的時候，performance 會掉的比較多
前面幾個 layer 影響還是比較小的
如果只 copy 前面幾個 layer，仍然跟沒有 copy 是持平的
這意味著，就算是 source domain 跟 target domain 是非常不一樣的
一邊是自然的東西，一邊是人造的東西
在 neural network 第一個 layer，他們仍然做的事情很有可能是一樣的
綠色的這條線，爛掉的這條線是假設前面幾個 layer 的參數是 random 的
會發生甚麼事情呢？得到的結果就是爛掉，這是一個 baseline
另外一個要講的是 multitask learning
multitask learning 跟 fine-tune 略有不同的地方就是
在 fine-tune 裡面我們 care 的是 target domain 做得好不好
在 source domain 上 learn 一個 model
在 target domain 上 fine-tune
care 的是 target domain 做得好不好，fine-tune 以後 source domain 壞掉就算了
在 multitask learning 裡面，其實同時 care target domain 跟 source domain 做得好不好
care 在這兩個 domain 上，能不能夠同時把它做好
如果是用 deep learning base 方法的話，特別適合來做這種 multitask 的 learning
可以丟一個 neural network 假設兩個不同的 task，他們用的是同樣的 feature
比如說，都做影像辨識，只是影像辨識的 class 不一樣的話
我就 learn 一個 neural network，input 就是兩個不同 task 同樣的 feature
但是中間會分叉出來，一部分的 network 他的 output 是 task A 的答案
一部分的 network 他的 output 是 task B 的答案
這麼做的好處是，task A 跟 task B 他們在前面幾個 layer 會是共用的
在前面幾個 layer，會同時使用 task A 跟 task B 的 data 去同時 train 前面幾個layer
前面幾個 layer 是用比較多的 data train 的，所以可能有比較好的 performance
做這件事的前提就是，要確定這兩個 task 有沒有共通性，是不是可以共用前面幾個 layer
有一些更 crazy 的 task，現在連 input 都是沒有辦法 share 的
今天兩種不同的 task，不同的 input 都用不同的 neural network
把它 transform 到同一個 domain 上面去 在同一個 domain 上，再 apply 不同的 neural network
一條路去做 task A，一條路去做 task B 中間可能有某幾個 layer 是 share 的
如果在這樣的 task 下，也可以做 transfer learning
就算是 task A、task B 的 input、output 完全不一樣
如果覺得中間幾個 layer 他們有共同的地方
還是可以用這樣子的 model 架構來處理
Multitask learning 一個很成功的例子是多語言的語音辨識
假設現在有一大堆不同語言的 data
假設有法文、德文、西班牙文、義大利文還有中文
在 train model 的時候，train 你拿來做語音辨識的 neural network 的時候
可以 train 一個 model，它同時可以辨識這五種不同的語言
而這個 model 前面幾個 layer 他們會共用參數
後面幾個 layer，每個語言可能有自己的參數
這麼做是合理的，因為雖然是不同的語言，但都是人類說的
前面幾個 layer 就可以 share 同樣的資訊、可以共用同樣的參數
其實在 translation 也可以用同樣的事情
假設要做中翻英、中文翻日文，那可以把這兩個 model 一起 train
在一起 train 的時候，在中翻英、中文翻日文
都要先把中文的 data 做process 那把中文的 data 先做process
那一部分的 neural network 就可以是兩種不同語言的 data 共同使用
這個 transfer 到底可以 transfer 的多廣？
比如說德文法文都是同樣語系的語言 所以他們是可以 transfer 的
但這些歐洲的語言跟中文或許是不能 transfer 的
目前在語音的發現是，幾乎所有的語言都可以 transfer
過去還有人蒐集了十幾種語言，把他們互相之間兩兩做 transfer
做了一個很大的 N x N 的 table，每一個 case 都有進步
目前發現大部分的 case，不同人類的語言 就算覺得他們不是非常像，他們也都可以互相 transfer
這邊舉的例子是從歐洲的語言 transfer 到中文上面
這邊中文的 data 不是太多，可能是十小時
橫軸是中文 training 的 data
縱軸是辨識的 character 的 error rate
一開始 data 很少，假設只有用中文直接 train 一個 model
現在 data 很少，Error rate 當然就很大，當 data 越來越多
到一百多小時的時候，error rate 就可以壓到 30 以下
今天如果有一大堆的歐洲語言，把歐洲語言跟中文一起做 multitask training
用這些歐洲語言的 data，來幫助中文 model 的前面幾層，讓他 train 得更好
會發現就算是中文 data 很少的情況下
有做 transfer learning，可以得到比較好的 performance
當中文的 data 越多的時候，中文本身的 performance 越好
就算是中文有一百小時的 data，借用一些歐洲語言來的 knowledge
對這個辨識還是有微幅的幫助的
這邊的好處是，假設做 multitask learning 的時候
會發現有一百多個小時，跟你只有這邊大概是五十個小時以內
如果有做 transfer learning 的話，你只需要二分之一以下的 data
就可以跟原來有兩倍的 data 做的一樣好
常常有人擔心說， transfer learning 會不會有負面的效應
會不會有 negative 的 transfer 的結果
是有可能的，如果兩個 task 不像的話 Transfer 就是 negative 的
兩個 task 不像，讓 neural network 同時做兩個 task 反而把結果弄糟了
有人總會思考，兩個 task 之間到底能不能夠 transfer 然後 try and error ，這樣很浪費時間
所以有人 propose 這個 progressive neural networks 這個 progressive neural networks 其實是很新的作法
這個是 16 年的放在 arXiv 上的 paper 會發現它是有很多問題的
這個方法是這樣子 現在有個 Task 1，先 train 一個 Task 1 的 neural network
藍色這個 neural network Train 好以後，它的參數就 fix 住了
現在要做 Task 2，一樣有一個 neural network
但是 Task 2 它的每一個 hidden layer 都會去接前面 Task 1 某一個 hidden layer 的 output
所以在 training 的時候，它的好處是就算 Task 1 跟 Task 2 非常的不像
首先，Task 2 的 data 不會去動到 Task 1 的 model
所以 Task 1 一定不會比原來更差
再來 Task 2 他去借用 Task 1 的參數，但是它可以把這些參數直接設成 0
這樣子也不會影響 Task 2 的 performance 最糟的情況下，就跟自己 train 的 performance 是差不多的
如果有 Task 3，也就做一樣的事情
Task 3會同時從 Task 1 和 Task 2 的 hidden layer 得到 information
你可能會覺得這個 model 怪怪的
如果有五個 Task，那第五個 Task 不是要同時接前面四個 Task 嗎？
對，他是怪怪的。作者也有說怪怪的，等待大家提出更好的想法
假設 target data 是 unlabeled
而 Source data 是 labeled 時候
這邊的 task 是這樣
在 source data 有 function 的 input，也有 function 的 output
但在 target data 只有 function 的 input，沒有 function 的 output
舉例來說，source data 是 MNIST 的 image
Target data 是另外一個 corpus，MNIST-M 的 image
把 MNIST 的 image 加上一些奇怪的顏色跟背景
MNIST 是有label的
我們知道每一張 image 對應到哪個 digit
但 MNIST-M 沒有 label
在這種情況下，通常把 source data 視作 training data
把 target data 視作 testing data
這邊的問題是 training data 跟 testing data 是非常的 mismatch
在這種 source data，MNIST 上 train 出來的一個 model
直接 apply 到另外一個 corpus 上面，他也 work 的
雖然另外一個 corpus 要做的事情，也是辨識數字零到九
但他們 input 的 image 是非常不一樣的
要怎麼把 source data 上 learn 出來的 model 也可以 apply target data 上面呢？
如果直接 learn 一個 model
Input 是一張 image
不管 mismatch training 跟 testing 也沒有關係
直接 learn 一個 model 下去看看會怎樣
會發現結果是會爛掉的
如果把一個 neural network 當作是一個 feature 的 extractor
Neural network 的前面幾層可以看作是在抽 feature 後面幾層可以看作是在做 classification
如果把 neural network 的前面幾層看作是在抽 feature 的話 把 feature 拿來看會發現甚麼事呢
會發現不同 domain 的 data，他的 feature 完全不一樣
如果把 MNIST 的 feature 丟進去的話，是藍色的這些點
藍色的這些點很明顯分成十群
零到九，十個數字
但如果把另外一個 corpus 的 image 丟進去的話
會發現抽出來的 feature 就是紅色這群
這邊有把 t-SNE 降維以後的結果
反正做出來就是紅色這一群
會發現作 feature extraction 的時候
原來 source domain 的 image 跟 target domain 的 image
他們根本就不在同一個位置裡面
抽出來的 feature 完全不一樣
後面的 classifier 雖然可以把藍色的部分做好
但紅色的部分就無能為力
這邊希望做到前面 feature extractor 可以把 domain 的特性去除掉
這招叫 domain-adversarial training 等一下會講為甚麼出現 adversarial 這個字
之前已經看過 adversarial 這個字 在前面講 GAN 的時候
GAN 就是 generative adversarial 的 model 這邊又出現 adversarial
這邊 adversarial 跟 GAN 的原理是非常像的 希望 feature extractor 可以把 domain 的特性消掉
也就是 feature extractor 的 output
不應該是紅色跟藍色的點分成兩群
不同的 domain 不應該是分成兩群
而是不同的 domain 都應該被混合在一起
希望 feature extractor 的 output 是可以把不同 domain 的 image 混在一起
也就是把不同 domain 的特性消掉
怎麼 learn 這樣的 feature extractor 呢
在後面接一個 domain 的 classifier
把 feature extractor 的 output 丟給 domain classifier
Domain classifier 也是一個 classification 的 task
他要做的是根據 feature extractor 給他的 feature
判斷這個 feature 來自於哪個 domain
在這個 task 就是要分辨這些 feature 來自於 MNIST 的 corpus
還是來自於 MNIST-M 的 corpus
這件事情就是有一個 generator 的 output
然後有一個 discriminator 讓他的架構非常像 GAN
但是跟 GAN 不一樣的是 之前在 GAN 的 task 裡
Generator 要產生一張 image 騙過 discriminator 這件事很難
但是在 domain-adversarial training 裡面
要騙過 domain classifier 太簡單了
有一個 solution 是不管看到甚麼東西，output 都 0 就騙過 classifier 了
所以只 train domain classifier 是不夠的
因為 feature extractor 可以輕易的騙過 domain classifier
所以要給 feature extractor 增加任務的難度
所以 feature extractor 他 output 的 feature
不只要同時騙過 domain classifier
還要同時讓 label predictor 做的好
Label predictor 吃 feature extractor 的 output
他的 output 就是十個 class
所以 feature extractor 不只要騙過 domain classifier
同時還要滿足 label predictor 的需求
他抽出來的 feature 不只是要把 domain 的特性消掉
同時還要保留原來 digit 的特性
如果把這三個 network 放在一起的話
實際上它只是一個大型的 neural network 而已
他有很多層就像一個在 multitask learning 會用到的 neural network 一樣
但是這個 neural network 是個各懷鬼胎的neural network
一般的 neural network 的每一個參數，要做的事情都一樣
有共同的目標，要 minimize loss
要 optimize accuracy
但是在這個 neural network 裡，他的參數是各懷鬼胎的
藍色這部分 label predictor 要做的是把 class 的分類正確率做的越高越好
Domain classifier 要做的是正確的 predict 一個 image 屬於哪個 domain
Feature extractor 要做的是同時 improve Label predictor 的 accuracy
同時 minimize Domain classifier 的 accuracy
所以 feature extractor 是一個不好的隊友
Domain classifier 想要做 domain classification
Feature extractor 想要捅他隊友一刀
他隊友想要選總統但他背後捅他一刀
他不想要讓他選總統
他想要做的事情跟他隊友要做的事情是相反的
他是一個會陷害他隊友的 model
Feature extractor 怎麼陷害隊友呢
這件事情很容易，只要加一個 gradient reversal 的 layer 就行了
也就是說在作 back propagation 的時候
Domain classifier 計算 back propagation 不是有 forward 跟 backward 兩個 path 嗎
在作 backward path 的時候 Domain classifier 傳給 feature extractor 什麼樣的 value
Feature extractor 就把他乘上一個負號 也就是 domain classifier 告訴你某一個 value 應該要上升
他就會故意下降 作一個跟 domain classifier 要求相反的事情
Domain classifier 因為看不到真正的 image 所以最後一定會 fail 掉
他所能看到的東西都是 feature extractor 告訴他的
所以最後一定會無法分辨 feature extractor 所抽來的 feature 是來自於哪個 domain
問題是，domain classifier 一定要奮力的掙扎
這個 model 原理講起來很簡單
實際上的 training 可能跟 GAN 一樣是沒有那麼好 train 的
Domain classifier 一定要奮力掙扎
他一定要努力判斷現在的 feature 是來自於哪個 domain
如果 domain classifier 他比較弱、懶惰
他一下就放棄不想做了
就沒有辦法把前面的 feature extractor 逼到極限
就沒有辦法讓 feature extractor 真的把 domain information remove 掉
如果 domain classifier 很 weak
他一開始就不想做了，他 output 永遠都是 0 的話
那 feature extractor 胡亂弄甚麼 feature 都可以騙過 classifier 的話
那就達不到把 domain 特性 remove 掉的效果
這個 task 一定要讓 domain classifier 奮力掙扎然後才死掉
這樣才能把 feature extractor 的潛能逼到極限
這個其實是很新的方法 引用的是 ICML,2015 的 paper 跟 JMLR,2016 的 paper
這是 paper 一些實驗的結果
作不同 domain 的 transfer
包括從 MNIST 上 transfer 到 MNIST-M 上
從這個數字的 corpus，transfer 到另一個數字的 corpus
從這個數字的 corpus，transfer 到 MNIST 上面
兩種不同的道路號誌的 data 互相 transfer
如果看實驗結果的話 縱軸代表用不同的方法
這邊有一個 source only 的方法 直接 source domain 上 train 一個 model
然後 test 在 testing domain 上
在這四個結果會發現如果只用 source only 的話
Performance 是比較差的
這邊比較另一個 transfer learning 的方法
留給大家自己去參考文獻
這 proposed 的方法是剛剛講的 domain-adversarial training
如果用 domain-adversarial training 得到的結果是這裡
我們現在最下面這行
最下面這行是直接拿 target domain 的 data 去做 training
會得到 performance 是最下面這個 row
這其實是 performance 的 upper bound
會發現如果用 source data 跟 target data train 出來的結果是天差地遠的
這中間有一個很大的 gap
如果用 domain-adversarial training 可以發現有很好的 improvement
在不同的 case 上面都有很好的 improvement
接下來要講的是 Zero-shot learning
在 Zero-shot learning 裡面跟剛才講的 task 是一樣的
只有 source data 有 label，target data 沒有 label 在剛才的 task 裡可以把 source data 當 training data
Target data 當作 testing data 但實際上 Zero-shot learning 他的 define 又更加嚴苛一點
他的 define 是他的 source data、target data，他的 task 是不一樣的
比如說，在影像上面，你的 source data 可能是要分辨貓跟狗
Source data 裡面可能有貓的 class、狗的 class 但 target data 裡面，image 是草泥馬
在 source data 裡是從來都沒有出現過草泥馬的
如果 source data 裡面從來都沒有出現過草泥馬的話
Machine 有辦法看到他就說是草泥馬嗎？ 這實在是太強人所難
這個 task 在語音上很早就有 solution
語音本來就常常會遇到 Zero-shot learning 的問題
假如把不同的 word 都當作一個詞彙、一個 class 的話
本來在 training、testing 的時候，就有可能看到不同的詞彙
Testing data 本來就有一些詞彙
英文的詞彙這麼多，至少十萬個不同詞彙
Testing data 本來就有一些詞彙，是在 training 的時候從來沒有看過
在語音上怎麼解決這個問題
在語音上的作法是不要直接辨識一段聲音屬於哪一個 word
辨識的是一段聲音屬於哪個 phoneme
如果不知道 phoneme，就想成音標就好
所以辨識的單位不要定成 word，而定成 phoneme
再做一個 phoneme 跟 table 之間對應關係的表
這個東西叫做 lexicon，也就是辭典
建一個文字跟 phoneme 對應關係的表根據人的 knowledge
在辨識的時候，只要辨識出 phoneme 就好
再去查表，這一段 phoneme 對應到哪一個 word
這樣就算有一些 word 是沒有出現在 training data 裡面的
只要他在你建好的 lexicon 裡面有出現過
你的 model 可以正確辨識出現在聲音屬於哪個 phoneme 的話
就可以處理這個問題
在影像上，可以把每一個 class，用他的 attribute 來表示
也就是說，有一個 database，database 裡面有所有不同
可能的 object 跟他的特性
假設現在要辨識動物
但 training data 跟 testing data 他們的動物不一樣
但有一個 database
這 database 告訴你每一種動物有什麼樣的特性
比如說，狗就是毛茸茸、四隻腳、有尾巴
魚有尾巴，沒有四隻腳、沒有毛茸茸
Chimp，黑猩猩有毛茸茸，沒有四隻腳、沒有尾巴
黑猩猩沒有尾巴，猴子才有尾巴
Attribute 要定的夠豐富，每一個 class 都要有獨一無二的 attribute
如果有兩個 class 他的 attribute 一模一樣的話
等於那個方法會 fail 掉
在 training 的時候，不直接辨識每一張 image 屬於哪一個 class
而是去辨識每一張 image 具備什麼樣的 attribute
Neural network learning 的 target 就是看到猩猩的圖
就要說這是一個毛茸茸的動物、沒有四隻腳的動物、沒有尾巴的動物
看到一個狗的圖，就要說這是毛茸茸的動物、有四隻腳的動物、有尾巴的動物
在 testing 的時候，就算來了一張從來沒有看過的 image
沒有關係，neural network 的任務也不是要 detect input 這張 image 是屬於哪一種動物
只要 input 這張 image，他有甚麼樣的 attribute
Input 一張沒有見過的動物，但只要能夠把他的 attribute 找出來
就查表看哪一種，在 database，哪一種動物的 attribute 跟 model 的 output 最接近
有時候可能沒有一模一樣，也沒有關係，就看誰最接近
哪一個動物的 attribute 跟 neural network 的 output 最接近 那個動物就是你要找的
有時候 attribute 可能非常複雜 Attribute 的 dimension 可能很大
甚至可以做 attribute 的 embedding 也就是說現在有一個 embedding space
把 training data 裡的每一張 image 都透過 transform
把他變成 embedding space 上的一個點
然後把所有的 attribute 也都變成 embedding space 上一個點
g 跟 f 都可以是 neural network
training 的時候就希望 f 跟 g 越接近越好
在 testing 的時候如果有一張沒有看過的 image
這個 image 的 attribute embedding 以後，跟哪一個 attribute 最像
那就知道他是什麼樣的 image
草泥馬翻譯就是 grass-mud horse
我們在這邊先休息一下
各位同學大家好，我們來上課吧 剛才講到 Attribute embedding
image 跟 attribute 其實都可以描述成 vector
所以想要做的事情是把
image 跟 attribute 投影到同一個空間裡面
可以想像成是對 image 的 vector，也就是這邊的 x
跟 attribute 的 vector，也就是這邊的 y
都做降維，降到同樣的 dimension
把 x 通過一個 function f 都變成 embedding space 上面的 vector
把 y 通過另外一個 function g 也都變成 embedding space 上面的 vector
要怎麼找 f 跟 g 呢
f 跟 g 可以說是 neural network
input 一張 image，他變成一個 vector
或 input 一個 attribute 的 vector，他變成一個 vector
f 跟 g 怎麼找呢
你的 training target 就是，你希望說，假設我們已經知道說
這個 y1 是 x1 的 attribute，y2 是 x2 的 attribute
希望找到 f 跟 g 可以讓 x1、y1 投影到 embedding space 以後越接近越好
x2、y2投影到 embedding space 以後越接近越好
如果已經把 f 跟 g 找出來了
假如有一張從沒見過的 image 叫 x3 在你的 testing data 裡
他也可以透過 f 變成 embedding space 上的一個 vector
接下來就看這個 embedding vector 他跟
這邊有一個錯，這邊應該是 x3
這個 x3 應該跟 y3 的 embedding 最接近
y3 就是他的 attribute
再看他對應到哪一個動物
比如說他是 grass-mud horse，所以就是草泥馬
有時候會遇到一個問題：如果根本沒有 database 呢
根本不知道每一個動物他的 attribute 是甚麼，怎麼辦
那可以借用 word vector
word vector 的每一個 dimension 就代表了現在這個 word
他的某種 attribute
所以不一定需要有個 database 去告訴你每一個動物的 attribute 是甚麼
假設有一組 word vector，這組 word vector
你知道每個動物他對應的 word 的 word vector
這 word vector 你就會拿一個很大的 corpus 比如說 Wikipedia train 出來
就可以把 attribute 直接換成 word vector
所以把 attribute 通通換成那個 word 的 word vector
再做跟剛才一樣的 embedding，就結束了
那其實剛才的 training，這邊可以稍微講一下
假設我們的 training的criterion 是說
我們要讓 x^n 通過f(x)
y^n 通過 g(y) 他的距離越接近越好
就是找一個 f 跟 g Minimize f(x^n) 跟 g(y^n) 的距離
這樣子你覺得 ok 嗎？其實這樣子是會有問題
因為這樣 model 就只會 learn 到他把所有不同的 x 跟所有不同的 y
都投影到同一點，這樣子距離最少，就停住了 所以如果 loss function 這樣定，其實是不行的
所以需要稍微重新設計一下 loss function 前面這個 loss function 只有考慮到說
如果我們知道 x^n 跟 y^n 是一個 pair，要讓他越接近越好
但沒有考慮到的是，如果知道 x^n 跟另外一個 y 不是同一個 pair
他們的距離應該要被拉大 前面這個 loss function 沒有考慮到這件事
所以應該要改一下這個 loss function 怎麼改這個 loss function 呢？
一個可能是把它改成 max 裡面的兩個 element 分別是 0 跟 k – f(x^n) ⋅ g(y^n) 加上
max，找一個 m 不等於 n f(x^n) ⋅ g(y^m)
k 是自己 define 的 margin k 是一個 constant，training 的時候必須要自己 define
今天有一個 max，其中一個 element 是 0 另外一個 element 是一個看起來很長的式子
他會從 0 跟這個式子裡面選一個最大的 所以這一項的最小值就是 0
甚麼時候會等於 0 當你不是零的這一項，他的值小於 0 的時候
這個loss就會是0 如果 k – f(x^n) ⋅ g(y^n) 再加上後面這一項
max，m 不等於 n，f(x^n) ⋅ g(y^m) < 0 的時候 這一項會是 zero loss
再整理一下，把這兩項移到右邊，把左右對調
就得到下面這個式子
如果看下面這個式子的話就比較清楚這一項的涵義是甚麼
下面這個式子的意思是，甚麼時候會有 zero loss 呢
當 f(x^n) 跟 g(y^n) 他的 inner product 值大於另外一個
假設在所有不是 y^n 的 y 裡面
找一個 m 出來
這個 m 是跟 f(x^n) 最接近的
就算他們的 inner product 最大
還是要比正確答案，小一個 k
如果 x^n 跟 y^n 之間的 inner product 值要很大
要有多大呢？要大過所有其他的 y^m 跟 x^n 的 inner product
而且要大過一個 margin，k
如果定這個式子
不只是把 pair 起來的 attribute 跟 image 拉近
同時也要把那些不成 pair 的東西，把它拆散
其實還有一個更簡單的 zero-shot learning 的方法
叫 Convex combination of semantic embedding
這個方法是說，也不要做甚麼 learning
假設有一個 off-the-shelf 語音辨識系統
跟一個 off-the-shelf 的 word vector
這兩個可能不是自己 train，或網路上載下來的，
就可以做這件事情
把一張圖丟到 neural network 裡面去
他沒有辦法決定他是哪一個 class
他覺得他有 0.5 的機率是 lion，0.5 的機率是 tiger
接下來再去找 lion 跟 tiger 的 word vector
把 lion 跟 tiger 的 word vector 用 1:1 的比例混合
0.5 tiger 的 vector 加 0.5 lion 的 vector
得到另外一個新的 vector
再看哪一個 word 的 word vector 跟混合的結果最接近
假設是 liger 這個字最接近的話
那這個東西就是 liger
liger 是甚麼？
liger 就是獅虎，老虎跟獅子生下的後代叫 liger
這個是 paper 裡面舉的例子
在這邊不需要做任何 training
只要有一組 word vector，一個語音辨識系統
就可以做這樣子的 transfer learning
以下是這個方法的實驗結果 我覺得其實頗驚人的
來比一下人類跟機器的差別 來看這個，你覺得他是甚麼
來問一下大家的意見吧 覺得是海豹的同學舉手
覺得是海象的同學舉手 覺得是海獅的同學舉手
機器覺得怎樣呢 假設沒有做 transfer learning
直接拿一個 CNN 他覺得最有可能是 sea lion，是海獅
其他也是奇奇怪怪的 cowboy boot，也滿像 cowboy boot
他說是海獅，如果你答海獅，你跟 machine 同一個等級 正確答案 Stellar sea lion，北海獅
這 ImageNet 其實是一個很崩潰的 corpus 這是很崩潰的 task
在這種 task 上就覺得說機器是有可能贏過人的 我覺得應該不可能有人答得出正確答案
DeViSE 是另一個方法
是剛才講的把 word vector 跟 image project 同一個 embedding space 上的結果
在這邊的結果並沒有很好 比較荒謬的答案比如說 flip-flop
這個是剛才講的完全不用 training 的方法 結果很驚人，他的前五名裡面有北海獅
而且其他答案也都是海獅
California sea lion，Australian sea lion，South American sea lion
這邊有舉另外一個例子，剛剛用草泥馬當例子
那個例子並不是亂取的，因為 ImageNet 裡面真的有草泥馬
這個大家都知道是草泥馬 machine的答案是這樣子
第一名是 Tibetan mastiff，是獒犬
然後是 titi monkey我也不知道是甚麼
然後 Koala，然後 chow-chow 好像是鬆獅狗之類的
他其實滿像獒犬的
llama 其實就是草泥馬
表面上看起來有答對，其實沒有答對
因為答案是另外一種 lama
如果你有看過草泥馬傳奇的話
其實是部落裡面最強的騎手才能騎牠
草泥馬有很多種，至少有三種
這種是最強的，反正不是 llama 就對
llama 的臉是尖的
這種的臉是圓的
這個方法得到比較怪的結果
其實這個 task 有點難，這可能是一個比較失敗的例子
這個 network 也沒有得到正確的結果
它得到最接近的 domestic llama
Zero-shot learning 剛才舉的例子都是影像的例子
最後舉一個文字的例子
這個結果滿新的，這個結果最近才被放到 arXiv 上面
是 google 的 paper
現在不只要放 arXiv 還要做漂亮的動畫在部落格上面
這個是做 machine translation
在 training 的時候
machine 看過如何把英文翻成韓文，有這種 data
它知道怎麼把韓文翻成英文，有這種 data
它知道怎麼把英文翻成日文，有這種 data
它知道怎麼把日文翻成英文，有這種 data
接下來它從來沒有看過日文翻韓文的 data，但是它可以翻
它從來沒有看過韓文翻日文的 data，所以它也可以翻
這件事情怎麼做到的
如果看部落格的文章，好像有第二版本的 title 很聳動
Google 的 machine，發明了自己的 sequence language
後來標題就被偷偷改了，後來看到有另外一個版本的標題
那篇部落格可能也不是 google 寫的
為甚麼 Zero-shot learning 在這個 task 是可行的
因為如果用同一個 model 做了不同語言之間的 translation 以後
machine 可以學到的事情是怎樣的呢
它可以學到對不同語言的 input
都把不同語言的不同句子 project 到同一個 space 上面
而在這個 space 上面，它是 language independent 的
這個 space 上面的位置，只跟這個句子的 semantic 有關
舉例來說，這個是 paper 裡面的例子
這個 paper 說根據 learn 好的 translator
translator 有一個 encoder，它會把 input 的句子變成一個 vector
一個 decoder，根據這個 vector，解回前一個句子，就是翻譯的結果
如果把不同的語言，都丟到這個 encoder 裡面
讓它變成 vector 的話
這些不同語言的不同句子，在這個 space 上的分佈有什麼樣的關係呢
比如說這三個，英文、韓文和日文的三個句子
這三個句子講的是同一件事情、是一樣的意思
通過 encoder 的 embedding 以後，他們在 space 上面其實是差不多的位子
其實就是在這個位置
在這個圖上，不同的顏色代表同樣的意思
這些句子雖然是同樣的意思，但可能來自於不同的語言
比如說這邊含紅色的這三條線
他們在這個 space 上面是在一起，代表同樣的意思
但其實他們來自於不同的語言
你說這樣子算 machine 發明了一種新語言
其實也算是，如果把 embedding space 當作一種新的語言的話
machine 做到的就是發現了一種 sequence language
對每一種不同的語言，都先轉成只有他自己知道的 sequence language
再從這種 sequence language 轉成另外一種語言
就算是有某一個發音的 task，input 語言和 output 語言是 machine 沒有看過的
它也可以透過這種它自己學出來的 sequence language 來做 translation
這邊是更多的 Zero-shot learning 的 paper給大家參考
其實 Zero-shot learning 像剛才看到用在 image 分類上面
其實現在也可以用在 caption generation 上
希望 machine 如果看到一個從來沒看過的東西
它也可以用自然語言來描述它現在到底看到甚麼、或看到甚麼動作
剩下一點點部分
所以你遇到一個狀況是
target data 有 label，source data 沒有 label
剛剛講的狀況都是 source data 有 label 的狀況
有時候會遇到 source data 沒有 label 的狀況
但是 target data 可能有 label，可能沒有 label
這種 target data 有 label，source data 沒有 label 的狀況
叫做 Self-taught learning
target data 有 label，source data 沒有 label，這種是 Self-taught learning
target data 沒有 label，source data 也沒有 label，這種是 Self-taught clustering
在 Self-taught learning 裡面，作者強調這跟一般的 transfer learning 是不一樣的
這個名詞都是大家自己定的
在 Self-taught learning 作者裡面，它的想法是 transfer learning 是有 label 的
在 source domain 是有 labeled data 的才叫 transfer learning
Self-taught learning 不算是一種 transfer learning
不過後人都把 Self-taught learning 當作是一種 transfer learning
這個只是文字上面枝枝節節的小問題
有一個要強調的是 Self-taught learning 跟 semi-supervised learning 有一些不一樣的地方
Semi-supervised learning 在 learning的時候，也是有一些 labeled data
一些 unlabeled data
可以說 source data 是 unlabeled data，target data 是 labeled data
這也是一種 semi-supervised learning
這種 semi-supervised learning 跟一般 semi-supervised learning 有一些不一樣
一般 semi-supervised learning 會假設那些 unlabeled data 至少還是跟 labeled data是比較有關係的
但在 Self-taught learning 裡面
那些 unlabeled data、那些 source data，跟 target data 關係比較遠
其實 Self-taught learning 概念很簡單 假設 source data 夠多，雖然它是 unlabeled
可以去 learn 一個 feature extractor 在原始的 Self-taught learning paper 裡面
他的 feature extractor 是 sparse coding 因為這 paper 比較舊，大概十年前
現在也不見得要用 sparse coding 也可以 learn
比如說 auto-encoder
用 auto-encoder 跟 decoder 來做feature extractor
總之，有大量的 data，他們沒有 label
可以做的是用這些 data learn 一個好的 feature extractor
用這些 data learn 一個好的 representation
用這個 feature extractor 在 target data 上面去抽 feature
在 Self-taught learning 原始的 paper 裡面
其實做了很多 task，這邊不要一一細講
這些 task 都顯示 Self-taught learning 是可以得到滿顯著的 improvement
台灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
