this is the introduction of the paper
entitled training course which in
language model with monolingual data
this is the outline of this presentation
we firstly introduce the task and
formulate the problem then we go through
the process of training language model
using modeling go data and we will
introduce generating pseudo data and
proposed approaches the experiment will
be presented in the end code switching
is about using two or more languages in
a single sentence or document however
code switching tasks service Fantasia
insufficient issue comparing the data
quantity collision data Israel and
modeling data monolingual data is easy
to collect and we wonder if it is
possible to take advantages of
monolingual data and conservation task
in this article we target new role based
language model as the task will turn the
coast region language model without
using conservation data we firstly
formulate all problem we collect three
date of data the first and the second
set are monolingual text in different
languages
the third step consists of constructing
sentences which are mixing languages of
each model lingo data a monolingual
sentences are used on training stage and
course which in sentences are used a
testing stage only and our goal is to
train a coach in language model the
elegant language model can be formulated
a sleaze equation we multiply an elegant
hidden state to a projection matrix W to
obtain a word distribution in time stamp
I length gradient descent is used to
update the parameters of our gun with
cross entropy loss function consider two
languages in quarantine language
modeling the numbers of order R K and M
in the vocabulary of each language and
the k plus M is revoke a precise inner
compass led the Alpha production metric
W is partitioned into
1 and W 2 with each color indicating the
latent representations of each word in
language 1 and 2 respectively let's see
what would happen if we simply trendy on
an English model on the neural encoding
a set of language 1 and language to our
training data is pure monolingual tags
in language 1 and language to the
projection metric is readily initialized
a simple diagram of it is plotted below
when we start to train the coast region
language model the input X would be
simple from the first molar english set
and the W 1 is updated or the input text
would be sampled on the second modeling
cassette and the W 2 is updated after n
steps training the distributions have
arbitrary shapes based only language
characteristics however without singing
bilingual word pairs the word
distributions may converge into layer
ownership without correlating to each
other
and we show the practical distribution
with PCA visualization here at the
impulse stage we use code-switching
sentences for testing when the prefix
word in language 1 is fed into the model
the output word distribution would focus
on the words in the selling which which
implies the hidden state H I is more
similar to the representation in W 1
space and because the hidden state is
dissimilar to W 2 space which makes
language model do not switch from
language 1 to language two however an
ideal language model should consider a
probability to predict words which
belongs to the other language which
means the hidden state should be similar
to the representations in both languages
spaces so we assume that over
in space benefit Coast region language
modeling to this end we attempt to bring
to his face w1 and w2 closer to each
other and here we introduce some simple
ways to obtain language mixing text with
the pseudo training data language model
system code switching sentences which is
supposed to bring two language spaces
closer to each other to synthesize
pseudo code switching data we can change
the language of words in modeling
context for example we can simple a
sentence from the first text set then we
can substitute some words to the other
language with a predefined probability
the new sentence is added to the
original corpus at the training data
however this method need an additional
beeping dictionary between two languages
so here we introduce the other methods
to synthesize pseudocode switch in data
without having an additional dictionary
we could apply sentence concatenation to
achieve the goal with simply two
sentences from each Moodle lingo set we
concatenate two sentences and the
synthetic result are added into the
original training data by doing so
language model can see some mixing
situation during training also we
propose some methods to train a
conservation language model without
using real or synthetic code switching
tasks as we mentioned previously the
overlapping spaces may help on cost
virtual language modeling the first
proposal is to minimize the distance
between two language spaces we use
symmetric KL divergence and constant
distance as the metric both of land are
treated as a regularization term during
optimizing Coast region language model
the other proposal is to normalize the
projection metric in training stage that
is H what representation is divided by
its l2 known to possess unit know which
implies lacked the onion hidden state
would be similar when we train language
model using monolingual text only and
let's see our experiment we use a course
which in koppers nancy me as our
experimental data simi consists of two
main languages the first part is monaco
text containing pure mentoring and pure
English transcriptions which are used in
training stage and the second part is
called switch in text which is for
testing our baseline language model is
training without seeing code switching
sentences we also utilize pseudo code
switching data for training the
evaluation metric explicity lower
propensity indicates higher confidence
in the predicted target and we show
three kinds of perplexity the first CS
ppl is low perplexity and code switching
sentences the second is CSP ppl which
stand for perplexity and collusion point
code switching point occurs when
language of next word is different from
current world and the third one is
capacity of whole corpus due to the
difference between CS ppl and CSP ppl
links capacities are separated measured
clearly improvement in CS ppl do not
necessarily translate to improvements in
CSP ppl as cost within sentences often
contain a majority of non core switching
point CS ppl is likely to benefit more
from improving monolingual perplexity
LEM improving CSP ppl and here is the
experimental result the baseline model
resulted in the worst of laxity and we
observe learning with pseudo code
switching sentences indeed health in Koh
solution perplexity which is reasonable
because the language model has seen cost
which encases during training even
though the trendy license
however applying cosine distance and
symmetric KL divergence constraint
resulted in better propriety than
language model trend with pseudo code
switching data using synthetic code
switching text and applying constraint
are not conflict to each other so we
show two result with combine techniques
above as we can see using sentence
concatenation method and applying
symmetric KL divergence resulted in the
best capacity we further show the result
with normalization technique we can see
the baseline is improved with
normalization with further applying
techniques no matter which kind of
pseudo code switching tags are used for
training constraining projection metric
can improve perplexity especially using
symmetric KL divergence as the
constraint and here are some PCA
visualisation result of the projection
metric under 2d plan the leftmost one is
the result of baseline model which shows
linear separable between two language
spaces the middle one is the result of
using pseudo code switching data for
training which shows closer than the
baseline model but without excess
overlap and the rightmost one is the
result of applying symmetric KL
divergence which totally overlap please
visualize it a result correspond to a
numerical result mentioned previously
that is the closer the two spaces are
the lower the perplexity is to analyze
whether word with equivalent semantics
in different languages are meant
together with the proposed approaches we
conduct experienced and unsupervised
bilingual world induction
given a semantic align or pair in
different languages such as today in
English and Indian in Mandarin we
usually award roots and
chance to retrieve each other for
example we want to retrieve what why
using word X we compute the cosine
similarity between VX and
representations in w-2 and check whether
the world y is retrieved the precision
at ten are shown in the table as we can
see applying proposed approaches helps
our procedures list suggest that
constrain and normalization for a
collation language modeling indeed
enhance semantic mapping so this is a
brief introduction to our paper further
details can be found in the paper and
thank you for listening
[Music]
