臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.tw
所以，接下來就是要看的就是 shallow 的 network
竭盡全力的狀態
那我們剛才反覆講過很多遍了
一個 ReLU 的 network
就是一個 piecewise 的 linear function
我們現在要討論的就是
因為我們知道說，如果是一個 shallow 的 network
每一個 piece 我們都需要至少一個 neuron
才能夠製造出一個 piece
所以今天 piece 的數目愈少，需要的 neuron 就愈少
如果我們要想辦法 fit
y = x^2 的時候
到底最少要多少的 piece
才能做到這件事情呢
那我們剛才在討論的時候
我們在製造這個 piecewise 的 linear function 的時候
我們都是把我們目標的 function
是紅色這一條
紅色這條拿出來，然後在上面取幾個點
然後，把這些點連起來
就說這個是我們要去 fit target function 的
piecewise linear 的 function
那能不能做得更好呢？
我們假設其實是可以做得更好的
怎麼樣做得更好，我們不要讓這些線段呢
他的頭還有尾跟紅色的線相接
對不對，因為就是這邊這個黑色的線段
我們之前在做的時候
我們都強迫他的這個頭跟尾
一定要跟我們目標的線
假如我們現在目標的線是 y = x^2
我們就假設他的頭跟尾
一定要跟目標的線接再在一起
我們現在能不能夠把這一條線
把他往下挪一點
不要讓他們頭接在一起，不要讓他們的尾接在一起
如果你這樣做的話
你從這張圖上，你直覺一看就知道說
這個 error 比較大
這麼做其實 error 是比較小的
當然你可以，你可能會問說，左邊這個狀態
是確實是可以用 ReLU 產生這樣子的 network 的結構
但右邊這個狀態
這邊有一個非連續的東西
ReLU 能夠產生非連續的 output 嗎？
你仔細想想應該是沒有辦法的
那沒有關係，這個就是他的夢幻狀態這樣子
就是他在夢中可以使出彗星的一擊這樣子
所以，雖然實際上他只能這樣搞
他一定要每一個線段都接在一起
因為他必須要是連續的
但是，我們就假設夢幻的狀態是
夢幻的狀態是不需要連續的
那反正就先看看如果在不需要連續的狀況下
到底需要多少個片段
在不連續的狀況下到底需要多少個片段
我們才能夠去 fit
x^2 這個 function
所以，這是個夢幻的狀態
那在夢幻的狀態我們可以有比較小的 error
另外，我們之前有講過
今天滿足上面這個條件就自動滿足了
下面這件事情
滿足下面這件事情
不見得滿足上面這件事情，對不對
因為你這邊是
他們兩個最大差距 ≦ ε
所以，你這個積分可以看出平均嘛
平均會 ≦ ε
但是平均 ≦ ε 並不代表他們最大的差距 ≦ ε
那沒有關係，我們現在不要管這一項
不要管最大差距，我們只看平均能不能 ≦ ε
那你說跟剛才我們狀態不一樣不太能比
沒關係，這個就是再給那個 shallow 的 network
一些 benefit 這樣
就好像說他的裁判什麼，通通都是自己人這樣子
就給他各種不同的優勢
各種不同的，不可能達成的夢幻的狀態
看說，在給他這麼多的優勢的情況下
他到底可以做到多好
所以現在呢，我們不考慮 max
我們不考慮 max，我們只考慮這個積分的結果
我們只考慮他的這個 Euclidean 的 norm
就這個 Euclidean 的 norm
那接下來我們現在要問的問題是
假設這個紅色的線就是 y = x^2
然後，在上面取一個線，取一個片段
這個片段的長度是 l，他的起始是 x_0
他的終止的位置是 x_0 + l，他的長度是 l
然後，我要用某一條直線去 fit
紅色這個 y = x^2
我到底可以 fit 多好呢？
這條直線就如同我剛才講的，是一個夢幻的狀態
不需要頭對頭，不需要尾對尾
這個直線可以是任何一條直線
到底在給定 l 的狀態下，可以 fit 到多好呢？
那你可以想像說我們其實就是，解這樣一個問題嘛
我們就算說，積分從 x_0 到 x_0 + l
積分從 x_0 到 x_0 + l
然後，我們要算 x^2 就是紅色這一條線跟 ax + b
也就是某一條直線 a 跟 b 的值是多少，我們還不知道
x^2 跟 ax + b 這一條直線
他們的差距的平方和
就得到了他的 error 的平方
你就想，我們就是要算這一項
然後找出一個 a 跟 b 可以讓 e^2 的值最小
然後接下來，你就是怒算一波
你可以回去 check 一下，我有沒有算錯
算出來是 l^5/180 這樣子
如果這個是不對的，你再告訴我
我就不要把計算過程寫出來了
不然你等一下聽了就生氣了
這個是 l^5/180
所以，也就是說今天在
這個結果其實還蠻神奇的
就是給一個線段，他的長度是 l
如果我今天考慮的是
y = x^2 在任何區間裡面
他的誤差都只跟 l 有關
他的誤差最小就是 l^5/180
但他是怎麼做的呢？
這邊給大家一些方向
如果這一段你沒有聽懂的話沒有關係
就是你沒有聽懂就算了的意思
大家記不記得我們在線性代數的時侯，我們這樣學
我們學說，有兩個 function 叫做 w 跟 u
然後，我們希望要 minimize
我們希望把 w 跟 u，做 linear combination
找一個 w 跟 u 的 linear combination 的結果
aw + bu，然後希望這個新的
linear combination 得出來的 vector
跟 v 的距離越近越好
那怎麼找呢？我們就是找出
w 跟 u 他們展開的那個 space
你要找，就假設 u 跟 w 就是
他們是 orthogonal basis 的就可以直接展開這樣子
如果不是，你就另外想辦法
反正就是把 u 跟 w 展開成一個 basis
展開成一個 basis，然後
把 v project 到那一個 basis 上面
你就可以知道 aw + bu 是哪一個了
就這樣，我們在線性代數學到這個東西
那其實在線性代數課本的最後一章有說
每一個 function 都是一個 vector 對不對
我們都可以用線性代數學到的 operation
來處理這些 function，所以我們假設
某一個
function 就是 x^2 ，x^2 是一個 function 叫做 fv
然後 x 是一個 function 叫做 fw
然後，常數項是一個 function 叫做 fu
我們現在要做的事情是希望
找到一組 a 跟 b，把 fw 跟 fu 做 linear combination
讓他跟 fv 的距離越接近越好
那我們要定義什麼叫做兩個 function 間的距離
那我定義兩個 function 間的距離
就是這個樣子
就是計算從積分 x0 到 x0 + l
這兩個 fumction 的差的平方和
就是他們之間的距離
然後，接下來，你就可以說我們找出
afw + bfu 的這個展開的那個 space
然後把 fv project 到那一個 space 上面
然後，你就可以找出 a 跟 b
然後，怒算一波，就結束了這樣子
就這樣子
總之，無論如何就變成
反正就跳過去
然後算出來結果，就是
總之 error 是 l^5/180
那接下來狀況是這個樣子
我們假設，我們可以放 n 個線段
那 n 個線段放上去的時候，我們得到的 error 的最小值
就我們要如何分配這 n 個線段
使得他的 error 的值越小越好呢？
如果可以的話，可以小到什麼樣的地步
我們如何分配這 n 個線段
使得 error 的值最小，所以
我們在 0 到 1 之間，就給他切 n 個線段
這 n 個線段不用是一樣長的
反正就想辦法讓
想辦法分配這個 n 個線段的位置
想辦法分配 l1 到 ln 的長度
我們希望最後算出來的 error 是最小的
那 l1 加到 ln 的和阿
會等於 1，這是唯一的 constraint
因為我們要考慮的就是 0 到 1 的區間嘛
我們把他分成 n 個區間，把 0 到 1 之間切 n 份
那每一份的長度就是 l1、l2 一直到 ln
l1 + l2 + ln 合起來
他的值應該要是 1
那假設這邊的長度是 l1 的話
那他的 error, e1^2 就是 l1^5/180
e2^2 就是 l2^5/180
e3^2 就是 l3^5/180，以此類推
所以 total 的 error
我們用 e^2 代表 total error 平方
就是每一項 e1, e2 一直到 en
每一項 e 的平方和
那每一項 e, ei 是 li^5/180
所以 e^2 是 summation n = 1 到 n，li^5 /180
那接下來的問題就是，怎麼分配 l1 到 ln
使得這個值最小這樣
就假設我們現在只能夠用 n 個片段
來 fit 這個 function
那我們要 fit function 是 y = x^2
我們怎麼分配 l1 到 ln 使得
total 的 error 的值最小
我們知道 total error 的值，寫成這樣
summation i = 1 到 n，li^5/180
li 唯一的限制是這樣子
我講到這邊，大家有問題想要問的嗎？
其實就算沒有證明，你直覺知道這一題的
答案是什麼呢？應該是平均分配吧，對不對
你問為什麼，沒有為什麼，平均分配就
其實等一下，我告訴你為什麼
但我猜你大概也不想知道這樣子
你直覺想就知道平均分配一定
可以讓他的 e^2 最小這樣
我直覺就覺得應該是平均分配會讓 e^2 最小
如果平均分配的話
那 li 就等於 1/n
那 li = 1/n，把 1/n 代進去
代進去，所以 e^2
就是 summation i = 1 到 n，1/n^5/180
然後這邊這個重複 n 次嘛，對不對
這項重複 n 次所以乘個 n
所以 1/180 * 1/(n^4)
講到這邊大家有問題要問的嗎？
你可能問說為什麼要平均分配，這邊有個
warning of math 告訴你為什麼要平均分配
怎麼平均分配呢？
這邊要用一個 Holder's inequality 這樣子
怎麼說呢？我們現在的
就是說，前面這邊有一個 180 這樣
我們不要管那個 180
因為這個不重要，他不會影響你的結果
我們只要知道說 summation i = 1 到 n li^5
要怎麼讓他的值最小這樣子
怎麼做呢？
有一個 inequality 是這樣子說的
我們現在有 n 個值 a1 到 an
有 n 個值 b1 到 bn
有兩個數值 t 跟 q 然後
1/t + 1/q 的值會等於 1
這個時候有一個 inequality 告訴我們說
ai * bi 的絕對值
從 i = 1 加到 n 會小於等於
ai^p 從 i = 1 加到 n
再開 p 次方
然後 bi^q summation i = 1 到 n，再開 q 次方
然後，接下來，我們只需要把
a1 跟 an 還有 b1 跟 bn 換成我們想要的東西
把 ai 跟 an 換成 l1 到 ln
b1 到 bn 通通換成 1
我們就可以簡化上面這個式子
ai * bi，li * 1 就是 li
那 l 一定是正的，所以絕對值也不用了
因為 l 一定是正的嘛
他是線段的長度，所以他一定是正的
所以，絕對值也不用
那 ai 就是 l 把他放到這邊
bi 就是 1 把他放到這邊
所以，我們得到了這一個不等式
那這個不等式可以輕易的簡化他
因為根據我們對 l 的限制
summation i = 1 到 n li 他的值
就是 1，所有線段 total 的和就是 1
1^q summation i = 1 到 n 就是 n
所以，這邊這一項是 1
這一項是 n 的 1/q 次方
把 n 的 1/q 次方拿到左邊去
變成 n 的 -1/q 次方，這邊是 n 的 1/q 次方
拿到左邊去，變成 n 的 -1/q 次方
小於等於 summation i = 1 到 n li 的 p 次方
再取 1/p 次方
然後，這邊是 1/p 次方
然後，兩邊都取 p 次方
所以這邊 1/p 次方就拿到左邊
變成 n 的 -1/q 次方
小於等於 summation i = 1 到 n li 的 p 次方
然後這個 p/q 是多少呢？
你就知道說 1/p + 1/q = 1
我們要把 p/q 製造出來
所以左右同乘以 p
所以 p/p 是 1，p/q 是 p/q
然後把 p 乘過來
p 在右邊，左右都乘 p
所以 1 + p/q 就等於 p
所以，負的 p/q 就是這一項，等於多少呢？
把 p/q 拿到右邊去變成 1 - p
所以這一項是 1 - p
然後 p 代 5
這邊就有 summation li^5 了
這是我們要 minimize 的這一項
我們知道說，他的最小值
就是 n 的 1 - p 次方
也就是 n 的 -4 次方
跟我直覺想的是一樣的這樣
這沒什麼好笑的
就是說，你直覺也覺得說應該是
每一個片段一樣應該會最小
果然怒導一波以後，是這個樣子
這個不重要，假設你剛剛沒有聽懂的話真的就算了
那現在得到的結果阿
是這個樣子的
E 的最小值
我的滑鼠還在嗎？
E 的 E^2 的最小值是 (1/180) * (1/n^4)
所以 E 的最小值阿
他的最小值就是 sqrt(1/180) * (1/n^2)
這是他的 error
他的 error 的 lower bond 是這個樣子
那我們希望這個 lower bond
他小於等於我們給定的誤差 ε
那我們希望這個 E 這一項
小於等於我們給定的誤差 ε 的話
那我們需要多大的 n 呢？
我們 sqrt(1/180) * (1/n^2) 要 ≦ ε 的話
那我們會希望把 n^2 項拿上來
ε 項拿下去
我們希望 n^2 ≧ sqrt(1/180)*(1/ε)
我們希望 n 大於等於，就再開一次根號
變成 1/180 開 4 方根號，乘上 sqrt(1/ε)
所以，我們需要這麼多的 n
這麼多的 linear 的 piece
才能夠讓
才能夠讓 error ≦ ε
那我們知道說每一個 piece 都需要一個
在 shallow 的狀況下，都需要一個 neuron 來製造
所以，我們這邊仍然需要
O(1/sqrt(ε)) 的 neurons 才能夠去 fit y=x^2
所以現在，我們剛才疑慮就消失了
我們本來不知道 shallow 的最佳的狀態是什麼
那我們現在說，就算是給他一些不公平的狀態
給他一個夢幻的狀態
其實他就只能夠做到這樣
所以，我們剛才看到那一個不知道是好還是壞的狀態
已經是 shallow 的最佳的狀態
而另外一方面 deep 這一個狀態
我們不知道他是最好還是
沒有很好這樣子
我們只是可以這樣做到
我們有一個方法可以用這麼多的 neuron
就去 fit y = x^2
接下來就可以 fit polynomial
我們只需要這麼多的 neuron 就可以 fit y = x^2
能不能更少呢？ 不知道
但我們至少知道，隨便找一個方法就這樣子了
那 shallow 的方法竭盡全力也就只能夠做到這樣了
所以，這邊就是告訴我們說
deep 是比 shallow 好的
後面有一些現在有的理論
然後，今天講的是
現在有的理論的異常簡化版這樣子
那你可以自己去看看
那些 paper 裡面講的都還蠻複雜的
那我這邊就是用流水帳的方式呢
帶過去就那些理論是長什麼樣子
那這邊都是簡化的版本
那實際上的，更詳盡的說明
你再自己去看看那些文章
在最早的時候
我看找到最早的跟這些 deep 的
deep fit function 的 power 有關的文件
是從 2016 年開始
那這邊是發表在 COLT
他是我找到的最早的一篇
那他說，他證出來的理論是這樣
這個是比較早的結果
他證出來的理論是這樣
他是說，有某一個 function
他是由三個 layer 的 feedforward network 所組成的
那他這個 function 他沒有辦法
被兩個 layer 的 network 所逼近
但是他只是說，有存在這樣的 function
並不是說所有的function 都是這樣子的，注意一下
你隨便找一個很簡單的 function
比如說 linear 的 function，比如說
就是一條平的水平線的 function
那這樣比起來這個 deep 跟 shallow 比起來
在 fit 那種 function 上是沒有任何優勢的
所以這邊的理論只是告訴說，存在某一個 function
他可以用三個 layer 的 network 來表示
但卻沒有辦法被兩個 layer 的 network 來表示
除非那個兩個 layer 的 network
非常的巨大
那他的證明是個 general 的證明
我們剛才討論的都是在 ReLU 的 case 上面
他不限於 ReLU
他可以 try 在其他的 activation function 上面
然後，他告訴我們說
假設那個三個 layer 的 network
他的寬度，每一層的寬度是 k
那兩個 layer 的 network 要怎麼樣才能夠
逼近那個三個 layer 的 network
所構築出來的那個 function 呢？
你要把那個寬度
他的那個寬度，你要把那個 k
取 4/19 次方
然後放在指數
然後這邊有兩個常數項 A 跟 B
然後，你需要這麼多的 neuron
兩個 layer 的 network 需要這麼多的 neuron 才可以去
approximate 這個三個 layer 的network
你會發現說，這個三個 layer 的寬度
居然在兩個 layer 這邊是放在指數項的
雖然他這邊有取一個 4/19 次方會讓
這個 k 的值變小，但是他是被放在指數項
好那這邊是只有證三個 layer 跟兩個 layer
所以，他是可能是比較 limit
這邊有一個更 general 的
他也是在 COLT 的 2016
他說存在著一個 function
他可以被 deep network 所表達
但是沒有辦法被 shallow 的 network 所表達
跟剛才講的一樣除非那個 shallow 的 network 非常的大
他的證明一樣也不限於 ReLU 的 function
他的證明是這樣，他說
假設你有一個很 deep 的 network，他的
layer 的數目是 θ(k^3)
然後，他的
每一個 layer 的 node 的數目是 θ(1)
那他的參數是 θ(1)
你可能說，怎麼突然討論參數
而且這邊他還討論了參數的量
因為這邊指的是不同的參數
也就是說，你的不同的 layer 是可以 share 參數的
你可以不需要那麼多的參數
就構築出一個非常複雜的 deep network
那 shallow 的 network 呢？
shallow 的 network
如果他的 layer 的數目是 θ(k)
那他至少要 2^k 的 node
才可以逼近這個 deep 的 network
但是，這些 paper 裡面他們討論的都只有一直說
存在這樣子的一個 function
那接下來有人就會問說
那雖然說存在這樣的 function
那個 function 會不會非常的奇怪
然後，在真實的 case 上
完全派不上用場
所以接下來，有人就證了說
那存在的那一個 function
它不見得是一個很奇怪的 function
至少我們舉一個例子，他是一個
球狀的 function，就你做一個球
然後這個球裡面的值，你就做一個 function
他在這個球裡面都是 1，在球外面都是 0
他就是前述的那幾個理論
裡面講的 function 的其中一個
那這種球狀的 function，一看就很有用
因為你可以做一個，做分類的時候可以用上
就是某一個類別都落在球裡面
某一個類別都落在球外面
然後，這篇 paper 還做了實驗的證明
假設我們現在要去 fit 這一個 function
我們用兩個 layer 的 network
跟用三個 layer 的 network 來比較看看
那你會發現說，兩個 layer 的 network
你不管怎麼開他的寬度 100、200、400 到 800
都沒有辦法去 fit 那個球
那一弄到三層，寬度只有 100
你得到的 error 馬上就有很大的下降
那他只是用這個實驗來驗證這個理論而已
那剛才講的都是比較 specific 的 case
其實現在還有很多的證明
我就沒有把他們的理論
寫上來 check 一下，這幾篇 paper
那其實在這些 paper 裡面
你知道要證明所有的 function
都是 deep 比較好
shallow 比較差是不可能也不合理的
因為你只是隨便拿一個 function 出來
比如說，他是一個 linear function
所以，deep 就不見得會佔到優勢
所以，deep 要比較好的前提通常是
你對那個 function 還是要做一些限制的
就你不能夠證說
任何 function 都是 deep 好，shallow 差
你沒辦法證這件事，而且實際上也不是這個樣子
你只能是說，假設你的 function
有某種程度的複雜度
比如說他的 curvature 是多少那
每一篇 paper 證的就不太一樣
這樣子就是每一個 function
你要考慮的那一個 function
他在有某種複雜程度的前提之下
我們通常可以得到，類似以下這樣的結論
假設你有一個 network
你希望他的 L2 norm 的 accuracy 小於 ε
那他是一個 ReLU 的 network
他的深度是固定的
那你需要的 neuron 的數目
可能是 1/ε 的某一個 polynomial 項
1/ε 放到某一個 polynomial 的 function 裡面
但是，如果你今天的 network 可以很深
他的深度，是跟 ε 有關係的
這個時候，你的寬度只需要 log (1/ε)
代到 polynomial 裡面去
本來需要 1/ε 代到 polynomial 裡面去
現在是 log (1/ε) 代到 polynomial 裡面去
所以，他們的差別是差了一個 exponential
跟我們剛才在前面講得比較簡單的推導
其實，得到的結果是一致的
這一頁投影片，想要講的是
其實還有別的說法
這個是，他說什麼情況
這個細節，大家再 check 這個 paper
什麼情況下 deep 會比較好呢？
他說，如果那個 function
有一個 compositional 的 structure
那 deep 的會比 shallow 還要好
那我今天要講的差不多就是這樣
然後，我們就請助教來講一下作業
那在請助教來講作業之前
我還是會今天的課程下一個結論
就假設剛才講了那麼多東西，你覺得很煩燥
都沒有聽下去的話
那今天得到的結論就是這樣
你想要 fit 某一個 function
那 deep 會比 shallow 還要好
然後，他的好是 exponential 的那麼好
那要 fit 那個 function
就是說這樣子，結論不是就這樣子
那一個 function，我們今天證了 y = x^2
是那樣子的 function
那你可以很直覺的覺得說
也許比 y = x^2 還要簡單
比如說一次的，不符合剛才的說法
但是，比 y = x^2 複雜的就會符合
剛才那一個說法，而我們要考慮要 fit 的 function
一定都是比 y = x^2 複雜
所以 deep learning 是很有用的
就是這樣子
那接下來我們就請助教來講一下作業
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.tw
