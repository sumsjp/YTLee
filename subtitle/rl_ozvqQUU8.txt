臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心http://ai.ntu.edu.tw/
那剛才講的是說
在 reward 非常 sparse 的情況下
要怎麼做
我們接下來要講的是 Imitation learning
Imitation learning 就更進一步討論的問題是
假設我們今天連 reward 都沒有
那要怎麼辦才好呢？
這個 Imitation learning 又叫做
learning by demonstration
或者叫做 apprenticeship learning
apprenticeship 這個字是那個學徒的意思
那在這 Imitation learning 裡面
你的 setup 是這樣子
你有一些 expert 的 demonstration
那 machine 也可以跟環境互動
但它沒有辦法從環境裡面得到任何的 reward
他只能夠看著 expert 的 demonstration
來學習什麼是好，什麼是不好
那你說為甚麼有時候，我們沒有辦法從環境得到 reward
其實，多數的情況，我們都沒有辦法
真的從環境裡面得到非常明確的 reward
舉例來說，因為如果今天是棋類遊戲
或者是電玩，你有非常明確的 reward
但是其實多數的任務，都是沒有 reward 的
舉例來說，你說開自駕車
雖然說自駕車，我們都知道撞死人不好
但是，撞死人應該扣多少分數，這個你沒有辦法訂出來
撞死人的分數，跟撞死一個動物的分數顯然是不一樣的
但你也不知道要怎麼訂
這個問題很難，你根本不知道要怎麼訂 reward
或是 chat bot 也是一樣
今天機器跟人聊天，聊得怎麼樣算是好
聊得怎麼樣算是不好
你也無法決定
所以很多 task，你是根本就沒有辦法訂出reward 的
但是，雖然沒有辦法訂出 reward
但是，收集 expert 的 demonstration 是可能可以做到的
舉例來說，在自駕車裡面
雖然，你沒有辦法訂出自駕車的 reward
但收集很多人類開車的紀錄
這件事情是可行的
在 chat bot 裡面，你可能沒有辦法收集到太多
你可能沒有辦法真的定義什麼叫做好的對話
什麼叫做不好的對話
但是，收集很多人的對話當作範例
這一件事情，也是可行的
所以，今天 Imitation learning
其實他的使用性非常高，假設
你今天有一個狀況是，你不知道該怎麼定義 reward
但是你可以收集到 expert 的 demonstration
你可以收集到一些範例的話
你可以收集到一些很厲害的 agent
比如說人跟環境實際上的互動的話
那你就可以考慮 Imitation learning 這個技術
那在 Imitation learning 裡面
我們等一下，就是介紹兩個方法
第一個叫做 Behavior Cloning
第二個叫做 Inverse Reinforcement Learning
或者又叫做 Inverse Optimal Control
我們先來講 Behavior Cloning
其實 Behavior Cloning 跟 Supervised learning 是一模一樣的
舉例來說，我們以自駕車為例
今天，你可以收集到人開自駕車的所有資料
比如說，人類的駕駛跟收集人的行車記錄器
看到這樣子的 observation 的時候
人會決定向前，機器就採取跟人一樣的行為
也採取向前，也踩個油門就結束了
這個就叫做 Behavior Cloning
expert 做甚麼，人就做一模一樣的事
不是人做一樣的事，機器就做一模一樣的事
expert 做甚麼，機器就做一模一樣的事
那怎麼讓機器學會跟 expert 一模一樣的行為呢？
就把它當作一個 Supervised learning 的問題
你去收集很多自駕車
你去收集很多行車紀錄器
然後，再收集人在那個情境下會採取什麼樣的行為
你知道說人在state, s1 會採取 action, a1
人在state, s2 會採取 action , a2
人在 state, s3 會採取 action, a3
接下來，你就 learn 一個 network
這個 network 就是你的 actor
他 input si 的時候
你就希望他的 output 是 ai，就這樣結束了
他就是一個非常單純的 Supervised learning 的 problem
Behavior Cloning 雖然非常簡單
但是，他有甚麼樣的問題呢？
他的問題是，今天如果你只收集 expert 的資料
你可能看過的 observation 會是非常 limited
舉例來說，假設你要 learn 一部自駕車
自駕車就是要過這個彎道
那如果是 expert 的話
你找人來，不管找多少人來
他就是把車，順著這個紅線就開過去了
但是，今天假設你的 agent 很笨
他今天開著開著，不知道怎麼回事
就開到撞牆了
他永遠不知道撞牆這種狀況要怎麼處理
為甚麼？因為 taring data 裡面從來沒有撞過牆呀
所以，他根本就不知道撞牆這一種 case
要怎麼處理
或是打電玩，電玩也是一樣
讓機器，讓人去玩 Mario
那可能 expert 非常強
他從來不會跳不上水管
所以，機器根本不知道跳不上水管時要怎麼處理
人從來不會跳不上水管
但是機器今天如果跳不上水管時，就不知道要怎麼處理
所以，今天光是做 Behavior Cloning 是不夠的
但是，只觀察 expert 的行為是不夠的
需要一個招數，這個招數叫作 Data aggregation
我們會希望收集更多樣性的 data
而不是只有收集 expert 所看到的 observation
我們會希望能夠收集 expert 在各種極端的情況下
他會採取什麼樣的行為
如果以自駕車為例的話，那就是這樣
假設一開始，你的 actor 叫作 π1
然後接下來呢，你讓 π1
真的去開這個車
但是，車上坐了一個 expert
這個 expert 會不斷的告訴，不斷的做紀錄說
如果，今天在這個情境裡面
我會怎麼樣開
所以，今天 π1，machine 自己開自己的
但是 expert 會不斷地表示他的想法
比如說，在這個時候，expert 可能說，那就往前走
這個時候，expert 可能就會說往右轉
但是，π1 是不管 expert 的指令的
所以，他會繼續去撞牆，然後
expert 雖然說要一直往右轉
但是不管他怎麼下指令都是沒有用的
π1 會自己做自己的事情
因為我們要做的紀錄的是說，今天 expert
在 π1 看到這種 observation 的情況下
他會做甚麼樣的反應
那這個方法顯然是有一些問題的
因為每次你開一次自駕車
都會犧牲一個人這樣
那你用這個方法，你犧牲一個 expert 以後
你就會得到說，人類在這樣子的 state 下
在快要撞牆的時候，會採取甚麼樣的反應
再把這個 data 拿去 train 新的 π2
這個 process 就反覆繼續下去
這個方法就叫做 Data aggregation
那 Behavior Cloning 這件事情
會有甚麼的樣的 issue
他的 issue 是說，今天機器會完全 copy expert 的行為
不管今天 expert 的行為，有沒有道理
就算沒有道理，沒有什麼用的
這是 expert 本身的習慣
機器也會硬把它記下來
其實這是 Big Bang Theory 的一段
所以，其實機器也是一樣
機器就是你教他甚麼，他就硬學起來
不管那個東西到底是不是值得的學的
那如果今天機器確實可以記住
所有 expert 的行為
那也許還好，為甚麼呢？
因為如果 expert 這麼做，有些行為是多餘的
但是沒有問題，在機器假設他的行為 可以完全仿造 expert 行為
那也就算了，那他是跟 expert 一樣的好
只是做一些多餘的事
但是問題就是，他畢竟是一個 machine
他是一個 network，network 的 capacity 是有限的
我們知道說，今天就算給 network training data
他在 training data 上得到的正確率，往往也不是 100
他有些事情，他是學不起來
這個時候，什麼該學，什麼不該學
就變得很重要
舉例來說，以剛才的例子而言
在學習一個中文的時候
你看到你的老師，他有語音
他也有行為
他也有知識，但是今天其實只有語音部分是重要的
知識的部分是不重要的
也許 machine 他只能夠學一件事
也許他就只學到了語音
那沒有問題，如果他今天只學到了手勢
那這樣子就有問題了
所以，今天讓機器學習什麼東西是需要 copy
什麼東西是不需要copy，這件事情是重要的
而單純的 Behavior Cloning
其實就沒有把這件事情學進來
因為機器唯一做的事情只是複製 expert 所有的行為而已
他並不知道哪些行為是重要
是對接下來有影響的
哪些行為是不重要的
所以，接下來是沒有影響的
舉例來說 ，有一個人
他想要變得跟賈伯斯一樣的厲害
然後，就去讀了賈伯斯的傳記以後
就統計一下賈伯斯的人格特質
舉例來說，賈伯斯的特質有勤奮
有創造力，還有穿得很邋遢
脾氣很暴躁
然後，他就覺得說這些東西太多了
我沒有辦法每一件都學，也許就學其中一件這樣子
比如說，學習脾氣很暴躁，這樣就好
然後就什麼都沒有辦法做成
Behavior Cloning 就是這樣的道理
那 Behavior Coning 還有甚麼樣的問題呢？
在做 Behavior Cloning 的時候
這個你的 training data 跟 testing data
其實是 mismatch 的
我們剛才其實是有講到這個樣子的 issue
那我們可以用這個 Data aggregation 的方法 來稍微解決這個問題
那這樣子的問題到底是甚麼樣的意思呢？
這樣的問題是，我們在 training 跟 testing 的時候
我們的 data distribution 其實是不一樣
因為我們知道在 Reinforcement learning 裡面
有一個特色是你的 action 會採取
會影響到接下來所看到的 gain，對不對
我們是先有 state, s1，然後再看到 action a1
action, a1 其實會決定接下來你看到甚麼樣的 state, s2
所以，在 Reinforcement learning 裡面
一個很重要的特徵，就是
你採取了 action 會影響你接下來所看到的 state
那今天如果我們做了 Behavior Cloning 的話
做 Behavior Cloning 的時候
我們只能夠觀察到 expert 的一堆 state 跟 action 的 pair
我們可以觀察到 expert 的一堆 state 跟 action 的 pair
然後，我們今天希望說我們可以 learn 一個 π
我們 learn 出來的，假設叫做 π* 好了
我們希望這一個 π* 跟 π\head 越接近越好
如果 π* 確實可以跟 π\head 一模一樣的話
那這個時侯，你 training 的時候看到的 state
跟 testing 的時候所看到的 state 會是一樣
因為，雖然 action 會影響我們看到的 state
假設兩個 policy 都一模一樣， 在同一個 state 都會採取同樣的 action
那你接下來所看到的 state 都會是一樣
但是問題就是
你很難 learn 到讓你的 learn 出來的 π
跟 expert 的 π 一模一樣
expert 可是一個人，network 要跟人 一模一樣，感覺很難吧
今天你的 π* 如果跟 π\head 有一點誤差
這個誤差也許在一般 Supervised learning problem 裡面
每一個 example 都是 independent 的，也許還好
但是，今天假設 Reinforcement learning 的 problem
你可能在某個地方，就是失之毫釐，差之千里
你可能在某個地方，也許你的 machine 沒有辦法
完全複製 expert 的行為
它只複製了一點點，差了一點點
也許最後得到的結果，就會差很多這樣
所以，今天這個 Behavior Cloning 的方法
並不能夠完全解決 Imatation learning 這件事情
所以接下來，就有另外一個比較好的做法
叫做 Inverse Reinforcement Learning
這個方法一聽名字就覺得非常地長
有 Reinforcement Learning
現在有 Inverse Reinforce Learning
為甚麼叫 Inverse Reinforce Learning
因為原來的 Reinforce Learning 裡面
也就是有一個環境
跟你互動的環境，然後你有一個 reward function
然後根據環境跟 reward function
透過 Reinforce Learning 這個技術
你會找到一個 actor，你會 learn 出一個 optimal actor
但是 Inverse Reinforce Learning 剛好是相反的
你今天沒有 reward function
你只有一堆 expert 的 demonstration
但是你還是有環境的，IRL 的做法是說
假設我們現在有一堆 expert 的 demonstration
他這邊我們用這個 τ\head 來 代表 expert 的demonstration
如果今天是在玩電玩的話
每一個 τ 就是一個很會玩電玩的人
他玩一場遊戲的紀錄，如果是自駕車的話
就是人開自駕車的紀錄
如果是用人開車的紀錄，這一邊 就是 expert 的 demonstration
每一個 τ 是 一個 trajectory
把所有 trajectory expert demonstration 收集起來
然後，使用 Inverse Reinforcement Learning 這個技術
使用 Inverse Reinforcement Learning 技術的時候 機器是可以跟環境互動的
但是，他得不到 reward
他的 reward 必須要從 expert 那邊推論出來
現在有了環境，有了 expert demonstration 以後
去反推出 reward function 長甚麼樣子
之前 Reinforcement learning 是由 reward function
反推出什麼樣的 action，actor 是最好的
Inverse Reinforcement Learning 是反過來
我們有 expert 的 demonstration
我們相信他是不錯的
然後去反推
expert 既然做這樣的行為
那實際的 reward function 到底長甚麼樣子
我就反推說，expert 是因為甚麼樣的 reward function
才會採取這些行為
你今天有了reward function以後
接下來，你就可以套用一般的 Reinforcement learning 的方法
去找出 optimal actor
所以，Inverse Reinforcement Learning
裡面是先找出 reward function
找出 reward function 以後
再去實際上用 Reinforcement Learning 找出 optimal actor
有人可能就會問說
把 Reinforcement Learning， 把這個 reward function learn 出來
到底相較於原來的 Reinforcement Learning有甚麼樣好處
一個可能的好處是
也許 reward function 是比較簡單的
也許，雖然這個 actor
這個 expert 他的行為非常複雜
也許簡單的 reward function
就可以導致非常複雜的行為
一個例子就是
也許人類本身的 reward function 就只有活著這樣
每多活一秒，你就加一分
但是，人類有非常複雜的行為
但是這些複雜的行為，都只是圍繞著
要從這個 reward function 裡面得到分數而已
有時候很簡單的 reward function
也許可以推導出非常複雜的行為
那 Inverse Reinforcement Learning 實際上是怎麼做的呢？
首先，我們有一個 expert ，我們叫做 π\head
這個 expert 去跟環境互動
給我們很多 τ1\head 到 τn\head
如果是玩遊戲的話
就讓某一個電玩高手，去玩 n 場遊戲
把 n 場遊戲的 state 跟 action 的 sequence
通通都記錄下來
接下來，你有一個 actor
一開始 actor 很爛，他叫做 π
這個 actor 他也去跟環境互動
他也去玩了n 場遊戲
他也有 n 場遊戲的紀錄
接下來，我們要反推出 reward function
怎麼推出 reward function 呢？
這一邊的原則就是
expert 永遠是最棒的
今天是先射箭，再畫靶的概念
expert 他去玩一玩遊戲，得到這一些遊戲的紀錄
你的 actor 也去玩一玩遊戲，得到這些遊戲的紀錄
接下來，你要定一個 reward function
這個 reward function 的原則就是
expert 得到的分數，要比 actor 得到的分數高
先射箭，再畫靶
所以，我們今天就 learn 出一個 reward function
你要用甚麼樣的方法都可以
你就找出一個 reward function
這個 reward function 會使 expert 所得到的 reward
大過於 actor 所得到的 reward
你有了新的 reward function 以後
你就可以去 learn 一個 actor
你有 reward function 就可以套用一般
Reinforcement Learning 的方法
去 learn 一個 actor
這個 actor 會對這一個 reward function
去 maximize 他的 reward
他也會採取一大堆的 action
但是，今天這個 actor
他雖然可以 maximize 這個 reward function
採取一大堆的行為，得到一大堆遊戲的紀錄
但接下來，我們就改 reward function
先射箭，再畫靶的概念
這個 actor 就會很生氣
它已經可以在這個 reward function 得到高分
但是他得到高分以後
我們就改 reward function
仍然讓 expert 比我們的 actor，可以得到更高的分數
這個就是 Inverse Reinforcement learning
你有新的 reward function 以後
根據這個新的 reward function
你就可以得到新的 actor
新的 actor 再去跟環境做一下互動
他跟環境做互動以後， 你又會重新定義你的 reward function
讓 expert 得到 reward 大過，你請問
這邊其實就沒有講演算法的細節
那你至於說要，怎麼讓他大於他
其實你在 learning 的時候，你可以很簡單地做一件事
就是，我們的 reward function 也許就是 neural network
這個 neural network 它就是吃一個 τ
然後，output 就是這個 τ 應該要給他多少的分數
或者是說，你假設覺得 input 整個 τ 太難了
因為 τ 是 s 跟 a 一個很強的 sequence
也許就說 ，他就是 input s 跟 a
他是一個 s 跟 a 的 pair，然後 output 一個 real number
把整個 sequence，整個 τ 會得到的 real number 都加起來
就得到 total R，在 training 的時候，你就說
今天這組數字，我們希望他 output 的 R 越大越好
今天這個 ，我們就希望他 R 的值，越小越好
這邊對 Inverse Reinforcement Learning 的 framework
大家有沒有甚麼問題要問的呢？
你說，你說
我們假設說 expert 就是最好的
所以，什麼叫做一個最好的 reward function
最後你 learn 出來的 reward function ，應該就是
這個 expert 跟這個 expert ，它們在這個 reward function
都會得到一樣高的分數，這樣
最終你的 reward function 沒有辦法
分辨出誰應該會得到比較高的分數
這樣大家還有問題要問的嗎？請說
通常在這個 train 的時候，你當然是會 iterative 的去做
那今天的狀況是這樣
最早的 Inverse Reinforcement Learning
他對 R 的 function 有些限制
他是假設他是 linear
如果在 linear 的話，你可以 prove 說這個 algorithm 會 converge
但是如果不是 linear 的
你就沒有辦法 prove 說他會 converge
大家還有問題要問的嗎？
你有沒有覺得這個東西，其實看起來還頗熟悉呢？
其實你只要把他換個名字說
actor 就是 generator
然後說 reward function 就是 discriminator
其實他就是 GAN，他就是 GAN
所以你說，他會不會收斂這個問題
就等於是問說 GAN 會不會收斂
那你已經有做過作業三，你應該知道說也是很麻煩
不見得會收斂
但是，除非你對 R 下一個非常嚴格的限制
如果你的 R 是一個 general 的 network 的話
你就會有很大的麻煩就是了
那怎麼說他像是一個 GAN，我們來跟 GAN 比較一下
GAN 裡面，你有一堆很好的圖
然後你有一個 generator
一開始他根本不知道要產生甚麼樣的圖，他就亂畫
然後你有一個 discriminator，discriminator 的工作就是
expert 畫的圖就是高分
generator 畫的圖就是低分
你有 discriminator 以後
generator 會想辦法去騙過 discriminator
generator 會希望他產生的圖， discriminator 也會給他高分
這整個 process 跟 Inverse Reinforcement Learning 是一模一樣的
我們只是把同樣的東西換個名子而已
今天這些人畫的圖
在這邊就是 expert 的 demonstration
你的 generator 就是 actor
今天 generator 畫很多圖
但是 actor 會去跟環境互動，產生很多 trajectory
這些 trajectory 跟環境互動的記錄
遊戲的紀錄其實就是等於
GAN 裡面的這些遊戲畫面
不是遊戲畫面，就等於是 GAN 裡面的這些圖
然後，你 learn 一個 reward function
這個 reward function 其實就是 discriminator
這個 rewards function 要給 expert 的 demonstration 高分
給 actor 互動的結果低分
然後接下來，actor 會想辦法
從這個已經 learn 出來的 reward function 裡面得到高分
然後接下來 iterative 的去循環
跟 GAN 其實是一模一樣的
我們只是換個說法來講同樣的事情而已
那這個 IRL 其實有很多的 application
舉例來說，當然可以用開來自駕車
然後，有人用這個技術來學開自駕車的不同風格
每個人在開車的時候，其實你會有不同風格
舉例來說，能不能夠壓到線
能不能夠倒退
要不要遵守交通規則等等
每個人的風格是不同的
然後用 Inverse Reinforcement Learning
又可以讓自駕車學會各種不同的開車風格
這個是文獻上真實的例子，在這個例子裡面
Inverse Reinforcement Learning 有一個有趣的地方
通常你不需要太多的 training data
因為 training data 往往都是個位數
因為 Inverse Reinforcement Learning 他只是一種 demonstration
他只是一種範例
今天機器他仍然實際上可以去跟環境互動，非常的多次
所以在Inverse Reinforcement Learning 的文獻， 往往會看到說
只用幾筆 data 就訓練出一些有趣的結果
比如說，在這個例子裡面
在這個例子裡面，是要讓機器學會在這個
讓自駕車學會在停車場裡面停車
在停車場裡面停車
這邊的 demonstration 是這樣
這個藍色的圈圈，藍色的方形是一部車
不，藍色是終點，他的車是從這邊開始開的
開開開，開開開到這邊停車，然後從這邊開
從這邊開，開到這邊停車
從這邊開，開到這邊停車這樣
然後就是給機器只看一個 row
的四個 demonstration
然後讓他去學怎麼樣開車
怎麼樣開車
最後他就可以學出
在這個位置，如果他要停車的話，他會這樣開
今天給機器看不同的 demonstration
最後他學出來開車的風格，就會不太一樣，舉例來說
這個是不守規的矩開車方式
因為他會開到道路之外
這邊，他會穿過其他的車，然後從這邊開進去
所以機器就會學到說，不一定要走在道路上
他可以走非道路的地方
或是這個例子，機器是可以倒退的
他可以倒退嚕一下，他也會學會說，他可以倒退
那這種技術，也可以拿來訓練機器人
你可以讓機器人，做一些你想要他做的動作
過去如果你要訓練機器人，做你想要他做的動作
其實是比較麻煩的
怎麼麻煩，這邊有一個例子
這個沒有字幕阿
我開大聲一點好了，希望你聽得懂
所以，這個想要告訴我們的事情是說
過去如果你要操控機器的手臂
你要花很多力氣去寫那 program 才讓機器做一件很簡單的事
那今天假設你有 Imitation Learning 的技術
那也許你可以做的事情是
讓人做一下示範
然後機器就跟著人的示範來進行學習
這個是學會擺盤子
拉著機器人的手去擺盤子，機器自己動
這學會倒水
他只教他 20 次，杯子每次放的位置不太一樣
就是這樣子，所以用這種方法來教機械手臂
其實還有很多相關的研究，舉例來說
你在教機械手臂的時候，要注意就是
也許機器看到的視野，跟人看到的視野
其實是不太一樣的
在剛才那個例子裡面
我們人跟機器的動作是一樣的
但是在未來的世界裡面
也許機器是看著人的行為學的
剛才是人拉著，假設你要讓機器學會打高爾夫球
在剛才的例子裡面就是
人拉著機器人手臂去打高爾夫球
但是在未來有沒有可能
機器就是看著人打高爾夫球
他自己就學會打高爾夫球了呢？
但這個時候，要注意的事情是
機器的視野
跟他真正去採取這個行為的時候的視野，是不一樣的
機器必須了解到
當他是作為第三人稱的時候
當他是第三人的視角的時候
看到另外一個人在打高爾夫球
跟他實際上自己去打高爾夫球的時候
看到的視野顯然是不一樣的
但他怎麼把他是第三人的時候，所觀察到的經驗
把它 generalize 到他是第一人稱視角的時候
第一人稱視角的時候，所採取的行為
這就需要用到 Third Person Imitation Learning 的技術
那這個怎麼做呢？
細節其實我們就不細講，他的技術
其實也是不只是用到 Imitation Learning
他用到了 Domain-Adversarial Training
我們在講 Domain-Adversarial Training 的時候，我們有講說
這也是一個 GAN 的技術
那我們希望今天有一個 extractor
有兩個不同 domain 的 image
通過這個 extractor 以後
沒有辦法分辨出他來自哪一個 domain
其實第一人稱視角和第三人稱視角
Imitation Learning 用的技術其實也是一樣的
希望 learn 一個 Feature Extractor
當機器在第三人稱的時候
跟他在第一人稱的時候
看到的視野其實是一樣的
就是把最重要的東西抽出來就好了
其實我們在講 Sequence GAN 的時候
我們有講過 Sentence Generation 跟 Chat-bot
那其實 Sentence Generation 或 Chat-bot 這件事情
也可以想成是 Imitation Learning
機器在 imitate 人寫的句子
你可以把寫句子這件事情
你在寫句子的時候，你寫下去的每一個 word
你都想成是一個 action
所有的 word 合起來就是一個 episode
舉例來說， sentence generation 裡面
你會給機器看很多人類寫的文字
那這個人類寫的文字，你要讓機器學會寫詩
那你就要給他看唐詩 300 首
這個人類寫的文字
其實就是這個 expert 的 demonstration
每一個詞彙，其實就是一個 action
今天，你讓機器做 Sentence Generation 的時候
其實就是在 imitate expert 的 trajectory
或是如果 Chat-bot 也是一樣
在 Chat-bot 裡面你會收集到很多人互動對話的紀錄
那一些就是 expert 的 demonstration
如果我們今天單純用 Maximum likelihood 這個技術
來 maximize 會得到 likelihood
這個其實就是 behavior cloning，對不對？
用我們今天做 behavior cloning ，就是看到一個 state
接下來預測，我們會得到
甚麼樣的 action，看到一個 state
然後有一個 Ground truth 告訴機器說
甚麼樣的 action 是最好的
在做 likelihood 的時候也是一樣
Given sentence 已經產生的部分
接下來 machine 要 predict 說
接下來要寫哪一個 word 才是最好的
所以，其實 Maximum likelihood
在做這種 Sequence generation 的時候
Maximum likelihood 對應到 Imitation Learning 裡面
就是 behavior cloning
那我們說光 Maximum likelihood 是不夠的
我們想要用 Sequence GAN
其實 Sequence GAN 就是對應 到 Inverse Reinforcement Learning
我們剛才已經有講過說
其實 Inverse Reinforcement Learning
就是一種 GAN 的技術
你把 Inverse Reinforcement Learning 的技術
放在 Sentence generation，放到 Chat-bot 裡面
其實就是 Sequence GAN 跟他的種種的變形
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心
