hi everyone i'm poetry from national
taiwan university
i'm here to present our work as the
first author
the title of our paper is wg webnet
real-time high-fidelity speech synthesis
without gpu
so let us begin
recently neural network based models
have achieved steadily r performance
in many speech tasks like text-to-speech
voice conversion and speech separation
these models are typically composed of
two parts
the first part is a task-oriented model
that conducts the speech tasks
and generates acoustic features like
spectrogram or
f-zero frequencies the second part
referred to as a vocoder is a generative
model or a heuristic method
generating speech samples from acoustic
features
in this work we aim to propose a light
weight for coder for real-time
synthesizing high fidelity speech
the first neural network used as a
vocoder is webnet
an other regressive model that generates
an audio sample at time t
given samples before it
the ar architecture makes wavenet
capable of generating high quality audio
but the iteratively synthesis process
also leads to
extremely slow speed and influence time
some non-ar vocoders were proposed to
improve the efficiency
one is grave glow waveflow is an
invertible model
during training real speech is used as
the input
and the goal is to maximize the
likelihood of the real data
using the output and a given probability
distribution
a normal distribution here at inference
time
a random noise is sampled from the
distribution of the input
the model then can generate high quality
audio samples in parallel
webflow is a very deep model and
contains a large number of parameters
also big batch size is necessary for
model convergence
hence it consumes huge computational
resources during training
and influence time another non-ar
approach is parallel wavegen a
game-based neural vocoder
as a lot of you may have known there is
a generator and a discriminator in
again the discriminator is trained to
distinguish real and generated speech
the generator is trained to follow the
discriminator
parallel with can also utilize losses on
the frequency domain
to improve the generator dollar
generator is
much smaller compared with web global
during training
big computational resources are still
required to optimize
two models simultaneously the defects
above
motivates us to design a new vocoder
the model should be lightweight enough
and use less computational resources
also this model should be capable of
synthesizing good speech in real time
we refer to our model as wgf net given
a random noise z wg web net generates
speech conditioned on the male
spectrogram
the model is non-ar for high speed
synthesis
here are the two components in it for
being lightweight
we first apply a highly compressed web
glow model
and to further improve the speech
quality we add a non-ar
with net base post filter
we start with compressing web glow
the web glow model consists of several
transformations
to progressively map speech data to
gaussian space
each transformation contains an
invertible
one by one convolution layer and a fine
coupling layer
in the affine coupling layer is a series
of calculations
including a webnet like module
there are 12 transformations in web glow
means there are 12
webnet like modules the overall model is
huge and hard to train
we apply cross layer parameter sharing
to reduce parameters
and make the model more compact
in this compressed web globe
transformations share the same affine
coupling layer
as mentioned above these transformations
are processes
gradually map the input from real data
distribution
to normal distribution the input and
output distribution of
each transformation might be different
hence the invertible one by one
convolution layers
remain different across transformations
now we have a compressed waveflow this
model is trend using the same dos
function as in the original workflow
we denote it as lz during inference
the model generates speech given a
random noise c
sampled from the normal distribution
now the computational cost is much less
for the compressed workflow
it still takes a lot of time to converge
we propose to use a webnet based post
filter
to overcome this problem
the post filter can not only enhance the
speech quality
but also shorten the training time this
plus filter is trained to optimize the
losses on the frequency domain
the loss function ls is the
multi-resolution stft auxiliary loss
in parallel afghan with a little
modification
for a target speech and a generated
speech
we first extract the spectral grains of
them
and then calculate the distance on the
frequency domain
we can extract spectrograms with various
frequency and temporal resolutions
using different parameters for stft
so far the loss is identical to the
original
to make ls more representative to human
perception
we also calculate the distances of the
male spectrograms
we have introduced wgf net and the two
components
in it during training
the loss ls is calculated using the
generated x head
and real speech x and the loss lz
is calculated by maximizing the
likelihood of the real speech
x the final loss function l total
is the combination of l z and l s
here lambda is a scalar note that in
practice
we do not calculate ls in each iteration
we found that this setting gives a
better result from our experiments
the two components in wg webnet are
simply trend together
that is end-to-end training
in our experiments we compared the
proposed wg wavenet with four neural
vocal coders
with net webflow squeeze web and
parallel afghan
to see the efficiency of different
models we first evaluated the speed
and memory usage during training and
inference
all the environments were set to the
same
the second experiment evaluated the
quality of the generated speech
also we evaluated the ability of
generating high fidelity speech
in the last experiment we combined
different models with a tacotron 2
to evaluate the performance of vocoders
we start with
speed and computational cost for webnet
workflow and squeeze web in list and the
following experiments
we used pre-trained models from public
implementations
parallel webcam was trend following the
original paper
in webglo and wgfnet the input
is reshaped to groups of a samples
inspired by squeeze web the input of g20
is reshaped to groups of 20 samples
this makes g20 a faster version of wg
webnet
we show the numbers of parameters of
different models
the crosstalk parameter sharing methods
obviously make wg webnet much
more compact the compact architecture
results in much less gpu memory consumed
while training
and with the help of the post filter the
model can converge
in less than 4 days although we did not
trend
webgl and squeeze web the usage of gpu
were reported
in the original papers we can see that
wg webnet is much more economical
for influence speed using only cpu or
gpu
we can find that the auto regressive
model is much slower than
non-auto-regressive models
the speed of 0.1 khz means it takes
about 10 minutes
to generate a 3 seconds audio clip
all the non-ar models achieve real-time
synthesis with gpu
while only the proposed wg web net and
squeeze web make it without gpu
the next experiment is audio quality
comparison
the ar model webnet has the highest mos
which is much closer to the net of the
ground truth data
and there is a performance gap between
the ar and non-ar models
the mos of wg webnet is close to the
best non-ar model parallel web game
and wg webner is faster than parallel
with game
while considering both the mos and
influence speed
we found that the mos decreased rapidly
when the generating efficiency improves
among these models wgwebnet has a faster
speed
with good speech quality this indicates
the proposed computer web that can
greatly increase the efficiency while
preserving the speech quality
we plot the mos and cpu influence speed
of different models on a 2d plan
only wgfnet and squeezeweb are capable
of generating
22khz and 44khz speech in real time
and wgwebnet reached a good balance
between the speed and
mos since we've shown that wgwebnet can
generate
44khz speech in real time the next
experiment is the quality analysis
on high fidelity audio generation
we first compared recordings sampled at
different rates
and found that sampling rates
significantly affect
perceptual results the results of
parallel webcam
and wg wavenet are quite disappointing
we try to fix this problem by studying
the effects
of different parameters for extracting
male spectrograms
in the previous experiment for
generating 22khz
speech the fft size hub size and window
size for sdft
are 2048 200 and 800.
for an audio with 1200 samples the
extracted spectrogram contains
three frames here is frame one
frame two and france 30.
for 44khz audio signals with the same
duration
the number of samples doubles in this
case
the parameters also double to extract
spectrograms with the same number of
ramps here is frame 1
ram 2 and brand 3.
so for extracting male spectrograms for
44khz
audio signals the parameters double to
keep the same temporal resolution
another method is simply keeping the
same parameters as in the 22khz
case with the parameters we have frame 1
frame 2 frame 3 and so on
note that we skipped some signals in
this figure
finally we got 9 frames in this
spectrogram
as you can see keeping the same
parameters increases the temporal
resolution
these two different parameter sets are
denoted as w1600
and w800 now let's back to this table
we've shown that the results of w1600
parameters are not good
the w800 parameters in our experiments
unfortunately doesn't improve the
performance of parallel web gain
but when it comes to wgf net the mos is
improved
and close to that of 16khz recordings
finally our fastest wg wavenet reaches
4.01 mos
again we would like to mention this
model can real time generate 44khz
speech
without gpu the last experiment is
texture speech
the mos test shows their weftness still
outperforms other methods
but the performance gap between ar and
non-ar models narrows
the buildup net has a comparable mos and
faster influence speed
this indicates the advantage of wgfnet
as a vocoder for
fast high quality speech synthesis
in this work we proposed wgfnet
a fast and lightweight vocoder which is
capable of generating
high quality 22khz and 44khz
audio samples faster than real time
without gpu
last but not least here are some useful
links
for anyone that is interested and that's
all
thank you for your attention
