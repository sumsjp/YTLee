臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw/
今天的規劃是這樣， 我們剩下一點點 Q learning 的部分把它講完
接下來要講 Actor-critic
講一些 Actor-critic 之後，之後要講說假設在 reinforcement learning 有碰到
reward 非常 sparse 的情況怎麼辦
然後最後會講說，如果完全沒有 reward 的時候
要怎麼辦， 完全沒有 reward 的狀況叫做 Imitation learning
今天首先跟大家繼續講一下 Q-learning
那其實跟 policy gradient based 方法比起來
Q learning 其實是比較穩的
policy gradient 其實是沒有太多遊戲是玩得起來的
你想想看我們作業 4-1，是要做 Pong
4-2 是要做 Breaking
那會什麼 4-1 不用 Breaking 呢？
因為 policy gradient 在 Breaking 上面是做不太起來的
所以 policy gradient 其實比較不穩，尤其在沒有 PPO 之前
你很難用 policy gradient 做什麼事情
Q learning 相對而言是比較穩的
可以看最早 Deep reinforcement learning 受到大家注意
最早 deep mind 的 paper 拿 deep reinforcement learning 來玩 Atari 的遊戲
用的就是 Q-learning
那我覺得 Q-learning 比較容易，比較好train 的一個理由是
我們說在 Q-learning 裡面，你只要能夠 estimate 出Q-function
就保證你一定可以找到一個比較好的 policy
也就是你只要能夠 estimate 出 Q-function
就保證你可以 improve 你的 policy
而 estimate Q function 這件事情，是比較容易的，為什麼？因為她就是一個 regression 的 problem
在這個 regression 的 problem 裡面， 你可以輕易地知道說
你現在的 model learn 的是不是越來越好
你只要看那個 regression 的 loss 有沒有下降，你就知道說你的 model learn 的好不好
所以 estimate Q function 相較於 learn 一個 policy
是比較容易的，然後你只要 estimate Q function
就可以保證說，你現在一定會得到比較好的 policy
所以一般而言 Q learning 是比較容易操作
那 Q learning 有什麼問題呢？他一個最大的問題是
他不太容易處理 continuous action
很多時候你的 action 是 continuous 的
什麼時候你的 action 會是 continuous 的呢？
在作業裡面，我們都只玩 Atari 的遊戲
你的 agent 只需要決定
比如說上下左右，這種 action 是 discrete 的
那很多時候你的 action 是 continuous 的
舉例來說假設你的 agent 要做的事情是開自駕車
它要決定說它方向盤要左轉幾度， 右轉幾度，這是 continuous 的
假設你的 agent 是一個機器人
它的每一個 action 對應到的就是它的， 假設它身上有 50 個 關節
它的每一個 action 就對應到它身上的這 50 個關節的角度
而那些角度，也是 continuous 的
所以很多時候你的 action，並不是一個 discrete 的東西
它是一個 vector，這個 vector 裡面
它的每一個 dimension 都有一個對應的 value
都是 real number，它是 continuous 的
假設你的 action 是 continuous 的時候
做 Q learning 就會有困難
為什麼呢？因為我們說在做 Q-learning 裡面
一個很重要的一步是
你要能夠解這個 optimization 的 problem
你 estimate 出Q function，Q of (s, a) 以後
必須要找到一個 a，它可以讓 Q of (s, a) 最大
假設 a 是 discrete 的，那 a 的可能性都是有限的
舉例來說 Atari 的小遊戲裡面，a 就是上下左右跟開火
它是有限的，你可以把每一個可能的 action 都帶到 Q 裡面
算它的 Q value
但是假如 a 是 continuous 的
你會很麻煩這個 continuous 的 action
你無法窮舉所有可能 continuous action 試試看那一個 continuous action 可以讓 Q 的 value 最大
所以怎麼辦呢？
在概念上，反正我們就是要能夠解這個問題
但是怎麼解這個問題呢？
就有各種不同的 solution
第一個 solution 是
假設你不知道怎麼解這個問題，因為 a 是很多的
a 是沒有辦法窮舉的，怎麼辦，用 sample 的
sample 出大 N 個 可能的 a
一個一個帶到 Q function 裡面
那看誰最快？這個方法其實也不會太不 efficient， 因為其實你真的在運算的時候
你會用 GPU，所以你一次會把 N 個 continuous action
都丟到 Q function 裡面
一次得到 N 個 Q value，然後看誰最大
那當然這個不是一個 非常精確的做法
因為你真的沒有辦法做太多的 sample， 所以你 estimate 出來的 Q value
你最後決定的 action，可能不是非常的精確， 這是第一個 solution
那第二個 solution 是什麼呢？
今天既然我們要解的是一個 optimization 的 problem
你會不會解這種 optimization 的 problem 呢？
你其實是會的，因為你其實可以用 gradient decent 的方法
來解這個 optimization 的 problem
我們現在其實是要 maximize 我們的 objective function
我們是要 maximize 一個東西， 所以這是 gradient decent
這是 gradient ascent，我的意思是一樣的， 我相信大家都很清楚
你就把 a 當作是你的 parameter
然後你要找一組 a 去 maximize 你的 Q function
那你就用 gradient ascent 去 update a 的 value
最後看看你能不能找到一個 a 去 maximize 你的 Q function
也就是你的 objective function
當然這樣子你會遇到的問題，就是
global maximum 的問題， 就不見得能夠真的找到最 optimal 的結果
而且這個運算量顯然很大， 因為你要 iterative 的去 update 你的 a
我們 train 一個 network 就很花時間了
今天如果你是用這樣子 gradient ascent 的方法呢
來處理這個 continuous 的 problem， 等於是你每次要決定要 take 哪一個 action 的時候
你都還要做一次 train network 的 process
這個顯然運算量是很大的，這是第二個 solution
那第三個 solution 呢
這是有點神奇，第三個 solution 是
特別 design 一個network 的架構
特別 design 你的 Q function
使得解那個 arg max 的 problem
變得非常容易
也就是這邊的 Q function 不是一個 general 的 Q function
特別設計一下它的樣子
讓你要找哪一個 a 可以讓這個 Q function 最大的時候，非常容易
那這邊是一個例子，這邊有我們的 Q function
然後這個 Q function 它的作法是這樣
input 你的 state s，通常它就是一個 image， 它可以用一個向量，或是一個 matrix 來表示
input 這個 s，這個 Q function 會 output 3 個東西
它會 output mu of s，這是一個 vector
它會 output sigma of s， 這個 sigma of s，是一個 matrix
它會 output V of s，V of s，是一個 scalar
output 這 3 個東西以後， 我們知道 Q function 其實是吃一個 s 跟 a
然後決定一個 value，對不對
Q function 意思是說在某一個 state，take 某一個 action 的時候，你 expected 的 reward 有多大
到目前為止這個 Q function 只吃 s，它還沒有吃 a 進來
a 在那裡呢，a 在當這個 Q function 吐出 mu
sigma 跟 V 的時候
我們才把 s 引入，用 a 跟這 3 個東西互相作用一下
你才算出最終的 Q value
a 怎麼和這 3 個東西互相作用呢？
它的作用方法就寫在下面這個地方
所以實際上 Q of (s, a)，你的 Q function 的運作方式是
先 input s，讓你得到 mu
sigma 跟 V
然後再 input a
然後接下來的計算方法是把 a 跟 mu 相減
注意一下 a 現在是 continuous 的 action
所以它也是一個 vector，假設你現在是要操作機器人的話，這個 vector 的每一個 dimension
可能就對應到機器人的某一個關節，它的數值
就是那關節的角度
所以 a 是一個 vector
把 a 的這個 vector
減掉 mu 的這個 vector，取 transpose
所以它是一個橫的 vector，它是倒下來的 vector
sigma 是一個 matrix
然後 a 減掉 mu of s
這兩個都是 vector，減掉以後還是一個豎的 vector
然後接下來你把這一個 vector
乘上這個 matrix，再乘上這個 vector
你得到的是什麼？你得到是一個 scalar 對不對？
把這個 scalar 再加上 V of s
得到另外一個 scalar
這一個數值就是你的 Q of (s, a)，就是你的 Q value
那接下來假設我們的 Q of (s, a) 定義成這個樣子
我們要怎麼找到一個 a
去 maximize 這個 Q value 呢？
其實這個 solution 非常簡單
因為我們把 formulation 寫成這樣
那什麼樣的 a， 可以讓這一個 Q function 最終的值，最大呢？
因為這邊這一項，a 減 mu 乘上 sigma
再乘上 a 減 mu 這一項一定是正的
然後前面乘上一個負號
所以第一項
就假設我們不要看這個負號的話
第一項這個值越小，你最終的這個 Q value 就越大
因為我們是把 V 減掉第一項
所以第一項
假設不要看這個負號的話，第一項的值越小
最後的 Q value 就最大，越大
怎麼讓第一項的值最小呢？
你直接把 a 帶 mu
讓它變成 0，就會讓第一項的值最小
你這個問題問得很好
假設它是不是正定的是嗎？
你說如果它不是正定的話， 它有可能會出現 negative 的 value
對，其實有假設它是正定的，這邊少講一個東西
因為你知道這個東西，就像是那個 Gaussian distribution
所以 mu 就是 Gaussian 的 mean
sigma 就是 Gaussian 的 various
但是 various 是一個 positive definite 的 matrix
所以其實怎麼樣讓這個 sigma，一定是 positive definite 的 matrix 呢？
其實在 Q(pi) 裡面
它不是直接 output sigma
就如果直接 output 一個 sigma， 它可能不見得是 positive definite 的 matrix
它其實是 output 一個 matrix
然後再把那個 matrix 跟另外一個 matrix
做 transpose 相乘， 然後可以確保它是 positive definite 的
所以這邊確實是漏講了一塊，感謝你有特別提出來
這邊要強調的點就是說，實際上它不是直接output 一個 matrix
你再去那個 paper 裡面 check 一下，它的 trick
它可以保證說 sigma 是 positive definite 的
所以今天前面這一項
因為 sigma 是 positive definite，所以它一定是正的
所以現在怎麼讓它值最小呢？你就把 a 帶 mu of s，
你把 a 帶 mu of s 以後呢
你可以讓 Q 的值最大
所以這個 problem 就解了， 所以今天假設要你 arg max 這個東西
雖然 in general 而言，若 Q 是一個 general function， 你很難算，但是我們這邊
philosophically design 了 Q 這個 function
所以 a 只要設 mu of s
我們就得到 maximum 的 value
你在解這個 arg max 的 problem 的時候
就變得非常的容易
所以其實 Q learning 也不是不能夠用在 continuous path
是可以用的，只是就是有一些侷限，就是你的 function 就是不能夠隨便亂設，它必須有一些限制
那第 4 招是什麼，第 4 招就是不要用 Q-learning
用 Q learning 處理 continuous 的 action 還是比較麻煩
那到目前為止，我們講了 policy based 的方法
我們講了 PPO，這是上上周講的
上周講了 value based 的方法，也就是 Q learning
但是這兩者其實是可以結合在一起的， 也就是 Actor-Critic 的方法
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
