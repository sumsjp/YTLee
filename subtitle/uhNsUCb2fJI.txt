好,那今天的課程有兩個部分
前半段終於要深入語言模型的內部
看一下語言模型內部的那個類神經網路
也就是大名鼎鼎的Transformer
它背後做的事情是什麼
特別需要把Transformer這段提出來講
是為了要為下半段的課程做鋪墊
因為在下半段的課程
我們要來探討大型語言模型
他的心裡究竟在想些什麼
但在探討他的心裡在想些什麼之前
你必須要先真的瞭解他的運作的機制
我們才能夠深入他的內心
所以我們先來講今天語言模型是怎麼做文字接龍的
在這個課堂上我們已經再三跟大家強調過說
語言模型做的事情就是文字接龍
我們也跟大家講過說
這個語言模型文字接龍的能力
就是從大量的訓練資料
所學出來的
語言模型
它是一個函式
內部有很多未知的參數
而我們可以用這些訓練資料
找出這些未知的參數
但是
這個語言模型
背後的這個函式
到底長得什麼樣子呢
我們今天就要來看看
到底內部運作的過程是什麼
那可能很多同學都知道
今天的語言模型
它背後用的就是
類神經網路的技術
也就是它背後的函式就是一個
類神經網路
那類神經網路有很多種
那今天語言模型最常用的
類神經網路的類型
叫做Transformer
那我們今天就要來看看Transformer
背後運作的過程
語言模型不是一直都使用Transformer這樣的類神經網路
這些投影片上講的是語言模型的演進史
最早的時候在深度學習還沒有開始發達以前
那時候人們還不知道要用深度學習的技術
用類神經網路來做語言模型
在有類神經網路之前
那時候通常是用一個叫做N-Gram的模型
它就不是類神經網路的技術
那後來呢就用了類神經網路的技術
比如說Feed Forward Network
那在Feed Forward Network之後
人們知道要用RNN
Recurrent Neural Network
那這些太過冗長
所以今天我們都不會細講
假設你想要知道語言模型的演進史
可以參看我過去上課的錄影
我把錄影的連結留在投影片上
那今天呢我們主要會Focus在Transformer上面
那ChatGPT的這個T啊
他指的就是Transformer的T
所以可見這個Transformer
對於大型語言模型是非常重要的
那今天講的是一個簡化的版本
並不是Transformer的全部
如果你真的想要更深入的瞭解Transformer
那我這邊附上了兩段過去上課的錄影
大家可以看這兩段上課的錄影
更瞭解Transformer內部運作的原理
好,那我們現在先來看一下Transformer這個模型
它整體運作的過程
我們先來對Transformer做一個概述
當一句話被語言模型讀進去的時候
經過了哪些步驟才得到輸出的機率分佈呢?
首先第一個步驟叫做Tokenization
Tokenization這個步驟是把文字轉為Token
那等一下在下一頁投影片會再跟大家講得更清楚
這邊所謂的Token指的是什麼
那把文字轉成Token之後
接下來語言模型有一個input layer
會去理解每一個Token
但是光理解每一個Token還不夠
我們還要考慮Token和Token間的關係
怎麼考慮Token跟Token間的關係呢
這邊是透過一個叫做Attention的模組來進行的
Attention這個模組會理解上下文
那Attention理解上下文之後
再經過一個Feedforward的Network
你可以想成它是把Attention模組的輸出
進行整合再進一步思考
那Attention加Feedforward合起來
叫做一個Transformer的Block
那一個Transformer裡面
有很多很多的Block
那Block的數目是你自己定的
那Block會被使用很多次
那代表說機器在做反覆的思考
那最後有一個output的layer
把這個transformer block的輸出轉成機率的分佈
那接下來我們就來仔細看每一個block在做什麼樣的事情
第一個block是把文字轉成token
那因為對一個語言模型來說
它真正處理的單位叫做一個token
它的輸入和輸出的單位都是以Token這個東西來作為單位的
所以當你把一個句子給語言模型的時候
它做的第一件事情是把這句話轉成Token的sequence
它才好做進一步的處理
舉例來說假設輸入的句子是Introduction of Generative AI
那經過Tokenization之後它就會被切成一個一個Token
比如說introduction可能會切成int跟reduction兩個token
off本身就是一個token
generative會被切成genre跟ative兩個token
那ai本身就是一個token
所以說一段話,它會變成一個token的sequence
那怎麼把一句話切成token的sequence呢?
我們怎麼知道一個語言裡面有哪些的token呢?
你要先準備一個token的列表
所以一開始你在打造你的語言模型的時候
你就會根據你對語言的理解
準備一個Token的列表
那這個Token的列表
就是你的語言模型處理文字的基本單位
那這個Token的列表怎麼來呢
它是根據你對這個語言的理解
自己定的
所以不同的語言模型
它的這個Token的列表是不一樣的
那很多這個中文的模型呢
往往就會把一個中文的方塊字
當作一個Token
因為對我們這個中文的使用者來說
把方塊字當作單位往往是最自然的
所以很多中文的模型是以一個方塊字當作它的token
但是像ChatGPT它並不是特別為中文所打造的
它是為了各種不同語言所打造的
它就不是拿一個中文的方塊字當一個token
在對ChatGPT來說
好幾個token才能夠組成一個中文的字
那Token呢其實也有辦法自動取得
有一個演算法叫做Byte Pair Encoding BPE
它可以從大量文字裡面找出常常出現的Pattern
那把這些Pattern當作Token
那BPE並不是唯一找Token的方法啦
還有很多其他的方法
BPE只是特別有代表性的一個
那BPE這個方法精神就是
看看你先收集大量的文字
這些文字裡面有沒有哪些英文的有沒有哪一些符號常常一起出現
比如說
Gener常常一起出現
Active常常一起出現
他們就是一個Token
那如果有一些比如說像Generative這樣的這個
符號的序列沒有很常出現那他就不會被
當作一個Token
所以可能Active
很常出現是一個Token
Generative他出現的次數還不夠多
所以就沒有被當作一個Token來
那總之呢要怎麼做Tokenization
這個要問你自己
也就是要問這個打造語言模型的人類
那這個部分沒有需要訓練的參數
它是一個人定好的module
反正就是把一個句子轉成Token的sequence
Token本身是一個比較模糊的概念
它是人定出來的
它本身是一個比較模糊的概念
所以你如果拿一個符號來問我說
它是不是一個Token
我其實是沒有辦法回答你的
而每個語言模型用的Token也都是不一樣的
如果你想要知道現在的GPT-3.5跟GPT-4用的Token有什麼
那其實有一個網站
OpenAI提供了一個網站
你可以把一篇文章貼給他
他會告訴你這篇文章裡面有哪些Token
比如說我貼給他這篇文章
他就告訴我這篇文章總共有65個Token所組成
他會告訴你說哪一些字是一個Token
比如說Language本身是一個Token
Model本身是一個Token
但Probabilistic不是一個Token
Probabilistic是被拆成兩個Token
那像這邊1980這個年代
也是被拆成兩個Token
198是一個Token
那0呢是被當作另外一個Token
那裡面沒有1980這樣一個Token
所以1980會被拆成198跟0兩個Token
那至於為什麼會這樣
這個都是OpenAI事先訂好的
我們現在把一句話拆解成Token之後
下一個步驟是要知道每一個Token是什麼意思
那怎麼知道每一個Token是什麼意思呢
這邊會把每一個Token用一個向量來表示
也就是每一個Token本來是一個符號
但是丟進這個第二個模組之後
每一個Token會變成一串數字
數字 每一個token會變成一個向量
那這個把token變成一串數字
也就把token變成一個向量的過程
叫做embedding
embedding的意思就是把本來一個物件
一個東西變成一個向量
這個過程呢就是embedding
那把這些token變成向量有什麼好處呢
原本每一個token都是獨立的符號
Apple是個符號,Cat是一個符號
這邊本來是想要打狗啦,想要打Dog啦
不然就寫成Dog啦,也無所謂
Round是一個符號,Jump是一個符號
那雖然Round跟Jump都是動作
感覺他們應該跟Round跟Jump彼此比較相近
跟Cat跟Apple可能距離比較遠
他們比較不相近
但是從Token的角度來看
因為每一個Token都是一個獨立的符號
所以如果我們是用Token來處理語言的話
RUN跟JUMP並沒有什麼特別的關係
RUN跟APPLE也沒有什麼特別的關係
就當你用Token來看待一個語言的時候
每一個Token都是獨立的
它們彼此這些詞彙
這些Token彼此之間的關聯性
並沒有辦法被反映出來
所以怎麼辦呢
今天我們把每一個Token呢
表示成一個向量
那有時候這些向量
我們就直接把它稱之為embedding
每一個token會被表示成一個embedding
而語意越相近的token
它的embedding距離就越接近
所以如果你把
這一些token的embedding拿出來畫成圖的話
你可能會發現
所有的動物
他們的token聚集成一群
而所有植物可能聚集成另一群
所有的動詞可能聚集成一群
名詞又聚集成另外一群
所以透過把這些本來獨立的Token變成Embedding
可以讓接下來在處理的時候
接下來的這個類神經網路
接下來剩餘的模組
在處理這些Token的時候
能夠知道這些Token跟Token之間的關聯性
好那我們已經知道說相鄰的Token
我們希望給他相近的Embedding
但是到底是怎麼把相鄰的Token
給他相近的Embedding呢
這邊你會有一個Token Embedding的列表
所以在Transformer裡面
你會有一個Table裡面存了所有的Token
跟它每一個Token所對應的Embedding
每一個Token所對應的項量
所以在做Embedding的時候
實際上你做的事情就是查表
看看有什麼樣的Token
去這個表裡面查一下
看看INT對應的Token是什麼
就把它拿出來
看INT對應的embedding是什麼就把它拿出來
看AI對應的embedding是什麼就把它拿出來
但下一個問題是
這一個表是從哪裡來的呢
我們怎麼知道每一個token應該對應的向量是什麼呢
這個每一個token所對應的向量
就是這個語言模型裡面的參數
還記得嗎 我們說語言模型裡面有很多未知的參數
這些未知的參數是要透過訓練資料找出來的
而每一個token所對應的embedding
就是我們待找的參數的一部分
所以這些token的embedding你並不需要人去設定它
它是自動由訓練資料所找出來的
但是到目前為止
我們雖然可以把token轉成embedding
但這些embedding有一個限制
有一個問題就是沒有考慮上下文
同一個token
他的embedding就是固定的
就是同一個比如說bank
我們知道bank這個詞會有兩個意思
他可能指的是銀行
也可能指的是河岸
但是不管現在bank在句子裡面
是表示銀行還是河岸
他用的embedding都是同一個
他用的token embedding都是同一個
那在第三階段
我們才會開始把上下文考慮進來
那剛才是考慮了語意的資訊
那其實還需要考慮位置的資訊
我們需要知道每一個token
是在句子裡面的哪一個位置
因為同一個token
在句子裡面的不同位置
它可能會代表了不同的意思
所以我們需要把位置的資訊
也加到每一個token的embedding裡面去
那怎麼把位置的資訊加到Token的Embedding裡面去呢?
一個可能的做法是為每一個位置也設置一個向量
設置一個向量 設置一串數字代表位置1
另外一串數字代表位置2
另外一串數字代表位置3
以此類推
接下來就把位置1的Token的Embedding
加上代表位置1的向量
那就等於是把位置1的資訊加進去
把位置2的資訊加進去
把位置3的資訊加進去
以此類推
那這樣子你的embedding裡面
就不只有包含語意的資訊
也包含了位置的資訊
這些向量啊
這些向量
每一個位置會給它一個獨特的向量
那這些向量呢
又叫做positional embedding
它們是代表了位置的embedding
那怎麼找到這種positional embedding呢
你可以用人設最早的Transformer
他的Token Embedded就是人設計的
你自己決定好說位置1他的向量要長什麼樣子
位置2他的向量要長什麼樣子
這個近年來Positional Embedded也是可以學的
你可以把Positional Embedded也當作參數的一部分
用你的訓練資料自動的找出這些Positional的Embedded
好接下來第三步呢
我們就要來考慮上下文
那這個可以考慮一整個句子
考慮上下文資訊的模組呢
叫做Attention
那剛才我們有講過說
最開始的這個Token Embedding
是沒有考慮上下文資訊的
所以不管是蘋果電腦的果
還是來吃蘋果的果
他的Token Embedding都是一模一樣的
不管這個果是代表蘋果電腦
還是代表一個可以吃的蘋果
他的Token Embedding都是一樣的
但在經過這個Attention之後
Attention會考慮整個句子的上下文
那蘋果電腦的果跟萊斯蘋果的果
因為他們上下文不一樣
所以這兩個果通過了Attention之後
他們的Embedding就會變得不一樣
這種有考慮上下文的Embedding呢
又叫做Contextualized Embedding
你用Contextualized意思就是有考慮上下文的
這個時候同一個Token 語意不一樣
那比如說Bank有可能是銀行 有可能是河岸
當它代表的意思不一樣的時候
可能就可以有不同的Embedding
這種Embedding叫做Contextualized Token Embedding
好 講到這個Attention
就有一篇赫赫有名的文章
叫做Attention Is All You Need
那這也是上古時代的文章啦
是2017年發表的
那很多人誤以為是
這一篇文章提出了Attention這個想法
那其實這是一個誤解啦
在早在有這篇文章之前
Attention就是語言模型裡面
常常使用的一個機制
只是過去覺得說
Attention要搭配RNN一起使用
才能夠發揮比較好的結果
那Attention is All You Need這篇paper
而他真正的貢獻是告訴世人說
其實可以把RNN拿掉
他說我把RNN拿掉
結果沒有比較差
那時候大家都驚呆了
原來可以不用RNN
那因為RNN本身訓練的時候
有一些訓練上的效能的限制
那所以當時人們是希望可以拿掉RNN的
所以Attention is All You Need
真正的貢獻並不是發明瞭Attention
而是發現不需要使用RNN
那從他的名字裡面也可以看出來
他的名字叫做Attention is All You Need
就告訴你說他只用Attention就搞定了語言模型這個任務
他不需要再使用RNN這樣的技術
好 那接下來我們就來看一下這個Attention是怎麼運作的
是怎麼考慮上下文的
那假設等一下講的東西你聽不太懂的話
反正你就記住Attention做的一件事情
你就記住Attention做的事情
就是輸入一排向量
在這一排向量裡面每一個向量都代表一個Token
那他會輸出另外一排一樣長度的向量
那對每一個token來說就是把上下文的資訊
加進他的embedded裡面去
假設你等一下講的東西聽不懂的話
就記得這個大原則就可以了
那接下來我們就來深入看一下Attention是怎麼運作的
我們來看一下當我們看中間這個embedded
進入Attention的時候是發生了什麼事
讓他得出輸出的這個embedded
然後在Attention這個模組裡面
第一件要做的事情
是要先從整個句子裡面
找出相關的Token
所以會有一個叫做
計算相關性的模組
這個模組它做的事情
就是把你現在要考慮的
這個Embedding讀進去
再把整個句子裡面的
某一個Token的Embedding讀進去
它就會輸出一個分數
這個分數代表這兩個Token
Token之間的關聯性
那這個分數是怎麼計算出來的呢
那這個計算相關性的模組
他也是一個函式
我們這邊用F來表示他
他輸入就是兩個embedded
輸出就是一個分數
那這個F裡面呢
是有參數的
所以他也是需要學的
這個F裡面是有參數的
怎麼計算相似性
也是透過訓練資料所得到的
總之我們現在有一個可以計算相似性的模組
給他兩個Token Embedding
他就告訴你說這兩個Token Embedding有多相近
那接下來呢
你就會把這個向量
跟這個句子裡面的每一個向量
包括他自己都來計算一下相關性
跟每一個項量都計算相關性
那這個相關性呢
有一個名字叫做Attention Weight
總之中間這個向量會跟每一個項量計算一個相關性
這個相關性呢 叫做Attention的Weight
那計算出這個Attention的Weight以後
下一步要做什麼呢
下一步就是根據這個Attention的Weight
把這個句子裡面的Embedding加起來
這一個Embedding 它的Weight是0.1
這個Embedding是0.5 這個Embedding是0.4
你就把0.1乘上這一個項量
把0.5乘上這個項量
0.4乘上這個項量全部加起來
就是你的Attention的模組的輸出
所以輸入這個項量
怎麼得到它的輸出呢
就是先計算出Attention
根據Attention的權重做weighted sum
然後就得到你現在這個Attention的模組的輸出
等於就是先看看哪些資訊是相關的
再把相關的資訊集合起來
就是Attention這個模組的輸出
那剛才是舉說輸入中間這個怎麼得到輸出
那我們再反覆看一下
那如果是得到右邊數來第二個向量
怎麼得到它的輸出呢
那跟剛才的原理是一樣的
就拿這個向量去跟其他所有的向量
都去計算相關性
有了這些相關性以後
你就可以做weighted sum
把綠色的向量乘0.3
加黃色的向量乘0.3
加藍色的向量乘0.4
全部加起來就得到
這個位置Attention Module的輸出
把相關的資訊集合起來
就得到這個位置Attention Module的輸出
這個是Attention這個模組的基本概念
當然這邊其實做過一些簡化啦
如果你真的想了解它
可以再參看原始論文或我過去上課的錄影
好 那根據剛才的講法
我們需要計算所有Token兩兩之間的相關性
也就我們需要計算所有Token兩兩之間的Attention Weight
所以第一個Vector要跟這五個Vector計算Attention Weight
第二個Vector要跟這五個Vector計算Attention Weight
以此類推
所以現在假設有五個Vector的話
我們就會計算出5x5總共25個Attention Weight
那這些Attention Weight集合起來也是有名字的
叫做Attention的Matrix
但實際上語言模型在實作的時候
我們只會考慮每一個Token的左邊
也就是每一個Token前面的那些Token
我們計算某一個Token的Attention Weight的時候
不會去管它右邊的Token
只會管它左邊
也就只會管它在這個句子前面的Token
那為什麼會這樣
等一下再為大家揭曉
我們先無視這件事
那這種只考慮前面
也就是左半邊的Token的這種Attention
它是有名字的 它叫做Koso的Attention
那Attention裡面其實還有一個特別的設計
叫做Multi-Head Attention
那今天其實我們的語言模型用的
都是Multi-Head Attention
那為什麼要考慮Multi-Head Attention呢
因為相關性是不只一種的
你要問兩個詞彙的相關性
那所謂的什麼叫做相關
是可以從不同面向來看待的
比如說Dog跟Cat都是動物
所以它們相關
Dog跟Bark它也相關
為什麼 因為Bark是狗可以做的一個動作
所以它們也是相關
但這個跟狗跟Cat的關聯性是不一樣的
所以關聯性有很多種
只用一個計算關聯性的模組
可能沒有辦法準確的計算關聯性
所以怎麼辦呢
我們需要有多個計算關聯性的模組
所以剛才只用藍色的這個計算關聯性的模組
計算出Attention Weight
這是不夠的
還需要其他的計算關聯性的模組
這邊假設有另外一個黃色的計算關聯性的模組
它可以計算出另外一組的Attention的位
就是另外一個計算關聯性的模組
它裡面也有它自己的參數
這些參數也是透過訓練資料學出來的
那如果透過訓練資料學出另外一組參數
那這個黃色的模組就會計算出
另外一組跟藍色的模組不一樣的Attention的weight
實際上在實作的時候
這個Multihead Attention也不會只有兩組
通常也至少會弄個16組這麼多
所以是非常多組的這個Attention Weight
那接下來就根據每一組Attention Weight去做Weighted Sum
那在剛才的例子裡面只針對第一組Attention Weight做Weighted Sum
那如果我們有第二組Attention Weight的話
你也要對第二組Attention Weight做Weighted Sum
第二組Attention Weight說藍色的要乘0.3
紅色的要乘0.4 黃色的要乘0.3
加起來你就得到另外一個輸出
所以實際上這個Attention的module給我們的不是只有一組輸出
而是多組輸出
根據每一種關聯性都會給我們一組輸出
那Attention的這個module呢
Attention的這個模組給我們多種輸出以後
接下來我們需要再整理一下
把多種輸出把它整合起來
所以這邊需要另外一個模組
這個模組是一個 feed forward的network
那這個Feed Forward Network做的事情
就是把這個Attention的輸出呢
這個多個向量呢
通通丟進去
綜合考慮一下
最終輸出一個向量出來
把多個Embedding綜合考慮一下
輸出一個Embedding出來
那Attention
加上Feed Forward這個模組
Feed Forward的這個模組
也是一個Neural Network
其實它裡面也有很多層
那裡面也是有很多的參數
是透過訓練資料被學習出來的
然後呢,這個Attention的模組
加上Feedforward的模組合起來呢
就叫做一個Transformer的Block
所以這是一個Transformer裡面的單位
基本單位
那一個Transformer裡面
不會只有一個Transformer Block
會有多個Transformer Block
所以一開始我們通過這個
第一個模組是做Tokenization
第二個模組是Token Embedding
得到第一組的Token Embedding以後
你就會通過Transformer的Block
裡面有Attention、有Feedforward Network
你會得到另外一組Embedding
這組Embedding是考慮過上下文的Embedding
光通過一個Transformer Block是不夠的
你會把他再通過下一個Transformer Block
一直一直下去
就看你要兜幾個Transformer Block
你就可以通過幾次Transformer Block
那通常在文獻裡面
每一個Transformer Block
我們會稱為一個Layer
雖然實際上如果從Neural Network的角度來看
一個Transformer Block裡面
也有很多層的
已經有很多層的Neural Network
不過在文獻上
通常一個Transformer Block
就視為一個Layer
所以輸入的這個Embedding
會通過多個Transformer Block
一直到通過最後一個Transformer Block之後
把句尾的這一個向量拿出來
再把句尾的向量呢
通過一個Output Layer
Output Layer它也是一個
Function,這個Function裡面做的是呢
是,不好意思,這邊打錯了
這是Linear Transform
不是Linear Transformer
Linear Transform
這個Output Layer裡面呢
有一個Linear Transform
有一個Soft Max
然後呢,會得到一個
機率的分佈
那如果這個地方你沒有聽得很懂的話
反正就是把這個Transformer Block
最後一層的最後一個輸出
丟給另外一個模組
這個模組呢
就會輸出一個機率分佈給我們
好那所以到目前為止呢
我們就知道了一句話
一個句子從輸入到Transformer
一直到最終輸出一個機率分佈
代表下一個Token
應該接哪一個Token的機率
這整個過程到底發生了什麼事
接下來我要回答的問題是
為什麼語言模型在做Attention的時候
只考慮他左半邊
只考慮他前面的Token呢
那這要回到語言模型的運作的
基本邏輯了
我們知道語言模型在運作的時候
他就是做文字接龍
所以當你問ChatGPT問語言模型一個問題的時候
他實際上看到的輸入可能是user冒號
你打的文字AI冒號
然後接下來語言模型要做文字接龍
他可能先接出第一個token叫做W1
那接下來他會把W1再當作輸入
再讓語言模型根據這一整串的文字
再做一次文字接龍就輸出W2
再把W2貼到剛才的句子 剛才的輸入後面
變成有這一整串輸入
再讓語言模型再去做一次文字接龍
以此類推 直到產生結束這個符號
這是語言模型產生答案的過程
那現在我們已經知道語言模型背後運作的機制了
那我們再來想一次 語言模型是怎麼運作的
當你輸入這一串文字 是怎麼產生W1的呢
裡面有一大堆的Transformer的Block
每一個Transformer的Block裡面
有Attention跟Free Forward Network的模組
那在Attention這個模組裡面
AI這一個Token
它是在這整個句子最右邊的那個符號
所以它會跟在它左邊的所有的符號
去計算相關性
去計算Attention的Weight
那這個計算Attention的Weight的過程
所以在每一個transformer block裡面反覆發生
transformer block有好多個
直到最後產生一個token出來
然後產生這個token之後
你會把這個token變成輸入
你會把這個token串在原來的輸入後面
再把這一串句子輸入給同樣一個語言模型
那因為輸入的語言模型是一樣的
所以你再想想看
AI這個token
他跟左邊的這一些Token之間的相關性
剛才已經算過了
所以當你輸入一個新的句子
是User輸入AI
後面再加上W1的時候
AI跟左邊這一些Token
他之間的關聯性已經不用再計算第二次了
所以這邊的關聯性
AI跟左邊這些Token的關聯性是算過的
完全不需要再算一次
把剛才在前一個步驟
在剛才在前一個時間點產生W1的這一些Attention把它記錄下來就好了
你不需要再重算第二次
你需要重算AI冒號這個Token跟W1之間的Attention
因為這是剛才在前一步產生W1的時候
沒有計算過的東西
這是需要耗費額外的資源來進行計算的
但能不能夠乾脆就不要算了呢
可以 現在就是發現說
你根本不需要再去管AI跟W1的Attention
管AI跟他左邊的Attention就好了
你可能想說 欸 如果我Attention算多一點
我能不能夠用算力來交換更好的結果呢
讓AI不只考慮
讓AI這個Token不只考慮左半邊這些Token
連新書的這些Token也考慮
我應該會得到更精確的結果吧
那現在的實驗就是告訴你說
以過去確實有一些模型採取了類似的做法
他不只考慮左半邊的Attention
他會把完整的Attention都算出來
但是現在看起來這些模型的表現並沒有比較好
而ChatGPT那個系列的語言模型
都是隻考慮左半邊的Attention
而他們的表現是非常驚人的
所以現在的認知就是
你其實不需要再多計算額外的Attention
把剛才有計算過的跟左邊這些Token的Attention
保留起來就足夠了
那你現在瞭解了語言模型的機制以後
你就知道為什麼處理超長的文本
會是一個巨大的挑戰
你知道今天各路各大語言模型
在炫耀他們語言模型的效能的時候
往往其中一個炫耀的指標
就是他可以讀多長的文本
那為什麼輸入超長的文本會是一個挑戰呢
如果你瞭解Attention的運作機制
你就會知道到底困難出在哪裡
到底困難出在哪裡呢
困難出在說
假設你今天輸入是100K個Token
對於你的語言模型來說
它裡面的Attention的模組
不是把100K個Token讀過就算了
他需要把100K個Token兩兩之間去計算Attention
所以他需要計算Attention的次數
是你輸入長度的平方是100K的平方
所以假設你把文本的長度當作橫軸
計算Attention的次數當作縱軸的話
你會發現說計算Attention的次數
他的增長是非常快的
計算Attention的次數是跟文本的長度的平方乘正比的
所以你會發現說計算Attention的次數
它的成長是非常迅速的
這就是為什麼要處理長的文本
會需要耗費大量的算力
而這個算力的耗費
並不是跟文本的長度乘正比
而是跟文本長度的平方乘正比
這就是為什麼你處理超長的文本
會是一個挑戰
好 那今天這一堂課呢
就是跟大家非常簡單的介紹了
Attention 的機制
那為下一堂課呢
做鋪墊
那有一些
特別值得關注的研究方向
那比如說我剛才講說
Attention
它是一個非常耗費算力的東西
因為它跟輸入文本的長度的平方
成正比
所以怎麼加快Attention的計算
就變成一個非常關鍵的議題
那如果你想了解怎麼加快Attention的計算的話
可以看我過去上課的錄影
我對加快Attention計算的方法
做了一個非常完整的整理
那近年來也有一些文獻表明
他們期待能夠做到無限長度的Attention
那這個聽起來就非常帥
他可以做無限長度的Attention
那這件事要怎麼做呢
我這邊就是留了兩篇文獻給大家參考
這兩篇文獻採取的是截然不同的做法
就告訴你說要研究怎麼把Attention做好
今天仍然有非常多可以思考的方向
另外呢 要收集超長的文本
這超長的文本你可能很難收集到非常多啦
我們有沒有可能在訓練語言模型的時候
訓練在比較短的文本上
但是在測試的時候
他讀得懂比較長的文本呢
雖然他在練習閱讀的時候都只看過短篇小說
但是他能不能夠在練習完之後讀長篇小說
也沒有問題呢
能不能夠train short test long呢
那有一系列的技術來告訴你
怎麼達到transhort test long這件事
那我就把一些比較重要的文獻
列在這邊給大家參考
那Transformer不一定是語言模型最終的答案
今天還有其他可能的類神經網路的架構
正在研究中看看有沒有辦法取代Attention
那其中比較知名的就是Memba系列
Memba系列它其實跟Recurrent Neural Network很像
只是它去掉了Recurrent Neural Network訓練的時候
比較沒有效率的那個問題
所以感覺如果Memba復興的話
就像是Recurrent Neural Network又回來了
之前Attention取代了Recurrent Neural Network
現在Recurrent Neural Network又要回來取代掉Attention了
那還有一個 最近有一個模型叫做JAMBAR
它就是MEMBAR的一個延伸啦
你可能會問說Attention跟Recurrent Neural Network一定是互斥的嗎
能不能把它結合起來
其實JAMBAR就是把那個MEMBAR
就是類似Recurrent Neural Network的東西
跟那個Transformer結合起來
這個都是非常新的論文
總之這個部分我們今天就不適合在今天的課堂上細講
這個日後有機會再跟大家分享
那今天最主要想跟大家講的就是Transformer背後的運作機制
一個句子讀入語言模型的時候
它是怎麼通過一連串的步驟求出下一個Token的機率分佈的
