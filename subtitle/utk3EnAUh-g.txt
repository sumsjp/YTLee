好 那這一堂課呢
要跟大家講的是 Network Compression
那在這一個課堂上
我們已經看過了很多碩大無朋的模型
舉例來說 Bert 或者是 GPT
那在這一節課裡面
我們要跟大家分享的事情是
我們能不能夠把這些碩大無朋的模型
把它縮小
我們能不能夠簡化這些模型
讓它有比較少量的參數
但是跟原來的效能其實是差不多的呢
這就是 Network Compression 想要做的事情
那為什麼我們會在意
Network Compression 這件事呢
因為很多時候
我們會需要把這些模型
用在 Resource-constrained 的環境下
用在資源比較有限的環境下
什麼樣的環境是資源比較有限的呢
有時候你會需要把這些機器學習的模型
舉例來說 跑在智能手錶上
舉例來說 跑在 Drone 上面
那在這些 Edge Device
在這些 IoT 的 Device 上面
只有比較少的 Memory
只有比較少的 Computing 的 Power
所以我們的模型如果太過巨大
你的手錶可能會是跑不動的
所以我們會需要比較少的模型
那講到這邊有人就會問說
為什麼我們會需要
在這些 Edge Device 上面跑模型呢
我們為什麼不把資料傳到雲端
直接在雲端上做運算
再把結果傳回到 Edge Device
比如說你的手錶就好了呢
為什麼一定要在手錶上面做運算呢
那一個常見的理由是 Latency 的問題
假設你今天需要把資料傳到雲端
雲端計算完再傳回來
那中間就會有一個時間差
那假設你今天的應用
你今天的 Edge Device 上面
你今天的 Edge Device 是自駕車的一個 Sensor
那也許自駕車的 Sensor
需要做非常即時的回應
你需要把資料傳到雲端再傳回來
那中間的 Latency 太長了
也許會長到是不能接受的
那接下來有人又會問說
在未來 5G 的時代會不會 Latency
根本就可以忽略不計呢
那這個時候呢
有人會給你另外一個
我們需要在 Edge Device 上面
做 Computing 的理由
這個理由就是 Privacy
就如果我們今天需要把資料傳到雲端
那這個雲端的這個系統持有者
不就看到我們的資料了嗎
也許我在做什麼事情
我不想讓雲端系統持有者知道
所以為了保障隱私
也許在智能手錶上直接進行運算
在智能手錶上直接進行決策
是一個可以保障隱私的做法
好 那這邊呢
就跟大家講一下
Network Compression 的種種理由
那在這份投影片裡面呢
會跟大家介紹五個
Network Compression 的技術
那這五個技術
都是以軟體為導向的
我們只是在軟體上面
對 Network 進行壓縮
那我們都不考慮硬體加速的部分
當然另外一個支線的研究是
我們想辦法在硬體上加速模型的運算
讓 Edge Device 上面
跑深度學習的模型更加有效率
不過這是另外一個研究的面向
我們這邊就不討論
任何跟硬體有關係的東西
我們只討論跟軟體有關係的東西
好 那第一個要跟大家分享的
是 Network Pruning 這個技術
那我們今天講完 Network Pruning 呢
我們就下課
Network Pruning 顧名思義就是
我們要把 Network 裡面的一些參數
把它剪掉
Pruning 就是修剪的意思
我們把 Network 裡面的一些參數
把它剪掉
為什麼我們可以把
Network 裡面的一些參數剪掉呢
因為你知道俗話說的好
樹大必有枯枝
一個這麼大的 Network
裡面有很多很多的參數
那每一個參數不一定都有在做事
參數這麼多的時候
也許很多參數
它就只是在划水
它是打醬油的
它什麼事也沒有做
那這些沒有做的參數
放在那邊就只是佔空間而已
浪費運算資源而已
何不就把它們剪掉呢
所以 Network Pruning 的基本概念
就是把一個大的 Network
其中沒有用的那些參數把它找出來
把它扔掉
那我小時候啊
在也不算小時候
高中的時候
在生物學課本上看過這個圖
這個是跟 Network Pruning 好像也有一點關係
這個圖是告訴我們說
人剛出生的時候
腦袋是空空的
這些圖呢
這些是腦的神經元
腦袋空空的
神經元跟神經元間沒什麼連結
在六歲的時候會長出非常多的連結
但是隨著年齡漸長
有一些連結就慢慢消失了
這個跟我們等一下要做的 Network Pruning
有異曲同工之妙
那其實 Network Pruning 這件事
不是太新的概念
早在這個 90 年代
Yann Le Cun 就有一篇 Paper
是講 Network Pruning
那篇 Paper 的 Title是 Optimal Brain Damage
他把 Brain
他把這個 Network Pruning
把它剪掉一些 Weight 看成是一種腦損傷
Brain 的 Damage
那 Optimal 的意思就是
我們要找出最好的 Pruning 的方法
讓一些 Weight 被剪掉之後
但是對這個腦的損傷是最小的
好 那 Network Pruning 的概念
大概是怎麼樣進行的呢
它的 Framework 大概是這樣子的
首先呢
你先 Train 一個最大的 Network
你 Train 一個大的 Network
然後接下來呢
你去量這個大的 Network 裡面
每一個參數
或者是每一個 Neuron 的重要性
去評估一下有沒有哪些參數
它是沒在做事的
或有沒有哪些 Neuron
它是沒在做事的
怎麼評估某一個參數有沒有在做事呢
怎麼評估某一個參數重不重要呢
最簡單的方法也許就是
看它的絕對值
如果這個參數的絕對值越大
那它可能越能
對整個 Network 的影響越大
或者如果它的絕對值越接近零
那也許對整個 Network 的影響越小
也許對我們任務的影響越小
或者是其實你也可以套用
Lifelong Learning 那邊的想法
你記得在 Lifelong Learning 裡面
我們不是也要看說
哪一些參數比較重要嗎
我們不是說一大堆的方法
看哪些參數重要
哪些參數不重要
然後決定 bi 那個首位的值嗎
也許我們也可以就把每個參數的 bi 算出來
那我們就可以知道那個參數重不重要
然後把不重要的參數剪掉
那也可以評估每一個神經元的重要性
我們也可以把神經元當作修剪的單位
那怎麼看一個神經元重不重要呢
你就可以比如說
計算這個神經元輸出不為零的次數等等
那總之有非常多的方法
來判斷一個參數或一個神經元是否重要
那我們在這邊就不細講
好 那把不重要的神經元呢
或是不重要的參數就剪掉
就把它從模型裡面移出
那你就得到一個比較小的 Network
但是你做完這個修剪以後
通常你的正確率
你的模型的效能就會掉一點
因為有一些參數被拿掉了嘛
所以這個 Network 當然是受到一些損傷
所以正確率就掉一點
但是我們會想辦法
讓這個正確率再回升一點
怎麼讓正確率再回升一點呢
就是把這個比較小的 Network
把剩餘沒有被剪掉的參數
再重新做微調
就把你的訓練資料拿出來
把這個比較小的 Network 呢
再重新訓練一下
然後訓練完之後
其實你還可以重新再去評估一次
每一個參數的正確性
你還可以再 Remove 掉
再剪掉更多的參數
然後再重新進行微調
那這個步驟呢
可以反覆進行多次
那為什麼我們不一次剪掉大量的參數呢
因為在實驗上發現說
如果你一次剪掉大量的參數
可能對你的 Network 的傷害太大了
可能會大到你用 Fine-Tune 也沒有辦法復原
所以一次先剪掉一點參數
比如說只剪掉 10% 的參數
然後再重新訓練
然後再重新剪掉 10% 的參數
再重新訓練
反覆這一個過程
你可以剪掉比較多的參數
當你的 Network 夠小以後
那整個過程就完成了
你就得到一個比較小的 Network
而且這個比較小的 Network
也許它的正確率
跟大的 Network是沒有太大的差別的
那我們剛才講到說
修剪的單位可以以參數為單位
也可以以神經元來當作單位
那用這兩者當作單位有什麼不同呢
用這兩者當作單位在實作上
會是有蠻顯著地差距的
如果我們現在是以參數當作單位
會發生什麼事
假設我們是要評估說
某一個參數要不要被去掉
某個參數對整個任務而言重不重要
能不能夠被去掉
那我們把這個不重要的參數去掉以後
我們得到的 Network
它的形狀可能會是不規則的
所謂不規則的意思是說
舉例來說 我們來看紅色的這個 Neuron
它連到接下來三個綠色的 Neuron
但第二個紅色的 Neuron
它只連到兩個綠色的 Neuron
或這個紅色的 Neuron
它的輸入只有兩個藍色的 Neuron
而這個紅色的 Neuron
它的輸入有四個藍色的 Neuron
如果你是把參數當作單位來進行修剪的話
那你修剪完以後的 Network
它的形狀會是不規則的
形狀不規則會造成什麼樣的問題呢
最大的問題就是你不好實作啊
你想想看
你用 PyTorch
要實作這種形狀不規則的 Network
你好實作嗎
你不好實作啊
因為在 PyTorch 裡面
你在第一個 Network 的時候
你的定義方法都是
每一層有幾個 Neuron 對不對
你都是定義說
我現在每一層要輸入幾個
輸入
輸入有幾個 Neuron
輸出有幾個 Neuron
或者輸入多長的 Factor
輸出有多長的 Factor
這一種形狀不固定的 Network
你根本就不好寫啊
你自己實作的時候
你根本就不好實作
而且就算你硬是把這種形狀不規則的 Network
把它實作出來
你用 GPU 加速也不好加速
GPU 在加速的時候
就是把 Network 的運算
看成一個矩陣的乘法
但是當 Network 是不規則的時候
你就不容易用矩陣的乘法來進行加速
你不容易用 GPU 來進行加速
那所以實際上啊
在做 Weight Pruning 的時候
在實作上
你可能會把那些 Prune 掉的 Weight 直接補零
就是 Prune 掉的 Weight
它不是不存在
它的值只是設為零
那這樣的好處就是你的實作就比較容易
你就比較容易用 GPU 加速
那這樣的問題是什麼呢
這樣的問題是
你根本就沒有真的把 Network 變小啊
你這邊說這個 Network
它的這個 Link
這個 Weight 它的值是零
你還是存了這個參數啊
你還是存了一個參數
在你的 Memory 裡面
你並沒有真的把 Network 變小
你只是在想像中把它變小
你在自 High
你覺得這個 Network 有變小
但實際上這個 Network 並沒有真的變小
所以這個是以參數為單位來做 Pruning 的時候
你在實作上會遇到的問題
這個文獻上的實驗
就是想要跟你展示說
以參數為單位做 Pruning 的時候
你會遇到什麼樣的問題
我們先來看紫色的這一條線
紫色的這一條線呢
它說是 Sparsity
這個 Sparsity 是什麼意思
這個 Sparsity 就是有多少百分比的參數
現在被 Prune 掉了
那你發現說這邊啊
這個紫色的這條線
它的值都很接近 1
什麼意思
代表有接近大概 95% 以上的參數
都被 Prune 掉了
這個 Network Pruning 的方法
其實是一個非常有效率的方法
往往你可以 Prune 到 95% 以上的參數
那但是你的 Accuracy 只掉 1~2% 而已
所以這邊參數 Prune 的是
Prune 得非常凶的
有 95% 的參數都被丟掉了
照理說丟掉了 95% 的參數
只剩下 5% 的參數
這個 Network 變得很小
它的運算要很快吧
但實際上你發現根本就沒有加速多少
甚至可以說根本就沒有加速
如果你看這些長條圖
這些長條圖顯示的是
在三種不同的 Computing 的 Resource 上面
你 Speedup 的程度
你加速的程度
那加速的程度要大過 1 才有加速嘛
加速程度小於 1 其實是變慢的
結果你發現說
在多數情況下根本就沒有加速
多數情況下其實都是變慢
也就是你把一些 Weight Prune 掉
結果你的 Network 形狀變得不規則
然後你真的用 GPU 加速的時候
你反而沒有辦法真的加速它
所以 Weight Pruning
不見得是一個特別有效的方法
那 Neuron Pruning
以神經元為單位來做 Pruning
也許是一個比較有效的方法
如果我們用神經元做單位來 Pruning
丟掉一些神經元以後
你 Network 的架構仍然是規則的
簡單來說就是你用 PyTorch 比較好實作
你實作的時候
你只要改那個每一個 Layer
Input Output 的那個 Dimension 就好了
所以你比較好實作
也比較好用 GPU 來加速
這個是在 Network Pruning 實作上
可能會遇到的問題
好 我們來看一下大家有沒有問題要問
我看一下
剪枝
有同學問說這個剪枝
應該是剪掉的剪
還是減少的減
在我認知裡面是剪掉的剪
但是如果我有說錯
你再來糾正我
那個有同學問說
請問 CNN 中做完 Pruning
剪掉遮罩中一些參數
它的運算量是不是沒變
不好意思我沒有非常懂這個問題
遮罩的參數是什麼
我這邊沒有非常了解
也許你可以重新再 Formulate 一下你的問題
然後我等一下再來看
有同學問說
這個 Pruning 有沒有效率是函式庫的問題
對啦 是函式庫的問題
那如果你可以想辦法寫一個
Irregular的 Network
也很有效的函式庫的話
那你就可以用 Weight Pruning
但是問題是
大家都沒有要自己寫函式庫
你都是(0:14:46) PyTorch 嘛
所以你用 Weight Pruning 就沒有辦法加速
好 如果反過來想
在處理一個任務時
把 Network 慢慢擴大
可以比直接用大 Network 有更好的表現嗎
有一個同學問說
如果我們先 Train 一個小的 Network
再把它慢慢變大
會不會比較好呢
因為我們剛才講
Network Pruning 的時候是把大的 Network
慢慢變小
那如果我們先 Train 小的 Network
再慢慢變大會不會比較好呢
答案是不會
我們在下一頁投影片就會回答你的問題
然後同學說可變 Network 本來就比較難做
對啊 可變 Network 本來就比較難做
好
好 那接下來我們就要問一個問題
你說我們先 Train 一個大的 Network
再把它變小
而且說小的 Network 跟大的 Network
它們的正確率沒有差太多
那我們怎麼不直接
Train 一個小的 Network 就好了呢
Train 一個小的
直接 Train 一個小的 Network 比較有效率吧
還 Train 大的 Network 變小幹嘛
根本是捨本逐末
為什麼不直接 Train 小的 Network
好 那一個普遍的答案是
大的 Network 比較好 Train
你會發現說
如果你直接 Train 一個小的 Network
你往往沒有辦法得到
跟大的 Network 一樣的正確率
你可以先 Train 一個大的 Network
再把它變小
沒有 正確率沒有掉太多
但直接去那個小的 Network
你得不到跟 Pruning
大的 Network Pruning 完
變得小的 Network 一樣的正確率
那至於大的 Network 為什麼比較好 Train
那也可以參看以下這個影片的連結
我在過去的課程有試圖解釋這件事
好 但是為什麼大的 Network 比較好 Train 呢
那這邊有一個假說叫做大樂透假說
那不過它既然叫做假說
就代表說它不算是完全被實證的一個方
不算是一個被實證的理論
如果它是被已經被證明出來的
那它就是一個理論
但它現在只是一個假說而已
那這個大樂透假說是怎麼解釋
為什麼大的 Network 比較容易 Train
直接 Train 一個小的 Network
沒有辦法得到跟大的 Network 一樣的效果
一定要大的 Network Pruning 變小
結果才會好呢
大樂透假說是這樣說的
我們知道說訓練 Network
是一個看人品的事情
我們現在大家都做過這麼多作業了
我相信你都一定有很多的心酸血淚
你知道 Train Network 就是看人品的
每次 Train Network 的結果不一定會一樣
你抽到一組好的 Initial 的參數
你就會得到好的結果
抽到一組壞 Initial 的參數
就會得到壞的結果
那像這種看 這樣就好像說
這個樂透也是一個看人品的東西
但是怎麼在樂透這個遊戲裡面
得到比較高的中獎率呢
是不是就是包牌
買比較多的彩券
就可以增加你的中獎率
所以對一個大的 Network 來說也是一樣的
大的 Network
可以視為是很多小的 Sub-network 的組合
我們可以想成是一個大的 Network 裡面
其實包含了很多小的 Network
當我們去訓練這個大的 Network 的時候
我們等於是在訓練
同時訓練很多小的 Network
那每一個小的 Network
不一定可以成功的被訓練出來
所謂成功的訓練出來是說
它不一定可以
透過 Gradient Descent
找到一個好的 Solution
我們不一定可以訓練出一個好的結果
我們不一定可以讓它的 Loss 變低
但是在眾多的 Network 裡面
眾多的 Sub-network 裡面
只要其中一個人成功
就可以一人得道 雞犬升天
其中一個 Sub-network 成功
大的 Network 它就成功了
而今天大的 Network 裡面
如果包含的小的 Network 越多
那就好像是去買樂透的時候
包牌包比較多的彩券
買比較多的彩券一樣
彩券越多
中獎的機率就越高
所以一個 Network 越大
它就越有可能成功的被訓練起來
那這個大樂透假說
它在實驗上是怎麼被證實的呢
它在實驗上的證實方式
跟 Network 的 Pruning 非常有關係
所以我們就直接看一下在實驗上
是怎麼證實大樂透假說的
好 你現在有一個大的 Network
在這個大的 Network 上面一開始的參數
是隨機初始化
好 把參數隨機初始化以後
得到一組訓練完的參數
訓練完的參數我們用紫色來表示
接下來你用 Network Pruning 的技術
把一些紫色的參數丟掉
而得到一個比較小的 Network
如果你現在
直接把這個小的 Network 裡面的參數
再重新隨機的去 Initialize
也就是你重 Train 一個一樣大小的
小的 Network
就是你把這個 Network 複製一次
一樣大小
但是參數完全不一樣 重新再訓練一次
重新再訓練一次
你會發現訓練不起來
直接訓練這個小的 Network 訓練不起來
訓練一個大的再把它變小
沒問題
但是直接訓練小的訓練不起來
但是假設這一個小的 Network
我們再重新 Initialize 參數的時候
我們用的跟這組紅色的參數
是一模一樣的
就訓練得起來
大家可以了解這兩者的差別嗎
就是這兩組參數
雖然都是 Random Initialize 的
但是這組綠色的參數
跟這組紅色的參數是沒有關係的
而這邊這些 Random Initialize 的參數
是直接從這邊的紅色參數裡面
選出對應的參數
就是這邊有四個參數
我們就是把這邊對應到的這四個參數
直接把它複製過來
這邊有四個參數
我們就把這裡面對應到的四個參數
直接複製過來
把這裡面的參數直接複製過來
就訓練了起來
那如果用大樂透假說來解釋的話
就是這裡面有很多 Sub-network
而這一組 Initialize 的參數
就是幸運的那一組
可以 Train 得起來的 Sub-network
所以當我們今天用
把這些 Net
把這個大的 Network Train 完
再 Pruning 掉的時候
你留下來的就是幸運的那些參數
可以訓練得起來的那些參數
所以這一組初始化的參數
它是可以訓練得起來的一個 Sub-network
但是如果你再重新隨機初始化的話
那你如果運氣比較不好
你就抽不到可以成功訓練起來的參數
所以這個就是大樂透假說
那大樂透假說非常地知名
它在 ICLR 2019
應該是 2019 沒錯
得到 Best Paper Award
所以它是一個非常知名的一個假說
後面也有很多後續的研究
比如說有一篇有趣的研究叫做
Deconstructing Lottery Tickets
解構這個大樂透
那這個解構大樂透裡面有什麼有趣的結論呢
我們就直接講它的結論
第一個
它試了不同的 Pruning Strategy
然後發現說
某兩個 Pruning Strategy 是最有效的
那在這個細節我們就不講
它做了一個非常完整的實驗告訴你說
Pruning 有哪些可能的 Strategy
然後它發現說
如果訓練前跟訓練後
它的絕對值差距越大
那 Pruning 那些 Network
得到的結果是越有效的
那另外一個比較有趣的結果是
到底我們今天這一組好的 Initialization
是好在哪裡呢
它發現說
如果我們
我們只要不改變參數的正負號
就可以訓練起來
就小的 Network 只要不改變正負號
就可以訓練起來
什麼意思呢
就是假設你 Pruning 完以後
剩下的這個數
假設你 Pruning 完以後
那你再把原來
Random Initialize 的那些參數拿出來
它的值是這個樣子
0.9 3.1 -9.1 8.5
你可以完全不管它的數值
直接把正的數值
大於 0 的通通都用 +α 來取代
小於 0 的都用 -α 來取代
用這組參數
去 Initialize 你的 Model
這樣也 Train 得起來
會跟用這組參數去 Initialize 差不多
所以這個實驗告訴我們說
正負號是初始化參數
能不能夠訓練起來的關鍵
它的絕對值不重要
正負號才重要
然後它在那個文章的
那個標題上 它在章節標題上
特別取了一個
就是它
如果你乍看之下會以為它想要講
Significance Of Initial Weights
但它故意在這個 Sign 後面加了一個 -
告訴你說是 Sign-ifincance
就是這個正負號是很重要的
它想要玩個一語雙關
不過好像沒什麼人注意到就是了
然後最後一個神奇的發現是
它發現 就既然我們在想說
一個大的 Network 裡面
有一些 Network
有一些 Sub-network
它是特別是好的初始化的參數
它訓練起來會特別地順利
那會不會一個大的 Network 裡面
甚至其實已經有一個 Sub-network
它連訓練都不用訓練
直接拿出來就是一個好的 Network 呢
我們完全不用訓練 Network
我們直接把大的 Network Pruning 一 Pruning
就得到一個可以拿來做分類的 Classifier 了
有沒有可能是這個樣子的呢
就好像米開朗基羅說
他是怎麼雕出大衛像的呢
他不是雕出大衛像
他是把大衛像從石頭裡面釋放出來
大衛像原來就是石頭裡面
原來就在石頭裡面
他只是把它 把多餘的地方把它雕
把多餘的地方把它去掉而已
那會不會在整個大的 Network 裡面
算參數都是隨機的
其中已經有一組參數
它就已經可以做分類了
把多餘的東西拿掉
直接就可以得到好的分類結果的呢
答案是 是 這樣子
你可以自己去讀一下那個文章
其實可以得到跟 Supervise
其實很接近的正確率
其實我看到這篇文章的時候
這個結論已經沒有讓我覺得非常神奇了
因為在這個
解構大樂透這篇文章發表的前幾個月
就有一篇文章叫做
Weights Agnostic Neural Networks
它是說
它弄了一個神奇的 Network
這個 Network 裡面
所有的數值要嘛是隨機的
要嘛通通都設 1.5這樣
結果這個 Network
也可以得到一定程度的結果
所以看起來就算
你的 Network 裡面參數都是隨機的
或根本就給它一個 Constant
得到的
也有可能可以得到好的 Performance
所以這個結論其實沒有讓人特別訝異
因為在幾個月之前就已經有同樣的文章了
這邊放了這兩篇文章
Arxiv 就有類似的說法了
這邊放了這兩篇文章的 Arxiv 連結
這邊放的連結是
它們最後一個上傳到 Arxiv 的版本
所以如果你看這個
上最後一次 最後一個版本上傳的月份的話
你會覺得
Weight Agnostic Networks 是後出來
然後解構大樂透是先出來
但如果你看第一個上傳的版本的話
是先有 Weight Agnostic Networks
然後才有解構大樂透
所以我是先讀到這篇才讀到解構大樂透的
但是大樂透假說它一定是對的嗎
不一定
有一篇文章是打臉大樂透假說
這篇文章叫做
Rethinking The Value Of Network Pruning
而且神奇的是
這篇文章跟大樂透假說是同時出來的
它們同時出現在 ICLR 2019
所以就是在 ICLR 2019 裡面有兩篇文章
它們得到了不太一樣的結論
那篇文章說的是什麼呢
這篇文章說 好 它試了兩個 Dataset
還有好幾種不同的模型
好 然後呢
它說
這個是沒有 Pruning 過的 Network 的正確率
然後它試著去 Pruning 了一下 Network
然後再重新去做 Fine-tuned
小的 Network
可以跟大的 Network 得到差不多的正確率
然後它說一般人的想像是
如果我們直接去 Train 這個小的 Network
正確率會不如大的 Network
Pruning 完以後的結果
它試了第一次實驗
叫做 Scratch-E
Scratch-E 的意思就是
它的參數是隨機初始化的
要注意一下它的隨機初始化
跟大樂透假說裡面隨機初始化是不一樣
它的隨機初始化就是真的是隨機初始化
大樂透假說裡面的隨機初始化是從
原來的那一組隨機的參數裡面
去借來那種隨機的參數
我知道這個拗口
希望你知道我在說什麼
好 這個
這個 Scratch
這邊這個 Scratch-E 的意思是說
我們就真的隨機初始化參數
訓練一個小的 Network 跟這個
有 Pruning 過的 Network 它的大小是一樣的
發現果然差了一點
跟多數人的想像好像是一樣的
但是接下來它說
如果我們在 Update 的時候
多 Update 幾個 (A part) 會怎樣呢
之前的人設定的 (A part) 數目
小的 Network 的 (A part) 的訓練數目
都跟大的 Network 一樣
但是如果小的 Network 的 (A part) 的數目
多設一點會怎樣呢
多設一點
就比 Pruning 過以後的結果就好了
所以之前
覺得大的 Net
小的 Network 訓練不起來
要先訓練大的再做 Pruning
這個會不會就是個 Illusion
就是個幻覺
是個都市傳說
直接訓練小的 Network
(A part) 設多一點
反正就是訓練得起來
就是這樣子
其實這篇文章裡面
這篇文章放在那個
在 ICLR 審查的時候
當然就有 Reviewer 去問說
你這個跟大樂透假說正好是相反的
你有沒有什麼 Comment
那其實在這篇文章裡面
它也有對大樂透假說做出一些回應
它覺得大樂透假說是比較有
大樂透假說觀察到的現象
也許只有在某一些特定的情況下才觀察得到
那根據這篇文章的實驗是說
只有在 Learning Rate 設比較小的時候
還有 Unstructured 的時候
Unstructured 的時候
就是我們 Pruning 的時候是
以 Weight 作為單位來做Pruning 的時候
才能觀察到大樂透假說的現象
它發現說 Learning Rate 調大
它就觀察不到大樂透假說的這個現象
所以到底大樂透假說
有多正確 是真是假
這個未來尚待更多的研究來證實
