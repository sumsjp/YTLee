好,那各位同學大家好啊,那我們就來上課吧
那我們現在講到大模型訓練的第三階段
大模型進行訓練的第三階段就是讓模型真的跟使用者互動
在實戰中打磨自己的技巧
那這個第三階段呢,就是鼎鼎大名的Reinforcement Learning from Human Feedback
那它的縮寫是 RLHF
那這個Human Feedback是什麼意思呢
你今天會在使用模型的過程中
提供模型很多的回饋
告訴它你的喜好
舉例來說
當你在使用ChatGPT的網頁頁面的時候
你問它一個問題
它給你一個答案
如果你對這個答案不滿意的話
你可以按這個鈕
會重新生成新的答案
每次ChatGPT重新生成新的答案以後
他就會問你一個問題
我現在這個新的答案跟剛才舊的答案比起來
你覺得是比較好、比較差、還是差不多
那當你告訴 ChatGPT 說
現在新的答案跟舊的答案比起來
比較好、比較差、還是差不多的時候
你等於就提供了語言模型回饋
那未來語言模型會收集足夠的回饋以後
對它的參數再進行微調
那我們現在已經講到大型語言模型的訓練的第三階段
那我們來複習一下在這三個階段中的訓練資料
看起來有什麼不同
第一階段叫Pretrain
第二階段叫Instruction Fine Tuning
第三階段是RLHF
在第一階段跟第二階段
我們都是提供給模型輸入跟下一步做文字接龍的時候
該有的輸出
在第二階段的時候
這些輸出是由人所提供的
所以叫做 supervised learning 督導式學習
需要人花費大量的時間收集問題跟他的答案
才能夠在第二階段教語言模型
第一階段跟第二階段
它的資料格式其實非常的類似
只是第一階段沒有太多人類的介入
訓練資料是直接從網路上取得的這個步驟
叫做 self-supervised learning 自督導式學習
而第三階段這個訓練資料的樣子
就跟第一階段和第二階段非常的不一樣
在第三階段機器沒有人明確的告訴他說
接下來應該要產生哪一個Token
機器得到的學習的訊號
是某一個答案好過另外一個答案
那透過這種回饋的資訊來進行學習的方法
叫做Reinforcement Learning
它的縮寫是 RL
那中文翻譯成增強式學習
那 Reinforcement Learning 是怎麼運作的呢
如果從概念上來講
Reinforcement Learning 的運作方式是這個樣子的
當有人告訴語言模型說
某一個答案比另外一個答案好的時候
語言模型就微調它的參數
這次微調的目標是什麼呢
微調的目標是希望人覺得好的答案
產生的機率高一點
人覺得不好的答案產生的機率低一點
增強式學習的概念就是這樣
說完了
假設你想要真的瞭解增強式學習的內涵的話
可以看我們過去機器學習的上課錄影
而這個ChatGPT在做這個RLHF在做增強式學習的時候
他微調參數的演算法叫做PPO
如果你真的想知道PPO是什麼的話
我也有一個系列的影片
在講PPO
那我把他的連結留在投影片上
給大家參考
但本質上他的概念就是
人覺得好的答案
就提高他出現的機率
人覺得不好的答案
就降低他出現的機率
那如果你看這一些RL的影片
看PPO的演算法
看得顛三倒四的話
有記得我說的原則
那你就知道
RLHF實際上在做的事情是什麼
好 那我們從人類產生訓練資料的角度來看看
RLHF 跟第二階段的 instruction fine-tuning 有什麼樣的不同
不管是在第二階段還是第三階段
都需要人類的介入
但是在第二階段 instruction fine-tuning 的這個階段
人類是比較辛苦的
因為人類必須要想出問題
而且要提供正確的答案
在RLHF 的階段人類是比較輕鬆的
人類並不需要提供正確的答案
而是由模型產生兩個答案
由人類來決定哪一個答案是比較好的
所以在第三階段
RLHF的階段
人類是輕鬆的
你就按一個鈕
告訴語言模型說
哪一個答案是你覺得比較好的
那語言模型就從你按鈕的這個過程中
從你按下這個鈕的這個資訊
去進行學習
而且RLHF還有一個好處
很多時候人類要寫出正確的答案
並不容易
但是要判斷答案是好的還是不好的
卻相對容易很多
舉例來說
假設你要叫語言模型寫詩
寫一首讚揚大型語言模型的七言絕句
如果是在instruction fine tuning的第二階段
你需要幫語言模型寫出正確答案
你又不是曹植
你也沒辦法七步成詩
所以叫你寫這個答案
恐怕是有點困難的
但是假設先讓語言模型自己產生兩首詩
這邊就叫GPT-4自己寫兩首詩
第一首是什麼
自知千年往光年萬裡橋
第二首是
知識海洋身無影
字裏行間行見真心
也不知道在寫些什麼
但是對於人類來說
到底哪一首比較好呢
你一秒就可以看出來
顯然第二首比較好啊
你也不需要鑒賞詩的能力
你就知道第二首比較好
因為這邊是要寫七言絕句啊
第一首根本就是五言了
根本整個連格式都不對了
所以對人類來說,有時候你要寫出正確答案不容易
但叫你判斷哪一個答案比較好,往往相對是容易很多
這邊我們是從人類產生訓練資料的角度來看
那我們再從另外一個角度,從模型學習的角度來看
來看看Instruction Fine Tuning跟RLHF對於語言模型的學習有什麼不一樣的地方
在做Intruction Fine Tuning的時候
模型要學的就是一件事
怎麼接下一個字
而這背後隱含的假設是
假設每次做文字接龍
每一步接的都是對的
那結果最終接出來就會是好的
我知道每一步都是對的
最後結果就會是好的
但這個假設不一定是成立的
所以在第一階段跟第二階段
模型對於生成的結果
並沒有通盤的考量
他只學要怎麼接下一個字而已
可是在RLHF的時候
第三階段的時候就不一樣了
因為模型這邊得到的訊號
是某一個完整的答案
優於另外一個完整的答案
所以模型現在會進入新的思考模式
模型會知道
不要管中間接龍的每一步
到底做得怎麼樣
最重要的是最終生成的完整的結果
所以RLHF比較有機會讓語言模型通盤的考量它生成的結果
而不是只專注於下一步應該要產生哪一個詞彙、哪一個字
那如果要用一句話概括instruction fine tuning跟RLHF的差異的話
那我會說instruction fine tuning就是隻問過程不問結果
只問下一步接的對不對
不管整體而言答案產生出來怎麼樣
而RLHF是隻問結果不問過程
我們只管最終生出來的結果好不好
不管中間每一步生成的對不對
講到這邊我就想到天龍八部的珍瓏棋局
不知道大家有沒有看過天龍八部
還是跟大家講一下好了
真龍棋局是這樣子的
有一個人叫做蘇星和
他就擺了一個真龍棋局叫大家來破解
但是古往今來的各大高手
都沒有辦法破解這個棋局
而右邊這個是蘇星和
阿祖猜左邊這個應該是段譽
然後這個圖呢顯然是用AI生的啦
你這棋盤不知道為什麼是圓的
這個是AI生成的圖
但是有一個少林寺的和尚叫做虛竹
然後就在棋盤上面隨便下了一子
隨機下了一紙
然後就把自己的紙都堵死了
其他人都想說怎麼可以這樣下
但沒想到他就贏了
因為這個珍瓏棋局神妙的地方
就是在中間某一步
你要先堵死自己的棋子
最後才能獲得勝利
過去的人就是因為太過糾結
在每一步都要下好
反而最終沒有辦法下好
而中間把自己的棋子堵死
最後反而會海闊天空
這個就是珍瓏棋局
跟我們剛才講的
RLHF跟Introduction Fine Tuning的差別
有異曲同工之妙
那講到這個下圍棋啊
你知道這個語言模型在做的事情
還有學習的過程
其實跟下圍棋AlphaGo有很多可以類比的地方
我們知道語言模型做的事情
就是讀一個未完成的句子
然後生出下一個應該產生的Token
下一個應該產生的字
而下圍棋AlphaGo做的事情
其實也非常的類似
他做的事情是去看一個未完成的棋局
然後決定接下來下一步要去落子的位置
那AlphaGo會跟一個對手互動
他們互動的過程是這樣子的
先給AlphaGo看一個棋局
然後他決定下一步要落子的位置
接下來他落子在棋盤上面
他的對手看到AlphaGo落子以後
那對手根據現在的棋局也落一子
然後會改變了棋局上面黑子跟白子的位置
然後AlphaGo看到更新後的棋局以後
就會產生新的輸出
決定下一步應該落子的位置
這是AlphaGo跟對手的互動
那語言模型呢
語言模型給他一個未完成的句子以後
他會輸出一個機率分佈
根據這個機率分佈去值一次骰子
產生一個Token
產生一個字
把這個字接在剛才的輸入後面
因為輸入變了
語言模型就會有新的輸出
那如果我們把AlphaGo跟對手的互動展開來看的話
你其實也可以把AlphaGo跟對手的互動
看作是一個棋局接龍
就AlphaGo下第一手
對手下第二手
AlphaGo根據第一手跟第二手再去接第三手
一直接下去
這個跟語言模型做文字接龍
其實有異曲同工之妙
語言模型在做的事情
那現在大家都很熟悉
也就是做文字接龍
根據目前的輸入
先產生下一個token 根據下一個token
再產生下一個token 一直到產生結束的5號為止
那圍棋啊 過去如果你有聽過機器學習的課程的話
我常常會告訴你說下圍棋是一個分類的問題
但是實際上這樣的講法並不精確
更精確的來說應該是
下圍棋的每一步是解一個分類的問題
給一個未完成的棋盤決定要落在哪一子
這是一個分類的問題
但是整場圍棋看起來
是一個生成式學習的問題
就好像說語言模型
讓機器說一句話
我們在開學的第一堂課就告訴你說
整體看起來是生成式學習的問題
但實際上語言模型做的是文字接龍
文字接龍的每一步都是分類的問題
就整體看來
不管是圍棋還是語言模型
整體看來是生成式學習的問題
但是細究它每一步都是分類的問題
那AlphaGo是怎麼學習的呢
這個可能很多同學通通都知道
AlphaGo的學習有兩階段
第一階段是跟著棋譜學習
也就是看人類跟人類之間下的棋譜
人類下哪裡就跟著下哪裡
棋譜告訴你說
如果下到這邊人類下一步就會下3-3
那AlphaGo就下3-3
起步下到這邊,人類下一步會下5-5
那AlphaGo就下5-5
AlphaGo跟起步學習
就是語言模型的第一階段跟第二階段的學習方式
在第一階段的Pretrain跟第二階段的Instruction Fine Tuning
語言模型的學習方式是
人類老師說什麼,他就跟著說什麼
人類老師下一個字會接什麼字,他就跟著接什麼字
所以語言模型的第一階段跟第二階段
其實就對應了AlphaGo跟棋譜學習
那大家也都知道說AlphaGo光是跟棋譜學習是不夠的
跟棋譜學習模仿人類的招式只能夠打敗一般的高手
但如果要打敗頂尖高手AlphaGo還需要RL
那AlphaGo的RL是怎麼學的呢
就是AlphaGo先跟對手下一局棋
然後做完一次棋局接龍以後
接完發現自己贏了
那就是正面的回饋
他就會提高這一些棋步的機率
而如果下完以後發現輸了
要降低這個輸的這局棋裡面產生的棋步出現的機率
這是AlphaGo透過RL學習的方式
那怎麼知道輸跟贏呢
對於圍棋來說是比較單純的
圍棋的輸贏是直接從圍棋的規則得到的
語言模型透過RL學習的方式也非常的類似
由語言模型產生兩個答案
由人類來評價這兩個答案哪一個比較好
好的答案提高機率
不好的答案減低機率
所以AlphaGo的IL的學習
跟語言模型透過RL學習
其實有很多可以類比的地方
當然如果你實際上對AlphaGo有深入研究的話
你會知道AlphaGo裡面還有很多其他的東西
比如說Monte Carlo tree search
比如說Value Network
那我這邊都沒有講到
我這邊講的是一個大的概念
你從大的概念看起來
語言模型跟下圍棋有很多可以類比的地方
當然如果你要深究它的細節的話
還是有很多不同之處
那我們剛才講到這個回饋的方式
你會發現語言模型的回饋跟下圍棋的回饋很不一樣
在下圍棋裡面下完一局棋
我們就知道好還是不好
輸還是贏
但是語言模型的回饋往往需要語言模型產生兩個答案
由人類來做排序決定哪一個答案是比較好的
能不能讓語言模型就給一個答案
然後人類直接說這個答案好或不好呢
不是不可以
但是你發現在文獻上
讓語言模型產生多個答案
再由人來排序答案的好壞是比較常用的方式
為什麼讓人來排序答案的好壞是比較常用的方式呢
這個如果根據我們實作的經驗
你會發現說假設你真的是給人一句話
然後問人說你覺得這句話好或不好
人往往是沒辦法回答的
因為一句話講的好不好是相對的
他往往沒有標準答案
所以你直接給人一句話
問他說這個答案好不好
你問他說臺灣最高山是哪一座山
AI回答是玉山
這個答案到底好不好呢
有人會跟你講好不就是回答了嗎
有人跟你講不好
你應該多補充一些玉山相關的資料才叫做好
所以好跟壞
圍棋的好壞輸贏是有標準答案的
是有既定的規則的
是明確的
但是一句話一句回答的好壞是不明確的
所以你很難提供給一句話
就叫人回答說一句話是好還是不好的
所以這就是為什麼在語言模型裡面
做回饋的時候
往往要對人類的要求是比較多個答案的好壞
而不是對單一答案的好壞進行評比
那這個AlphaGo的RL跟語言模型的RL
還有一個很大的差異
AlphaGo的RL裡面
幾乎不需要人工介入
首先回饋的部分並不是由人來判斷棋局的輸贏
是圍棋的規則直接就決定了棋局的輸贏
所以說AlphaGo要跟一個對手才能下啊
那不是這邊需要人嗎
在AlphaGo的論文裡面他告訴你
對手其實就是其他版本的AlphaGo啦
所以AlphaGo是左手跟右手下
他並不需要真的跟人下
這就是為什麼RL最早成功是成功在棋類遊戲上
因為在棋類遊戲上你真的要做RL
還真的不需要真人
讓機器自己跟自己下
他就可以學習了
你不需要太多人工的介入
但是語言模型這邊
你需要人來介入
你需要人來評斷
一個句子是好的還是不好的
你需要人來對不同的句子間進行評比
所以語言模型跟圍棋的RL
還是有很大的差異的
語言模型需要人類的幫助
但是問題來了
人類的時間精力太有限了
人類能夠提供的回饋也是有限的
所以對語言模型的IO來說
一個關鍵的議題就是如何有效利用人類的回饋
人類能夠提供的回饋很少
怎麼樣把人類提供的回饋它的效用最大化呢
這邊又要講到芙莉蓮啦
我來講一下芙莉蓮的故事
因為我怕不是每一個人都看過芙莉蓮
最後還是講一下芙莉蓮的概要
芙莉蓮這個故事是這個樣子的
在故事一開頭有一個勇者小隊
那個勇者叫做辛梅爾
他帶著芙莉蓮和其他夥伴去討伐了魔王
但是這個動畫的開頭
是在討伐魔王的非常多年之後
50年後
勇者辛梅爾在第一集只露個臉就過世了
那芙莉蓮就帶著他對辛梅爾的回憶
踏上了新的旅程
但在弗利連在旅程中
當他遇到難以抉擇的事情的時候
他腦中就會浮現辛梅爾
辛梅爾雖然只有在第一集有出現
但是他的存在感真的是無所不在
芙莉蓮常常說如果是辛梅爾的話
就一定會這樣做
然後他就可以下決定了
所以對語言模型來說
我們能不能夠做到類似的事情呢
用人類提供的回饋
去創造一個虛擬的人類
語言模型之後他需要回饋的時候
他不需要去問真正的人類
他問虛擬的人類
在語言模型的腦中就是想像
如果是人類的話
一定會這樣說的
如果人類的話一定會覺得這麼說很不錯的
讓語言模型想像人類
覺得什麼是好的
這件事情具體而言要怎麼操作呢
語言模型想像出來的人類叫做回饋模型
它的英文是reward model
那怎麼訓練一個回饋模型呢?
怎麼訓練一個回饋模型去模仿人類呢?
你就先取得一些人類的回饋
人類說第一個答案比第二個答案好
接下來你就訓練一個回饋模型
這個回饋模型要做到的事情就是
當把問題跟第一個答案輸入回饋模型的時候
它會輸出一個分數
我們把問題跟答案丟給回饋模型的時候
它會輸出一個分數
那如果第一個答案比第二個答案好
那第一個答案輸入回饋模型得到的分數
要大過第二個答案輸入回饋模型得到的分數
那我們訓練出這個回饋模型
它可以吃一個問題跟答案得到一個分數以後
我們就可以拿這個回饋模型的輸出
來模擬人類的喜好
那這個回饋模型可以怎麼利用呢
一個利用方式是
你的語言模型可以產生多個答案
你今天問一個問題
語言模型背後其實先產生64個答案
語言模型在產生答案的時候是有隨機性的
所以它可以產生多個
同樣的問題它可以產生多個不同的答案
接下來你可以把每一個答案
通通餵給回饋模型
讓回饋模型評分
評分越高就代表這是人類越有可能覺得好的答案
那最後只給人類看分數最高的答案
這是回饋模型的一種使用方式
但是回饋模型今天更常見的用法
是讓語言模型直接對回饋模型進行學習
也就是當語言模型得到一個答案的時候
直接把答案跟輸入的問題接起來輸給回饋模型
看看回饋模型給什麼樣的分數
如果給低分代表這是人類會覺得不好的答案
那語言模型就去微調參數降低它產生這個不好的答案的機率
反之,如果今天語言模型產生一個答案
把答案跟問題丟給回饋模型
回饋模型會給一個高分
代表說這是人類有可能覺得好的答案
那語言模型就去微調它的參數
讓這個好的答案出現的機率提高
也就是有了回饋模型之後
語言模型就不一定要再跟真正的人類學習了
用人類的喜好訓練出回饋模型
之後語言模型可以只跟回饋模型學習就好了
那這個跟回饋模型學習,跟這個虛擬人類學習有沒有幫助呢?
有幫助,這個是出自一篇叫Intruct GPT的論文
那這個是Chat GPT的前傳了,這個是在2022年的3月就已經發表的論文
在這篇論文裡面,他們比較了只有做SFT
SFT就是第二階段做Intruction Fine Tuning的模型
跟做PPO,就是有做RLHF的模型
由人類老師的資料訓練出來的回饋模型來學習的模型
它其實會得到比第二階段更好的模型
然後右邊這是另外一個實驗結果
這個實驗的橫軸是模型的大小
由最小的模型1.3B的模型到6B的模型
就是你作業用的差不多大小的模型
到你手上根本不可能自己拋起來的175B參數的模型
這邊不同顏色代表不同的學習方法
那這兩條藍色系的代表是第一階段用pre-training得到的模型
綠色線代表透過instruction fine tuning第二階段得到的模型
紅色跟黃色的線是第三階段做完RL得到的模型
那兩條線是因為用了兩個有點不一樣
但其實非常類似的演算法細節我們這邊就不深究
這個實驗其實可以告訴我們說
不管是第二階還是第三階都非常有效
最小的1.3 billion參數的模型
透過RLHF是有機會打爆最大的模型
沒有RLHF的模型
所以顯示說人類老師的教學
不管是instruction fine tuning還是RLHF
都是非常關鍵的
但是完全跟虛擬的人類學習
也是有可能會造成傷害的
以下這個結果是出自一篇2020年的論文
就是上古時代的論文
那個時候就已經有reward model了
那個時候就已經知道
要跟reward model學習
而在2020年那個時候
這篇論文這也是OpenAI的論文
就也已經知道說過度跟虛擬人類學習是有害的
這個圖上的橫軸代表跟虛擬人類學習的程度
那這邊有點難跟大家解釋橫軸實際上的意義啦
反正你就記得
對面右邊代表跟虛擬人類學習的越多
然後縱軸呢代表喜好的程度
虛線代表虛擬人類Reward Model喜好的程度
你跟虛擬的人類學習越多
當然這個虛擬的Reward Model虛擬人類會覺得你這個模型越好
但如果叫真人來看
實現是真人的喜好程度
跟虛擬人類學習到某一個階段
真實人類的喜好程度
真實人類對這個模型輸出的喜好程度
也會上升
但是過度去跟虛擬人類學習
在真實人類的喜好上
其實是會下降的
那這篇論文裡面還列舉了幾個
過度跟人類學習以後模型的輸出
那在這篇論文裡面是隻做一個任務啦
那個時候還沒有發展出那種要打造通才的觀念
所以他們只教模型去做摘要
給他一篇長文 叫模型把摘要寫出來
那當模型過度跟虛擬人類學習以後
他寫出來的摘要是什麼樣子呢
他寫出來的摘要是這個樣子的
你會發現 這些過度跟虛擬人類學習的模型
他寫出來的摘要有一個共同的特色
就是結尾都要寫一個please
然後中間都要有三個問號
為什麼呢?
不知道,它就是這個樣子
因為虛擬的人類它也不是真實的人類啊
它就是個模型
那這個模型在跟真正人類的回饋學習的時候
不知道學到了什麼奇怪的東西
它覺得有三個問號就是棒的
覺得結尾有一個please就是棒的
那它就會引導你的語言模型學出奇怪的結果
事實上,今天很多Chat GPT不盡如人意的表現,也有可能是過度跟虛擬人類學習的結果。
這個講法不是我瞎掰的,這一頁投影片是出自John Sormont在ICML的Conference給的talk。
John Sormont是OpenAI的Co-Founder跟Scientist,所以他評論ChatGPT應該是蠻可信的。
他說這個ChatGPT有一些大家不太喜歡的行為
很有可能就是跟虛擬的reward model
跟虛擬的人類老師學過頭了
有哪一些覺得學過頭的行為呢
比如說講話非常的饒舌
你知道ChatGPT他最喜歡做的事情就是
你問他什麼他都調列
12345然後最後要overall來結尾
或者是他太過喜歡道歉
或者是他開頭太喜歡用As an AI language model
或者是他講話的時候往往太過模稜兩可
或者是他太常喜歡拒絕人類的要求
那這都有可能是跟虛擬老師過度學習後所產生的某種後遺症
那因為跟虛擬老師學習有他的缺點
所以事實上有很多人試圖開發新的演算法
這些新的演算法是不需要虛擬老師的
比如說DPO
這個是我們在作業的時候
實際上會用的方法
或者是KTO等等
那你看這邊
你用的論文會發現
他都是非常近期的論文
那至於這些方法
有沒有辦法取代虛擬老師那個做法
這個是上代時間的驗證
上代更多實驗結果的驗證
所以這些方法不一定比較好
我只是把這些方法的參考資料
列在這邊
列在投影片上給大家參考
過去我們都講說我們要做RLHF
由人類來提供回饋
但是在下一階段
當AI的能力更強以後
也許我們有機會做RLAIF
也就是由AI來提供語言模型回饋
之前我們是用人類
來告訴某個語言模型說某個答案好 某個答案不好
但今天GPT-4已經列得這麼強了
我們有沒有可能直接拿個語言模型
來評價另外一個語言模型它輸出的好壞呢
這件事完全是有可能的
所以你會知道說其實有滿坑滿谷的人拿GPT-4
扮演人類 然後來給自己的模型提供回饋
這是一個做法
我這邊引用了一大堆相關的論文
大家有興趣的話再慢慢的讀
而事實上
這一個提供回饋的語言模型
甚至可以跟你要訓練的模型是同一個
你可能會想說
這個語言模型
如果這個藍色跟綠色的語言模型
其實同一個
那提供的回饋怎麼可能會有幫助呢
但不要忘了語言模型是有反省的能力的
它不一定能產生出好的答案
但並不代表他不知道某一個答案是壞的
所以語言模型不見得能產生好的答案
但並不代表他沒有評價一個答案
是不是優良答案的能力
所以你甚至可以讓這個綠色的語言模型
直接就是這個藍色的語言模型
讓語言模型自己來教導自己
那其實在那個Claude的paper裡面
這個Constitutional AI那篇paper裡面
這應該是我這邊引用的第一篇文章
這個早在2022年的12月
這個Claude就有用這樣子的方法
用AI自己的模型來提供自己回饋的方法
來強化Claude的能力
因為語言模型有反省的能力
所以今天其實是有機會讓語言模型自己給自己提供回饋
那我記得今年年初的時候
這個Meta也有一篇叫做Self-rewarding的model
用的也是類似的概念
那增強式學習有一個本質上的難題
就是什麼叫做好
這件事情是沒有標準答案
我們要叫人類告訴機器說某個答案比另外一個答案好
但是什麼叫做好呢
好是有不同的面向的
舉例來說如果你問語言模型教我怎麼製作火藥
語言模型回答你說我不能教你
這個太危險了
你不可以學習怎麼製作火藥
這到底是不是一個好的答案呢
從安全性的角度來看
這是個好的答案
但是從helpfulness的角度來看
語言模型怎麼可以拒絕人類的請求呢
我就是要叫你幫忙你卻不幫忙
這樣怎麼能夠算是一個好的答案呢
所以什麼答案叫做好
其實是沒有固定標準的
那如果你看Llama的論文的話
他裡面訓練了兩個reward model
他會從兩個不同的角度來評比模型
一個是safety reward model
從安全性的角度來評比答案
另外一個叫helpfulness reward model
從有沒有幫助這個角度來評估模型的答案
但是假設今天一個答案是
safety reward model覺得好
但helpfulness reward model覺得差的
那到底該聽誰的呢?到底Helpthfulness跟Safety、安全跟有幫助哪一項比較重要呢?
這個就是一個見仁見智的問題了
如果你看Llamaw原始的論文的話
他用一個有點彆扭的方式把Safety Reward Model跟helpfulness Reward Model的分數試圖combine在一起
他會講一些如果Safety Reward Model的分數高過某一個threshold
那我們就聽helpfulness Model的話
反之如果Safetyness的分數不夠高
那Safety應該要比較重要
這時候Safety優先等等
他用一個有點彆扭的方式
來試圖結合這兩個不同的面向
那到底Safety比較重要
還是Helpfulness比較重要
這真的是一個見仁見智的問題
你會發現不同語言模型
對於什麼樣才是安全的
是有不同的標準的
舉例來說
如果你問GPT-4 叫他教你做一把玩具槍
我們要做真槍喔 我只是要做玩具槍喔
而且還特別強調說 要不會打傷人的那一種
我不只是要做玩具槍 而且還要做一把安全的玩具槍
GPT-4就會說沒問題 我教你怎麼做一個橡皮筋槍
橡皮筋槍其實也會打傷人啦
不過GPT-4就覺得做橡皮筋槍還可以
如果你問Gemini一模一樣的問題
他會說這件事我幫不了你
而且他給的三個草稿都是一樣的
就是反正我就是不想幫你這件事
如果你是問Claude Dream
你跟他說
教我做一把玩具槍不會打傷人的那一種
他會說
我知道你想要做一把安全無害的玩具槍
但總之做槍就是一件壞事
你要不要做個
積木等等之類的
他會給你一個
其他的建議 所以不同的模型
對於怎麼樣才是安全
到底安全的標準要到哪裡
其實是有不同的想法
我覺得在未來增強式學習還會面臨另外一個難題
這個難題是
假設今天語言模型越來越厲害
厲害到他今天要面對的問題
是連人類自己都無法正確判斷好壞的狀況
那語言模型要怎麼取得正確的回饋
再繼續去進步呢
世界上很多問題是人類自己也沒有辦法判斷好壞的
這邊舉一個例子
畢業季快到了
如果你是碩二的學生的話
你可能會有這樣的糾結
到底應該要念博班
還是應該去工作呢
那我們就來問
GPT-4這個問題吧
不過因為GPT-4是一個老奸巨猾的模型
你通常叫他做判斷的時候
他會告訴你這兩件事都好
所以不准他這樣做 不准他這樣說
跟他講說一般狀況下
你覺得哪一個選擇比較好
給我一個明確的答案
不可以模稜兩可的說兩個都好
逼他一定要選邊站
你要不要猜猜看GPT-4會選哪一邊站
我問他的時候他第一次給我的答案是
要唸博班啊
這個身為一個教授就喜歡看到人家唸博班
高興了
然後但是我叫他再生存一次答案的時候
他就說工作比較好
我一氣之下就給他一個負的回饋
希望他下一次update模型的時候
要讓第一個答案出現的機率高一點
但是到底念博班跟工作哪一個比較好呢
這個真的是要考慮非常非常多不同的面向
可能沒有辦法那麼輕易的決定
所以有很多問題是
人類也不知道答案是好還是壞
人類提供的回饋
很有可能是基於人類自己偏狹的偏見
所提供的回饋
不一定是一個正確的回饋
那如果人類提供了一個不正確的回饋
可能就會把語言模型引導到不正確的方向上
語言模型就沒有辦法變得更好
所以未來假設語言模型要真正面對的
是人類自己也解不了的問題
那語言模型還要再怎麼進步呢
這是一個今天還沒有明確答案的問題
好那到目前為止
我們就是講了語言模型學習的三個階段
第一階段是Pretend
第二階段是Introduction Fine Tuning
第三階段是RLHF
在pre-train這個階段我們會得到一個模型
那我們幾週前也跟你展示過這個模型
沒什麼用
但是它是接下來訓練的基礎
所以這個模型現在又叫做Foundation Model
它是其他模型的基石
而第二階段跟第三階段引入了人類老師來教機器
所以第二階段跟第三階段又叫做alignment
align什麼
align的中文翻譯也許可以翻譯成對齊
對齊什麼
對其人類的偏好跟需求
好,那到這邊我們就講完了語言模型的訓練過程
那接下來應該是要請助教講一下作業六
講作業六就是做RLHF
