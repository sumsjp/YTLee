臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
各位同學大家好，我們來上課吧！
好，那接下來呢，我們來講一下 Recurrent Neural Network
那 Recurrent Neural Network呢 它其實也可以做到我們在前一堂課裡面
講的 sequence labeling 的 task 那最後呢，我們再來說他們有
甚麼樣的不同
那我們這邊要舉的例子呢 是 Slot Filling，那我們知道說
現在很流行呢，做一些智慧客服之類的東西
譬如說，做一些智慧的訂票系統 那這種智慧客服或智慧訂票系統裡面呢
你往往需要 Slot Filling 這個技術 Slot Filling 指的是甚麼呢
Slot Filling 指的是說 比如說假設有一個人對你的訂票系統說
I would like to arrive Taipei on November 2nd 那你的系統要自動知道說
你的系統裡面有一些 slot 比如說，在這個
訂票系統裡面呢 它會有一個 slot叫做 Destination
它會有一個 slot 叫做 Time of Arrival 那你的系統要自動知道說呢
這邊的每一個詞彙，它屬於哪一個 slot
那你的系統要知道說 Taipei 屬於 Destination 這個 slot
那你的系統要知道說 November 2nd 屬於 Time of Arrival 這個 slot
那其他的詞彙呢，就不屬於任何 slot 裡面
那這個問題要怎麼解呢，其實這個問題
你當然也可以用一個 Feedforward 的 Neural Network 來解。也就是說，我疊一個
Feedforward 的 Neural Network
然後它的 input 就是一個詞彙
比如說，你把 Taipei 變成一個 vector 丟到這個 Neural Network裡面去
但是你要把一個詞彙丟到Neural Network 裡面去 你必須要先把它
用一個向量 vector 來表示
那怎麼把一個詞彙用一個向量來表示呢
方法實在太多了，最 naive的方法呢
就是 1-of-N encoding 我想這個呢，應該就不需要再細講
當然你用 word vector來表示一個詞彙呢
也是可以的，那或者是呢，有一些
Beyond 1-of-N encoding 的方法，比如說
有時候，如果你只是用 1-of-N encoding 來描述一個詞彙的話，你會遇到一些問題
因為有很多詞彙，你可能從來都沒有見過
所以你會需要在1-of-N encoding 裡面
多加一個 dimension，這個 dimension 代表
代表 other，然後所有的詞彙如果它不是在我們的
辭典裡有的詞彙，就歸類到 other 裡面去
比如說， Gandalf 不在我們的 vocabulary 裡面
它就歸類到 other
或者是
Sauron 不在這個 vocabulary 裡面，它就歸類到這個 vector
那你也可以用某一個詞彙的字母，來表示
它的 vector ，那如果你用某一個詞彙的字母的 n-gram 來表示那個 vector 的話呢
你就不會有某一個詞彙不在辭典中的問題 比如說，你有一個詞彙叫做 "apple"
那 apple 呢，它裡面有出現 "app"
有出現 "ppl", 有出現 "ple"
那在這個 vector 裡面，對應到 "app" 的 那個 dimension 就是 1
對應到 "ppl" 的 dimension 就是 1 那其他的都是 0，無論如何
假設我們可以把一個詞彙表示成一個 vector
那就可以把這個 vector 丟到一個 Feedforward 的 Network 裡面去，那在 slot filling 這個 task 裡面
你就會希望你的 output 是一個 probability distribution
這個 probability distribution 代表說我們現在 input 的這個詞彙
屬於 哪一個 slot 的機率，舉例來說
Taipei 屬於 destination 的機率， 還有 Taipei 屬於 time of departure 的機率，等等
但是呢，光只有這樣是不夠的 Feedforward Network 呢
沒有辦法 solve 這個 problem 為甚麼呢？假設現在，有一個使用者說
要 arrive Taipei on November 2nd
那 arrive 是other , Taipei 是 destination ，on 是 other
November 2nd 都是時間，但是如果另外一個
使用者說 leave Taipei on November 2nd 那 Taipei 這個時候
它應該是 place of departure 它應該是出發地而不是目的地
但是對 neural network 來說，input 一樣的東西
output 就是一樣的東西，而你 input 台北這個詞彙
output 要馬都是 destination 的機率最高
要馬就都是 place of departure
要馬就都是目的地的機率最高 要馬就都是出發地的機率最高
你沒有辦法讓它有時候出發地的機率最高 有時候目的地的機率高
那怎麼辦呢？這個時候呢，我們就希望
我們的neural network 它是有記憶力的
如果今天 neural network 是有記憶力的
它記得它在看過這個紅色的台北之前
它就已經看過 arrive 這個詞彙
它記得它在看過這個綠色的 Taipei 之前
它就已經看過 leave 這個詞彙 它就可以根據一段話的上下文
產生不同的 output 所以如果我們讓我們的 neural network
它是有記憶力的話，它就可以解決input 同樣的詞彙
但是 output 必須是不同的這個問題
好那這種有記憶力的 neural network 呢
就叫做 Recurrent Neural Network
它的縮寫，是 RNN 那在 Recurrent Neural Network 裡面呢
每一次我們的 hidden layer
每一次我們的 hidden layer 裡面的 neuron 產生 output 的時候
這個 output 都會被存到 memory 裡面去
這邊呢，用藍色的方塊來表示 memory
當這些 hidden layer 裡面的 neuron 有 output 的時候
它就會被存到這個藍色的方塊裡面去，那下一次
如果當有 input 的時候，這個 hidden layer
這些 neuron 它不是只會考慮
input 的這個 x1 跟 x2，它還會考慮
存在這些 memory 裡面的值。
對它來說，除了 x1 跟 x2 以外
這些存在 memory 裡面的值呢
a1、a2 也會影響它的 output
那我想直接舉個例子，大家可能會比較清楚
假設我們現在圖上的這個 network 它所有的 weight 都是 1
然後所有的 neuron 都沒有任何的 bias 值
然後假設所有的 activation function 都是 linear
這樣可以不要讓計算太複雜
那現在假設我們的 input 是一個 sequence
我們的 input 是 [11] [11] [22]
那我們把[11] [11] [22] 這個 sequence
input 到這個 Recurrent Neural Network 裡面去
會發生甚麼事呢？
那首先呢，在你開始要
使用這個 Recurrent Neural Network的時候呢
你必須要給 memory 起始值
你必須要給 memory
在還沒有放進任何東西的時候， 你必須要給它一個初始值，比如說這邊呢
我們在假設還沒有放進任何東西之前 memory 裡面的值是 0
那現在輸入第一個輸入 [1 1]
那接下來會發生甚麼事呢
對這個 neuron 來說
它除了接到 input 的 [1 1] 以外
它還接到 memory 的 [0 0]， 那因為我們說所有的 weight 都是 1
所以它的 output 就是 2， 那這個 neuron 的 output 也一樣是 2
那接下來呢
接下來呢，因為所有的 weight 都是 1
所以紅色這兩個 neuron 呢，它們的 output 就是 4
所以 input [1 1] 的時候，它的 output 就是 [4 4]
接下來呢，Recurrent Neural Network 會把這些
綠色的 neuron 他的 output 存到 memory 裡面去
所以呢
memory 裡面的值就被 update 成 2 這個2呢，會被寫進來；這個2，會被寫進來
所以 memory 裡面的值就 update 變成 2，接下來呢
再輸入 1 跟 1，這時候綠色的 neuron會有什麼樣的輸出呢？
它的輸入有 4 個
[1 1] 跟 [2 2]，然後 weight 都是 1， 所以你把 2 + 2 + 1 + 1
得到的結果呢，是 6
那最後呢
紅色 neuron 的輸出就是 6 + 6 是 12 所以當輸入 [1 1] 的時候
第二次再輸 [1 1] 的時候，輸出就是 [12 12]
所以對 Recurrent Neural Network 來說
你就算是輸入一樣的東西，你就算給它一模一樣的 input
在這個 case 裡面都是 1 跟 1
你就算給它一模一樣的 input 在這個 case 裡面都是 1 跟 1， 它的 output 是有可能會不一樣的
因為存在 memory 裡面的值呢，是不一樣的
那原來的值呢，在這個綠色的 neuron 的 output 是 6 跟 6
那接下來 6 跟 6 呢，就會被存到 memory 裡面去， 就會被存到 memory 裡面去
所以 2 就被洗掉，變成 6
那接下來呢，我們的 input 是
2 跟 2，假設 input 是 [2 2]，這邊每一個綠色的 neuron
它考慮的 4 個 input ：2 跟 2 跟 6 跟 6
所以 6 + 6 + 2 + 2 是多少呢，得到的值是 16
那紅色 neuron 的 output 就是 32
所以 input 2 跟 2 的時候呢，output 是 32
那今天在做 Recurrent neural network 的時候呢 有一件很重要的事情就是
這個 input 的 sequence
Recurrent neural network 在 考慮它的時候，並不是 independent
今天如果你任意調換 input sequence 的順序 比如說把 2 跟 2 挪到最前面來
那 output 呢，是會完全不一樣的
所以在 Recurrent neural network 裡面，
它會考慮 input 這個 sequence 的 order
所以今天如果我們要用 Recurrent neural network 來處理 slot filling 這個問題的話
它看起來就像是這樣，有一個使用者說 arrive Taipei on November 2nd
那 arrive 就變成一個 vector，丟到 neural network裡面去
neural network 的 hidden layer 它的 output 這邊寫成 a1
這個 a1 是一排 neuron 的 output，所以它其實是一個 vector
然後根據這個 a1，我們產生 y1
這個 y1 就是 arrive 屬於哪一個 slot 的機率
接下來呢，a1 會被存到 memory 裡面去
接下來呢，Taipei 會變成 input
那這個 hidden layer 會同時考慮 Taipei 這個 input
跟存在 memory 裡面的 a1，得到 a2
然後再根據 a2 產生 y2 y2 是 Taipei 屬於哪一個 slot 的機率
這個 process 呢，就以此類推
我們再把 a2 存到 memory 裡面
再把 on 丟進去
那 hidden layer 同時考慮 input "on" 這個詞彙的 vector
跟存在 memory 裡面的 a2，得到 a3，然後a3 再得到 y3
它代表 on 屬於哪一個 slot 的機率， 那這邊要注意的事情是
有人看到這個圖就說，這邊有3個 network
這個不是3個 network，這個是同一個 network
在三個不同的時間點，被使用了3次
我這邊呢，特別把同樣的 weight，用同樣的顏色來表示
同樣的 weight，就用同樣的顏色來表示， 希望大家看得出來
那，所以如果我們有了 memory 以後
剛才我們講的輸入同一個詞彙， 我們希望它 output 不同的這個問題
就有可能被解決，比如說， 如果同樣是輸入 Taipei 這個詞彙
但是因為紅色 Taipei 前面接的是 leave， 綠色 Taipei 前面接的是 arrive
因為 leave 跟 arrive 它們的 vector 不一樣
所以 hidden layer 的 output 也會不同
所以存在 memory 裡面的值呢，也會不同
雖然現在 x2 是一模一樣的 但是因為存在 memory 裡面的值不同
所以 hidden layer 的 output 也會不一樣 所以最後的 output 也就會不一樣
好那這個是 Recurrent Neural Network 的基本觀念
當然 Recurrent Neural Network 的架構 你是可以任意設計的
比如說，它當然可以是 deep，我們剛才看到 Recurrent Neural Network 它只有一個 hidden layer
當然它可以是 deep 的 Recurrent Neural Network
比如說，我們把 x1 丟進去以後
它可以通過一個 hidden layer，在通過第二個 hidden layer
以此類推，通過很多個 hidden layer 以後， 才得到最後的 output
那每一個 hidden layer 的 output 都會被存在 memory 裡面
在下一個時間點的時候呢，每一個 hidden layer
會再把前一個時間點存的值，再讀出來
再把前一個時間點存的值呢，再讀出來，最後得到
最後的 output，這個 process 就一直持續下去
這個 deep 你要疊幾層呢
都是可以的
那 Recurrent Neural Network 有不同的變形 我們剛才講的呢
叫做 Elman Network， 如果我們今天是把 hidden layer的值
存起來，在下一個時間點再讀出來
那麼這個叫做 Elman Network，那有另外一種呢
叫做 Jordan Network
Jordan Network 它存的是整個 network 的 output 的值
那它再把 output 的值，在下一個時間點
再讀進來，它是把 output 的值存在 memory 裡面
那傳說呢，Jordan Network 可以得到比較好的 performance
因為
這邊的 hidden layer，它是沒有 target 的
所以有點難控制說 它學到什麼樣的 hidden 的 information
它學到把甚麼東西放到 memory 裡面
但是這個 y ，它是有 target 的 所以我們今天可以比較清楚我們
放在 memory 裡面的，是甚麼樣的東西
這個 Recurrent Neural Network ， 它還可以是雙向的
甚麼意思呢
我們剛才看到 Recurrent Neural Network
你 input 一個句子的話，它就是從句首一直
讀到句尾，假設句子裡面的每一個詞彙 我們都用 x^t 來表示它的話
它就是先讀 x^t，再讀 x^(t+1)，再讀 x^(t+2)
但是，其實它的讀取方向也可以是反過來
它可以先讀 x^(t+2)，再讀 x^(t+1)
再讀 x^t
你可以同時 train 一個正向的 Recurrent Neural Network
又同時 train 一個逆向的 Recurrent Neural Network
然後把這兩個 Recurrent Neural Network
的 hidden layer 拿出來
把這兩個 Recurrent Neural Network 的 hidden layer 拿出來，都接給一個 output layer
得到最後的 y
所以你把正向的 network，在 input x^t 的時候的 output
跟逆向的 network，在 input x^t 的時候的 output
都丟給 output layer
讓 output layer 產生 y^t 然後產生 y^(t+1) 完，產生 y^(t+2)
那用 Bidirectional RNN 的好處呢
你的 network 呢，它在產生 output 的時候，它看的範圍
是比較廣的，如果今天你只有正向的 network
在產生 y^t , 跟 y^(t+1) 的時候
你的 network 只看過 x1 一直到 x^(t+1) 的部分
但是如果我們今天是 Bidirectional 的 RNN
在產生 y^(t+1) 的時候，你的 network 不只是看了
x1 到 x^(t+1) 所有的 input，它也看了從句尾
一直到 x^(t+1) 的 input
那你的 network 等於是看了整個 input 的 sequence
假設你今天考慮的是 slot filling 的話
你的 network 等於是看了整個 sentence 以後， 才決定每一個詞彙的 slot 應該是甚麼
那這樣當然會比只看句子的一半， 有更好的 performance
那我們剛才講的 Recurrent Neural Network，其實只是
Recurrent Neural Network 一個最 simple 的版本
那其實只是一個最 simple 的版本
那我們剛才講的 memory 呢，是最單純的
就是我們隨時都可以把值存到 memory 裡面去
也可以隨時把值從 memory 裡面讀出來
但現在比較常用的 memory 呢
稱之為 Long Short-term 的 memory
這種 Long Short-term 的 memory 呢 它的簡寫是 LSTM
這種 Long Short-term 的 memory 呢 它是比較複雜的
這個 Long Short-term 的 memory 呢 它有 3 個 gate
當外界，當 neural network 的其他部分
某個 neuron 的 output 想要被寫到 memory cell 裡面的時候
它必須先通過一個閘門，通過一個 input gate
那這個 input gate 呢，它要被打開的時候
你才能夠把值寫到 memory cell 裡面去
如果它被關起來的時候，
其他 neuron 就沒有辦法把值寫進去
那至於這個 input gate 它是打開還是關起來
這個是 neural network 自己學
所以它可以自己學說甚麼時候要把 input gate 打開
甚麼時候要把 input gate 關起來
那輸出的地方呢
輸出的地方也有一個 output gate
那這個 output gate 會決定說
外界、其他的 neuron 可不可以從 memory 裡面把值讀出來
那當 output gate 關閉的時候，就沒有辦法把值讀出來
只有 output gate 被打開的時候，才可以把值讀出來
那跟 input gate 一樣，output gate 甚麼時候是打開
甚麼時候是關起來，network 也是自己學到
那還有第三個 gate 呢，叫做 forget gate
那 forget gate 是決定說，甚麼時候 memory
要把過去記得的東西忘掉
或是它甚麼時候要把過去記得的東西做一下 format
把它 format 掉
那這個 forget gate 甚麼時候 會把存在 memory 裡面的值 format 掉
甚麼時候會把存在 memory 裡面的值繼續保留下來
這個也是 network 自己學到的
那整個 LSTM 呢，可以看成它有4個 input
1 個 output，這 4 個input 是甚麼呢，一個是
想要被存到 memory cell 裡面的值
然後它不一定存得進去，這 depend on input gate 要不要讓這個 information 過去
跟操控 input gate 的這個訊號
操控 output gate 的這個訊號 和操控 forget gate 的訊號
所以一個 LSTM 的 cell 呢
它有 4 個 input，那有這 4 個 input 但它只會得到一個 output
那這邊有一個小小的冷知識
就是這個 dash，你覺得它應該被放在哪裡
我投影片把它放在這邊，但並不代表我投影片就是對的
我有可能只是突然發現我寫錯了想要改一下這樣子
好你覺得這個 dash
應該放在 long 和 short 之間的舉手一下
沒有
那你覺得它應該放在 short 跟 term 之間的同學舉手一下
好，手放下，沒錯它應該放在 short 跟 term 之間
有時候我會看到有人放在 long 和 short 之間
那其實這是比較不 make sense 的 應該放在 short 跟 term 之間
因為它其實呢，還是一個 short-term 的 memory
它只是比較長的 short-term memory
所以按照這個字面意思， 它是比較長的 short-term memory
因為之前我們看那個 Recurrent neural network 阿
它的 memory 在每一個時間點
都會被洗掉
只要每次有新的 input 進來，在每一個時間點呢
Recurrent neural network 都會把 memory 洗掉 所以這個 short-term 是非常 short
它只記得前一個時間點的事情
但是如果是 long short-term 的話，它可以記得
比較長一點，只要 forget gate不要決定要 format 的話
它的值就會被存起來
好那這個 memory 的 cell 呢，如果
更仔細地來看它的 formulation的話，它長得像這樣
這個是外界的 input， 外界要存到 cell 裡面的 input，這個是 input gate
這個是 forget gate, 這個是 output gate
那我們假設呢，現在讓要被存到 cell 裡面的 input 叫做 z
操控 input gate 的 signal 叫做 zi
好這個所謂操控 input gate 的 signal， 它也就是一個 scalar
也是一個數值
那我們等一下會講說這個數值它是從哪裡來的
反正這邊就是有一個數值
被當作這個 cell 的 input，好那這個 forget gate
有一個操控它的數值是 zf， output gate 有一個操控它的數值是 zo
綜合這些東西以後，最後會得到一個 output
這邊寫作 a
好，假設我們現在 cell 裡面呢
在輸入這些，在有這 4 個輸入之前
它裡面已經存了值 c
那現在呢，假設要輸入
輸入的部分呢，輸入z
那三個 gate 呢，分別是由zi, zo, zf 所操控的
那 output a 會長甚麼樣子呢？
我們把 z 呢，通過一個 activation function
得到 g(z)，然後把 zi 通過
另外一個 activation function，得到 f(zi)
那這邊呢
這3個 zi, zf, zo 他們通過的這3個 activation function f 阿
通常我們會選擇 sigmoid function 那選擇 sigmoid function 它的意義就是
sigmoid 的值是介在 0~1 之間的，而這個 0~1 之間的值
代表了這個 gate 被打開的程度
如果這個 f 的 output
這個 activation function 的 output 是 1 代表這個 gate 是處於被打開的狀態
反之呢，代表這個 gate 是被關起來的
接下來呢，我們就把 g(z) 乘上這個 input gate 的值
f(zi) 得到 g(z)*f(zi)
那這個 forget gate 的 zf 呢
zf 這個 signal 也通過這個 sigmoid activation function
得到 f(zf)，接下來呢
我們把存在 memory 裡面的值
c 乘上 f(zf)
得到 c*f(zf)，然後接下來
把 c*f(zf) 加上 g(z)*f(zi)
把這兩項加起來，得到 c'
c' 就是新的存在 memory 裡面的值
新的存在 memory 裡面的值就是 c'，所以
根據到目前為止的運算可以發現說呢 這個 f(zi) 就是 control 這個 g(z)
可不可以輸入的一個關卡，因為假設 f(zi) = 0
那 g(z) * f(zi) 就等於 0，就好像是沒有輸入一樣
若 f(zi) 是等於 1，那就等於是直接把 g(z) 當作輸入
那這個 f(zf)，就是決定說我們要不要
把存在 memory 裡面的值洗掉，假設 f(zf) 是 1
假設 f(zf) 是 1
也就是 forget gate 是被開啟的時候
forget gate 被開啟的時候呢，這個時候
c 會直接通過，就等於是
把之前存的值，還是記得， 那如果是 f(zf) = 0
也就是 forget gate 被關閉的時候， 0 乘上 c 過去存在 memory 裡面的值呢，就會變成 0
然後把這兩個值呢，加起來當作
我們把這兩個值加起來，寫到 memory 裡面得到 c'
那 forget gate 它的開關呢，跟我們直覺的想法
是相反的，這個 forget gate 它打開的時候代表
是記得，它被關閉的時候，代表的是遺忘
所以我覺得它的名字取的有點怪， 或許不該叫它 forget gate
不過反正，習慣上呢，就是這麼做
好那把這個 c' 通過 h，得到 h(c')
然後接下來呢，這邊有一個 output gate
這個 output gate 受 zo 所操控
zo 通過 f 得到 f(zo)
f(zo) 如果是 1 的話，那這邊我們會把 f(zo)
跟這個 h(c') 乘起來，如果 f(zo)是 1，就等於是
h(c') 可以通過這個 output gate，如果 f(zo) 是 0
就等於這個 output 會變成0 就代表存在 memory 的值呢
沒有辦法通過 output gate，被讀取出來
也許這樣你還是沒有很清楚， 所以後面呢，我打算做一個韌體 LSTM
我從來沒有在其他地方看過韌體 LSTM
所以你可以想到我這個投影片是做很久
那我們先講一下我們要舉的例子
等一下我們要舉的例子是這樣子
在 network 裡面，只有一個 LSTM 的 cell
那我們的 input 都是三維的 vector
output 都是一維的 vector
那這個三維的 vector，它跟 output 還有 memory 裡面的值的關係是甚麼呢
這個關係是這樣子
假設
第二個 dimension x2 的值是 1 的時候
x1 的值就會被寫到 memory 裡面去
那x2 是1的時候，x1的值就會被存到 memory 裡面去
假設 x2 的值是 -1 的時候
memory 就會被 reset
memory 存的值就會被遺忘
假設 x3 等於 1 的時候
你才會把 output gate 打開，才能夠看到輸出
所以呢
假設我們原來存在 memory 裡面的值是 0
那當這邊是1的時候
當 x2 = 1 的時候，3會被存到 memory 裡面去
這裡得到的值呢，就變成 3
那這邊又出現一次 1
所以 4 會被存到 memory 裡面去，所以就得到 7
x3 = 1，所以 7 會被輸出
所以得到 7，那這邊是 -1，如果是 -1 的話呢
就會把 memory 裡面的值洗掉 所以看到 -1，下一個時間點的值就變成 0
然後看到 1 就會把 6 存進去，所以得到的值是 6
這邊 1呢是輸出，所以得到的值是 6
那我們就來實際做一下運算
那這個是一個 memory cell，一個 LSTM 的 memory cell
那我們知道 LSTM 的 memory cell 呢，總共有 4 個 input
這 4 個 input 都是 scalar
這 4 個 input 的 scalar 是怎麼來的呢
這 4 個 scalar是我們 input 的三維的 vector 乘上一個
linear 的 transform 以後，所得到的結果
你就把這 3 個 vector 乘上這 3 個值
再加上 bias，就得到這邊的 input，這三個值
再乘上三個 weight，再加上 bias，就得到它的 input
以此類推
那這些值，就是 input 的 x1, x2, x3
要乘上哪些值，還有 bias 的值應該要是多少這件事情呢
是透過 training data，透過 gradient descent 去學到的
那我們今天只是假設說，我已經知道這些值是多少
然後我現在用這樣的輸入，它會得到怎麼樣的輸出
那我們就來實際地運算一下， 不過在實際運算之前
我們先根據它的 input，根據這些參數呢
來分析一下我們可能會得到的結果， 那你看，在這個地方
x1 乘 1，其他都是乘 0，所以呢
這邊呢，就是直接把 x1 當作輸入 好，那我們看 input gate 的地方
它是 x2 * 100
bias 是 -10，也就是說呢，假設 x2 沒有值的時候
因為 bias 是 -10，所以通常input gate 呢，是被關閉的
如果 bias 是 -10的話，那通過 activation function 以後呢
通過 sigmoid 的 activation function 之後 它的值會接近 0
所以呢，代表它是被關閉的
那只有在 x2 有值的時候，如果 x2 有值 它就會比 bias 的這個 -10 還要大
如果 x2 代 1的話呢，它就會比 bias大 這個時候呢，input 就會是很大的正值
代表 input gate 被打開
那 forget gate 呢
forget gate 平常都是被打開的，你會發現說，因為它 bias 是 10
所以平常呢，它都是被打開的 所以平常都會一直記得東西，只有在 x2
給它一個很大的負值的時候，它會壓過這個 bias 才會把 forget gate 關起來
那 output gate 呢
output gate 平常也都是被關閉的， 因為它的 bias 是很大的負值
但是如果今天 x3 有一個很大的正值的話
它就可以壓過 bias，把 output gate 打開
所以我們就實際地來 input 一下看看
我們假設 g 跟 h 都是 linear 的，這樣計算比較方便
假設存在 memory 裡面的初始值是 0
好那我們現在 input 第一個 vector [3 1 0]
那 input [3 1 0] 會發生甚麼事呢， 3 乘上 1，所以這邊進來的值是 3
然後 1 乘 100 減 100，所以這邊的 input gate 約等於 1
所以它是被打開的，那 forget gate 呢
1*3，通過 input gate 以後得到的值是 3
那 forget gate 呢，input 是 [3 1 0]，forget gate 呢
是被打開的，所以 forget gate 是被打開的
把 0 乘上 1 加上 3，所以 forget gate 是被打開的 不過裡面本來就沒有存值
也沒有甚麼影響，0 * 1 + 3 所以存在 memory 裡面的值變成 3
然後接下來呢，看 output gate，[3 1 0]
output gate 還是被關起來的，3無法通過，所以輸出就是 0
好，接下來呢 input
input [4 1 0]，這個 input 的地方還是 4
然後這個 [4 1 0] 會把 input gate 打開
forget gate 也會被打開
forget gate 被打開的關係，所以 3 * 1 + 4 所以 memory 裡面存的值會變成 7
那 output 仍然是被關閉的
所以 7 呢，仍然無法被輸出， 所以整個 memory 的輸出仍然是 0
那接下來呢，input [2 0 0]，會發生甚麼事呢？
input [2 0 0 ]，所以現在 input 變成 2
這個 input gate 會怎樣呢，input gate 現在是 [2 0 0]
所以它 activation function 的 input 是 -10
所以 output 是趨近於0，0 * 2 = 0
等於 input 的 2 被 input gate 擋住了，那 forget gate 呢
[2 0 0] 得到的 forget gate，得到 activation function 的 input 是 10
所以 forget gate 還是打開的，所以 7 * 1 + 0
原來存在 memory 裡面的值是不動的，還是 7
那這個 7 它沒有辦法被輸出，因為 output gate 仍然是關閉的，所以 output 仍然是 0
好接下來呢，input 是 [1 0 1]
那 input [1 0 1]會發生甚麼事呢？這邊 input 仍然是 1
那這個 input gate 是被關閉的，那 forget gate 呢
forget gate 這個時候仍然跟原來一樣，它是被打開的
所以 memory 裡面存的值是不變的
那 output gate 呢，當你 input [1 0 1] 的時候
你會打開 output gate，這時候 activation function 的 input 變成 90
通過 activation function 以後呢，得到 1
那 1 * 7 = 7 這樣子
所以 output 的地方會變成是有值的， memory 裡面的值呢
存在 memory 裡面的值 7 呢，會被讀取出來
最後，讓我們試一下 [3 -1 0]
[3 -1 0] 這個 3 呢，就被讀進來
input gate 會被關起來，那 forget gate 呢？
因為這個值是 -1，所以 forget gate 的 activation function input 是 -90
activation output 就是 0
所以呢， memory 裡面存的值會被洗掉
memory 裡面存的值會乘上 forget gate 的 output， 會被洗掉變成 0
那 output gate 呢，這時候仍然是關起來的，不過它有開沒開也沒差，因為反正現在存在 memory 裡面的值變成 0
那它讀出來的值也是 0
好那你看到這邊你可能會有一個問題，這個東西
跟我們原來看到的 neural network 感覺很不像阿
它跟原來的 neural network 到底有甚麼樣的關係呢， 你可以這樣想
在我們原來的 neural network 裡面會有很多的 neuron
我們會把 input 乘上很多不同的 weight
然後當作是不同 neuron 的輸入
然後每一個 neuron 它都是一個 function
它輸入一個 scalar，output 另外一個 scalar
但是如果是 LSTM 的話呢
你其實只要把那個 LSTM 的那個 memory cell 想成是一個 neuron 就好
所以如果我們今天要用一個 LSTM 的 network
你做的事情只是把原來一個簡單的 neuron
換成一個 LSTM 的 cell
而現在的 input x1, x2 它會乘上不同的 weight
當作 LSTM 的不同的輸入
也就是說 x1, x2 乘上某一組 weight，變成
假設我們現在這個 hidden layer 只有兩個 neuron
也就是只有兩個 LSTM，但實際上你不會只有兩個 neuron
你可能會有比如說 1000 個 neuron， 1000 個 LSTM 的 memory cell
現在假設只有兩個neuron，那 x1, x2乘上某一組 weight
會去操控第一個 LSTM 的 output gate，乘上另外一組 weight，操控第一個 LSTM 的input gate
乘上一組 weight，當作第一個 LSTM的input，乘上另外一組 weight，當作另外一個 LSTM的forget gate的 input
第二個 LSTM 也是一樣
x1, x2 乘上某一組 weight 操控它的 output，它會操控它的 input
操控它的 output gate，操控它的 input gate， 操控它的 input，操控它的 forget gate 等等
所以我們剛才講過說 LSTM 它就是有 4 個 input ，1 個 output
而對一個 LSTM 來說
它的這 4 個 input 是不一樣的，這 4 個 input 都是不一樣的
這 4 個 input 都是不一樣的，在原來的 neural network 裡面
一個 neuron 就是一個 input，一個 output， 在 LSTM 裡面它需要 4 個 input
它才能夠產生一個 output
就好像說有的機器呢，他只要插一個電源線它就可以跑，那像 LSTM 呢，它就要插 4 個電源線才能跑
那所以 LSTM 因為它需要 4 個 input
而這 4 個 input 都是不一樣的
所以 LSTM 它需要的參數量，假設你現在用的 neuron 的數目
假設 LSTM 的 network 跟原來的
neuron 的 network，他們的 neuron數目是一樣的時候
LSTM 需要的參數量會是一般的 neural network 的 4 倍
那從這個圖上，你可以很明顯地
看出來，一般的 neural network 只需要這個部分的參數，只需要這個部分的參數
但 LSTM 還要操控另外三個 gate，所以他需要 4 倍的參數
不過這樣講你可能還是沒有辦法很了解，你沒有辦法體會的可能是跟
Recurrent Neural Network 的關係是甚麼
這個好像看起來不太像 Recurrent Neural Network
所以呢，我們要畫另外一個圖呢來表示它， 你可以想像這個圖呢
也是要畫非常久，假設我們現在有一整排的
neuron，假設有一整排的 LSTM
那這一整排的 LSTM 裡面， 它們每一個人的 memory 裡面
都存了一個值，每一個 LSTM 的 cell
它裡面都存了一個 scalar
把所有的 scalar 接起來，把這些 scalar 接起來
它就變成一個 vector，這邊寫成 c^(t-1)
那你可以想乘這邊每一個 memory 它裡面存的 scalar
就是代表這個 vector 裡面的一個 dimension
現在，在時間點 t
input 一個 vector, x^t
這個 vector，它會先乘上一個 linear 的 transform
乘上一個 matrix，變成另外一個 vector z
你把 x^t 乘上一個 matrix 變成 z
那這個 z，也是一個 vector， 那 z 這個 vector 代表甚麼呢
z 這個 vector 的每一個 dimension 呢
z 這個 vector 的每一個 dimension 呢，就代表了操控
每一個 LSTM 的 input
所以 z 它的 dimension 就正好是 LSTM 的
memory cell 的數目，那正好就是它的數目
那這個 z 的第一維就丟給第一個 cell， 第二維就丟給第二個 cell，以此類推
希望大家知道我的意思
好那這個 x^t 會再乘上另外一個 transform
得到 z^i
然後這個 z^i 呢，它的 dimension 也跟 cell 的數目一樣
z^i 的每一個 dimension
都會去操控一個 memory，所以 z^i 的第一維就是
去操控第一個 cell 的 input gate，第二維 就是操控第二個 cell 的 input gate，最後一維
就是操控最後一個 cell 的 input gate， 那 forget gate 呢？
跟 output gate 也是一樣，這邊就不再贅述
把 x^t 乘上一個 transform，得到 z^f z^f 會去操控每一個 forget gate
然後 x^t 乘上另外一個 transform，得到 z^o z^o 會去操控每一個 cell 的 output gate
好，所以我們把 x^t 乘上 4 個不同的 transform
得到 4 個不同的 vector，這 4 個 vector 的 dimension
都跟 cell 的數目是一樣的，那這 4 個 vector 合起來
就會去操控
這些 memory cell 的運作
好那我們知道一個 memory cell 就是長這樣
那現在 input 分別是 z, z^i, z^f, z^o, 那注意一下就是這 4 個
這 4 個 z 其實都是 vector
丟到 cell 裡面的值呢， 其實只是每一個 vector 的一個 dimension
那因為每一個 cell 他們 input 的 dimension 都是不一樣的
所以他們 input 的值都會是不一樣的，但是
所有的 cell 是可以共同一起被運算的 怎麼一起共同被運算呢
我們說 z 要乘上 z^i
要把 z^i 先通過 activation function， 然後把它跟 z 相乘
所以我們就把 z^i 先通過 activation function，跟 z 相乘
這個乘呢，是這個 element-wise 的 product 的意思
element-wise 的相乘，好那這個 z^f 也要通過
forget gate 的 activation function，z^f 通過這個 activation function
它跟之前已經存在 cell 裡面的值呢，相乘這件事情
它跟原來存在 memory cell 裡面的值相乘，它把它跟它相乘
然後接下來呢，也要把這兩個值加起來
你就是把 z^i 跟 z 相乘的值加上 z^f
跟 c^(t-1) 相乘的值，把他們加起來
好那 output gate 呢， z^o 通過 activation function，然後呢
把這個 output 跟相加以後的結果呢，
再相乘，最後就得到最後的 output 的 y
這個時候相加以後的結果，就是 memory 裡面存的值
相加以後的結果，也就是 memory 裡面存的值 也就是 c^t
那這 process 呢，就反覆地繼續下去
在下一個時間點，input x^(t+1)
然後呢，你把 z 跟 input gate 相乘
你把 forget gate 跟存在 memory 裡面的值相乘
然後再把這個值跟這個值加起來， 再乘上 output gate 的值
然後得到下一個時間點的輸出這樣子
那你可能覺得說這已經很複雜了，如果你自己 做投影片的話，顯然是要做非常久
這個不是 LSTM 的最終型態
這個只是一個 simplified 的 version
真正的 LSTM 會怎麼做呢
它會把這個地方
這個地方的輸出呢，把它接進來
它會把這個 hidden layer 的輸出把它接進來 當作下一個時間點的 input
也就是說，下一個時間點操控這些 gate 的值，不是只看
那個時間點的 input x，也看前一個時間點的 output h
然後其實還不只這樣， 還會加一個東西呢，叫做 "peephole"
這個 peephole 是甚麼呢？ 這個 peephole 就是把存在 memory cell 裡面的值呢
也拉過來，所以在操縱 LSTM 的 4個 gate 的時候
你是同時考慮了 x, 同時考慮了 h, 同時考慮了 c
你把這 3 個 vector 並在一起，乘上4個 不同的 transform，得到這4個不同的 vector
再去操控 LSTM
那 LSTM 通常不會只有一層
現在胡亂都要疊個五、六層才爽這樣
所以他就長的大概是這個樣子
然後每一個第一次看到這個東西的人啊
他的反應都是這樣子
就我記得，大家知道 sequence 有 sequence model 嗎
Google Brain 的 proposed，然後我有聽過他的這個 talk
他說他第一次看到 LSTM 的時候， 他的想法呢，就跟這個圖上是一樣的
這個太複雜了，這應該不 work 吧，我認識的每一個人第一次看到 LSTM 都覺得說
這個應該不 work 這樣， 但是他其實現在還 quite standard 這樣
當有一個人告訴你說，當有一個人說
我用 RNN 做了甚麼事情的時候， 就是你不要去問他說為甚麼你不用 LSTM
因為他其實就是用 LSTM ， 因為現在當你說你在做 RNN 的時候
其實你指的就是用 LSTM
所以呢，他其實是比較 standard 的
其實 Keras 裡面有支援 LSTM 啦，所以就算是
剛才講得這麼複雜的東西你沒聽懂就算了 在 Keras 裡面就是打 LSTM 這4個字母
然後就結束了
那 Keras 它其實支援三種 Recurrent neural networks，一個是 LSTM
一個是 GRU
GRU 是 LSTM 的一個稍微簡化的版本
它只有兩個 gate
據說少了一個 gate，但是 performance 跟 LSTM 差不多，而且少了 1/3 的參數
所以比較不容易 over-fitting
如果你要用一般的、我們這堂課最一開始講的那種 最簡單的 RNN 的話
也要說是 SimpleRNN 才行這樣子
那我想我們今天就上到這邊好了，那我們就下課
謝謝
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
