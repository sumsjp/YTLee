好 我們來講一下那個下一個主題
我們來講那個 Adversarial 的 Attack
Adversarial Attack 要講這個主題
要講的是什麼呢
我們這邊要講的是
我們在作業裡面
我們已經訓練了非常多
各式各樣的類神經網路
那我們當然期待說
我們可以把這些技術用在真正的應用上
但是光是把這些 Network 用在真正的應用上
要把這些 Network 用在真正應用上
光是它們正確率高是不夠的
它們還需要什麼能力呢
他們需要能夠應付來自人類的惡意
什麼叫做來自人類的惡意呢
有時候你的 Network
它的工作是為了要偵測一些有惡意的行為
如果今天它的工作是要偵測有惡意的行為
這些它要偵測的對象
會去想辦法騙過 Network
Network 在一般的情況下
都可以得到高的正確率是不夠的
它要在有人試圖想要欺騙它的情況下
也得到高的正確率
舉例來說 我們今天都會用 Network
來做 E-Mail 的 Filtering
你會用 Network 來做偵測一封郵件
它是不是垃圾郵件
那今天對於一個垃圾郵件的發信者而言
他也會想盡辦法避免
他的郵件被分類為垃圾郵件
如果今天有人他想辦法去更改郵件的內容
想要去欺騙過 Network 的話
那 Network 到底能不能不要被欺騙呢
所以今天我們希望我們的類神經網路
它光是正確率高還不夠
我們希望它可以應付來自人類的惡意
舉例來說 這個是蟻王 知道嗎
牠叫做梅路艾姆
牠是站在生物頂點
牠是生物頂點的一個存在
牠非常地強
人類沒有辦法打贏牠
那牠會消滅掉所有的人類
但是人類並沒有跟牠打
人類是不講武德的
人類就直接放一個核彈就把牠炸死
然後故事就結束了 就這樣
好 所以這告訴我們什麼
這告訴我們說
Network 光是正確率高是不夠的
它必須要能夠應付來自人類的惡意
好 接下來我們就要講人類的惡意
可能是什麼樣子
我們先來看一個真正的例子
我們今天在好多個作業裡面
我們都已經訓練了影像辨識的模型
也就是說你有一個影像辨識的系統
給它一張照片
它可以告訴我們說
這張照片屬於什麼樣的類別
那今天我們要做的事情是
在這張照片上面加入一個非常小的雜訊
一張照片可以被看作是一個非常長的向量
我們在這個非常長的向量上加入
每一個維度都加入一個小小的雜訊
把這個有加入雜訊以後的照片丟到 Network
看看會發生什麼樣的事情
那一般呢 這個雜訊都非常非常地小
小到什麼地步呢
最好小到人肉眼沒有辦法看出來
所以這個例子裡面加的雜訊其實太大了
一般這個雜訊是小到人肉眼看不出來的
有被加雜訊的照片叫做 Attacked Image
有被攻擊的照片
那還沒有被加雜訊的照片呢
我們一般就叫做 Benign Image
它是好的 Image
它是還沒有被攻擊的圖片
Benign Image 丟到 Network 裡面
它的輸出是貓
那我們期待說 Attacked Image
就我們現在是攻擊方 我們是壞人
我們希望 Attacked Image 丟到 Network 裡面
它的輸出不可以是貓
要變成其他的東西
那攻擊呢 大致上可以分成兩種類型
一種是沒有目標的攻擊
沒有目標的攻擊是
原來的答案是貓
只要你能夠讓 Network 的輸出不是貓
你就算是成功了
但是還有另外一種更困難的攻擊
是有目標的攻擊
也就是說我們希望 Network
不止它輸出不能是貓
還要輸出別的東西
比如說 我們希望加了一個雜訊以後
Network 輸出呢 是海星
它把貓錯誤判斷成一隻海星
才算是攻擊成功
好 那這樣子的攻擊真的有可能做到嗎
我們真的可以加入一個人肉眼看不到的雜訊
去改變 Network 的輸出嗎
實際上是可以的
這邊用的 Network 並不是一個很弱的 Network
它是 50 層的 ResNet
當我們把一個 Benign Image 沒有被攻擊的圖片
丟到 50 層的 ResNet 的時候
它的輸出是 Tiger Cat
你知道今天這種影像辨識的系統
它的輸出都不只會告訴你它是什麼動物
還會告訴你它是哪一個品種的動物
所以它給它這隻貓
它不只說它是 Cat
還說它是 Tiger Cat
不過據說這個答案其實是錯的
據說這個貓不是 Tiger Cat
不過沒有關係
反正它都認得出這個是一隻貓就對了
至少知道是一個貓科的動物
好 那它還有一個信心的分數
信心的分數是 0.64
這個信心的分數是什麼呢
這個信心的分數就是
做完那個 Soft Mask 以後
得到的那個分數
就假設你的影像分類的類別有 2000 類
2000 個類別都會有一個分數
那這個分數呢 一定介於 0 到 1 之間
而且 2000 個類別的分數合起來
會剛剛好是 1
那既然有 2000 個類別那麼多
Tiger Cat 可以拿到 0.64 的分數
那其實算是挺高的
接下來呢 我們在這個 Benign Image 上面
加入一些雜訊
我們現在希望成功攻擊的目標
是把 Tiger Cat 變成海星
而被攻擊以後的圖片長的是這個樣子的
你可能問說 雜訊加在哪裡呢
雜訊已經加進去了
但它非常非常地小
小到人的肉眼根本沒有辦法看出來
把這張圖片丟到 ResNet 以後
會發生什麼事呢
ResNet 的 Output 變成 Star Fish
而且它的信心分數是 100 % 啊
本來它還沒有那麼確定這是不是一隻貓
現在它百分之百確定
它就是海星
那為了要證明說
這兩張照片還是有一些不一樣的
我們把它做一下相減
光做相減是不夠的
做完相減以後
還要把它們的差距放大 50 倍
你會得到這樣子的結論
所以這兩張照片確實有些不一樣
我並不是把同一張照片複製兩次來騙你
它們是有一點不一樣的
但是人根本看不出
這點不一樣會造成什麼樣的影響
但是對 ResNet 而言
它卻有了天差地遠的輸出
那也許有人會覺得說
啊 也許是因為貓跟海星有什麼特別的關係
我們可以把貓變成海星只是一個特例
這不是一個特例
你可以把這隻貓輕易地變成任何的東西
我完全可以加上另外一個雜訊
就讓這隻貓變成一個鍵盤
它一樣信心分數高達 0.98
本來不太確定它是不是貓
現在加入另外一個雜訊以後
Network 98% 確定它就是一個鍵盤
那有人可能會覺得說
欸 怎麼會發生這麼離譜的行為
會不會是這個 Network 太爛了
我要告訴你 它可是有 50 層的喔
它可不是一個非常爛的 Network
如果你加入的只是一般的雜訊
它並不一定會犯錯
這個是原來的圖片
我們現在加入一個雜訊
這個雜訊是你肉眼可見的
你可以很明顯地看到
這張圖片裡面被加入了雜訊
這個時候 ResNet 覺得
它看到的是 Tabby Cat
這可能才是正確答案
但無論如何 它都知道是貓科動物
把雜訊加得更大一點
它說這是 Persian Cat 這是波斯貓
可能雜訊加得大一點
這個貓看起來毛茸茸的
所以 ResNet 覺得它看到了波斯貓
把雜訊再加更大一點
你可能已經不知道這是什麼東西了
這個時候 ResNet 說
它看到了 Fire Screen
Fire Screen 是什麼呢
我 Google 了一下發現
Fire Screen 長這個樣子
這裡完全可以理解機器為什麼會犯錯
它覺得前面的雜訊是這個屏風
而後面這個橙色的貓就是火焰
它雖然犯錯 它錯的是有尊嚴的
它錯的是有道理的
但不知道為什麼
加入一個人肉眼看不到的雜訊的時候
它卻產生了天差地遠的結果
那接下來我們在講為什麼這件事會發生之前
我們來看看剛才所說的攻擊
究竟是如何做到的
我們到底是怎麼加入了一個非常微小的雜訊
而這個非常微小的雜訊
可以讓 Network 產生非常錯誤的結果呢
而這個是我們的 Network
它是一個 Function 我們叫它 f
這個 Function 輸入是一張圖片
我們叫它 x0
它的輸出是一個 Distribution
這個是這個分類的結果
那我們叫它 y0
那我們這邊假設 Network 的參數就是固定的
我們不討論 Network 的參數的部分
Network 的參數不是我們今天的重點
所以它是固定的
如果是 Non-Targeted Attack 的話
我們要怎麼找出 Non-Targeted Attack 的雜訊呢
我們現在要做的目標就是
我們要找到一張新的圖片
這張新的圖片呢 我們用 x 來表示
當我們把 x 丟到這個 Network f 的時候
它的輸出是 y
而我們希望 y 呢 跟正確答案
我們正確的答案叫做 ŷ
我們希望 y 跟 ŷ 它的差距越大越好
我們現在是 Non-Targeted Attack
沒有目標的攻擊
我們只要知道說正確答案是 ŷ
今天 Network 的輸出跟正確答案的差距越大
我們就算是攻擊成功了
那怎麼做到這件事呢
怎麼找到這個 x
丟到一個 Network 裡以後
它產生的 y 跟 ŷ 差距越大越好呢
我們一樣要解一個 Optimization 的問題
這個跟我們訓練的 Network
其實是非常類似的
我們先定一個 Loss Function
這個 Loss Function 呢
叫做 L
這個 L 是什麼呢
這個 L 呢 是 y 跟 ŷ 之間的差距
取一個負號
舉例來說
我們一般在做這個 Classification 的時候
我們訓練的目標 y 跟 ŷ
都是看它的 Cross Entropy
那我們這個 -e(y, ŷ)
這一項代表的就是 y 跟 ŷ 之間的 Cross Entropy
但是我們希望這個 Cross Entropy 越大越好
所以我們今天在 Cross Entropy 前面加一個負號
那這個負的 Cross Entropy 就是我們的 Loss
而我們期望這個 Loss 越小越好
我們希望找到一個 x
x 可以讓 L(x) 越小越好
L(x) 越小 就代表說 y 跟 ŷ
它們的 Cross Entropy 越大
也就是 y 跟 ŷ 它們的距離越大
這個是沒有目標的攻擊
如果是有目標的攻擊的話
那我們會先設定好我們的目標
我們用 y target 來代表我們的目標
那 ŷ 其實是一個 One-Hot Vector
y target 也是一個 One-Hot Vector
那我們現在希望這個 y 不止跟 ŷ 越遠越好
我們還要跟 y target 越近越好
所以假設你的 y target 是一個 Fish
那你希望你輸出的這個 y 啊
它不止 Cat 的機率越低越好
Fish 的機率還要越高越好
那你的 Loss Function 就寫成這樣
我們的 Loss Function
是負的 y 跟 ŷ 之間的 Cross Entropy
希望這一項越大越好
同時你又希望 y 跟 y target
它們越小越好
你把這兩項加起來就是你的 Loss
你希望找一個 x
去 Minimize 這個 Loss
但光是找一個 x
去 Minimize Loss 是不夠的
因為我們其實還期待說
我們加入的雜訊越小越好
也就是我們新找到的圖片
可以欺騙過 Network 的圖片
跟原來的圖片要越相近越好
x 跟 x0 要越近越好
所以我們在解這個
Optimization 的 Problem 的時候
我們還會多加入一個限制
這個限制是 d(x0,x)
它小於等於 Σ
那這個 d(x0,x) 小於等於 Σ
是什麼意思呢
它的意思就是
我們希望 x 跟 x0 之間的差距
小於某一個 Threshold
小於某一個閾值
那這個閾值是根據什麼東西來決定的呢
通常就是根據人類的感知能力來決定
我們就假設說如果 x 跟 x0
它們的差距大於這個 d(x0,x)
就代表它們兩個之間的差距
那等一下下一頁投影片我會講說
怎麼計算兩張圖片之間的差距
如果 x0 跟 x 之間的差距大於 Σ
我們假設人就會看到這個雜訊
人就會發現有一個雜訊存在
所以我們要讓 x0 跟 x 它的差距
小於等於 Σ
小於等於人類可以感知的極限
那我們就可以產生一張圖片
人類看起來 x 跟 x0 是一模一樣的
但產生的結果對 Network 來說
是非常不一樣的
好 那怎麼計算 x 跟 x0 之間的差距
它們之間的距離呢
d(x0,x) 就代表它們之間的距離
有各式各樣不同的算法
那為了等一下符號的方便起見呢
我們假設 x 是一個向量
因為它是個圖片 所以它是個向量嘛
x0 是另外一張圖片
它也是一個向量
這兩個向量相減 我們叫它 Δx
那這個距離啊
你可以定 L2-Norm 當做它們的距離
也就是說你可以計算 Δx 的 L2-Norm
Δx 的 L2-Norm 就是把這個 Δx 的第一位
拿出來取平方
第二位拿出來取平方
第三位拿出來取平方
在這邊你其實要開根號也可以啦
就看你的 L2-Norm 的定義是怎樣
你要開根號也是可以的
好 那另外還有一個定義呢 是 L-Infinity
L-Infinity 是怎麼看的呢
它就是把這個 Δx 拿來
然後看裡面哪一個維度它的絕對值最大
那這一個就是L-Infinity
就把 Δx1 Δx2 Δx3
也就是 Δx 的每一維都拿出來取絕對值
看誰最大
最大的那一個就代表 x 跟 x0 之間的距離
好 那有各種不同的方法
可以計算兩張圖片之間的距離
但是我們在決定要使用哪一種方法
來計算圖片的距離的時候
其實我們應該把人類的感知把它考慮進來
那 L2 跟 L-Infinity 到底哪一個
在 Attack 的時候是比較好的距離呢
以下我舉一個例子來跟大家說明
好 這是一張圖片
假設這個圖片只有四個 Pixel 而已
現在啊
我們把這張圖片做兩種不同的變化
第一個變化是這四個 Pixel 的顏色
都做了非常小的改變
第二種變化是只有右下角這個 Pixel
它的顏色被改了
而且改的是比較大的
如果我們今天在計算 L2 的 Norm 的時候
這兩張圖片的 L2-Norm
和這兩張圖片的 L2-Norm 是一樣的
上面這張圖片是下面這張圖片
四個 Pixel 都改過以後的結果
上面這個中間這個圖片是下面這個圖片
就下面這個圖片是中間這個圖片
只有右下角的 Pixel 改動以後的結果
那這個改動是比較大的
這兩個變化他們的 L2 的 Norm 是一樣的
而但是如果你看 L-Infinity 的話
它們是不一樣的
因為 L-Infinity 只在意最大的變化量
那對於 L-Infinity 而言
這個改變它的最大的變化量
跟這個改變它最大的變化量
下面這個改變它最大的變化量是比較大的
上面這個改變最大的變化量是比較小
那如果從這個例子來看 L-Infinity 跟 L2
哪一個比較接近人類的感知能力呢
也許應該是 L-Infinity 吧
因為對你來說
其實這兩張圖片
我相信多數人你可能都看不出
它們之間有什麼差別
那我跟你保證它們兩個之間是有差別的
就它們是有非常非常微小的差別
只是它的差別是分布在每一個 Pixel 上面
而這下面這兩個改變呢
你會很明顯的看到右下角這個綠色
它的顏色變深了
雖然這另外這三個 Pixel 的顏色是固定的
右下角的顏色一變深
你就發現有圖片有變化
就發現這個圖片有做到
有做了某種修改
所以看起來 L-Infinity 也許更符合實際的需求
我們要避免被人類
我們要避免被人類發現
光是 L2 小是不夠的
我們要讓 L-Infinity 小才是最好的
才是比較不會被發現
所以在作業裡面我們是用 L-Infinity
來當做我們的限制
來當做攻擊
那我們作業就是要去攻擊一個
JudgeBoi 上面的那個
像辨識系統產生出來的圖片
我們會有所限制說
新的圖片跟舊的圖片
跟原來 Benign 圖片的差距
要小於某一個 Threshold
那我們在定這個差距的時候
我們就是選擇 L-Infinity
那實際上這個差距要怎麼定才是比較好
這個也要憑 Domain Knowledge
我們剛才舉的例子是影像上的例子
如果我們今天要攻擊的對象
其實是一個跟語音相關的系統
我們的 x 跟 x0 其實都是聲音訊號
那什麼樣的聲音訊號
對人類來說聽起來有差距
那就不見得是 L2 跟 L-Infinity 了
你就要去研究人類的聽覺系統
看看人類對什麼頻態的變化特別敏感
那根據人類的聽覺系統來制定
比較適合的 x 跟 x0 之間距離的衡量方式
那這個部分就是需要用到 Domain Knowledge
好 那我們現在已經有了
我們的 Optimization 的問題
我們要做的事情就是
我們要去 Minimize 是一個 Loss
那現在我們要找一個 x 去 Minimize 這個 Loss
但是這個 x 我們是有限制的
x 跟 x0 它們的 Distance 要小於等於 x
那這個問題到底要怎麼解呢
我們先把這個限制拿掉
如果把這個限制拿掉
你會不會解這個問題呢
你其實會解這個問題
因為這跟我們 Train 一個模型其實沒有什麼差別啊
我們在第一堂課的時候
就列過這個 Optimization 的問題給你看
告訴你說你可以調你的 Network 的參數
去讓一個 Loss 最小
我們今天只是把參數改成
Network 的 Input 而已
就這樣
你就把 Input 那一張 Image
看作是 Network 參數的一部分
然後 Minimize 你的 Loss Function 就結束了
現在 Network 的參數是固定的
我們只去調 Input 部分
讓 Input 的部分去改變
去 Minimum 一個 Loss 就結束了
用的一樣是 Gradient Descent
怎麼做呢
你就這樣做啦
就是你要先有個 Initialization 嘛
我們現在找的對象不是 Network 參數
是 x
是你 Input 的 Image
但是它還是需要一個初始化的值 對不對
你還是需要一個
做 Gradient Descent 的時候初始化的值
那初始化的值設什麼樣的數值比較好呢
你可能不會從隨機的 Image 開始
你可能會從 x0 開始
因為我們本來就希望說
我們新找到的 x 應該跟 x0 越接近越好嘛
那你何不就從 x0 開始找呢
你從 x0 開始找
你接下來找出來的 x 可能就會跟 x0 比較接近
所以你初始化的 x
你會初始化的這個 x0 就直接設 x0
然後接下來就跟
一般的 Gradient Descent 是一模一樣的
我們就是 Iterative 去 Update 你的參數
你就設一個 Iteration
t 等於 1 到 T
然後在每一個 Iteration 裡面
你都會計算 Gradient
只是這個 Gradient 不是 Network 參數
對 Loss 的 Gradient
我們現在已經不管 Network 參數了
而是 Input 那一張 Image x
對於 Loss 的 Gradient
那 Input 這個 x
它也是一個很長的向量嘛
它裡面就是有 x1 x2 x3 嘛
你就去計算這個 Input Image 裡面
每一個數值對 L 的偏微分
就 x1 對 L 的偏微分
x2 對 L 的偏微分
算出來
算出一個 Gradient
用這個 Gradient
去 Update 你的 Image 就結束了
所以你本來的 Image x0
它就減掉這個 Gradient
那前面你也會乘上一個 Learning Rate
就跟一般 Gradient Descent 是一模一樣的
只是要做 Gradient Descent 的對象
從參數換成 Input 而已
其他都是一樣的
也有 Learning Rate 那些什麼東西統統都有
乘上一個 Gradient
乘上 Learning Rate
減掉原來的 Image
然後就得到新的 Image
你可以 Iteration 地跑
就跟一般的 Gradient Descent 是一模一樣的
但是這個是在沒有 Constraint 的前提
接下來我們得把 Constraint 加進去
因為一般我們在做 Gradient Descent 的時候
我們並沒有把那個
Gradient Descent 的對象做什麼限制
我們並沒有設限說
我們的參數一定要長什麼樣子
那現在我們是有限制的
我們限制說 x 跟 x0
他們的差距一定要小於等於 x0
那要怎麼處理這個問題呢
你就在你的 Gradient Descent 裡面
再加一個 Module
欸 我發現說快那個 5 點 20 了
今天我們需要在 5 點 20 左右就結束了
所以我們等一下可能會需要
講到一個段落就結束它這樣子
好 這個我們要跑 Gradient Descent 這個演算法
但是我們要同時考慮 x0 跟 x 之間的差距
怎麼考慮這件事情呢
這邊呢
這個方法說穿了不值錢
非常地簡單
如果你 Update 完你的參數以後發現 x0 跟 X
我這邊應該要用 xt 比較正確啦
我這邊應該要用 xt 比較正確
你 Update 完你的參數以後發現
你的 xt 跟 x0 的差距大於 ε 以後
你就做一個修改
把 xt 做個修改
把它改回符合限制就結束了
舉例來說
假設我們現在用的是 L-Infinity
我們的這個 x0 在這個地方
那我們的 x 它可以存在的範圍
就只有這個方形框框的範圍 對不對
因為 L-Infinity 是考慮 x0 跟 x 之間最大的差距
所以出了這個框架的差距就會超過 ε
所以今天呢你在做完這個 Gradient Descent
用 Gradient 去 Update 你的 x 以後
它一定還是得要落在這個框框裡面才行
那怎麼保證 Update 以後
一定落在這個框框裡面呢
你就
只要 Update 超出了框框
就把它拉回來就結束了
所以今天這個步驟如果做完
你發現你得到藍色這個點跑出框框了怎麼辦
在框框裡面找一個跟藍色的點最近的位置
把藍色的點拉進來就結束了
就結束了
好 那其實這種 Attack 有非常多不同的變形
我想你在文獻上可以找到
各式各樣的 Attack 方法
但其實它們的精神都不脫
我們今天講的這個事情
那它們通常不一樣的地方都是
要嘛是 Constraint 不一樣
要嘛是 Optimization 的方法不一樣
但是通常都還是用 Gradient Descent
它們的精神還是一樣的
只是這邊你可能會有不同的 Optimizer
這邊你可能會有不同的限制
它就變成不同的 Attack 方法
但它們精神都不脫
我們今天跟大家舉的這個例子
好 那接下來呢
我們跟大家介紹一個最簡單的 Attack 的方法
也是作業裡面你要過
這個應該是過 Medium Baseline 所用的方法
然後這個方法是什麼呢
這個方法它叫做 FGSM
它可以過 Medium Baseline 嗎
喔 它不能過 Medium Baseline
它只能過 Simple Baseline
好 這個 FGSM 它是怎麼做的呢
非常地簡單
它叫做 Fast Gradient Sign Method 縮寫
它怎麼做呢
它就像是一個一拳超人一樣
它只用一擊
本來一般你在做 Gradient Descent 的時候
你要 Update 參數很多次
但是 FGSM 它厲害的地方就是
它決定只 Update 一次參數
看看能不能夠一擊必殺
一擊就找出一個可以 Attack 成功的 Image
所以首先呢
本來要 Iterative 的去 Update 參數
但是現在不用
我們只做一次的攻擊
我們只做一次的 Attack
然後 G 這邊呢
它做了一個特別的設計
那至於為什麼做這個特別設計
大家再去看一下原始文獻
可以了解當初為什麼會有這樣的想法
它說我們不要直接用這個
Gradient Descent 的值
我們給它取一個 Sign
這個 Sign 是什麼意思
這個 Sign 的意思是說
如果括號裡面的值大於 0
我們就輸出 1
括號裡面的值小於 0
我們就輸出 -1
所以加了 Sign 以後
這個 g 這個 Vector 啊
它裡面要嘛是 1 要嘛是 -1
本來如果你是算 Gradient
它的值可以是任何的 Real Number
但現在取 Sign
它要嘛是 1 要嘛是 -1
所以 g 裡面就都是 1 或者是 -1
然後 Learning Rate 呢
Learning Rate 就設 ε
就看你這邊的這個 ε 設多大
這邊 Learning Rate 直接設一個一模一樣的
直接設個一模一樣的會得到什麼效果呢
會告訴
會得到的效果就是
你攻擊完以後
你一定落在這個藍色框框的四個角落的地方
因為你想想看哦
這個 G 它要嘛是 1 要嘛是 -1
它每一維要嘛是 1 要嘛是-1
那前面會乘上 ε
所以乘完 ε 以後
你今天的 x0
要嘛就是往右邊移 ε
要嘛就是往左邊移 ε
要嘛就是往上移 ε
要嘛就是往下移 ε
所以今天做完一次攻擊以後
你的這個 x0 做完一次攻擊以後
它一定會挪到這個四方形的四個角落的地方
它一定是這四個角落的其中一個
那光做這件事
光做這個一擊往往就可以必殺
所以這個你可以過 Simple Baseline
那有同學就會問說一擊必殺有什麼好呢
如果我多攻擊幾次
多跑幾個 Iteration 結果不會更好嗎
會更好
所以多跑幾個 Iteration
就過 Medium Baseline 就這樣子
好 所以怎麼多跑幾個 Iteration
你就把本來是只跑一個 Iteration 啊
現在就多跑幾個 Iteration 啊
幾個 Iteration
你要跑幾個 Iteration
高興都是你自己設
就設個比如說 3 啊 5 啊 10 啊
多跑幾個 Iteration
那但是多跑幾個 Iteration 的壞處是
你有可能一不小心就出界
有可能一不小心就跑出了這個四方形的範圍
那跑出四方形的範圍後怎麼處理呢
非常簡單
把它拉回來就結束了
你就看說在這
如果這個藍色的點 Update 以後
跑出這個四方形
你就看這四個角落
哪一個角落跟藍色的點最近
就選那個角落
就結束了
好 這個就是 Iterative 的 FGSM
它可以幫你過 Medium 的 Baseline
好 那講到這邊呢
因為也五點半了
也許正好告一個段落
我們在這邊就先下課了
那大家有問題呢
其實還是可以繼續留下來問
那我們其實就到外面去討論
好 我們就下課了
