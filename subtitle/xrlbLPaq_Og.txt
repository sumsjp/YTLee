好 那我們來上課吧
那上週
我們講到 Network Pruning
那接下來呢
我們講下一個
可以讓 Network 變小的方法
這個方法呢
叫做 Knowledge Distillation
那等一下你聽完 Knowledge Distillation
會發現它的精神啊
跟 Network Pruning 其實也有一些類似的地方
好 那 Knowledge Distillation 是什麼呢
它的概念是這樣
我們先 Train 一個大的 Network
那這個大的 Network 啊
在 Knowledge Distillation 裡面
叫做 Teacher Network
它是老師
那你要 Train 的
你真正想要的那個小的 Network
叫做 Student Network
你先 Train 一個大的 Network
叫做 Teacher Network
再根據這個大的 Network
來製造 Student Network
那在 Network Pruning 裡面
你是直接把那個大的 Network
做一些修剪
把大的 Network 裡面
其中一些參數拿掉
就把它變成小的 Network
那在 Knowledge Distillation 裡面呢
是不一樣的
這個小的 Network
這個 Student Network
是去根據這個 Teacher Network 來學習
因為 Student Network
是根據 Teacher Network 來學習
所以一個就叫做 Teacher
一個就叫做 Student
好 那這個 Student
是怎麼根據這個 Teacher 來學習的呢
這個學習的方法是這個樣子的
假設我們現在要做的
就是手寫數字辨識
那你就把你的訓練資料
都丟到 Teacher 裡面
然後 Teacher 呢
就產生 Output
那因為今天是一個分類的問題
所以 Teacher 的輸出呢
其實是一個 Distribution
其實是一個分布
舉例來說
Teacher 的輸出可能是
看到這張圖片
1 的分數是 0.7
7 的分數是 0.2
9 這個數字的分數是 0.1 等等
好 那接下來學生要做的事情就是
給學生一模一樣的圖片
但是學生呢
不是去看這個圖片的正確答案來學習
它把老師的輸出
就當做正確答案
也就是老師輸出 1 要 0.7
7 要 0.2
9 要 0.1
那學生的輸出呢
也就要儘量去逼近老師的輸出
儘量去逼近 1 是 0.7
7 是 0.2
9 是 0.1
這樣的答案
那你可能會問說
那如果老師犯錯了 怎麼辦呢
如果老師答案是錯的 怎麼辦呢
不要管它
學生就是根據老師的答案學
就算老師的答案是錯的
學生就去學一個錯的東西
好 那講到這邊
你就會想說
那為什麼不直接 Train 小的 Network 就好了呢
為什麼不直接把小的 Network
去根據正確答案學習
而是要多加一個步驟
先讓大的 Network 學
再用小的 Network 去跟大的 Network 學習呢
那這邊的理由跟 Network Pruning 是一樣的
記不記得上週
我們講 Network Pruning 的時候
我們講說
為什麼不直接訓練一個小的 Network 呢
最直覺
最直覺最簡單的答案就是
因為你直接 Train 一個小的 Network
往往結果就是沒有從大的 Pruning 來得好
那 Knowledge Distillation 的概念是一樣的
為什麼不直接 Train 小的 Network
因為直接訓練一個小的 Network
沒有小的 Network 根據大的 Network 來學習
結果要來得好
那其實 Knowledge Distillation 這個技術啊
它也不是新的技術
Knowledge Distillation 最知名的一篇文章
是這個 Hinton
在 15 年的時候
就已經發表了
很多人會覺得 Knowledge Distillation
算是 Hinton 提出來的
因為 Hinton 有一篇文章
他的 Title 呢
就是 Knowledge Distillation of Neural Network
那但是其實在 Hinton 提出
Knowledge Distillation 這個概念之前
我其實就有看過其他文章
使用了一模一樣的概念
舉例來說
在一篇文章叫做
它的 Title 很 Fancy
Title 叫做 Do Deep Nets Really Need to be Deep
是一篇 13 年的文章裡面
他也提出了 Network Distillation 的想法
那這個顯然是一個比較早年的文章
在 13 年的時候
那個時候還不是所有人都已經被說服說
Deep Learning 就是很棒的
所以那個時候呢
你還需要寫一些文章
做一些研究
來 Justify 說 Deep Learning 是好的
所以這篇文章
它的標題就是 Deep Network
到底需不需要是 Deep
到底 Deep 的好處在哪裡
好 那為什麼 Knowledge Distillation 會有幫助呢
一個比較直覺的解釋是說
Teacher Network
其實會提供這個 Student Network 額外的資訊
那如果你直接跟 Student Network 說
這個是 1
可能太難了
因為 1 可能跟其他的數字
比如 1 跟 7 也有點像
1 跟 9 也長得有點像
所以對 Student Network 來說
你告訴它說
看到這張圖片你要輸出 1
然後 7 啊 9 啊
它們的分數都要是 0
可能很難
它可能學不起來
所以讓它直接去跟老師學
老師會告訴它說
就算 1 呢
你沒有辦法讓它是 1 分
也沒有關係
其實 1 跟 7
是有點像的
老師都分不出 1 跟 7 的差別
老師說 1 是 0.7
7 是 0.2
學生只要學到 1 是 0.7
7 是 0.2 就夠了
那這樣反而可以讓小的 Network
學得比直接 Trained From Scratch
直接根據正確的答案要學
來得要好
那其實 Knowledge Distillation 有些神奇的地方
如果你看那個 Hinton 的 Paper 裡面
它甚至可以做到
光是 Teacher 告訴 Student
哪些數字之間
有什麼樣的關係這件事情
就可以讓 Student
在完全沒有看到某些數字的訓練資料下
它就可以把那一個數字學會
假設今天訓練資料裡面
完全沒有 7 這個數字
但是 Teacher
它在學的時候有看過 7 這個數字
但是 Student 從來沒有看過 7 這個數字
但光是憑著 Teacher 告訴 Student 說
1 跟 7 有點像
7 跟 9 有點像
這樣子的資訊
都有機會讓 Student 可以學到 7 長什麼樣子
就算它在訓練的時候
從來沒有看過 7 的訓練資料
好 這個是 Knowledge Distillation 的基本概念
好 那這個 Teacher Network
不一定要是單一一個巨大的 Network
它甚至可以是多個 Network 的 Ensemble
Ensemble 是什麼呢
到目前為止這門課
我們其實沒有正式介紹過 Ensemble 這個詞彙
雖然它是 Deep Learning 裡面
尤其是在機器學習的比賽裡面
常常用到的一個技巧
不過這個技巧非常地簡單
所以其實也不太需要真的花太多時間介紹
它的概念很簡單 就是
你就訓練多個模型
然後你輸出的結果就是多個模型
投票的結果就結束了
或者是把多個模型的輸出
平均起來的結果
當做是最終的答案
那這個就是 Ensemble
那雖然在比賽裡面
常常會使用到 Ensemble 的方法
今天如果在一個機器學習的比賽裡面
你要名列前茅
往往憑藉的
就是超級的 Ensemble
就是訓練個
100 個模型啊
1000 個模型啊
把那麼多的模型的結果通通平均起來
往往你要在機器學習的
這種 Leaderboard 上面名列前茅
靠的就是這種 Ensemble 的技術
但是在實用上啊
Ensemble 會遇到的問題就是
你訓練了 1000 個模型
進來一筆資料
你要 1000 個模型都跑過
再取它的平均
這個計算量也未免太大了吧
打比賽還勉強可以
要用在實際的系統上
顯然是不行的
那怎麼辦呢
你可以把多個 Ensemble 起來的 Network
綜合起來變成一個
那這個就要用 Knowledge Distillation 的做法
你就把多個 Network Ensemble 起來的結果
當做是 Teacher Network 的輸出
然後讓 Student Network
去學這個 Ensemble 的結果
就 Ensemble 的輸出是什麼樣子
就讓 Student Network
去學這個 Ensemble 的輸出
那這樣子你就可以讓 Student Network
去逼進這個 Ensemble
一堆 Network Ensemble 起來
有的這個正確率
那這個呢
是 Knowledge Distillation 在 Ensemble 上的應用
那在使用 Knowledge Distillation 的時候
有一個小技巧啦
這個小技巧是
你會稍微改一下 Softmax 的 Function
你會在 Softmax 的 Function 上面
加一個 Temperature
這個 Temperature 是什麼呢
我們馬上就會看到
好 那跟大家簡單的複習一下 Softmax
那我們呢
在開學期初的時候
在講這個分類的時候
就跟大家介紹過 Softmax
Softmax要做的事情就是
你把每一個 Neural 的輸出
都去 Exponential
然後再做 Normalize
得到最終 Network 的輸出
那這樣可以讓你的 Network 的輸出
變成一個機率的分布
Network 的輸出
最終的輸出
分子都是介於 0 到 1 之間的
好 那所謂的 Temperature 的意思就是
在做 Exponential 之前
把每一個數值呢
都除上一個 T
那這個 T 呢
是一個 Hyperparameters
T 呢
是一個你需要調的參數
那除這個 T 有什麼作用呢
假設 T 大於 1 的話
那這一個 T
也就是 Temperature
它的作用就是
把本來比較 Sharp
比較集中的分布
讓它變得比較平滑一點
那為什麼要讓集中的分布
變得比較平滑一點呢
我們這邊舉一個例子
假設你的 y1 y2 y3
在做 Softmax 之前
它的值分別是 100 10 跟 1
那做完 Softmax 以後
你會發現
y1′ 就是 Softmax 的結果
y1′ 是 1
y2′ y3′ 都趨近於 0
那假設這是你的輸出
這個是你的 Teacher Network 的輸出
你叫你的學生
要叫你的 Teacher Network 去跟這個結果學
那跟直接和正確的答案學
完全沒有不同啊
跟 Teacher 學的一個好處就是
老師會告訴你說
哪些類別其實是比較像的
讓 Student Network 在學的時候不會那麼辛苦
但是假設老師的輸出
非常地集中
就是其中某一個 Class 是 1
其他都是 0
那這樣子跟正確答案學有什麼不同呢
就沒有不同啦
所以你要取一個 Temperature
假設我們這邊 Temperature 設為 100
那你就是把 y1 y2 y3 都除 100
就變成 1
0.1 跟 0.01
那對於老師來說
其實加上這個 Temperature 分類的結果
是不會變的
做完 Softmax 以後
最高分的還是最高分
最低分的還是最低分
所有 Class 的
這個排序
是不會變的
分類的結果是完全不會變的
但好處是
每一個類別得到的分數
會比較平滑
比較平均
那你拿這一個結果去給 Student 學
才有意義
才能夠把 Student 呢
學得好
那這個是 Knowledge Distillation 的一個小技巧
好
好 那我們來看一下
有沒有同學有問題要問的
那我剛才發現說呢
我犯了一個錯誤
是什麼呢
就是我忘了把
我忘了在手機上
把大家的那個留言打開啦
所以呢
我現在趕快
請容我打開一下
好 我看一下哦
我剛才發現說
他的聲音讓它放出來了
好
來 我看一下
有沒有同學有
問題要問的
欸 講到那個教育
剛才在聊天室裡面
我們有提到教育部的那個
那個測驗嘛
那我突然想到說
假設你覺得說
這個台灣的機器學習課程
應該要有更多的運算資源
這樣大家才能夠做作業更順利的話
欸 你去跟教育部講
你去跟教育部講
好
那個
有同學問說
助教有說作業九
什麼時候公布嗎
應該下週就可以公布了
剛才有
其實有回答了這個問題
好 我看看哦
好 有同學問說
那拿 Softmax 前的輸出
拿來 Train
會發生什麼事呢
你完全可以拿 Softmax 前的輸出
拿來 Train
其實還會有人拿
這個 Network 的每一層
都拿來 Train
你可以說
欸 我有一個大的 Teacher Network
它是第 1 層
它有 12 層
小的 Student Network 有 6 層
那我要讓 Student Network 第 6 層
像大的 Network 的第 12 層
Student Network 的第 3 層
像大的 Network 的第 6 層
可不可以呢
可以
那往往你做這種比較多的限制
其實可以得到更好的結果
那這是一個常用的
在 Knowledge Distillation 常用的小技巧
好
那有同學想問 Lifelong Learning 的問題
可以
你可以問 Lifelong Learning 的問題
我看你的問題是什麼
上週有提到用 Generator 產生資料
讓模型學習
但這樣模型只要再學一次舊的資料
有達到 Lifelong Learning 的宗旨嗎
我可以了解你的問題
你說 Lifelong Learning 就是
不要一看到舊的資料啊
那我現在再產生一些舊的資料
加到訓練資料裡面
有沒有違反 Lifelong Learning 的精神呢
這是一個
見仁見智的問題啦
那假設我們把 Lifelong Learning 定義成
你不能去看真正的舊的資料
但是如果你的 Network
自己有辦法去產生舊的資料
它並沒有去真的把舊的資料存下來啊
那個舊的資料是它自己生出來的
那至少今天在 Lifelong Learning 的 Community
在如果你要寫 Lifelong Learning 的 paper 的話
這樣子的方法是
會被接受是一個 Lifelong Learning 的方法
因為你並沒有真的用到舊的資料
舊的資料是被產生出來的
有一個同學問說
如果學生和老師差太多的話
可以加一個介於中間的 Network
讓學生學那個 Network 嗎
可以
就是可以
那這樣子的技術啊
等一下在助教的錄影裡面
助教會跟大家提示
有這麼樣的一個方法
同學問到說 T 太大的話
模型會不會改變很多
Temperature 太大的話
會不會改變很多
會
你想
假設你 Temperature 是個接近無窮大
那這樣所有的 Class 的分數就變得差不多啦
這樣子 Student Network 也學不到東西了
所以 T
又是另外一個 Hyperparameters
它就跟 Learning Rate 一樣
這個是你在做 Knowledge Distillation 的時候
要調的參數
好 有同學問說
老師
請問 Ensemble 的 Destination
只有把 Logistic 合在一起的方式嗎
我猜這位同學想要問的問題是說
在做 Ensemble 的時候
就我剛才在舉例
告訴你什麼是 Ensemble 的時候
我是舉說
我們可以把很多的 Network 呢
它的輸出平均起來
這就是一種 Ensemble
那有沒有其他 Ensemble 的方式呢
其實是有的
這個說起來就有點話長啦
我之後
這個 Ensemble 的這個技術
其實之前在過去的錄影
其實也是有提過的
那我之後可以把這個錄影呢
也貼在社團上
讓大家再來研究
那其實 Ensemble 呢
是有很多各式各樣不同的變形的啦
舉例來說
你在做 Ensemble 的時候
剛才講的
是在那個
Network 的 Output 上面做平均
但事實上還有其他做法
比如說
你有看過有人直接在
Network 的參數上面做平均
那這個 Network 參數上面做平均的方法
在這一門課裡面
你甚至已經有用過了
在那個 Translation 那個作業裡面
其實助教的這個程式 就是有
就是已經有幫你做好 Ensemble
就是有把不同的參數做平均
這樣子的小技巧
那這一招呢
在 Translation 上面
不知道為什麼特別有用
所以在助教的程式裡面
有實做這個方法
好 希望這樣子有回答到大家的問題
好 那如果大家沒有問題的話
那我們就繼續啦
那剩下的
這個有關
那個 Network Compression 的部分沒有太多啦
我們就趕快把它講完
接下來就可以放一下助教的錄影
好 那接下來
下一個跟大家講的小技巧啊
叫做 Parameter 的 Quantization
不過這一招呢
在我們的作業是用不上的
欸 為什麼在我們的作業是用不上的呢
因為我們的作業啊
對大家 Network 大小的限制
並不是看那個
Network 的本身的大小
而是看你用了幾個參數啦
所以你如果不是減少參數的量
單純是用其他方法來壓縮 Network 的話
在這個作業裡面
其實是不會得到比較好的結果的
是不會比較有利的
好 不過還是跟大家介紹一下
這個 Parameter Quantization 的做法
那這個 Parameter Quantization 的做法
是什麼意思呢
它的意思是說
我們能不能夠只用比較少的空間
來儲存一個參數
舉例來說
你現在存一個參數的時候
你可能是用 64 個 Bits
你可能是用 32 個 Bits
真的有必要精度這麼高嗎
會不會用 16 個 Bits 就夠了
會不會用 8 個 Bits 就夠了
會不會甚至更少的 Bits 就夠了
所以 Parameter Quantization
最簡單的做法就是
本來如果你存 Network 的時候
舉例來說
你是 16 個 Bits 存一個數值
現在改成 6 個 Bits
8 個 Bits 存一個數值
欸 那你的儲存空間
你的 Network 的大小
直接就砍半了
而且你的那個 Performance 呢
不會掉很多
甚至有時候
你把你的這個儲存那個參數的精度變低
結果還會稍微更好一點
好 那還有一個
再更進一步壓縮你的參數的方法呢
叫做 Weight Clustering
Weight Clustering 的意思是什麼呢
我們直接舉例來跟大家說明
假設這些
是你的 Network 的參數
然後呢
你就做 Cluster
你就做分群
按照這個參數的數值來分群
數值接近的
放在一群
那要分幾群呢
你會先事先設定好
舉例來說
我們這邊事先設定好說
我們要分四群
那你就會發現說
比較相近的數字
就被當做是一群
那接下來每一群
我們都只拿一個數值來表示它
也就是只要分到黃色這一群的
原來它的數值是多少
我們不管
我們就說它都是 -0.4
反正其他人可能
其他參數也都跟 -0.4 的結果
可能都差不多了
這個 -0.4 呢
可能就是這裡面所有數值的平均
假如這裡面所有數字的平均是 -0.4
我們就用 -0.4
來代表所有黃色的參數
用 0.4 來代表所有藍色的參數
用 2.9 來代表分到綠色這群的參數
用 -4.2 來代表分到紅色這群的參數
這樣做有什麼好處呢
這樣做的好處是
你今天在儲存你的參數的時候
你就只要記兩個東西
一個是一個表格
這個表格是記錄說
每一群代表的數值是多少
那另外一個呢
你要記錄的就是
每一個參數
它屬於哪一群
那假設你群的數目設少一點
比如說你設四群而已
那這樣子你要存一個參數的時候
你就只要兩個 Bits
就可以存一個參數了
你就可以把
本來你存一個參數
你可能要 16 個 Bits
8 個 Bits
再進一步壓縮到
存一個參數
只需要兩個 Bits 就好
好 那你其實還可以把參數
再更進一步做壓縮
那假設你對通訊比較熟悉的話
那你可能學過一個東西
叫做 Huffman Encoding
那 Huffman Encoding 的概念就是
比較常出現的東西
就用比較少的 Bits 來描述它
比較罕見的東西
再用比較多的 Bits 來描述它
這樣的好處就是常出現的東西
用少的 Bits 來描述
罕見的東西
用多的 Bits 來描述
這樣平均起來
你需要儲存你的資料
需要的 Bits 的數目就變少了
所以這個就是 Huffman Encoding
所以你可以用這些技巧呢
來壓縮你的參數
讓你儲存每一個參數的時候
需要的空間呢
是比較小的
好 那到底可以壓縮到什麼程度呢
最終極的結果就是
你可以只拿一個 Bits
來存每一個參數
你可以說你的 Network 裡面的 Weight
要嘛就是 +1
要嘛就是 -1
只有這兩種可能
沒有其他的了
假設你可以做到說
你所有的 Weight
就只有正負 1 兩種可能
那你每一個 Weight
就只需要一個 Bits
就可以存下來了
那像這樣子的這種 Binary Weights 的研究啊
其實還蠻多的
那我這邊就是列了一些 reference 給大家參考
那至於實際上
要怎麼訓練出這種 Binary 的 Network
那這個細節
我們就不講
那一般人會對 Binary Network 有的印象就是
哇 這個一個參數值
要嘛是 +1
要嘛是 -1
會不會這個 Network 的 Performance 很差啊
這 Network 應該很慘吧
它應該做得很爛吧
這倒未必
這邊呢
就是取那個 Binary Network
裡面的其中一個經典的方法
叫做 Binary Connect
它的結果來跟大家分享一下
在 Binary Connect 的這篇 Paper 裡面
他跟你說
他把 Binary Connect 這個技術
用在三個影像辨識的問題上
從最簡單的 MNIST
還有稍微難一點的 CIFAR-10
他設計的 Corpus
那他發現了什麼
這個第一排是正常的 Network
不是 Binary 的 Network
一般的 Network
在 MNIST 上
你的錯誤率是 1.3%
這邊是錯誤率啦
所以值越小越好
之後在 CIFAR-10 上是 10%
他發現
用 Binary Connect
也就是每一個參數
要嘛是 +1
要嘛是 -1
結果居然是比較好的
所以你用 Binary Network
結果居然還比正常的 Network 的 Performance
好一點
那這邊的理由可能是說
當我們用 Binary Network 的時候
我們給了 Network 比較大的限制
我們給 Network Capacity 比較大的限制
所以它比較不容易 Overfitting
所以用 Binary 的 Weights
它反而可以達到防止 Overfitting 的效果
所以這邊是想要跟大家分享一下
Binary Network 的
厲害的地方
好 那我們看一下大家有沒有問題要問
好 看一下
好 有同學問到說
關於上週 Lifelong Learning
講到 EWC 的方法
在 Train 每個新的 Task 之前
為了要算每個 Parameter 的重要性
都會用之前的 Data 去算 Gradient
這樣是不是算跟
GEM 一樣
存有部分之前的 Data 呢
這邊是不一樣的
因為那個 Gradient
它不是看到新的任務才算的
那個 Gradient
就是我們要算 Parameter 的重要性這件事情
在一個任務訓練完之後
你就會馬上計算了
所以你並不是等
要解新的任務的時候
才去舊的任務的 Data 上
去計算說每一個參數的重要性
是舊的任務一訓練完
馬上就把參數性記錄下來
把參數的重要性記錄下來
那把參數的重要性記錄下來以後
舊的任務的所有的資料
就都可以被丟掉了
所以它跟 GEM
還是不太一樣的
不過我上週有提示說
你如果是用 EWC 這種方法
其實你也用
有用到額外的儲存空間啦
所以
雖然它沒有存資料
但是它需要儲存參數的重要性
那這個也是會耗用一些儲存的空間的
這邊要從消息理論那邊講吧
我猜你是指 Huffman Coding
好啊
也可以從消息理論那邊講
資料壓縮啊 等等
你知道這個就是
就是因為這個 Weight Quantization
這一系列的方法跟
這個課程的主軸比較沒有關係啦
所以在作業裡面也
我們就沒有特別強調
Weight Quantization 這一件
這一件事情
我只想跟大家講說
反正有這些技術就是了
好 有同學問說
Weight Clustering 要怎麼做 Update
每次 Update 都要重新分群嗎
其實為 Weight Clustering 有一個很簡單的做法
你可能以為那個 Weight Clustering
是需要在訓練的時候就考慮的
但是有一個簡單的做法是
你先把 Network 訓練完
再直接做 Weight Clustering
但你這樣直接做
可能會導致你的這個
就是 Cluster 後的參數
跟原來的參數相差太大
所以有一個做法是什麼呢
有一個做法是
我們在訓練的時候啊
要求 Network 的參數
彼此之間比較接近
你可以把這個訓練的 Quantization 當做是
Loss 的其中一個環節
直接塞到你的訓練的過程中
讓訓練的過程中達到呢
這個參數呢
有群聚的效果
那這件事情是可以做到的
好 有同學問說
要壓到什麼程度
才不會丟掉太多的資訊
那這個
你就要自己算算看啦
那你就要
到底要壓到什麼程度
那你做壓縮的時候
難免就是會掉一些資訊嘛
但到底掉多少資訊你能夠接受
那這個就要看你的應用了
好 那個 Weight Clustering 裡面
每個 Cluster 的數字要怎麼決定呢
決定好每個 Cluster 的區間之後
取它們的平均嗎
對 就是決定好每個 Cluster 的區間之後
取它們的平均
好 同學問說
Binary Weight 加
（00：27：38）
會比較好嗎
不好意思
我不太確定你指的（00：27：42）是指什麼
好 剛剛有一個同學問說
關於剛剛 EWC 的問題
想請問老師
那樣不同的 Task Data 對 Model 的重要性
會不會衝突
怎麼把不同 Task 的 Importance Merge
這邊這個解法
怎麼把不同 Task 的 Importance Merge
在文獻上的做法
比你想像的要簡單
就是
每一個 Task 都會計算出一個重要性
然後把所有 Task 的重要性加起來
就結束了
就是每個參數
在每一個 Task 訓練完以後
都會得到一個首位的值
都會得到一個 b
把每一個任務的 b 通通加起來
就結束了
好 希望這樣有回答到
（00：28：40） 是加入 Noise
反而可以保留更多的資訊
好 我沒有看過有人研究過
（00：28：52）加 Binary Weight 這個問題
那所以這邊我沒有很好的答案
這聽起來像是一個蠻有創意的方法
如果你有試出什麼結果
再告訴我好了
好 那希望有回答到大家的問題
那我們就繼續囉
接下來啊
要跟大家講的
是 Network 架構的設計
透過 Network 架構的設計
來達到減少參數量的效果
那等一下要跟大家介紹的
叫做 Depthwise Separable Convolution
那這個會是這次作業的主力啦
你要過 Strong Baseline
就靠這個方法了
好 那這一個方法啊
在講這個方法之前
我們先幫大家很快地秒複習一下 CNN
那我們在開學的期初的時候
已經跟大家講過 CNN
那我們說 CNN 是什麼呢
在 CNN 的這種 Convolution Layer 裡面呢
你每一個 Layer 的 Input
是一個 Feature Map
那在這個例子裡面呢
我們的 Feature Map 有兩個 Channel
如果你的 Feature Map 有兩個 Channel
那你的每一個這個 Filter
它的高度也得是 2
而且這個 Filter 並不是一個長方形
而是一個立方體
你的 Channel 有多少
你的 Filter 就得有多厚
然後你再把這個 Filter 呢
掃過這個 Feature Map
你就會得到另外一個 Mattress
另外一個正方形
你有幾個 Filter
那 Output Feature Map 就有幾個 Channel
這邊有 4 個 Filter
每一個 Filter 都是立方體哦
那 Output Feature Map 就有 4 個 Channel
那在這個例子裡面總共有多少參數呢
我們總共有 4 個 Filter
每一個 Filter 的參數量是 3 x 3 x 2
要注意一下每一個 Filter
其實立方體
所以總共的參數量是 3 x 3 x 2 x 4
總共 72 個參數
那等一下跟大家介紹的方法
叫做 Depthwise Separable 的 Convolution
那這個 Depthwise Separable Convolution
它為什麼會好呢
這個等一下再跟大家講它的原理
在講它的原理之前
我們先直接講它的操作
這個 Depthwise Separable 的 Convolution
它分成兩個步驟
第一個步驟呢
叫做 Depthwise 的 Convolution
Depthwise Convolution 做的是什麼呢
它要做的事情是
有幾個 Channel
我們就有幾個 Filter
而每一個 Filter 只管一個 Channel
因為每個 Filter 只管一個 Channel
所以有幾個 Filter
有幾個 Channel 就有幾個 Filter
舉例來說
假設 Input Feature Map 是兩個 Channel
那在 Depthwise 的 Convolution Layer 裡面
我們就只放兩個 Filter
不像之前在一般的 Convolution Layer 裡面
Filter 的數目跟 Channel 的數目是沒有關係的
在前一個投影片的例子裡面
Channel 只有兩個
但 Filter 可以有四個
但在 Depthwise Convolution 裡面
幾個 Channel
幾個 Filter
然後每一個 Filter 就只負責一個 Channel 而已
那其實我覺得這個 Depthwise Convolution
比較像是大家一般對 CNN 的誤解啦
就假設有人對 Convolution 不太熟悉的話
一般都會誤以為 Convolution 的算法
就跟 Depthwise Convolution 是一樣的
其實不是
你已經修過這一門課了
已經花很多時間跟大家介紹過 CNN 了
所以你一定知道說 CNN 不是這樣算的
但是 CNN 裡面一個特別的變形
減少參數量的變形
其實跟大家一般對 CNN 的誤解蠻像的
就是每一個 Filter 只去管一個 Channel 就好
那一個 Filter 怎麼管一個 Channel 呢
假設這個淺藍色的 Filter 管第一個 Channel
那就淺藍色的 Filter
在第一個 Channel 上面滑過去
滑過去 滑過去
然後就算出一個 Feature Map
然後深藍色的這個 Filter 管第二個 Channel
那它就在第二個 Channel 上面做 Convolution
然後也得到另外一個 Feature Map
所以在 Depthwise Convolution 裡面
你 Input 有幾個 Channel
你 Output 的 Channel 的數目會是一模一樣的
這個跟一般的 Convolution Layer 不一樣
一般的 Convolution Layer 裡面
Input 跟 Output 的 Channel 數目可以不一樣
但在 Depthwise Convolution 裡面
Input 跟 Output 的 Channel 數目是一模一樣的
好 但是你會發現
如果你只做 Depthwise Convolution
它會遇到一個問題
這個問題是 Channel 和 Channel 之間
沒有任何的互動
假設今天有某一個 Parton
是跨 Channel 才能夠看得出來的
那 Depthwise Convolution 對這種
跨 Channel 的 Parton 是無能為力的
所以怎麼辦呢
再多加一個 Pointwise 的 Convolution
Pointwise
Pointwise Convolution 的意思是說呢
我們現在一樣有一堆 Filter
這個跟一般的 Convolution Layer 是一樣的
但我們這邊做一個限制是
我們的 Filter 的大小
它的 Kernel 的 Size 通通都是 1 x 1
在一般的 Convolution Layer 裡面
你的 Filter 大小可能開 2 x 2, 3 x 3, 4 x 4
但是在 Pointwise Convolution 裡面
我們限制大小一定是 1 x 1
那這個 1 x 1 的 Filter
它的目標 它的作用
就是去考慮不同 Channel 之間的關係
所以第一個這個 1 x 1 的 Filter
它做的事情就是去掃過這個
Depthwise Convolution 出來的 Feature Map
然後得到另外一個 Feature Map
好 那這邊有另外三個 Filter
它們做的事情也是一樣
每一個 Filter 會產生一個 Feature Map
所以 Pointwise
Pointwise Convolution
跟一般的 Convolution Layer 是一樣的地方是
輸入跟輸出的 Channel 數目可以不一樣
但是 Pointwise Convolution
有一個非常大的限制
是我們強制要求說
Filter 的大小只準是 1 x 1
你只要考慮 Channel 之間的關係就好了
你就不要考慮同一個 Channel 內部的關係了
同一個 Channel 內部的關係
已經 Depthwise Convolution 做完了
所以 Pointwise Convolution
只專注於考慮
Channel 跟 Channel 間的關係就好
好 那我們先來計算一下
這個方法的參數量
你看在 Depthwise Convolution 裡面
兩個 Filter
每一個 Filter 大小是 3 x 3
所以就 3 x 3 x 2
總共是 8 個參數
那在 Pointwise Convolution 裡面
四個 Filter
每一個 Filter 的大小是 2
每個 Filter 只用了兩個參數
所以總共是 8 個參數
好 左邊這個圖
這個是一般的 Convolution
右邊這個圖
是Depthwise 加 Pointwise 的 Convolution
那我們現在來比較一下
這兩者參數量的差異
我們現在假設我們先預設好說
Input Channel 數目就是 I 個 Channel
Output Channel 數目就是 O 個 Channel
然後呢 這一個 Case 跟這一個 Case
這邊的 Kernel Size 都是 k x k
那如果是一般的 Convolution
你要有 k x k 的 Kernel Size
Input I 個 Channel
Output O 個 Channel
那你到底會需要多少參數呢
那你能先算一下
每一個 Filter 的大小
每一個 Filter 的大小應該是 k x k
Kernel Size 乘上 Input Channel 的數目
也就是 k x k x I
如果你要 Output O 個 Channel
O 個 Channel
那你需要 O 個 Filter
所以總參數量是 k x k x I
一個 Filter 的參數量再乘以 O
如果是 Depthwise 加 Pointwise 的 Convolution
要達到 Input I 個 Channel
Outout O 個 Channel
那要怎麼做呢
Depthwise Convolution
它的這個 Filter
它的 Filter 是沒有那個厚度的
它的 Filter 是沒有厚度的
所以你會發現說 Depthwise Convolution
所有的 Filter 加起來
它的參數量只有 k x k x I 而已
跟一般的 Convolution 裡面的
一個 Filter 的參數量是一樣的
然後我們看 Pointwise 的 Convolution
這邊是 I x O
就是假設你 Input Channel 的數目是 I 的話
那每一個 1 x 1 的 Convolution
它的高度會是 I
那 O 是哪來的呢
假設你要 Output O 個 Channel 的話
那你就要有 O 個 1 x 1 的 Convolution
所以今天 Pointwise Convolution
它的總參數量是 I x O
那你把這兩者進行比較
你把 k x k x I + I x O
去除掉 k x k x I x O
你把這兩者相除
經過一番計算你會發現
這兩者的比例是 1 除以 O
加 1 除以 k x k
那因為 O 通常是一個很大的值
你的 Channel 數目你可能開個 256 啊 512 啊
所以我們先把這個 1 除以 O 忽略不計
那 k x k 通常是比較
在這一整項裡面
它可能是比較關鍵的數值
這一整項的大小
可能跟 1 除以 k x k 是比較有關係的
那假設你的 Kernel Size
今天常用的 Kernel Size 可能是 3 x 3
或者是 2 x 2
假設你選 2 x 2
你把一般的 Convolution
換成 Depthwise 加 Pointwise 組合的話
那 Network 的大小就變成 2 x 1/2
假設你的 Kernel Size 是 3 x 3
當你從 Convolution 變成
Depthwise 加 Pointwise 的時候
你的 Network 的大小就變成原來的 1/9 了
好 那接下來想要跟大家解釋的是
為什麼這招有用呢
這一招的原理是哪來的呢
那這個啊
這個 Depthwise 加 Pointwise 這個招數
它其實是其來有自
在過去還沒有這個
Depthwise Separable 的 Convolution 之前
就已經有一個方法
是用 Low Rank Approximation
來減少一層 Network 的參數量
那你不知道什麼是 Low Rank Approximation
沒有關係
這邊就直接告訴你
這個是怎麼操作的
假設你有某一個 Layer
那這個 Layer 呢
輸入有 N 個 Neuron
輸出有 M 個 Neuron
那假設 N 或者是 M 其中一者非常大
這邊假設是 M 非常大
那你的這個參數量就會非常地可觀
這邊的參數量 W 呢
假設你的 Input 是 N 個 Neuron
Output 是 M 個 Neuron
那你的參數量是多少呢
這兩層之間的參數量是多少呢
是 N x M 對不對
那只要 N 跟 M 其中一者很大
那你這個 W 的參數就很多了
那怎麼減少這個參數量呢
有一個非常簡單的方法
是在 N 跟 M 中間再插一層
這一層就不用 Advection Function
直接多插一層
這一層 Neuron 的數目是 K
原來本來只有一層
現在拆成兩層
那這兩層裡面的第一層呢
我們用 V 來表示
第二層我們用 U 來表示
你可能會想說
這個把一層 Network 拆成兩層 Network
這個參數量不是變多了嗎
你仔細算算看
這個兩層的 Network
參數量反而是比較少的
怎麼說呢
你看原來一層的 Network
你的參數量是 M x N
現在我們拆成兩層 Network
第一層是多少
第一層是 N x K 對不對
N x K
第二層是多少
第二層是 K x M
K x M
如果你今天的 K 遠小於 M 跟 N
那你就會發現說
U 跟 V 的參數量加起來
是比 W 還要少的多
W 是 N x M
那 U 跟 V 的參數量加起來是 K x N + M
就 N 跟 M 就沒有相乘了
那你只要可以 K 夠小
那整體而言 U + V 的參數量就會變少
所以你常常
之前過去常有的做法就是
N 1000 M 1000
沒關係我可以塞個 20 塞一個 50
那這個參數量就減少蠻多的
那像這樣子的方法
雖然可以減少參數量
當然它還是會有一些限制
什麼樣的限制
因為你會發現說
當你把 W 拆解成 U x V 的時候
當你把 W 分成
用 U 跟 V 兩層來分開表示的時候
你會減少 W 的可能性
本來 W 它可以放任何的參數
但假設把 W 拆成 U 跟 V 的話
那這個 W 這個矩陣
它的 Rank 會小於等於 K
那你不知道 Rank 是什麼也沒有關係
反正就是 W 會有一些限制
所以會變成說它不是所有的 W
都可以變成用在
不是所有的 W 都可以變成當作你的參數
你的參數會變成有一些限制
好 那這個方法呢
就是拿來減少參數的一個非常常用的做法
那其實剛才講的
Depthwise 加 Pointwise 的 Convolution
其實它用的就是我們這邊的概念
就是把一層拆成兩層這樣的概念
好 怎麼說呢
我們先來看一下原來的 Convolution
在原來的 Convolution 裡面
紅色的這一個矩陣
左上角的這個參數是怎麼來的
是不是有一個紅色的這個 Filter
放在 Feature Input 的
Feature Map 的左上角以後所得到的
那今天在這個例子裡面
一個 Filter
一個 Filter 的參數量是多少
一個 Filter 的參數量是 3 x 3 x 2 也就是 18
你把 Filter 裡面的 18 個參數
跟 Input Feature Map 左上角的這 18 個數值
做 Inner Product 以後
就會得到這邊
Output Feature Map 左上角這個值
所以你的每一個 Filter 有 18 個參數
如果今天拆成
Depthwise 加 Pointwise 兩階的話
那會怎麼樣呢
如果拆成 Depthwise 加 Pointwise 兩階的話
左上角 Output Feature Map
左上角這個數值來自於中間的
Depthwise Convolution 的 Output
所以左上角這個值來自於中間這一個
Depthwise Convolution 的 Output
左上角的這兩個值
而這兩個值來自於 Input Feature Map
第一個 Channel 左上角這 9 個值
跟第二個 Channel 左上角這 9 個值
所以怎麼從這邊變到這邊呢
你可以看成是
我們有兩個 Filter
這兩個 Filter 呢
它分別是 9 個 Input
然後得到輸出
然後接下來這兩個 Filter 的輸出
再把它綜合起來
得到最終的輸出
所以本來是直接從這 18 個數值
變成一個數值
現在是分兩階
18 個數值變兩個數值
再變一個數值
或者是如果我們今天看
黃色的這個 Feature Map
左下角這個參數
黃色的 Feature Map 左下角這個數值
來自於哪裡呢
來自於 Depthwise Convolution 的 Output
左下角這兩個數值
而左下角這兩個數值來自於這個 Filter 左下
這來自於這個 Depthwise
來自於 Input 的這個 Feature Map
左下角的這 18 個數值
所以其實今天你又可以再看成說
是我們 我們把這個
當我們把這個一般的 Convolution
拆成 Depthwise 加 Pointwise 的時候
我們就可以看成是把一層的 Network
拆解成兩層的 Network
所以它的原理
跟剛才在前一頁投影片看到的
Low Rank Approximation 是一樣的
我們把一層拆成兩層
這個時候它對於參數的需求反而是減少了
好 那這個是有關 Network Architecture 的設計
那其實有關 Network Architecture 的設計
還有非常多論文可以參考啦
那這些文獻
其實也都放在助教的投影片裡面
那等一下助教不會細講這些 Network 的架構啦
但是在這個作業裡面
如果你可以成功實作出
剛才講的這個
這個 Depthwise 加 Pointwise 相疊的話
把一層 CNN 拆成兩層 CNN 的話
那你就很有機會可以破 Strong Best Line
好
我來看看
看看同學們有沒有問題
好 那如果同學沒有問題的話呢
那我們就繼續吧
最後一個要跟大家分享的
是 Dynamic Computation
那 Dynamic Computation 要做的事情
跟前幾個方法想要達成的目標不太一樣
在前幾個方法裡面想要做的事情
就是單純的把 Network 變小
那在 Dynamic Computation 裡面
要做的事情是什麼呢
在 Dynamic Computation 裡面
要做的事情是
我們希望 Network 呢
可以自由地調整它需要的運算量
為什麼我們期待 Network
可以自由地調整它需要的運算量呢
因為有時候你可能同樣的模型
會想要跑在不同的 Device 上面
而不同的 Device 上面
它有的運算資源是不太一樣的
所以你期待說
你訓練好一個 Network 以後
你放到新的 Device 上面
你不需要再重 Train 這個 Network
因為這個訓練一個神奇的模型
這個神奇的模型
本來就可以自由調整
它所需要的運算資源
所以運算資源少的時候
它就只需要少的運算資源就可以運算
運算資源大的時候
它就可以充分利用
充足的運算資源來進行運算
那另外一個可能是
就算是在同一個 Device 上面
你也會需要不同的 Computation
比如說你有
舉例來說假設你的手機非常有電的時候
那你可能就會有比較多的運算資源
假設你的手機沒有電的時候
那你可能就需要把運算資源
留著做其他的事情
那 Network 可能可以分到的運算資源就比較少
所以就算是在同一個 Device 上面
我們也希望一個 Network
可以根據現有的運算資源
比如說手機現在的電量還有多少
來自由地調整它對運算量的需求
好 那有人可能會問說
為什麼我們不直接準備
一大堆的 Network 就好了呢
假設我們今天需要各種
應付各種不同運算資源的情況
我們為什麼不訓練 10 個 Network
從運算量最少的到運算量最大的
然後根據我們的運算的狀況
去選擇不同的 Network 呢
可是你知道假設今天
假設你是在同一台手機上
你需要根據不同的狀況做不同的因應
那你可能就需要訓練一大堆的 Network
而手機上的儲存空間有限
那你今天我們就是要減少我們的運算量
但是如果你需要訓練一大堆的 Network
那你需要一大堆的儲存空間
那這個可能不是我們要的
所以我們其實期待可以做到說
一個 Network 它可以自由地調整
它對運算資源的需求
那怎麼讓 Network 自由地調整
它對運算資源的需求呢
一個可能的方向是
讓 Network 自由地調整它的深度
怎麼讓 Network 自由調整它的深度呢
你可以訓練
一般我們就是訓練一個很深的 Network
然後它有 Input
比如說在做 Image Classification 的話
它就是輸入一張圖片
然後輸出呢
就是圖片分類的結果
然後呢
你可以在這個 Layer 和 Layer 中間
再加上一個額外的 Layer
這個額外的 Layer 它的工作呢
是根據每一個 Hidden Layer 的 Output
去決定現在分類的結果應該是什麼
那這樣子當你的運算資源比較充足的時候
你可以讓這張圖片去跑過所有的 Layer
得到最終的分類結果
當運算資源不充足的時候
你可以讓 Network 決定說
它要在哪一個 Layer 自行做輸出
比如說運算資源比較不充足的時候
通過第一個 Layer
就直接丟到這個 Extra 的 Layer 1
然後呢 就得到最終的結果了
那怎麼訓練這樣一個 Network 呢
那其實概念比你想像的還要簡單
我們訓練的時候都有 Label 的資料
那一般在訓練的時候
我們只需要在意最後一層 Network
它的 Output 是什麼
我們希望它的 Output
跟 Ground Truth 越接近越好
但是我們今天呢
也可以讓 Ground Truth 呢
跟每一個 Extra Layer 的 Output 越接近越好
那我們把所有的 Output
跟 Ground Truth 的距離通通加起來
把所有的 Output
跟 Ground Truth 的 Cross Entropy
通通加起來得到 L
然後再去 Minimize 這個 L
然後就結束了
好 那這樣子最基本的方法
它能夠運作得好嗎
這個訓練方法是可以用的
你確實可以用我剛才講的訓練方法
就是每一層接出來做訓練
然後把所有接出來的結果
去跟 Ground Truth 算距離
然後 Minimize 所有接出來的結果
跟 Ground Truth 的距離
確實可以用這個方法達到 Dynamic 的深度
但是其實它不是一個最好的方法
如果你想知道最好的方法是怎麼做的
這個也不一定是現在最好的方法
假設你想知道比較好的方法是怎麼做的
也許你可以參考 MSDNet 這篇文章
另外還可以讓 Network 自由地決定它的寬度
怎麼讓 Network 自由決定它的寬度呢
你就是有設定好幾個不同的寬度
然後呢 你今天同一張圖片丟進去
在訓練的時候
同一張圖片丟進去
那每一個 Network 呢
每個不同寬度的 Network 會有不同的輸出
我們在希望每一個輸出
都跟正確答案越接近越好就結束了
我們把所有的輸出跟 Ground Truth 的距離
加起來得到一個 Loss
然後我們要去 Minimize 這個 Loss 就結束了
那這邊要跟大家強調一下
雖然我在這個投影片上
我畫了三個 Network
但是這並不是三個不同的 Network
它們是同一個 Network
可以選擇不同的寬度
也就是說這個 Weight
就是這個 Weight
我標一樣顏色的
就是同一個 Weight
只是在最左邊這個狀況的時候
整個 Network
所有的 Neuron 都會被用到
但是在中間這個狀況的時候
你可能會決定說
有 25% 的 Neuron 會用到
但你就會視哪些 Neuron 不用到
你就事先決定好
你就事先決定好說
某一些 Neuron 在你選擇說
只要用這個 75% 參數的時候
那 25% 的 Neuron 我們不要用它
或拿 50% 的 Neuron 我們不要用它
然後在訓練的時候
就把所有的狀況一起考慮
然後所有的狀況都得到一個 Output
所有的 Output 都去跟 Ground Truth 計算距離
然後要讓所有的距離都越小越好 就結束了
但是實際上你會發現
這樣 Train 也是有問題的
所以需要一些特別的想法來解決這個問題
那有關 Depthwise Dynamic 的寬度的 Network
要怎麼訓練這件事
大家可以參考 Slimmable 的 Neuron Network
好 那剛才講的
是我們可以 Train 一個 Network
我們可以自由去決定
它的深度跟它的寬度
但是所謂的決定權還是在人這一邊
你要自己去決定說
今天電池電量少於多少的時候
我們就用多少層
或者是多寬的 Network
但是有沒有辦法讓 Network 自行決定
根據它的環境
根據我們的情境
決定它的寬度或者是深度呢
這個也是有辦法的
為什麼我們需要 Network
自己去決定它的寬度跟深度呢
那是因為有時候
就算是同樣是影像分類的問題
那有一些影像可能特別難
有一些影像可能特別簡單
對那些比較簡單的影像
也許你只要通過一層 Layer
Network 就已經可以知道答案了
對於一些比較難的問題
舉例來說同樣是貓
這隻貓是被做成一個墨西哥捲餅的樣子
所以這是一個特別困難的問題
也許這張圖片只通過一個 Layer 的時候
Network 會覺得它是一個捲餅
在通過第二個 Layer 的時候
還是一個捲餅
要通過很多個 Layer 的時候
Network 才能夠判斷它是一隻貓
如果是這種比較難的問題
你就不應該在中間停下來
那我們能不能讓 Network 自己決定說
這是一張簡單的圖片
所以通過第一層就停下來
這是一個比較困難的圖片
所以要跑到最後一層才停下來呢
我們是可以這麼做的
那假設你想要知道怎麼做的話
可以參考以下幾篇 Reference
所以像這樣子的方法
其實就不一定限制在那個
運算資源比較有限的狀況啦
有時候就算是你運算資源比較很充足
但是對一些簡單的圖片
如果你可以用比較少的 Layer
就可以得到你要的結果
那其實也就夠了
這樣你就可以省下一些運算資源
去做其他的事情
就好像說我可以了解說呢
在這門課裡面
因為我們今天把這個
做什麼事情會得到幾分都訂得非常地明確
所以大家就會知道說你做了哪些事情
大概就可以拿到 A+
然後所以後面的作業
你拿到 A+ 以後
可能後面的作業你就不想做了
所以你就跟上面這個情況一樣
做到中間你就停下來
得到 A+ 以後
你就 Output 你的結果就結束了
那這也是人之常情
我也是可以接受的啦
不過像以前
以前這個
這學期呢 這個成績是
這個原始成績就直接定義
就直接對應到那個等第了
那過去其實有一陣子
這個原始成績是沒有直接對應到等第
成績是相對的
這個時候大家就會很痛苦吧
有的同學就會
他原始成績拿到 100 多分
哇 結果 C- 這樣子
以前如果是直接按照比例分配
就前 1/4 的人 A+
以此類推的話
那就有可能有點痛苦吧
不過這學期
我們就直接把原始成績對應到等第
讓大家日子過得比較輕鬆一點
好 那以上呢
就是跟大家介紹的五個技術
那前面四個技術
都是讓 Network 可以變小
那這四個技術呢
它們並不是互斥的
其實你今天要在做 Network 壓縮的時候
你其實沒有什麼道理只做一個技術
你可以既用 Network 的 Architecture
也做 Knowledge Distillation
你還可以在做完 Knowledge Distillation 以後
再去做 Network Pruning
你還可以在做完 Network Pruning 以後
再去做 Parameter 的 Quantization
如果你今天真的想要
把 Network 壓縮到很小的話
這些方法並不是互斥的
它們都是可以一起被使用的
好 那以上呢
就是有關 Network Compression 的介紹
那我們其實在這邊呢
到這邊 Network Compression 呢
就講到一個段落
那來看看大家有沒有問題要問的
