那我猜剛才 Policy Gradient 的這個部分
你其實沒有聽得太懂
就算你聽得懂的式子
你也不知道要如何實作
好，所以我們來講一下，如果是實際實作的時候
你到底是怎麼做的
怎麼做呢？
首先，這整個大的 picture 是這樣
我們先有一個 actor，它的參數是 θ
一開始，你就先 random initialize 就好了
接下來，你有了這個初始的 actor, θ 以後
你就拿這個初始的 actor, θ 去玩 N 次遊戲
那玩 N 次遊戲，你就收集到 N 個 trajectory
那假設你收集到一個 τ^1 (trajectory 1)
那這個 τ^1 裡面有 state 1
這個 state 1 採取了 action a1
我把我的雷射筆叫出來
state 2 採取了 action a2
以此類推
然後，玩完這個遊戲以後呢
你可以算出一個 total reward, R(τ)
那有一個 τ^1，在 τ^2 裡面
你也有 τ^2 的 state 1
還有在 state 1 採取 action a1
你有 τ^2 跟 state 2，你有在 state 2裡面採取 action a2
你要可以算出 τ^2 的 reward
好，那這個東西都沒有什麼問題
你有一個 policy，random 初始的，去
這個實作上沒有什麼問題
你把 state action 都記錄下來
沒有什麼問題
好，那有了這一些 data 以後
你就拿這一些 data 去 update 你的參數，θ
怎麼拿這些 data 去 update 你的 參數 θ 呢
就用剛才我們在前一頁投影片裡面
推出來的這個式子
那等一下，我們會再詳細解釋一下這個式子
到底實作上，你要怎麼 implement
現在看起來有點複雜
或是，式子你懂，實際上要怎麼做
你也許不太清楚
那沒關係，反正就是，我們收集，我們等一下再解釋
反正我們就是收集到一堆 data 以後
我們可以拿這一些 data 去 update 我們的參數
update 完參數以後
你有了一個新的 actor
你有了一個新的 actor 以後
你再去玩 N 次遊戲
因為 actor 是新的
它跟之前的 actor 不一樣了
所以在玩 N 次遊戲的時候
你可能會得到不太一樣的分布
你會得到一個不太一樣的結果
你再把這個結果收集起來
再去調你的 model
然後，有了新的 model 以後再去跟環境互動 N 次
再收集資料，再調你的 model
就這樣，陷入一個循環這樣
所以我想，這個應該是很清楚的，對吧
這個大家有問題嗎 ？
如果大家沒有問題的話
接下來的問題就是
這一項， 到底是什麼意思
這個 summation over 所有的 trajectory 你知道
summation over 某一個 trajectory 所有的 time step
這個意思，你也知道
R(τ) 你也知道
就這個常數項你知道
但是，∇ log p( a(上標 n, 下標 t) | s(上標n, 下標t) )
到底是什麼，也許你有點困惑
那我們來考慮另外一個 case
我們假設，我們現在要做的是一個分類的問題
我們現在有一個 network，我們現在有一個 actor
我們把這個 actor 當作是一個 classify
這個 classify 做的事情是
given 一個畫面 S
它分類說，我們現在應該要採取哪一個 action
現在有 3 個可以採取的 action
就是說，現在是一個有 3 個類別的
3 個類別的，分類的問題
那我們說，我們在做分類的時候
你要 train 一個 classifier
你要有 labeled data， 你要給你的 network 一個 target
你要給你的 network 一個目標
那我們就說，現在的目標是 1、0、0
也就是 1，left 是正確的類別
right 跟 fire 是錯誤的類別
那我們把 network 的 output 叫做 yi
把 target 叫做 yi\head
那你記不記得
我們說，在做 classification problem 的時候
我們 minimize 的是什麼？
我們 minimize 的是 cross entropy
對不對
你記不記得，我們說
我們今天要 train 一個
可以拿來做 classification network 的時候
如果，我們在做 Regression 的時候
我們都做 minimize 的就是 cross entropy
那 cross entropy 就是
summation over 每一個 dimension、 summation over 所有的 class
然後，把 yi\head * log(yi)，前面取負號
但是因為多數的 yi\head 都是零
這邊 y2\head 跟 y3\head 都是 0
只有 y1\head 是 1
所以，實際上我們在做的事情
這邊本來是一個負號加 minimize
負號加 minimize
可以看作是 maximize
所以，我們實際上在做的事情就是
maximize log(yi)
實際上做的事情就是 maximize log(yi)
那我們知道說
所謂的 yi ，其實就是 P("left"|s)
所謂的 y1
這邊我該把它寫 y1 比較對
因為今天在這個 specific 的例子裡面，y1\head 是 1
所以，只有 log(y1) 會被留下
所以，這項應該是 log(y1)
log(y1) 其實就是 log[P("left"|s)]
就是這個 network output
這個 network 覺得要採取 action left 的機率
如果你今天要做的事情是
minimize 這個 cross entropy
或者是
同等的 maximize下面這一項
你會怎麼做呢？
你是不是就說
這是我們的 objective function
我要去 maximize 它
我當然就是對它算一個 gradient
所以，我們就對 log[P("left"|s)]
求它的 gradient
再乘上 learning rate
再加給你的參數
再用這項去 update 你的參數
所以當你看到你 update 的式子裡面
有這個項的時候
當你看到你 update 的式子裡面
有這個 log[P("left"|s)] 的時候
它的意思，其實是說
我們希望這一個 objective function 越大越好
或這個 objective function 越小越好
或者是說，我們其實是在解一個分類的問題
然後，我們希望我們 network 的 output
跟我們訂下來的 target 越接近越好
而在我們的 target 裡面
就是left，就是正確的類別
而其他的是錯誤的類別
這樣大家了解我的意思嗎？
假如你可以了解這個意思的話
那接下來，我們怎麼解釋這個式子呢？
怎麼解釋這個式子呢？
我們先把 R 拿掉
我們先把 R(τ^n) 拿掉，當作那一項等於 1
你就當作所有的 trajectory reward 都是 1
就不要管它了
那這個 ∇ log[P("left"|s)] 是什麼意思
它的意思就是說
我們現在 training data裡面有一個 s1、a1
有一個 s1，a1
那我們假設說 a1就是 left
那我們要做的事情就是
我們把 s1 丟到 network 裡面
它給我們 left、right 跟 fire 的機率
那我們希望這個機率跟 1、0、0 越接近越好
因為現在的 a1 是 left
a1 是 left
所以我們希望
所以就告訴我們說 left 是正確的
我們希望 left 的分數越大越好
然後，right 跟 fire 的分數，越接近 0 越好
那今天如果我們有一個 state 2
那如果今天
我們再另外一個 trajectory 裡面
我們有一個 s (上標 2, 下標 1)
那把 s (上標 2, 下標 1)
也丟到那個 network 裡面
這樣我們假設在
這個 s1 在 trajectory 的第一個 state 的時候
我們採取的 action 是 fire 的話
那意思就是說
我們希望 fire 的分數越大
我們希望這個 fire 的分數是 1
其他的分數是 0
所以，這就變成了一個分類的問題
你有發現嗎？這其實就是一個分類的問題
你就實際上，我們在 update 這個 式子的時候
我們真正在做的事情是
我們希望說
等於是 machine 告訴我們說
現在有一筆 training data
它的 input 就是這個樣子
它的 target 就是這個樣子
它的 input 就是這個樣子
它的 target 是這個樣子，input 就是這樣子
它的 target 就是這個樣子
然後，你把這個分類問題做對
之前，machine 看到 s1 的時候
它就採取，它採取 a1 這個 action
今天 machine learning 的目標
就是一樣， 今天 machine learning 的目標就是
希望在 input s1 的時候
它的目標就是要採取 action a1
那你可能會想說
你就跟 machine 原來做的事情是一樣的嗎 ?
machine 本來就 input s1，就會採取 a1
那你說你是要 learn 一個 network，它的目標就是
input s1，output 就是 target a1
那不就跟原來的 network
原來的 actor 做的事情是一樣的嗎
但是有一個不一樣的地方就是
我們前面有了 reward
我們會把每一個 example
在我們假設把它當成一個分類問題的話
我會把這個分類問題的每一個 example
前面都乘上 R(τ^n)
每一個分類問題前面
我們都乘上 R(τ^n)
那這件事情到底是什麼意思呢？
把每一筆的 training data 都乘上 R(τ^n)
到底是什麼意思呢？
如果簡單的想
假設現在 R(τ^1) 的值是 2
假設 R(τ^2) 的值是 1
我們說，把每一筆 training data 前面
都 weighted by R(τ^n)
意思就是說
我們現在把 input s (上標1, 下標1)
output 要是 left 這件事情
這個 example 複製兩次
因為它的 reward 是 2，所以就複製兩次
那 input s (上標 2, 下標 1)
output fire 這件事情
它的 reward 只有 1
所以我們就複製一次
然後 ，拿這個東西去 train 我們的 network
然後你就可以 update 一次你的 network 參數
然後再重新去 sample 得到
再重新 sample 得到新的 training data
再用這個方法去 update 參數
再去 sample data，再去 update 參數
就結束了這樣
這樣有沒有覺得很簡單呢
這樣有沒有覺得實做很簡單呢
你唯一需要的
你唯一需要的， 你只有你需要在 learn classify 的時候
給你的 training 的 example
給它 weight
你唯一需要改程式的
只有這個部分
那甚至 Keras，如果我沒有記錯的話
它其實有支援這個功能
那其他部分
你根本就不用就改了你的 code
這樣大家了解我的意思嗎？
所以，其實有人會覺得說
Reinforcement learning 很難
沒有很難
你不用改 code 就可以做 Reinforcement learning
只是很花時間，可能就是真的
因為，你每一次收集完 data 以後
你都要解一次分類的問題
train 一次 neural network
然後再去收集 data ，再 train 一次 neural network
跟我們之前做的分類的問題都不一樣
之前做分類問題
data 收集好，就在那一邊 train 完一次
就結束了
但是，在 Reinforcement learning 問題裡面
等於你要 train 你的 network 很多次
