臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw/
好，各位同學大家早
那我們就開始上課吧！
今天的第一堂課，我們要講的是 Gradient Descent
Gradient Descent 我們上次已經大概講過怎麼做了
但是有一些小技巧呢，你可能是不知道的
所以我們要再詳細說明一下，Gradient Descent 你要怎麼把它做得更好
好，那我們上次是這樣說的
在整個 machine learning 的第3個步驟
我們要找一個最好的 function
那找一個最好的 function 這件事呢
是要解一個 optimization 的 problem
也就是說，在第二步， 我們先定了一個 loss function： L
這個 loss function 呢，它是一個 functional unction
你把一個 function 代到這個 loss function 裡面
或者是你把一個操控 function 形狀的參數
我們現在，在這張投影片裡面，把那些參數寫成 θ
L 是 loss function，θ 是那些參數
你把一組參數代到一個 loss function 裡面
它就會告訴你說，這組參數有多不好
那你接下來，要做的事情呢，就是
從一開始，我就發現這張投影片，有一個地方寫錯了
這裡應該是 minimum 啦
因為它是 loss function 嘛！
那 loss function 我們是希望它越小越好
當然你也可以反過來定義一個 function 是
參數越好，它 output 的值越大
那這時候就不會叫它 loss function，就會叫他別的名字
比如說，叫它 objective function
好，這邊應該是 minimum
我們要找一組參數 θ，讓這個 loss function 越小越好
那這件事情怎麼做呢？
我們可以用 Gradient Descent
假設現在這個 θ 是一個參數的 set
那裡面有兩個參數：θ1 跟 θ2
首先，你先隨機的選取一個起始的點
隨機選取一組起始的參數
這邊寫成 θ(上標0, 下標1)、θ(上標0, 下標2)
用上標 0 來代表說，它是初始的那一組參數
那用下標代表說
這個是這一組參數裡面的 第 1 個 component 跟第 2 個 component
接下來，你計算 θ(上標0, 下標1) 跟 θ(上標0, 下標2)
對這個 loss function 的偏微分
計算他們的偏微分，然後把這個
θ(上標0, 下標1)、θ(上標0, 下標2) 減掉 learning rate
乘上這個偏微分的值，得到一組新的參數
這個參數，這邊寫做 θ(上標1, 下標1) 跟 θ(上標1, 下標2)
θ 上標1，代表說它是在第二個時間點
由 θ 上標0 update 以後得到的參數
下標代表說，它有兩個 component
那同樣的步驟，你就反覆不斷地進行
接下來你有 θ(上標1, 下標1) 跟 θ(上標1, 下標2) 以後呢
你就一樣去計算它們的偏微分
再乘上 learning rate，
再去減 θ(上標1, 下標1) 跟 θ(上標1, 下標2)
得到下一組參數，每個反覆進行這個 process
這個就是 Gradient Descent
那如果你想要也得更簡潔一點的話呢
其實你可以這樣寫
假設你現在有兩個參數：θ1 跟 θ2
這個應該是 L，不好意思， 這個 C 呢，這邊應該是寫成 L
你把這個 θ1 跟 θ2 對這個 loss function 做偏微分
你把這兩個偏微分得到的值串在一起， 變成一個 vector 以後
那這個 vector 呢，就叫做 Gradient
你把 L 前面，加一個倒三角形
這個東西呢，就叫做 Gradient
那它其實是一個 vector
所以你可以把這個式子寫成
你把這個東西，寫成一個 vector，θ 上標 1
這個東西，寫成一個 vector，θ 上標 0
這一項就是這一項
那你就寫成呢，L 在 (θ上標0) 這個地方的 Gradient
所以你可以把 update 參數呢
簡單寫成 (θ上標0) 減掉 learning rate
乘上 Gradient，就等於(θ上標1)
同理，(θ上標1) 減掉 learning rate 乘上 Gradient， 就得到 (θ上標2)
如果把它 visualize 的話呢
它看起來像是這個樣子
假設我們現在有兩個參數，θ1、θ2
那你隨機的選一個初始的位置，θ^0
然後接下來呢，你計算在 θ^0 這個點
這個參數對 loss function 的 Gradient
假設參數對 loss function 的 Gradient 是這個紅色的箭頭
Gradient 就是一個 vector，它是一個紅色的箭頭
那，如果你不知道 Gradient 是甚麼的話呢
你就想成，它是等高線的法線方向
這個箭頭，它指的方向就是
如果你把 loss 的等高線畫出來的話呢
這個箭頭指的方向，就是等高線的法線方向
那怎麼 update 參數呢？
你就把這個 Gradient 乘上 learning rate
然後再取一個負號
Gradient 乘上 learning rate，然後再取一個負號
就是這個藍色的箭頭
再把它加上 θ^0 ，就得到 θ ^1
那這個步驟就反覆地持續進行下去
再計算一遍 Gradient
你得到另外一個紅色的箭頭
紅色箭頭指向這個地方
那你現在走的方向呢，就變成是紅色的箭頭的相反
紅色箭頭乘上一個負號，再乘上 learning rate
就是現在這個藍色的箭頭 ，得到 θ^2
這個步驟就反覆一直進行下去
再算一下在 θ^2 這個地方的 Gradient
然後再決定要走的方向
再算一次 Gradient，再決定要走的方向
這個就是 Gradient Descent
這些，我們上次其實都講過了
接下來呢，要講一些 Gradient Descent 的 tip
首先，第一件事情就是
你要小心地調你的 learning rate
如果你已經開始做作業一的話呢 ，你會知道說
有時候，learning rate 是可以給你造成一些問題的
舉例來說，假設這個是我們的 loss function 的 surface， 假設長這樣子
如果你今天 learning rate 調剛剛好的話
你從左邊這邊開始，那你可能就是順著紅色的箭頭
很順利地走到了最低點
可是，如果你今天 learning rate 調的太小的話
那它走的速度呢，會變得非常慢
雖然只要給它夠多的時間，它終究會 走到這個 local minimum 的地方
但是如果它走得太慢的話呢
你會沒有辦法接受這件事情
你可能會來不及交作業
如果你今天這個 learning rate 調得稍微大一點
比如說，像綠色這個箭頭的話
那就變成說，它的步伐太大了
它變得像一個巨人一樣，步伐太大了
它永遠沒有辦法走到這個特別低的地方
它都在這個山谷的口上面震盪
它永遠走不下去，那甚至如果
你今天真的把 learning rate 調太大的話
它可能就，一瞬間就飛出去了
結果你 update 參數以後
loss 反而越 update，越大
那其實只有在你的參數是一維或二維的時候
你才能夠把這樣子的圖 visualize 出來
如果你今天是有很多維參數
這個 error 的 surface，在這個高維的空間裡面
你是沒有辦法 visualize 它的
但是，有另外一個東西，你是可以 visualize 的
什麼東西呢？
你可以 visualize 參數的變化對這個 loss 的變化
你可以 visualize 每次參數 update 的時候
loss 的改變的情形
所以，今天如果你 learning rate 設得太小的話
你就會發現說，這個 loss 它下降地非常非常慢
今天如果你 learning rate 調得太大的話
你在左邊這個圖會看到說 loss 先快速地下降
接下來呢，它就卡住了
所以，如果你 learning rate 調得太大的話
你把參數的 update 對 loss 的變化， 做出來會看到的是綠色這條線
你的 loss 很快地下降，但很快地卡住了， 很快地不再下降
那如果你今天 learning rate 是調得太大，你就會發現
你的 loss 就飛出去了，你需要調整它，讓它調到剛剛好
那你才能夠得到一個好的結果
所以你在做 Gradient Descent 的時候
你應該要把這個圖畫出來
沒有把這個圖畫出來，你就會非常非常地卡
有人就說，它就把 Gradient 的程式寫好
那寫好放下去之後開始跑，他去打一場 LOL
然後，打完回來就發現說：結果爛掉啦
然後，他也不知道爛在哪裡這樣子
所以如果你在做 Gradient Descent 的時候
你應該把這個圖畫出來
然後你要先看一下，它前幾次 update 參數的時候
它 update 的走法是怎麼樣
搞不好它，你 learning rate 調太大，它一下子就爆炸了
所以這個時候，你就知道你要趕快調 learning rate
你要確定它呢，是穩定地下降
才能去打 LOL 這樣子
好，但是要調 learning rate 很麻煩
有沒有辦法自動地調 learning rate 呢？
有一些自動的方法可以幫我們調 learning rate
最基本而簡單的大原則是
通常 learning rate 是隨著參數的 update 會越來越小的
為甚麼會這樣呢？
因為當你在剛開始的起始點的時候
它通常離最低點，是比較遠的
所以你步伐呢，要踏大一點
就是走得快一點，才能夠趕快走到最低點
但是，經過好幾次的參數 update 以後呢？
你已經比較靠近你的目標了
所以這個時候呢，你就應該減少你的 learning rate
這樣它能夠收斂在你最低點的地方
舉例來說，你 learning rate 的設法可能是這樣
好，你可以設成說
這個 learning rate 是一個 t dependent 的函數
它是 depend on 你現在參數 update 的次數
在第 t 次 update 參數的時候
你就把你的 learning rate 設成一個 constant η，除以 sqrt (t+1)
這樣當你參數 update 的次數越多的時候呢
這個 learning rate 就會越來越小
但是光這樣呢，是不夠的
你到這邊，我們需要因材施教
所以這每一個不同的參數
最好的狀況應該是，每一個不同的參數
都給它不同的 learning rate
這件事情呢，是有很多的小技巧的
其中，我覺得最簡單，最容易實作的，叫做 Adagrad
那 Adagrad 是這樣子的
他說，每一個參數的 learning rate 呢
都把它除上，之前算出來的微分值的 root mean square
什麼意思呢，我們原來的 Gradient Descent 是這樣
假設 w 是某一個參數
這個時候 w 不是一組參數，我們現在只考慮一個參數
因為我們現在，在做 Adagrad 這個做法的時候
它是 adaptive 的 learning rate
所以，每一個參數，它都有不同的 learning rate
所以呢，我們現在要把每一個參數都分開來考慮
每一個參數都分開來考慮
那 w 呢，是某一個參數
那 w 的 learning rate，在一般的 Gradient Descent 呢
depend on 時間的值，比如說 η^t
但是你可以把這件事情呢，做的更好
在 Adagrad 裡面呢， 你把這個 η^t / σ^t
這個 σ^t 是甚麼呢？ 這個 σ^t 是過去
這邊這個 g 呢，是這個偏微分的值，g 是偏微分的值
這個 σ^t 呢，是過去所有微分的值的 root mean square
是過去所有微分的值的 root mean square
這個值，對每一個參數而言，都是不一樣的
所以現在就會變成說，不同的參數
它的 learning rate 都是不一樣的
那我們實際舉個例子，來看看這件事情是怎麼實作的
假設你現在初始的值，是 w^0
假設你初始的值，是 w^0
那接下來呢，你就計算在 w^0 那點的微分
這邊呢，寫作 g^0
然後，它的 learning rate 是多少呢？
它的 learning rate 是 η^0 / σ^0
η^0 是一個時間 dependent 的參數 ，那 σ^0 是甚麼呢？
σ^0 是一個參數 dependent 的參數
σ^0 它是過去算過所有微分值的 root mean square
那在這個 case 裡面，我們過去只算過 一個微分值，就是 g^0
所以這個 σ^0，就是 g^0 的平方再開根號
那接下來呢，你再 update 參數
你把這個 w^0 更新變成 w^1
在 w^1 這個地方，你再算一次 Gradient，就是 g^1
那 g^1 的 learning rate 應該乘上多少呢？
它要乘 η^1 / σ^1
那 σ^1，它是過去所有微分值的 root mean square
過去我們已經算過兩次微分值
一次是 g^0，一次是 g^1
所以 σ^1，就變成 g^0 跟 g^1 的 root mean square
也就是把 g^0 平方再加 g^1 平方， 再取平均值 ，再開根號
那 w^3 一樣，你就是 update 參數就得到 w^2
你有了 w^2 以後，你可以算 g^2，
在 w^2 地方的微分值就是 g^2
它的 learning rate 就是 η^2 / σ^2
那這個 σ^2呢 ，就是過去 算出來所有微分值的 root mean square
過去算出 g^0, g^1, g^2，你就把 g^0, g^1, g^2 都平方
再平均，然後再開根號
得到σ^2，然後把它放在這邊，搭配參數得到 w^3
這個步驟呢，就反覆地一直繼續下去
到第 t 次 update 參數的時候
你有一個微分直 g^t
那這個 g^t 的 learning rate 就是 η^t / σ^t
這個 σ^t，是過去所有微分的值的 root mean square
過去已經算出 g^0, g^1, g^2 .... 一直到 g^t
你就把 g^0, g^1, g^2 .... 一直到 g^t
都取平方，再加起來，再平均，再開根號
得到 σ^t，就把它放在這邊
所以，現在如果我們用 Adagrad 的時候呢
它 update 參數的式子，寫成這樣子
這個 σ^t，我們就寫成這樣子
這個 root mean square，我們在前頁投影片已經看到的
那這個 time dependent 的 learning rate 呢？
這個 time dependent 的 learning rate， 你可以寫成 η / sqrt(t+1)
那你可以發現說，當你把這一項除以這一項的時候
因為他們都有：根號 (t+1)
所以根號 (t+1) 是可以刪掉的
所以整個 Adagrad 的式子，你就可以寫成
它的 learning rate，你就可以寫成，一個 constant η
除掉 根號(過去所有 gradient 的平方和)
但不用算平均這樣
因為平均這件事情，會跟上面 time dependent 的 learning rate 抵銷掉
所以你在寫 Adagrad 的式子的時候，你可以簡化成
不需要把 time dependent 的這件事情 explicitly 的寫出來
你就直接把你的 learning rate 寫成
η 除以 根號(過去算出來的 gradient 的平方)
再開根號就好
這個方法，你可以接受嗎？
講到這邊，大家有問題嗎？來，請說
在做 Adagrad 的時候，它在後面下降的速度， 慢到令人髮指
慢到令人髮指是嗎？
就是看的過1000筆資料，才降不到0.1
這是正常的嗎？
這其實是正常的
就是說 Adagrad 它的參數 update，其實 整體而言是會是越來越慢的
因為它有加上 time depend
如果你不喜歡這個結果的話
有很多比這個更強的方法
這個 adaptive 的 learning rate 其實是一系列的方法
今天講 Adagrad 其實是裡面最簡單的
還有很多其他的，他們都是用 Ada- 開頭這樣子
比如說 Adadelta, Adam 阿，甚麼之類的
所以，如果你用別的方法
比如說，Adam 的話，它就比較不會有這個情形這樣子
其實，如果你沒有甚麼特別偏好的話
你現在可以用 Adam 啦，它應該是現在我覺得最穩定的
但是它 implement 比較複雜就是了
但其實也沒有什麼啦，好，講到這邊大家有甚麼問題嗎？
好，那我其實有一個問題啦
我其實有一個問題
我們在做一般的 Gradient Descent 的時候
我們這個參數的 update 取決於兩件事情
一件事情是 learning rate，另外一件事情是 Gradient
我們的意思就是說 Gradient 越大，你參數 update 就越快
斜率算出來越大，參數 update 就越快
我相信你可以接受這件事情
但是在 Adagrad 裡面，你不覺得相當矛盾嗎？
你不覺得有某些怪怪的地方
這一項告訴我們，微分的值越大，你參數 update 越快
但是這一項它是相反的，對不對？
當這一項跟這一項，它們卻是不一樣
對不對，你有沒有覺得說，這邊有一些奇怪的地方
也就是說，今天當你的 Gradient 越大的時候
當 Gradient 越大的時候，你底下算出來的這項就越大
你底下算出來的這項越大， 你的參數 update 的步伐就越小
這不就跟我們原來要做的事情是有所衝突的嗎？
在分母的地方告訴我們說
Gradient 越大，踏的步伐越大 ，參數就 update 的越大
但是分母的地方卻說
如果 Gradient 越大，參數 update 的越小這樣
好，怎麼解釋這件事情呢？
有一些 paper 這樣解釋的
這個 Adagrad 它想要考慮的是： 今天這個 Gradient 有多 surprise
也就是所謂的"反差"這樣
反差，大家知道嗎？
就是比如說，反差萌的意思就是說
如果本來一個很兇惡的角色，突然對你很溫柔
你就會覺得它特別溫柔這樣，所以呢，對 Gradient 來說
也是一樣的道理
假設有某一個參數 ，它在第一次 update 參數的時候
它算出來的 Gradient 是 0.001
再來又算 0.001, 0.003, 等等...等等
到某一次呢，它 Gradient 算出來是 0.1
你就會覺得特別大，因為它比之前算出來的 Gradient 都大了 100 倍，特別大
但是，如果是有另外一個參數
它一開始算出來是 10.8, 再來算 20.9, 再來算 31.7
它的 Gradient 平常都很大
但是它在某一次算出來的 Gradient 是 0.1
這時候，你就會覺得它特別小這樣子
所以為了強調這種反差的效果
所以在 Adagrad 裡面呢，我們就把它除以這項
這項就是把過去這些 Gradient 的平方
把它算出來，我們就想要知道說過去 Gradient 有多大
然後再把它們相除，看這個反差有多大這樣
這個是直觀的解釋
那更正式的解釋呢，我有這樣的解釋
我們來考慮一個二次函數，來考慮一個二次函數
這個二次函數呢，我們就寫成這樣子
他只有一個參數，就是 x
如果我們把這個二次函數，對 x 做微分的話
把 y 對 x 做微分，這個國中生就知道，這是 2ax + b
如果它絕對值算出來的話，長這樣子
好，那這個二次函數的最低點在哪裡呢？
是 -(b/2a)，我國中就被過這個式子了
如果你今天呢，在這個二次函數上，
你隨機的選一個點開始 ，你要做 Gradient Descent
那你的步伐多大，踏出去是最好的？
假設這個起始的點是 x0
最低點是 -(b/2a)
那踏出去一步，最好的步伐，
其實就是這兩個點之間的距離
因為如果你踏出去的步伐，是這兩個點之間的距離的話
你就一步到位了
這兩個點之間的距離是甚麼呢？
這兩個點之間的距離，你整理一下，
它是 |2a x0 + b| / 2a
|2a x0 + b| 這一項，就是這一項
2a x0 + b 就是 x0 這一點的微分
x0 這一點的一次微分
所以 Gradient Descent 你不覺得說聽起來很有道理
就是說，如果我今天算出來的微分越大
我就離原點越遠
如果踏出去的(我最好的)步伐，是跟微分的大小成正比
如果踏出去的步伐跟微分的大小成正比
它可能是最好的步伐
但是，這件事情只有在，只考慮一個參數的時候才成立
如果我們今天呢，同時有好幾個參數
我們要同時考慮好幾個參數的時候
這個時候呢，剛才的論述就不見得成立了
也就是說，Gradient 的值越大就跟最低點的距離越遠
這件事情，在有好多個參數的時候 ，是不一定成立的
比如說，你想看看，我們現在考慮 w1 跟 w2 兩個參數
這個圖上面的顏色，是它的 loss
那如果我們考慮 w1 的變化
我們就在藍色這條線這邊切一刀
我們把藍色這條線切一刀，
我們看到的 error surface 長得是這個樣子
如果你比較圖上的兩個點，a 點跟 b 點
那確實 a 點的微分值比較大，那它就距離最低點比較遠
但是，如果我們同時考慮幾個參數
我們同時考慮 w2 這個參數
我們在綠色的這條線上切一刀
如果我們在綠色這條線上切一刀的話
我們得到的值是這樣子
我們得到的 error surface 是這樣子
它是比較尖的，這個谷呢，是比較深的
因為你會發現說，w2 在這個方向的變化是比較猛烈的
如果我們只比較在 w2 這條線上的兩個點 , c 跟 d 的話
確實 c 的微分比較大
所以，它距離最低點是比較遠的
但是，如果我們今天的比較是跨參數的話
如果我們比較 a 這的點對 w1 的微分
c 這個點對 w2 的微分
這個結論呢，就不成立了
雖然說，c 這個點對 w2 的微分值是比較大的
這個微分值是比較小的
但 c 呢，是離最低點比較近的，而 a 是比較遠的
所以，當我們 update 參數
當我們 update 參數選擇跟微分值成正比
這樣的論述是在，沒有考慮跨參數的條件下
這件事情才成立的
當我們要同時考慮好幾個參數的時候呢
我們這樣想呢，就不足夠了
所以，如果我們今天要同時考慮好幾個參數的話
我們應該要怎麼想呢？
如果你看看，我們說的最好的 step 的話
我們看最好的這個 step
它其實還有分母這一項 ，它的分母這一項呢，是 2a
這個 2a 哪來的呢？這個 2a 是甚麼呢？
這個 2a 呢，如果我們今天把這個 y 做2次微分
我們做一次微分得到這個式子
那如果我們做二次微分的話，就得到 2a
那它是一個 constant
這個 2a 呢，就出現在最好的 step 的分母的地方
所以，今天最好的 step，它不只是要正比於一次微分
它同時要和二次微分的大小成反比
如果你二次微分比較大
這個時候你參數 update 量應該要小
如果二次微分小的話，你參數 update 量應該要比較大
所以，最好的 step 應該要把二次微分考慮進來
所以，如果我們今天把二次微分考慮進來的話
你會發現說，在 w1 這個方向上
你的二次微分是比較小的
因為這個是一個比較平滑的弧
所以這個二次微分是比較小的
在 w2 的方向上
這個是一個比較尖的弧、比較深的弧
它是一個比較尖的弧，所以它的二次微分是比較大的
所以你光比較 a 跟 c 的微分值呢 ，是不夠的
你要比較 a 的微分值除掉它的二次
跟 c 的微分值除掉它的二次，再去比
如果你做這件事，你才能夠真正顯示
這些點跟最低點的距離這樣
雖然 a 這個點，它的微分是比較小的
但它的二次也同時是比較小的
c 比較大、二次是比較大的
所以，如果你把二次微分的值呢，考慮進去
做這個評檢、做調整的話
那你這個時候，才能真正反映， 你現在所在位置跟最低點的距離
好，那這件事情跟 Adagrad 的關係是甚麼呢？
如果你把 Adagrad 的式子列出來的話
你把 Adagrad 的式子列出來的話
它參數的 update 量是這個樣子的
η 是一個 constant，所以我們就不理它
這個 g^t 阿，它就是一次微分，對不對
下面這個，過去所有微分值的平方和開根號
神奇的是，它想要代表的是二次微分
那你可能會問說，怎麼不直接算二次微分呢？
你可以直接算二次微分
確實可以這麼做，也有這樣的方法， 而且你確實可以這麼做
但是，有時候你會遇到的狀況是
你在作業一裡面是比較簡單的 case
相信你都秒算，秒給你結果
但是，有時候你參數量大、data 多的時候
你可能算一次微分就花一天這樣子
然後你再算二次微分，你要再多花一天
有時候，這樣子的結果是你不能承受的
而且你多花一天 performance 還不見得會比較好
其實這個結果，是你不能承受的
所以，Adagrad 它提供的做法就是
我們在沒有增加任何額外運算的前提之下
想辦法能不能夠做一件事情
去估一下，二次的微分應該是多少
在 Adagrad 裡面，你只需要一次微分的值
那這個東西我們本來就要算它了
所以並沒有，多做任何多餘的運算
好，怎麼做呢？
如果我們考慮一個二次微分比較小的峽谷
跟一個二次微分比較大的峽谷
然後我們把它的一次微分的值，考慮進來的話
這個是長這樣
如果你只是在，這個區間和這個區間裡面
隨機 sample 一個點，算它的一次微分的話
你看不出來它的二次微分值是多少
但是如果你 sample 夠多點
你在某一個 range 之內，sample 夠多點的話
那你就會發現說，在這個比較平滑的峽谷裡面
它的一次微分通常就是比較小的
在比較尖的峽谷裡面，它的一次微分通常是比較大的
而 Adagrad 這邊，這一件事情
summation over 過去所有的微分的平方，這件事情
你就可以想成，在這個地方呢，做 sampling
就在這個地方呢，做 sampling
那你再把它的平方和呢，再開根號算出來
那這個東西，就反映了二次微分的大小
這個 Adagrad 怎麼做，其實我們上次已經有示範過了
那所以我們就不再示範
接下來我們要獎的另外一件事情呢， 是 Stochastic 的 Gradient Descent
那它可以讓你的 training 呢，更快一點
好，這個怎麼說呢？
我們之前講說，我們的 loss function
它的 loss function，它的樣子呢
如果我們今天做的是這個 Regression 的話
這個是 Regression 的式子
Regression 得到的 estimation 的結果
那你把 Regression 得到 estimation 的結果
減掉 y\head，再去平方
再 summation over 所有的 training data
這是我們的 loss function
所以，這個式子非常合理
我們的 loss 本來就應該考慮所有的 example
它本來就應該 summation over 所有的 example
有這些以後，你就可以去算 Gradient
然後你就可以做 Gradient Descent
但 Stochastic Gradient Descent，他的想法不一樣
Stochastic Gradient Descent 它做的事情是
每次就拿一個 x^n 出來
這邊你可以隨機取，也可以按照順序取
那其實隨機取的時候
如果你今天是在做 deep learning 的 case
也就是說你的 error surface 不是 convex
是非常崎嶇的，隨機取呢，是有幫助的
總之，你就取一個 example 出來
假設取出來的 example 是 x^n
這個時候呢，你要計算你的 loss
你的 loss 呢，只考慮一個 example
你只考慮你現在的參數，對這個 example 的 y 的估測值
再減掉它的正確答案，再做平方
然後就不 summation over 所有的 example
因為你現在只取一個 example 出來
你只算某一個 example 的 loss
那接下來呢，你在 update 參數的時候
你只考慮那一個 example
我們只考慮一個 example 的 loss function，我們就寫成
L^n，代表它是考慮第 n 個 example 的 loss function
那你在算 Gradient 的時候呢？
你不是算對 total 所有的 data，它的 Gradient 的和
你只算對某一個 example，它的 loss 的 Gradient
然後呢，你就很急躁的 update 參數了
所以在原來的 Gradient Descent 裡面， 你計算所有 data 的 loss
然後才 update 參數
但是在 Stochastic Gradient Descent 裡面
你看一個 example，就 update 一個參數這樣
你可能想說，這有啥好呢？
聽起來好像沒有甚麼好的
那我們實際來操作一下好了
剛才看到圖呢，它可能是這個樣子的
我們剛才看到的圖呢，它可能是這個樣子
原來的 Gradient Descent，你看完所有的 example 以後
你就 update 一次參數
那它其實是比較穩定
你會發現說，它走的方向
就是按照 Gradient 建議我們的方向呢，來走
但是如果你是用 Stochastic Gradient Descent 的話
你每看到一個 example ，你就 update 一次參數
如果你有 20 個 example 的時候
那你就 update 20 次參數
那這邊他是看完 20 個 example 才 update 一次參數
這邊是，每一個 example 都 update 一次參數
所以在它看 20 個 example 的時候
你這邊也已經看了 20 個 example， 而且 update 20 次參數了
所以 update 20 次參數的結果呢，看起來就像是這樣
從一樣的起始點開始，但它已經 update 了 20 次參數
所以，這個如果只看一個 example 的話
它的步伐是小的
而且可能是散亂的
因為你每次只考慮一個 example
所以它參數 update 的方向，跟這個 Gradient Descent
total loss 的 error surface 界定我們走的方向
不見得是一致的，但是因為我們可以看很多個 example
所以天下武功，為快不破。在它走一步的時候
你已經出 20 拳了，所以它走的反而是比較快的
然後呢，接下來我們要講的是第三個 tip
就是你可以做 Feature 的 Scaling
所謂的 Feature Scaling 意思呢是這樣子
假設我們現在要做 Regression
那我們這個 Regression 的 function 裡面
input 的 feature 有兩個，x1 跟 x2
比如說，如果是要 predict 寶可夢進化以後 CP 值的話
那 x1 是進化前的 CP值，x2 是它的生命值...等等這樣
你有兩個 input feature, x1 跟 x2
那如果你看你今天的 x1 跟 x2
它們分佈的 range 是很不一樣的話
那就建議你呢，把它們做 scaling
把它們的 range 分佈變成是一樣
比如，這邊的 x2 它的分佈是遠比 x1 大
那就建議你把 x2 這個值呢，做一下 rescaling
把它的值縮小，讓 x2 的分佈跟 x1 的分佈是比較像的
你希望不同的 feature，他們的 scale 是一樣的
為甚麼要這麼做呢？
我們舉個例子
假設這個是我們的 Regression 的 function
那我們寫成這樣，這邊這個意思跟這個是一樣的啦
y = b + w1*x1 + w2*x2
y = b + w1*x1 再加 w2*x2
那假設 x1 平常的值，都是比較小的，假設說 1, 2 之類的
假設 x2 它平常的值都很大
它 input 的值都很大，100, 200 之類的
那這個時候，如果你把 loss 的 surface 畫出來
會遇到甚麼樣的狀況呢？
你會發現說，如果你更動 w1 跟 w2 的值
假設你把 w1 跟 w2 的值都做一樣的更動
都加個 △w ，你會發現說
w1 的變化，對 y 的變化而言是比較小的
w2 的變化，對 y 的變化而言是比較大的
對不對，這件事情是很合理的
因為你要把 w2 乘上它 input 的這些值
你要把 w1 乘上它 input 的這些值
如果 w2 它乘的這些 input 的值是比較大的
那只要把 w2 小小的變化，那 y 就會有很大的變化
那同樣的變化，w1 它 input 的值是比較小的
它對 y 的影響呢，就變成是比較小的
所以如果你把他們的 error surface 畫出來的話呢
你看到的可能像是這個樣子
所以如果你把他們的 error surface 畫出來的話呢， 你看到的可能像是這樣
這個圖，是甚麼意思呢？
因為 w1 對 y 的影響比較小
所以 w1 就對 loss 的影響比較小
所以 w1 對 loss 是有比較小的微分的
所以 w1 這個方向上，它是比較平滑
w2 對 y 的影響比較大，所以它對 loss 的影響比較大
所以改變 w2 的時候，它對 loss 的影響比較大
所以，它在這個方向上，是比較 sharp 的
所以這個方向上，有一個比較尖的峽谷
那如果今天，x1 跟 x2 的值，它們的 scale 是接近的
那如果你把 loss 畫出來的話呢
它就會比較接近圓形
w1 跟 w2 呢，對你的 loss 是有差不多的影響力
這個對做 Gradient Descent 會有甚麼樣的影響呢？
是會有影響的
比如說，如果你從這個地方開始
其實我們上次已經有看到了，就是這樣
這種長橢圓的 error surface 阿
如果你不出些 Adagrad 甚麼的，你是很難搞定它的
因為就在這個方向上，和這個方向上
你會需要非常不同的 learning rate
你同一組 learning rate 會搞不定它
你要 adaptive learning 才能夠搞定它
所以這樣子的狀況，沒有 scaling 的時候， 它 update 參數是比較難的
但是，如果你有 scale 的話，它就變成一個正圓形
如果是在正圓形的時候 ，update 參數就會變得比較容易
而且，你知道說 Gradient Descent 它並不是向著最低點走
在這個藍色圈圈，它的最低點是在這邊
綠色圈圈最低點是在這邊
但是你今天在 update 參數的時候，走的方向是順著
等高線的方向，是順著 Gradient 箭頭的方向
所以，雖然最低點在這邊
你從邊開始走，你還是會走這個方向，再走進去
你不會只向那個最低點去走
那如果是綠色的呢，綠色的又不一樣
因為，它如果真的是一個正圓的話
你不管在這個區域的哪一個點
它都會向著圓心走
所以，如果你有做 feature scaling 的時候
你在做參數的 update 的時候呢
它是會比較有效率的
那你可能會問說，怎麼做 scaling
這個方法有千百種啦
你就選一個你喜歡的就是了
那常見的作法是這樣
假設我有 r 個 example, x^1, x^2 到 x^R
每一筆 example，裡面都有一組 feature
x^1 它第一個 component 就是 x(1,1)
x^2 它第一個 component 就是 x(2,1)
x^1 它第二個 component 就是 x(1,2) x^2 它第二個 component 就是 x(2,2)
那怎麼做 feature scaling？
你就對每一個 dimension i
都去算它的 mean，這邊寫成 m_i
都去算它的 deviation，這邊寫成 σ_i
然後呢，對第 r 個 example 的第 i 個 component
對第 r 個 example 的第 i 個 component
你就把它減掉，所有的 data 的 第 i 個 component 的 mean，也就是 m_i
你就把它減掉所有的 data 的 第 i 個 component 的 mean
再除掉所有的 data 的第 i 個 component 的 standard deviation
然後呢，你就會得到說
你做完這件事以後
你所有 dimension 的 mean 就會是 0
你的 variance 就會是 1
這是其中一個常見地做 localization 的方法
最後，在下課前呢，我們來講一下
為甚麼 Gradient Descent 它會 work
Gradient Descent 背後的理論基礎是什麼
那在真正深入數學部分的基礎之前呢
我們來問大家一個問題
大家都已經知道 Gradient Descent 是怎麼做的
假設，我問你一個這樣的是非題
每一次，我們在 update 參數的時候
我們都得到一個新的 θ
這個新的 θ，總是會讓我們的 loss 比較小
這個陳述，是對的嗎？
好，也就是意思就是說 θ_0 你把它代到 L 裡面
它會大於 θ_1 代到 L 裡面，它會大於 θ_2 代到 L 裡面
每次 update 參數的時候， 這個 loss 的值，它都是越來越小的
這陳述，是正確的嗎？
你覺得它是正確的同學舉手
那你覺得這個陳述，它是不對的同學舉手
好，手放下
大家的觀念都很正確，沒錯
就是 update 參數以後，loss 不見得會下降的
所以如果你今天自己 implement Gradient Descent
做出來，update 參數以後的 loss 沒有下降
那不見得是你的程式有 bug
因為，本來就有可能發生這種事情
我們剛已經看過說，如果你 learning rate 調太大的話
會發生這種事情
或許，我們可以在下課前，做一個 demo
好，那在解釋 Gradient Descent 的 Theory 之前
這邊有一個 Warning of Math ，意思就是說
這個部分，就算是你沒有聽懂，也沒有關係
太陽明天依舊會升起
好，那我們先不要管 Gradient Descent
我們先來想想看， 假如你要解一個 Optimization 的 problem
你要在這一個 figure 上面，找他的最低點
你到底應該要怎麼做？
那有一個這樣子的作法
如果今天給我一個起始的點 ，也就是 θ_0
我們有方法，在這個起始點的附近
畫一個圓圏、畫一個範圍、畫一個紅色圈圈
然後，在這個紅色圈圈裡面
找出它的最低點
比如說，紅色圈圈裡面的最低點，就是在這個邊上
這個意思就是說，如果你給我一整個 error function
我沒有辦法，馬上一秒鐘就告訴你說
我沒有辦法馬上告訴你說，它的最低點在哪裡
但是如果你給我一個 error function，加上一個初始的點
我可以告訴你說，在這個初始點附近，畫一個範圍之內
哦，有問題是嗎？
謝謝，謝謝，沒有問題
我們可以在那個附近，找出一個最小的值
那你假設找到最小的值以後
我們就更新我們中間的位置
中間的位置挪到 θ_1
接下來呢，再畫一個圓圈
我們可以在這個圓圈範圍之內
再找一個最小的點
假設呢，它是落在這個地方
然後，你就再更新中心點的參數
到 θ_2 這個地方
然後，你就可以再找小小範圍內的最小值
然後，再更新你的參數，就一直這樣下去
好，那現在的問題就是
怎麼很快的在紅色圈圈裡面
找一個可以讓 loss 最小的參數呢？
怎麼做這件事呢？
這個地方要從 Taylor series 說起
假設你是知道 Taylor series 的，那個微積分有教過
Taylor series 告訴我們什麼呢？
它告訴我們說，任何一個 function h(x)
如果它在 x = x_0 這點呢
是 infinitely differentiable
那你可以把這個 h(x) 寫成以下這個樣子
你可以把 h(x) 寫成
Σ(k=0, ∞)，這裡 k 代表微分的次數
(h 在 x_0 微分 k 次以後的值) / k!
然後 (x-x_0)^k
不過，把它展開的話，你可以把 h(x) 寫成 h(x_0)
+ h'(x_0) * (x - x_0)
+ h''(x_0) * (x - x_0)^2
那當 x 很接近 x_0 的時候
當 x 很接近 x_0 的時候
(x - x_0) 就會遠大於 (x - x_0)^2，就會遠大於 後面的 3次,、4次，到無窮多次
所以，這個時候，你可以把後面的高次項刪掉
所以，當 x 很接近 x_0 的時候
這個只有在 x 很接近 x_0 的時候才成立
h(x) 就可以寫成
h(x_0) + h'(x_0) * (x - x_0)
那這個是只有考慮一個 variable 的 case
那其實，我這邊有個例子
假設 h(x) = sin(x)
那在 x_0 約等於 (π/4) 的地方
sin(x) 你可以寫成什麼樣子呢？
你用計算機算一下，它算出來是這樣子
這個 sin(x)，可以寫成這麼多這麼多這麼多項的相加
那如果我們把這些項，畫出來的話
你得到這樣子，一個結果
如果是 1/sqrt(2)，只有考慮 0 次的話
是這條水平線
考慮 1/sqrt(2) + (x-π/4)/sqrt(2)
考慮一次的話，是這條斜線
如果你有再把 2 次考慮進去，考慮 0 次, 1 次, 2 次的話
我猜你得到的，可能是這條線
如果你再把 3 次考慮進去的話，你得到這條線
如果你再把 4 次考慮進去的話，你可能得到橙色這條線
但是，雖然說，比如說如果你看
成色這條線，應該是 sin(x)，不好意思
好，你發現說，如果你只有考慮一次的時候
它其實跟這個 sin(x)，橙色這條線差很多啊，根本不像
但是，它在 (π/4) 2的附近
在這個地方附近，它是像的
因為，如果 x 很接近 (π/4) 的話
那後面這些項，平方項、三次方項這些都很小
所以就可以忽略它們，只考慮一次的部分
那這個 Taylor series 也可以是有好幾個參數的
如果今天有好幾個參數的話
那你就可以這樣子做
這個 h(x, y)，假設這個 function 有兩個參數
它在 x_0 和 y_0 附近
你可以把它寫成呢
這個 h(x, y)，你可以用 Taylor series 把它展開成這樣
就有 0 次的，有考慮 (x - x_0) 的
有考慮 (y - y_0) 的
還有考慮 (x - x_0)^2 跟 (y - y_0)^2 的
如果今天 x, y 很接近 x_0, y_0 的話
那平方項呢，就可以被消掉
就只剩這個部份而已
所以，今天 x, y 如果很接近 x_0, y_0 的話
那 h(x, y) 就可以寫成呢
約等於 h(x_0, y_0) 加上
(x - x_0) * (x_0, y_0) 對 x 做偏微分
(y - y_0) * (x_0, y_0) 對 y 做偏微分
這個偏微分的值，你不要看他這麼複雜
微分的值，它其實就是一個 constant 而已
就是一個常數項而已
這個是一個常數項，這個也是一個常數項
好，那如果我們今天考慮 Gradient Descent 的話
如果我們今天考慮我們剛才講的問題
如果，今天給我一個中心點，這是 a 跟 b
那我畫了一個很小很小的圓圈
紅色的圓圈，假設它是很小的
再這個紅色圓圈的範圍之內
我其實可以把 loss function 用 Taylor series 做簡化
我可以把 loss function, L(θ) 寫成
L(a, b) + θ_1 對 loss function 的偏微分， 在 (a, b) 這個位置的偏微分 ，乘上 (θ_1 - a)
加上 θ_2 對 loss function 在 (a, b) 這個位置 的偏微分，再乘上 (θ_2 - b)
所以在紅色的圈圈內，loss function 可以寫成這樣子
那我們把 L(a,b)，L 用 (a, b) 代進去， 它就是一個 constant，用一個 s 來表示
那 θ_1 對 L 的偏微分
在 (a, b) 這個位置，這也是一個 constant， 所以我們用 u 來表示
這也是一個 constant，所以我們用 v 來表示
這樣這個式子呢，看起來就非常簡單了
所以在這個範圍之內
L 對 θ 跟 θ_1
所以呢，在紅色圈圈範圍內呢
這個式子是非常簡單的
就寫成左下角這個樣子
再來，如果告訴你說，紅色圈圈內的式子都長這個樣子
你能不能秒算， 哪一個 θ_1 跟 θ_2 可以讓它的 loss 最小呢？
我相信你可以秒算這個結果
不過，我們還是很快地稍微看一下
好，L 寫成這樣
s, u, v 都是常數，我們就把它放在藍色的框框那裡面
不用管它值是多少
我們現在的問題，就是找
在紅色的圈圈內呢，找 θ_1 跟 θ_2 讓 loss 最小
那所謂的在紅色圈圈內的意思就是說
紅色圈圈的中心就是 a 跟 b
所以你這個 (θ_1 - a)^2 + (θ_2 - b)^2 ≦ d^2
他們要在這個紅色圈圈的範圍內
這件事情，其實就是秒算對不對
太簡單了，你一眼就可以看出來
如果你今天把 (θ_1 - a) 都用 △θ_1 表示
(θ_2 - b) 都用 △θ_2 來表示
s 你可以不用理它，因為它跟 θ 沒關係啊
所以你要找不同 θ 讓它值最小，不用管 s
好，如果我們看一下 L
你會發現說它是 u * △θ_1 + v * △θ_2
也就是說，它就好像是
它的值就是，有一個 vector ，叫做 (△θ_1, △θ_2)
有另外一個 vector，叫做 (u, v)
你把這個 vector 跟這個 vector 做 inner product
你就把 △θ_1 * u + θ_2 * v
你就得到這個值，如果我們忽略 s 的話， 你就得到這個值
接下來的問題就是，如果我們要讓 L(θ) 最小
我們應該選擇什麼樣的 (△θ_1, △θ_2) 呢？
我們要選擇什麼樣的 (△θ_1, △θ_2)
我們才能夠讓 L(θ) 最小呢？
這個，太容易了，就是選正對面的，對不對？
如果我們今天把 (△θ_1, △θ_2) 轉成跟 (u, v) 這條反方向
然後，再把 (△θ_1, △θ_2) 的長度增長
我們把它轉到反方向，再把它伸長
長到極限，也就是長到這個紅色圈圈的邊緣
那這個 (△θ_1, △θ_2) 跟 (u, v)
它們做 inner product 的時候，它的值是最大的
所以，這告訴我們說
什麼樣的 (△θ_1, △θ_2) 可以讓 loss 的值最小呢？
就是它是 (u, v) 乘上負號
再乘上一個 scale
再乘上一個 constant，也就是說你要把 (△θ_1, △θ_2)
調整它的長度，長到正好頂到這個紅色圈圈的邊邊
這個時候呢，它算出來的 loss 是最小的
這一項應該跟長度是成正比的
所以呢，我們再整理一下式子
△θ_1 就是 (θ_1 - a)，△θ_2 就是 (θ_2 - b)
所以，如果我們今天要再紅色圈圈裡面
找一個 θ_1 跟 θ_2 讓 loss 最小的話
那怎麼做呢？那個最小的值，就是中心點 (a, b)
減掉某一個 constant 乘上 (u, v)
中心點 (a, b) 減掉某一個 constant 乘上 (u, v)
所以，我們就知道了這件事
那你接下來要做的事，就是把 (u, v) 帶進去
把它帶進去，就得到這樣子的式子
那這個式子，你就發現它其實
exactly 就是 Gradient Descent
對不對？我們做 Gradient Descent 的時候，就是找一個初始值
算每一個參數在初始值的地方的偏微分
把它排成一個 vector，就是 Gradient
然後再乘上某一個東西，叫做 learning rate，再把它減掉
所以這個式子，exactly 就是 Gradient Descent 的式子
但你要想想看，我們今天可以做這件事情
我們可以用這個方法，找一個最小值
它的前提是什麼？ 它的前提是
你的上面這個式子，要成立
Maclaurin series給你的這個 approximation 是夠精確的
什麼樣 Taylor series 給我們的 approximation 才夠精確呢？
當你今天畫出來的紅色圈圈夠小的時候
Taylor series 給我們的 approximation 才會夠精確
好，才會夠精確
所以，這個就告訴我們什麼？
這個告訴我們說，你這個紅色圈圈的半徑是小的
那這個 η，這個 learning rate
它跟紅色圈圈的半徑是成正比的
所以這個 learning rate 不能太大
你 learning rate 要很小， 你這個 learning rate 無窮小的時候呢
這個式子才會成立
所以 Gradient Descent，如果你要讓你每次 update 參數的時候
你的 loss 都越來越小的話
其實，理論上你的 learning rate 要無窮小， 你才能夠保證這件事情
雖然實作上，只要夠小就行了
所以，你會發現說，如果你的 learning rate 沒有設好
是有可能說，你每次 update 參數的時候
這個式子是不成立的
所以導致你做 Gradient Descent 的時候， 你沒有辦法讓 loss 越來越小
那你會發現說
這個 L，它只考慮了
Taylor series 裡面的一次式
可不可以考慮二次式呢？
Taylor series 不是有二次、三次，還有很多嗎？
如果你把二次式考慮進來
你把二次式考慮進來
理論上，你的 learning rate 就可以設大一點
對不對，如果我們把二次式考慮進來
可不可以呢？是可以的
那有一些方法，我們今天沒有要講， 是有考慮到二次式的
比如說，牛頓法這樣子
那在實作上，尤其是假設你在做 deep learning 的時候
這樣的方法，不見得太普及，不見得太 practical
為甚麼呢？因為你現在要算二次微分
甚至它還會包含一個 Hessian 的 matrix
和 Hessian matrix 的 inverse，總之，你會多很多運算
而這些運算，在做 deep learning 的時候呢
你是無法承受的
你用這個運算，來換你 update 的時候比較有效率
會覺得是不划算的
所以，今天如果在做，比如說，deep learning 的時候
通常，還是 Gradient Descent 是比較普及、主流的作法
上面如果你沒有聽懂的話，也沒關係
在最後一頁，我們要講的是 Gradient Descent 的限制
Gradient Descent 有什麼樣的限制呢？
有一個大家都知道的是， 它會卡在這個 local minimum 的地方
它會卡在 local minimum 的地方
所以，如果這是你的 error surface
那你從這個地方，當作你的初始值，去更新你的參數
最後走到一個微分值是 0，也就是 local minimum 的地方
你參數的更新，就停止了
但是，一般人就只知道這個問題而已
那其實還有別的問題
事實上，這個微分值是 0 的地方， 並不是只有 local minimum 阿
對不對，settle point 也是微分值是 0
所以，你今天在參數 update 的時候
你也是有可能卡在一個不是 local minimum， 但是微分值是 0 的地方
這件事情，也是有可能發生的
但是，這什麼卡在 local minimum 或微分值是 0 的地方啊
這都只是幻想啦
其實，真正的問題是這樣
你今天其實只要
你想想看，你 implement 作業一了
你幾時是真的算出來，那個微分值 exactly 等於 0 的時候
就把它停下來了
也就是，你最多就做微分值小於 10^(-6)
小於一個很小的值，你就把它停下來了，對不對？
但是，你怎麼知道，那個微分值算出來很小的時候
它就很接近 local minimum 呢？
不見得很接近 local minimum 阿
有可能，微分值算出來很小， 但它其實是在一個高原的地方
而那高原的地方，微分值算出來很小，你就覺得說
哦，那這個一定就是很接近 local minimum
在 local minimum 附近，所以你就停下來了
因為你們真的很少有機會 exactly 微分值算出來是 0 嘛
對不對，你可能覺得說， 微分算出來很小，就很接近 local minimum
你就把它停下來，那其實搞不好，它是一個高原的地方
它離那個 local minimum 還很遠啊
這也是有可能的
講到這邊，有人都會問我一個問題
這個問題，我很難回答，他說
你怎麼會不知道你是不是接近 local minimum 了呢？
我一眼就知道說 local minimum 在這邊阿
呵呵，你怎麼會不知道呢？
所以我覺得這些圖都沒有辦法 表示 Gradient Descent 的精神
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心
