Now we are going to talk about
the optimization part.
So what we are going to talk about basically
has nothing to do with
overfitting.
We only discuss that in optimization,
how to make gradient descent better.
So why does optimization fail?
When you are doing optimization,
you will find that
as you update with your parameters,
your training loss will not decrease anymore.
But you are still not satisfied with this loss.
Like I just said,
you can compare the deep neural network
with the linear model
or with the shallow network.
You'll find that it did not do better.
So you think a deep network
doesn't utilize its full power.
Thus the optimization obviously has problems.
But sometimes, you will even find that
your model can't work well at the beginning.
No matter how you update your parameters,
your loss can't decrease anymore.
What happened?
A common guess in the past was that
because we had reached a place.
The differential of the parameter to the loss in this place was zero.
When your parameters differentiated to loss were zero,
gradient descent had no way to
update parameters again.
Training stopped at this moment.
Your parameters are no longer updated.
Of course, the loss would not decrease anymore.
Now when the gradient is zero,
the first thing that usually comes to people’s minds
may be a local minimum.
So people often say that when doing deep learning
with gradient descent,
you will get stuck in the local minimum.
So gradient descent does not work,
or deep learning doesn’t work.
But if one day you want to write a paper
related to deep learning.
You must not say anything like
getting stuck in a local minimum.
People will think you have no class.
Why?
Because it’s not just local minima
that make the gradient zero.
There are other possibilities that make the gradient zero,
for example, saddle point.
In fact, the saddle point means that
the gradient is zero,
but it's neither a local minimum
nor a local maximum.
It's like the red dot in this example.
It is relatively high in the left and right direction.
The front and back direction is relatively low.
It's like a saddle shape,
so it’s called a saddle point.
Then its Chinese is called a saddle point.
For a place like a saddle point,
its gradient is zero,
but it is not a local minimum.
For the point where the gradient is zero,
we call it a critical point.
So you can say that your loss
has no way to go down.
Maybe it's because it's stuck at a critical point.
But you can't say it's stuck in the local minimum
because the saddle point is also the point where the differential is zero.
Ok, but today if you find your gradient is
really close to zero.
It is stuck at a critical point.
Can we know
whether it is a local minimum
or a saddle point?
The answer is yes. It is possible to determine where we are on the hypersurface.
So why do we want to know
whether we are stuck at a local minium
or a saddle point?
Because if the model is stuck at a local minimum,
our model can not escape from that point
since everywhere nearby is higher.
Your are currently at a position
which is the point with lowest loss nearby,
so the loss will be higher no matter which direction you go,
thus the model doesn't know where to go for the next step.
However, if you are at a saddle point,
then it is all right.
If you are at a saddle point,
there remain some directions you can go towards,
which means it is still possible to make the loss lower.
As long as you are able to escape from the saddle point,
you may be able to improve your model's loss.
So,
it is worth investigating whether
a critical point is
a local minimum
or a saddle point.
So how do you determine whether a critical point
is a local minimum
or a saddle point?
We are going to use a little bit of math here.
But don't be afraid, the math here is not very hard.
It is just simple calculus and linear algebra.
If you failed to understand this part, you can just skip it.
This part has nothing to do with the contents later on.
Okay. So again, how do you determine whether a critical point
is a local minimum
or a saddle point?
To answer this, you need to know the hypersurface shape of the loss function.
But how do we figure out
how the loss function looks like?
The network itself is very complicated.
The loss function, which is calculated from the network,
is also very complicated.
Then, how do we find out
the shape of our loss function?
Although it is hard for us to find out
the entire shape of the loss function,
if a certain set of parameters is given,
for example, the blue θ' here,
we are able to explicitly write out
the loss function near θ'.
Its form is like this.
So the complete formula of L(θ) can’t be written out explicitly.
But if we restrict ourselves to the neighborhood of θ',
you can use this formula to approximate it.
You may ask: what is this?
There are so many colors in this formula.
What is this?
This is the
Tayler Series Approximation.
I assume you have learned this already
in calculus,
so I won't go into the details of this formula.
But maybe I can give you a brief picture of it.
In this formula,
the first term is L(θ').
This term indicates that
when θ is very close to θ',
L(θ) will approximate L(θ').
What's the second term?
The second term is the transpose of (θ minus θ') multiplied by g.
What is g?
g is a vector.
the gradient vector.
We talked about gradient last week.
We'll use a green g to
represent the gradient,
which is a vector.
So, what information can the gradient give us?
This gradient will make up for
the gap between θ' and θ.
Although we said that θ and θ'
should be close,
there are still some gaps between them.
This gap.
For the first term, we use the gradient to
represent the gap between them.
Okay, how about g?
Gradient will sometimes be written as ∇L(θ').
g
g is a vector.
Its i-th component
is the derivative of L
with respect to i-th component of θ.
By only considering g,
we cannot fully describe L(θ).
You have to look at the third term.
What is the third term? The third term is related to Hessian.
There is an H here. H is called Hessian.
It is a matrix.
This third term is the transpose of θ-θ' multiplied by H
multiplied by (θ-θ').
So the third term will make up for
the gap between the real L(θ)
after adding the gradient.
What is inside this H?
This H consists of the second derivative of L.
What is the value of
the i-th row the j-th column?
The value of
the i-th row the j-th column
is the derivative of L
with respect to the i-th component of θ.
Then put the derivative of L
with respect to the j-th component of θ.
Then put the derivative of L
with respect to the i-th component of θ.
The result of the secondary derivative is this Hᵢⱼ
If you don’t understand well here,
it doesn't matter.
Anyway, you just remember this L(θ).
This loss function.
This error surface near θ'
can be written like this.
This formula is related to two things.
It has something to do with gradient.
It has something to do with Hessian.
Gradient is first-order differential.
Hessian contains second-order differential.
If we move to one
critical point.
What does that mean?
It means the gradient is zero.
That is, the green term is completely gone.
g is a zero vector.
The green term is completely gone.
Only the red one is left.
So at the critical point,
this loss function
can be approximated as L(θ')
plus the red term.
We can have a picture of
the error surface near θ'
based on the red term.
Knowing what the error surface looks like,
I can judge
if θ' is a local minimum
or a local maximum
or a saddle point.
We can rely on this to understand
the topography of this error surface
approximately.
Knowing what its topography looks like,
we can know that
what state it is in now.
This is Hessian.
Then let’s take a look at that according to Hessian,
according to this red term,
how to know the shape of the error surface near θ'.
For convenience,
we use the vector v to represent θ-θ'.
θ is a vector and θ' is also a vector.
We use the vector v to represent
the result of θ - θ'.
For any possible vector v,
the value of its transpose multiplied by H and itself
is always greater than zero.
In other words, no matter what the value of θ is,
v is possible to be any vector.
It means that θ-θ' can be any value,
and θ can be any value too.
No matter what the value of θ is,
the value in this red box is always greater than zero.
What does it mean?
It means that the value in the red box is always greater than zero,
which also means that L(θ) > L(θ').
Regardless of the value of θ, as long as it is near θ',
L(θ) is always greater than L(θ').
What does it mean?
It means that L(θ') is the lowest point around it.
So it is a local minimum.
On the other hand,
for all possible v,
if the value of its transpose multiplied by H and itself is less than zero,
then that is, the value in this red box is always less than zero.
In other words, no matter what the value of θ is,
the value inside the red box is always less than zero.
What does it mean?
It means that all the possible values in this red box are less than zero.
It means that L(θ') is always greater than L(θ).
If L(θ) is always less than L(θ'),
then it means that L(θ') is the highest point arround it.
So it is a local maximum.
Suppose there is a third situation that
the value of v transpose multiplied by H and v
is sometimes greater than zero and sometimes less than zero.
With a different v and a different θ,
the value in the red box is sometimes greater than zero
and sometimes less than zero.
It means that arround θ',
sometimes L(θ) > L(θ') and sometimes L(θ) < L(θ').
Arround L(θ'),
there are some high regions and some low regions.
What does it mean?
This means that it is a saddle point.
But there is still a problem.
With the red term,
we can now know that whether it is a local minimum,
a saddle point,
or a local maximum.
But in this case, we have to substitute all the possible value for v,
and check that the value of v transpose multiplied by H and itself is greater than zero
or less than zero.
How can we check the results of
all possible v?
So there is an easier way
to check which condition
will happen.
We will tell you the conclusion directly.
It has been taught in linear algebra.
If for all possible v,
the results of v transpose multiplied by H and v are all greater than zero,
then this kind of matrix is ​​called positive definite.
What are the properties of a positive definite matrix?
All its eigenvalues ​​are positive.
So if you have calculated the Hessian,
you don’t need to multiply it by all possible v.
You just have to take a look at the eigenvalues of this H.
If you find that
all its eigenvalues ​​are positive,
it means that this condition is always true.
The result of v transpose multiplied by H and v
will be greater than zero,
which also means it is a local minimum.
So, you can see, from the Hessian matrix,
whether it is a local minimum.
You only need to calculate the Hessian matrix.
If its eigenvalues are positive,
then it is a local minimum.
That's it.
Vice versa.
If, in this situation,
for all v,
"v^t * H * v" is less than zero,
then H is negative definite,
which means that all eigenvalues ​​are negative
If all eigenvalues ​​are negative,
it enssures that it must be a local maximum.
If the eigenvalues are sometimes positive and sometimes negative,
it means it's a saddle point.
Supposing you don’t understand it well,
you can remember the conclusion that
all you need is to figure out
what the Hessian is.
It is a matrix.
If its eigenvalues
are all positive,
it means that we now reach a local minimum.
If they are sometimes positive and sometimes negative,
It means we now reach the saddle points.
As we mentioned,
we can judge
whether it is a local minimum or a saddle point.
If, as what we just talked,
you don't understand it well,
there is an example.
We now have the most useless network ever.
How useless is it?
We first enter an x,
which only comes with one new row multiplied by w1.
Furthermore, this new row
doesn't come with a vectorization function.
So, the output is merely x multiplied by w1
and then multiplied by w2.
This output is called y.
Anyway, this function is very simple.
"y = w1 * w2 * x".
This is the most useless neural network ever.
We also have the most useless
training data set ever.
This data set includes
only one record.
When it is "x is equal to 1",
its level is 1. So, if we put 1 into it,
you hope the final output to be as close to 1 as possible.
The most useless training ever.
Then, the most useless training ever,
Its error surface
is able to be showed directly.
Anyway, there are only two parameters: w1 and w2,
and there is no bias.
Assuming there is no bias,
and there are only two parameters: w1 and w2,
this network has only two parameters: w1 and w2,
we can exhaust all the values ​​of possible w1 and w2
to derive the loss of each combination.
Then, we can draw the error surface like this.
So, here is high. The loss here is high.
The loss here is also high.
The losses are high in the four corners.
Okay! Then, with this picture, you can see that.
There are some critical points.
Where are those critical points?
At this black spot (0, 0).
The origin point is the critical point.
Then, in fact,
there are also a row of critical points next to this line.
A row of critical points are next to this line.
If you go further to analyze
whether they are saddle points
or local minima,
at the center of the circle,
you will find that it is a saddle point at the original point.
Why is it a saddle point?
If you go in this direction, the loss will increase.
Go in this direction, the loss also increases.
Go in this direction, the loss decreases.
It is a saddle point.
And these two groups of critical points,
they are all local minima.
So in this valley,
there is a row of local minima.
There is another row of local minima in this valley.
And at the origin,
there is a saddle point.
This is the conclusion we get
by brute-force searching the error surface.
We brute force search all the parameters,
calculate the loss with the loss function,
and draw the error surface
to get this conclusion.
Now suppose we don’t use brute-force search.
How can we tell if a point
is local minimum or
a saddle point?
Well, we can write out the loss function.
This loss function, denoted L, is
ŷ, the correct answer, minus the model's output,
which is w₁w₂x.
We take square error here.
There is only one data,
so we won’t have to sum over
all training data
because there is only one piece of data anyway.
Plug in x=1 and ŷ=1.
I have said that having only one training data is pretty lame,
so since there is only one training data,
the loss function is (1–w₁w₂)².
Then you can solve for the gradient
of this loss function.
We won’t talk about the details here.
If you still remember calculus,
do the math and you will know that this is correct.
The partial derivative of L wrt. w₁
looks like this.
The partial derivative of L wrt. w₂ looks like this.
We call this thing g,
the so-called gradient.
When will the gradient be zero?
When will it reach a critical point?
For example, if w₁=0 and w₂=0,
at the center of the circle,
if you plug in 0 as w1 and w2,
the gradient will be zero.
If you plug in 0 as w1 and w2,
both partial derivatives of L
will be zero.
Then we can know
that the origin is a critical point.
We now know it is a critical point.
But is it a local maximum?
Is it a local maximum,
local minimum,
or a saddle point?
You will have to look at Hessian to answer this.
It depends on the Hessian matrix.
Of course, we have already brute-force searched
all possible w₁ and w₂.
So you already know
that it is obviously a saddle point.
But suppose we haven't searched for
all the possible losses.
So we have to see if we can use H,
use Hessian to see
which critical point it is.
How to calculate this H?
H is a matrix.
This matrix ​​collects
second derivative of L.
So the element located in the first row
and the first coloumn in this matrix
means that w₁ should differentiate L twice.
The element located in the first row and the second column
is to differentiate L with w₂ first,
then use w₁ to differentiate L.
Here is w₁ to differentiate L
and w₂ to differentiates L.
Lastly, w₂ should differentiate L twice.
Combine these four values
is our Hessian.
What is the value of this Hessian?
Here I have shown
the formula of Hessian.
You just need to substitute w₁ for 0 and w₂ for 0.
After that, at the origin you will obtain Hessian,
which is a matrix like [(0,-2)(-2,0)]
Okay, this matrix tells us,
this Hessian can tell us
whether it is a local minimum
or a saddle point.
To do so, you have to look at the eigenvalue of this matrix.
Calculate and find
that there are two eigenvalues for this matrix,
2 and -2, one of which is positive and the other is negative.
It means that it's a saddle point.
We just mentioned that one positive eigenvalue with a negative one
should be a saddle point.
Okay, so let's take an example
to show that how can we
use Hessian to distinguish between
the saddle point and the local minima
when we come across a critial point.
If you stuck in the saddle point today,
maybe you don't need to be worried about that.
Why?
Because if you find out
the reason to make you stop
is because of the saddle point,
there may be a chance to heave a sign of relief.
Why?
Because H can not only help us to judge
whether it is a saddle point or not,
but it also points out
the possible update direction of the parameters.
In the past, we always update the parameters
with the gradient, g.
However, sometimes, we go somewhere
and found that g becomes to 0 and can't use it anymore.
g disappears, gradient disappears.
But if it’s a saddle point,
You can still use H.
How to use H?
How can H tell us
how to update parameters?
Let us assume that u is the eigenvector of H,
and λ is the eigenvalue of u.
That is, H has an eigenvalue called λ,
and its corresponding eigenvector is called u.
Here if we replace v with u,
What will happen?
If we multiply u by H on the left side and
on the right of H respectively,
that is, u transpose multiplied by H and multiplied by u,
what will we get?
H multiplied by u will get λu.
Why?
Because u is an eigenvector.
So H multiplied by an eigenvector will get
λ eigenvalue multiplied by an eigenvector.
So here we will get uᵀ multiplied by λu.
Then tidy up,
multiply uᵀ with u can
get ‖u‖².
Finally, we get λ‖u‖².
So for this term,
suppose v
is an eigenvector.
If θ minus θ'
is an eigenvector.
You will find that this red term
is actually λ‖u‖².
What will happen
if the eigenvalue λ
is less than zero?
If the eigenvalue is less than zero
the term λ‖u‖² will be less than zero.
Because ‖u‖² must be positive
the eigenvalue is negative.
Then this term will be negative.
That is, the transpose of u multiplied by H and u
is negative.
That is the term in this red box
is less than zero.
Assuming that θ minus θ' is equal to u,
then this term is negative.
That is, L(θ) will be less than L(θ').
In other words, suppose θ minus θ' is equal to u,
that is, θ is equal to θ' plus u.
Adding u to the original parameter of θ'
updates the parameters along the direction of u and gets θ.
You can make the loss lower.
According to this formula
if θ minus θ' is equal to u
the loss will decrease.
So the only thing you need to do is to make θ equals to θ' plus u
then the loss will decrease.
You only need to update the parameters
along the direction of u
or the direction of the eigenvector
and then you can make the loss lower.
Although there is no gradient at the critical point
we don't need to panic
if we are at a saddle point.
You just need to find the negative eigenvalue,
then find out its corresponding eigenvector.
Add θ' with this eigenvector,
then you can find a new point
with a loss lower than the original point.
In case you are not totally clear.
Let's take a look at a real example.
We have just discovered that
the origin is a critical point.
Its Hessian looks like this.
Then I found out that
the Hessian has a negative eigenvalue.
This eigenvalue is equal to -2.
What does the corresponding eigenvector
look like?
Well, there are many.
In fact, there are infinitely many corresponding eigenvectors.
Take one for example.
[1 1]
is its corresponding eigenvector.
Then we only need to follow the direction of u.
Follow the direction of the vector [1 1] to
update our parameters.
Then you can find a point
with a loss lower than the saddle point.
In this example,
the saddle point is at (0,0).
You will have no gradient at this point.
So the gradient won't tell you
how you should update the parameters.
However, the eigenvector
of Hessian tells us to
update in the direction of [1 1].
Then you can make the loss smaller.
Just update along the direction
of the eigenvector u, which is [1, 1].
You can make your loss lower.
That means you can escape from the saddle point,
and then make your loss lower.
So from this perspective...
From this perspective,
the saddle point seems to become less frightening.
When you're training a model,
your gradient becomes zero
and the training process stops.
If it stops
because of a saddle point,
there seems to be hope for it.
But of course,
Hessian is seldom computed
in the actual implementation.
But why?
The second-order partial derivatives
need to be calculated for the Hessian,
so it involves a huge amount of computation.
Not to mention you have to find out
its eigenvalues and eigenvectors.
Thus in practice,
this method is hardly used
to escape from saddle points.
Next, we will talk about the other ways to
escape from saddle points.
All of them require much less computation
than the Hessian.
The reason we introduced
this saddle point
and the eigenvectors of the Hessian
is because I'd like to tell you
that you don't have to be too worried
even if it got stuck at a saddle point.
You can resort to it
in the worst case.
Well, here comes the question.
The question is
whether saddle points or local minima
occur more constanly during training.
As shown earlier,
saddle points are not that scary.
And it would be very nice
if we encounter them
more often than
local minima.
Which of them is more commonly seen
after all?
Here I am going to tell an irrelevant story.
Let's get started.
It was around the 16th century,
more concisely in 1543,
the story took place.
Constantinople fell that year.
Here is a painting depicting the Fall of Constantinople.
Constantinople, originally the capital of the Eastern Roman Empire,
was captured by the Ottoman Empire,
and so the Eastern Roman Empire collapsed.
During the siege of Constantinople
by the Turks,
the emperor of the Eastern Roman Empire,
Constantine XI,
was desperate for counterattacks against them.
And he was counseled to seek help
from a magician called Diorena.
By the way, this is a true story, you know
from The Three-Body Problem (novel).
Then...
Who is Diorena?
She has the same ability as Zhang Fei (a Three Kingdoms general),
who was able to decapitate the enemy commander by himself
despite being outnumbered.
So did she.
She could chop off the Sultan's head directly
without being noticed by anyone.
Everyone was wondering why she could do so.
Did she really have magical power?
So they made her
exhibit her power.
At that time she took out a holy grail,
which baffled everyone.
Why were they so surprised
after having seen the holy grail?
Because this holy grail
was originally placed in the basement of Hagia Sophia,
and it was placed in a sarcophagus.
This sarcophagus was sealed.
No one can open it.
But Diorena got the Holy Grail from it
and put a bunch of grapes in it.
Constantine XI wanted to verify
whether Diorena had this ability.
He brought some people to pry open this sarcophagus,
and he found that the holy grail was taken away.
There was a bunch of fresh grapes in it.
He knew that Diorena had the ability to
take the head of the enemy admiral.
Why could Diorena do these things?
The sarcophagus was closed from your perspective.
You are looking at it from a three-dimensional space.
From the perspective of three-dimensional space,
the sarcophagus was closed.
There was no way to get into it.
But Diorena could enter the four-dimensional space.
From the high-dimensional space,
there was a way to enter this sarcophagus.
It was not closed.
As for whether Diorena succeeded in assassinating the Sultan,
you can imagine that he failed.
That’s why Constantinople fell to the enemy occupation.
Why did he fail?
Please watch "Death's End" by yourself.
Anyway, from a three-dimensional perspective,
there is no way to go.
There is a way to go in a high-dimensional space.
Is the error surface the same?
In a one-dimensional space,
one-dimensional error surface with one parameter,
you will feel as if there are local minima everywhere.
But is it possible that when be viewed in a two-dimensional space,
it's just a saddle point.
People often draw pictures like this to
tell you that the training of deep learning
is very complicated.
If we move certain two parameters,
the error surface changes are very complicated.
It looks like this.
Obviously, it has a lot of local minima.
I have a local minimum now.
But is it possible that this local minimum
just looks like a local minimum
in a two-dimensional space?
In a higher-dimensional space,
it looks like a saddle point.
In a two-dimensional space,
we have no way to go.
Will there be a way in a higher dimension?
For higher dimension,
we can't visualize it,
We can't really show it.
Will there be a way to go
in a higher dimension?
If the dimension is higher,
are there more ways to go?
So, when we are training
a network,
the number of parameters is often more than one million or tens of millions,
so our error surface
is actually in a very high dimension.
Right?
The number of parameters
represents the dimension
of our error surface.
The parameter is 10 million, which means the dimension
of the error surface is ten million.
The dimension is so high.
Will there actually
be so many ways to go?
Since there are so many ways to go,
will there be few local minima
at all?
In fact,
if you do some experiments yourself,
you'll find this hypothesis to be correct in general.
This graph represents the result of training a certain network.
Each point represents
the Hessian of a network
after it has gone through training.
So, every point here
represents a network.
In short,
we train a network
until its gradient decreases to the point that
it is stuck on the critical point.
We then analyze that particular set of parameters
and see whether it is a saddle point
or a local minimum.
What about the vertical and horizontal axes?
The vertical axis represents the loss during training.
To be more precise,
it represents the minimum loss we can achieve
during the training process.
More often than not,
the loss can't be lowered anymore even if it is still high
since it is stuck at the critical point.
In some cases, the loss can be reduced a lot
before getting stuck at the critical point.
We have explained the vertical axis.
What about the horizontal axis?
The horizontal axis represents the minimum ratio.
What does the minimum ratio mean?
The minimum ratio is the number of positive eigenvalues
divided by the number of all eigenvalues.
So, if all eigenvalues ​​are positive,
it means the critical point
is a local minimum.
If both positive and negative eigenvalues exist, it means it is a saddle point.
In practice, it is almost impossible to
find a critical point with only positive eigenvalues,
and no negative eigenvalues.
In our example,
the minimum ratio,
whose definition
we have just defined,
is around 0.5 to 0.6 at max.
It means that only half of the eigenvalues are positive,
while the other half are negative.
So, even if it means
the further to the right we go on this graph,
the more the critical point looks like a local minimum,
none of the critical points
is truly a local minimum.
Even in the most extreme cases,
negative eigenvalues still account for
almost 50% of eigenvalues,
while the other 50% are positive eigenvalues.
This means that in all dimensions, half of the directions
would increase the loss, while the other half
would decrease the loss.
Judging from experience,
local minima aren't actually that common.
More often than not,
during the training process,
the gradient becomes too small
and the parameters stop updating
because we are stuck on a saddle point.
Now, we're going to talk about
whether there are any possible solutions
when we're stuck on the critical point.
Like I just said,
even if the occurrence of local minima
may not be that common,
being stuck on a saddle point
or somewhere near a saddle point that is also flat
could still make the gradient too small
and stop the parameters from updating.
Let's pause for a bit
and see if any of you have questions to ask.
