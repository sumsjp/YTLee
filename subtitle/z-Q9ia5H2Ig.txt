好 那上週呢 我們是講到這個地方
好 那到目前為止啊
我們在上課講的內容
其實都是 White Box 的 Attack
什麼意思呢
也就是說 我們要計算這個 Gradient
我們做 FGSN 要計算這個 Gradient 嘛
在計算 Gradient 的時候
我們需要知道模型的參數
知道模型的參數
才有辦法計算這個 Gradient
才有辦法去在 Image 上加上 Noise
那像這種知道模型參數的攻擊啊
叫做 White Box 的 Attack
那中文有時候就翻譯成白箱攻擊
那白箱就是一個動畫了 知道嗎
這個是白箱 沒有很重要
沒有很重要 不用管我
那但是你可能會覺得說
哇 這個攻擊需要知道 Network 的參數
看來這個攻擊呢 不是很危險
為什麼呢 因為一般線上的服務
你當然要攻擊一定是去攻擊別人的模型嘛
某一個線上的服務嘛
線上的服務它的模型
你又不知道參數是什麼
所以也許要攻擊一個線上的服務
並沒有那麼容易
所以其實如果我們要保護
我們的模型不被別人攻擊
也許我們只要記住
不要隨便把自己的模型放到網路上
公開讓大家取用
也許我們的模型就會是安全的
但真的是這樣嗎
不知道模型參數下的攻擊啊
叫做 Black Box Attack 也就是黑箱攻擊
黑箱攻擊是有可能的嗎
黑箱攻擊是有可能
怎麼做黑箱攻擊呢
我們到目前為止講說
我們在做攻擊的時候
都需要計算 Gradient
就像 Gradient 需要知道 Model 的參數
那黑箱攻擊是怎麼做到的呢
黑箱攻擊是這樣子做的
那我們的作業 剛才助教也有講過
就是黑箱攻擊
所以網路上有一個模型
這個模型你是沒有辦法拿到的
你根本不知道它的參數是什麼
這個其實就是（00：02：10）上面的那一個模型
你並不知道助教使用了哪一個模型
你並不知道它的參數是什麼
那怎麼辦呢
假設你知道這個 Network
是用什麼樣的訓練資料訓練出來的話
那你可以去訓練一個 Proxy 的 Network
也就是你訓練一個 Network
讓這個 Network 來模仿我們要攻擊的對象
那我們要攻擊的對象跟 Proxy 的 Network
如果都是用同樣的訓練資料訓練出來的話
也許它們就會有一定程度的相似度
如果 Proxy Network 跟要被攻擊的對象
有同樣的 有一定程度的相似程度的話
那我們只要對 Proxy 的 Network 進行攻擊
也許這個有被攻擊過的 Image
拿去丟到我們不知道參數的 Network 上
攻擊也會成功
那這個其實就是在我們作業裡面做的事情
所以在作業裡面做的事情是
你從某一個地方找來某一個
已經訓練好的影像辨識的模型
這個是你的 Proxy 的 Network
你自己在自己的機器上
你在（00：03：22）上攻擊這個自己的 Network
然後丟到（00：03：25）上面
看看這個攻擊能否成功
那有人可能會問說
那如果我根本就沒有訓練資料
我根本不知道現在要攻擊的對象
是用什麼樣的訓練資料的話怎麼辦呢
在作業裡面 我們知道是（00：03：41）
我們要被攻擊的對象
是用（00：03：43）訓練出來的
所以你只要用一個
（00：03：46）訓練出來的模型
你可能就可以攻擊成功
但是假設我們完全沒有訓練資料的話 怎麼辦呢
這也不是完全無解的
怎麼解呢
就是你就假設這是你要攻擊的影像辨識模型
你就把一堆圖片丟進去
然後看看它會輸出什麼
線上的 Service 就算是它不會告訴你
Network 的參數
你總是可以丟東西進去
看它輸出什麼嘛
再把輸入輸出的成對資料
拿去訓練一個模型
你就有可能可以訓練出一個類似的模型
當做 Proxy Network 進行攻擊
好 那這種黑箱攻擊容易成功嗎
蠻容易成功的
你在作業裡面就可以體會一下
這個黑箱攻擊其實非常容易成功
那這個是文獻上的結果
那這邊有 5 個不同的 Network
ResNet 152 層 ResNet 101層
ResNet-50 VGG-16 還有 GoogLeNet
總共有 5 個 Network
好 那這個 Column 啊
代表要被攻擊的 Network
總共有 5 個要被攻擊的 Network
那這個 Row 啊
這代表說我們有 5 個 Proxy 的 Network
那如果是對角線的地方
代表說 Proxy 的 Network
跟要被攻擊的 Network
它們是一模一樣的
所以這個情況就不是黑箱攻擊
對角線的地方其實是白箱攻擊
所以如果你拿 ResNet-152 當做 Proxy Network
攻擊的時候其實是攻擊一個
一模一樣的 Network
太容易成功了
這邊這個數字是正確率
是要被攻擊的那個模型的正確率
所以這個值呢 是越低越好
越低的正確率
代表你的攻擊越成功
你現在是站在攻擊方的
所以你不是負責 你不是訓練模型方的
你是攻擊方的
所以這個正確率越低
代表你的攻擊是越成功的
你發現對角線 也就是白箱攻擊的部分
White Box Attack 的部分
這個攻擊的成功率是百分之百
也就是模型的正確率是 0 %
你的攻擊總是會成功
但如果在非對角線的地方
也就是黑箱攻擊
舉例來說 你用 ResNet-101 當 Proxy Network
去攻擊 ResNet-152
得到的正確率是 19 %
或者是你拿 ResNet-152 當做是 Proxy Network
去攻擊 ResNet-50
你得到的正確率是 18 %
那這個非對角線的地方是黑箱攻擊
你會發現說 黑箱攻擊模型的正確率
是比白箱攻擊還要高的
但是其實這些正確率也都非常低
都是低於 50 %
所以顯然黑箱攻擊也有一定的成功的可能性
不過實際上黑箱攻擊是在
Non-Targeted Attack 的時候比較容易成功啦
Targeted Attack 就不太容易成功
就是假設你用 Proxy Network
說你要把一個狗變成一個兔子
那如果你把 Attacked Image
拿到那個你要攻擊的對象上面的話
你可能可以讓它辨識錯誤
你可能會讓機器辨識出不是狗
但你要指定它一定要變成兔子 就比較難
所以在黑箱攻擊的時候
這個 Targeted Attack 比較難成功
但 Non-Targeted Attack 還是非常容易成功的
那如果你要增加這個
Black Box Attack 的成功率怎麼辦呢
剛才助教也講了一個
可以過 Strong Baseline 的 Tip
就是 Ensemble 的 Network
那這個 Ensemble 的 Network 要怎麼做呢
這邊的這個表格的看法是這個樣子的
這個 Column 代表要被攻擊的 Network
那每一個 Row 是什麼意思呢
你會發現這個每一個模型的名字
前面放了一個減號
它是什麼意思呢
那就代表說
我們現在把這 5 個模型都集合起來
但拿掉 ResNet-152
我們要找一個攻擊的 Image
在 ResNet-152 以外的模型都是成功的
我們假設我們手上沒有 ResNet-152
但是有 ResNet-101 ResNet-50
VGG-16 跟 GoogLeNet
找一張 Image 攻擊這 4 個 Network
都是成功的
然後看看在 152 上會發生什麼事
所以其實今天在這個圖啊
這個下面這個表格
跟上面這個表格的看法是不一樣的啦
如果是下面這個表格的話
非對角線的地方是白箱攻擊
非對角線的地方有沒有發現
模型正確率都變成 0 %
就像我剛才說的
白箱攻擊非常容易成功
對角線的地方才是黑箱攻擊
所以這個地方是 我們要攻擊 ResNet-152
但我們沒有用 ResNet-152
這邊是要攻擊 ResNet-101
但沒有用 ResNet-101
但是用了另外 4 個 Network 以此類推
所以對角線的地方才是黑箱攻擊
那你發現說 當你有做 Ensemble 的時候
當你同時用多個 Network 的時候
當你找一個 Attacked Image
可以成功騙過多個 Network 的時候
騙過一個你不知道參數的黑箱的 Network
也非常容易成功
你看對角線上的正確率
基本上都是 10 % 以下
好 那這個是黑箱攻擊
好 你會發現說這個攻擊這件事啊
非常容易成功
到底是怎麼回事呢
為什麼連黑箱攻擊
你在 A Network 上攻擊
在 B Network 上都會成功
事實上這仍然是一個
可以說是未解之謎啦
還有很多可以研究的空間
那以下就是講一個很多人相信的結論
這邊有一個實驗是這個樣子的
這個圖上面的原點
代表一張小丑魚的圖片
就是這隻 就是尼莫
就是這個 小丑魚就是尼莫
就是尼莫的圖片 在這邊
然後這個橫軸跟縱軸分別是什麼呢
分別是把這張圖片往兩個不同的方向移動
就是一張圖片是一個非常高維的向量
把這個高維的向量
往某一個方向移動 是橫軸
往另外一個方向移動 是縱軸
那這邊的橫軸跟縱軸
分別是什麼樣的方向呢
這邊橫軸啊 是在 VGG-16 上面
可以攻擊成功的方向
而縱軸就是一個隨機的方向
那你會發現說呢 雖然這個橫軸啊
是讓 VGG-16 可以攻擊成功
但是在其他的 Network 上面
ResNet-50 ResNet-101
ResNet-152 GoogLeNet 上面
你看這個圖
我後來發現它們有很大的類似之處
它們中間這個深藍色的區域都還蠻相近的
這個深藍色的區域是什麼呢
這個深藍色的區域啊
這個深藍色的區域是
會被辨識成小丑魚的圖片的範圍
也就是說 如果你把這個小丑魚的圖片
加上一個 Noise
你把這個高維的向量
在高維的空間中往這個方向移動
基本上 Network 還是會覺得
它是小丑魚的圖片
不管對每一個 Network 來說
只要往這個方向移動
它是一個隨機的方向
基本上都會被認為是小丑魚
但是如果你是往可以攻擊成功
VGG-16 的方向來移動的話
那基本上其他 Network
好像也是有蠻高的機率可以攻擊成功的
你發現這個小丑魚這一個類別
它在這個攻擊的方向上
它就是特別窄
只要你把這個高維的向量
這張圖片稍微移動一下
它就掉出會被辨識成小丑魚的
區域範圍之外了
它就會掉出會被辨識成小丑魚的
區域範圍之外
會被辨識成其他的類別
對每一個 Network 來說
看起來這個攻擊的方向
對不同的 Network 影響都是蠻類似的
那所以啊 有不止一篇論文
它們對於攻擊這件事
它們的認知是這個樣子的
你從這篇文章的開頭就可以看出來
它說這個
Adversarial Example Are Not Bugs,
They Are Features.
所以一個 有一群人是主張說呢
這個攻擊這件事情會成功
它最主要的問題來自於你的 Data
而不是來自於模型
不同的模型訓練出來的結果
看起來是還蠻相近的
而攻擊會成功這件事情
不是只有對 Deep Learning 有一樣的問題
對 Linear 的 Network
對 SDM 也都有類似的問題
所以也許攻擊會這麼容易成功這件事情
變成這個主因未必出現在模型上面
可能是出現在資料上
為什麼 Machine 會把這些非常小的雜訊
誤判為另外一個物件
那可能是因為在資料上面
本身它的特徵就是這樣
在有限的資料上
機器學到的就是這樣子的結論
所以也許 Adversarial Attack 會成功的原因
是來自於資料上的問題
當我們有足夠的資料
也許就有機會避免 Adversarial Attack
不過這個其實只是這個某一個
就是它並不是所有人都同意這樣啊
同意這個觀點啊
這只是某一群人的想法而已
也許過幾年以後 你再來修同一堂課
我講的結論又會不太一樣
那這邊只是告訴你說
有一群人他們的認知的觀點
是認為 Data 是造成 Attack 會成功的元凶
好 那 Attack 的 Signal
我們希望它越小越好
到底可以小到什麼樣的程度呢
那在文獻上有人成功地做出 One Pixel Attack
所謂 One Pixel Attack 的意思就是說
你只能動圖片裡面的一個 Pixel 而已
舉例來說 在這張圖片裡面
他們動了一個 Pixel
他會特別把 Pixel 有改變的地方把它框起來
把它框起來 把它框起來 把它框起來
希望說動了圖片中的一個 Pixel
影像辨識系統的判斷就必須要有錯誤
不過你其實如果從這個圖片的
這個在這個圖片上這個黑色的部分啊
代表的是正確的 攻擊前的
這個影像辨識的結果
藍色代表是攻擊後的影像辨識結果
那你會發現說
One Pixel Attack 看起來還是有一些侷限的啦
它的攻擊並沒有說
真的非常非常成功 怎麼說呢
舉例來說 這是一個 Teapot
它是一個茶壺
做 One Pixel Attack 在這個地方
某一個 Pixel 的顏色被改變了
機器呢 把 Teapot 變成 Joystick
Joystick 是什麼呢 Joystick 是搖桿
那你會發現說
欸 這個錯其實還錯的是有點道理
不像我們一開始舉的什麼
貓變成海星 貓變成鍵盤 那麼荒謬
這個錯還有點道理
所以感覺這個攻擊呢
並沒有非常地 Powerful
這個是 One Pixel Attack
那其實還有更狂的攻擊方式
叫 Universal Adversarial Attack
Universal 的 Attack 是什麼意思呢
我們在到目前為止
每一張圖片 你的這個都是客製化的
你有 200
作業裡面有 200 張圖片
200 張圖片
你會分別找出不同的 Attacked Signal
那有人就問說
有沒有可能用一個 Signal
就成功攻擊所有的圖片呢
因為如果你說
每一張圖片都要有不同的 Signal
那如果你今天要 Hack 某一個監視系統
你要讓某一個監視系統它的辨識是錯的
那你可能需要真的
Hack 進去那個監視系統
然後每次進來不同的影像的時候
你都要客製化 找出一個 Attacked Signal
那這個運算量可能會非常地大
如果 Universal Attack 可以成功的話
你其實只要把這個訊號
貼在這個監視器的攝像頭上
那如果這個訊號
這個 Attacked Signal 非常強
只要加上這個 Attacked Signal
不管什麼樣的影像都可以攻擊成功的話
你只要把這個 Signal 直接放在攝像頭上
貼在攝像頭上
那這個攝像頭它就直接壞掉了
不管看到什麼東西它都會辨識錯誤
那 Universal Attack 有可能成功嗎
你可以看看這篇論文
Universal Attack 是有可能成功的
在這篇論文裡面 他們找了一個 Noise
找了一個 Attacked Signal
這個 Attacked Signal
加在非常多不同的圖片上
都可以讓影像辨識系統辨識錯誤
好 到目前為止啊
我們舉的例子通通都是影像的例子
那有人可能會覺得說
會不會是影像才有這種會被攻擊的問題
會不會其他的類型的資料
就比較不會有這種問題呢
其實不是 其他類型的資料也有類似的問題
以語音為例
大家都知道說現在會做 Defect
有人會模擬出這個用語音合成的技術
或用語音轉換的技術
去模擬出某些人的聲音
藉以達到詐騙的效果
那為了偵測這種 Defect 的狀況
於是有另外一系列的研究在研究說
怎麼偵測一段聲音是不是被合成出來的
今天雖然語音合成的系統
往往都可以合出以假亂真的聲音
但是這些以假亂真的聲音
還是有非常大的可能性
可以用機器抓出來的
這些合成出來的訊號
它還是有固定的 Pattern
跟真正的聲音訊號
還是有一定程度的差異
人耳聽不出來 但機器可以抓出來
但是這些可以偵測語音合成的系統
可以偵測一段聲音訊號
是不是合成的系統
也會被輕易的攻擊
那以下是真實的例子
然後先放一段合成的聲音
這是一段合成的聲音
任何人都聽得出這是一段合成的聲音
這段聲音是故意合壞的
如果今天語音合成的系統都可以合出
人聽不出來是真是假 以假亂真的聲音
所以剛才那一段顯然合得很差
所以你用這個偵測是否是語音合成的系統
它可以正確地告訴你說
這段聲音訊號顯然是合成的
好 但是如果我們在剛才那段聲音訊號裡面
加入一點點雜訊
它聽起來是這樣
你可能問說
這個新的聲音 加入雜訊的聲音
跟原來有什麼不同呢
人耳完全聽不出它之間的差異
那個雜訊非常非常地小
沒有任何人可以聽出
這兩段聲音訊號有什麼樣的差異
而這段聲音訊號加上這個微小的雜訊以後
它聽起來也沒有合成得更好
但是同一個偵測合成的系統
會覺得剛才那段聲音是真實的聲音
而不是合成的聲音
好 剛才舉的是語音的例子
那文字上也會被 Attack 嗎
文字也會被 Attack
那我們在作業裡面
有一個作業是做 Question Answering
就是給機器讀一篇文章
問它一個問題
看看它可不可以給你正確的答案
那有一篇論文就發現說
它發現在所有文章末尾貼上
Why How Because To Kill American People
接下來不管你問它什麼問題
它的答案都是 To Kill American People
所以你可以在文字上進行 Adversarial Attack
直接讓這個 QA 的系統
怎麼回答都是 To Kill American People
所以不管是什麼樣的 Modelity
今天都有可能被攻擊成功
好 那到目前為止啊
我們的攻擊都發生在虛擬的世界中
都發生在數位的世界中
你是把一張影像讀到電腦裡面以後
你才把雜訊加上去
而攻擊這件事情
有沒有可能發生在真實的世界中呢
有沒有可能發生在三次元的世界中呢
舉例來說 現在有很多人臉辨識系統
那如果你是要在數位的世界發動攻擊
那你得 Hack 進那個人臉辨識的系統
說有一個人臉進來
你自己再去加一個雜訊
你才能夠騙過那個人臉辨識的系統
但是這個攻擊 這個雜訊
有沒有可能加在三維的世界中呢
有沒有可能加在三次元的世界中呢
有沒有可能有人在臉上畫某一個妝
就把人臉辨識的系統騙過去呢
這件事情是有可能的
不過化妝比較困難
因為你知道 化妝你一流汗可能就花掉了
所以化妝也許不是一個特別好的方法
有一人發現說 可以製造神奇的眼鏡
戴上神奇的眼鏡以後
你就可以去欺騙人臉辨識的系統
那這個眼鏡看起來沒有什麼特別的
它就是花花綠綠的
看起來特別潮
但是左邊這個男的他戴上這副眼鏡以後
人臉辨識系統就會覺得
他是右邊這一個知名藝人
那我其實不知道 忘記這個知名藝人是誰了
沒有關係 這不重要
我對三次元世界其實也沒有太熟悉
我也不知道他是誰
好 但是如果你仔細去讀這篇文獻的話
你會發現說 它們考慮了很多
物理世界才會有的問題
第一個是 在物理的世界
我們在觀看一個東西的時候
可以從多個角度去看
過去有人會覺得說
Adversarial Attack 也許不是那麼危險
為什麼 因為影像就是一張
然後你加入某一個特定的雜訊
才能夠讓這張影像被辨識錯誤
但在真實的世界中
你可以從多個角度去看同一個物體
也許你的雜訊騙過了某一個角度
但沒有辦法在所有的角度
都騙過影像辨識的系統
但這篇論文它其實是有考慮這個觀點的
所以並不是從某一個角度看這個人
他才會被辨識成右邊這個知名藝人
從所有的角度
從各式各樣的角度去看這個有戴眼鏡的人
他都會被辨識成右邊這個人
不過這件事其實你現在也不會太驚訝
因為我剛才有告訴你說
Universal Attack 是有可能成功的
所以你有可能找得到某一種雜訊是
這個人戴上這個眼鏡以後
不管從什麼角度看這個人
這個攻擊都是成功的
好 所以這是第一個考慮物理世界的部分
那第二個 考慮物理世界特性
在這篇論文裡面有做的事情
是它有考慮到說
今天你的攝像頭它的解析度還是有限的
所以如果你今天在這個眼鏡上面
加的那個訊號非常地小
比如說 你只加一個非常小的斑點
那有可能你的攝像頭根本沒有辦法看到
或者是如果你的相鄰的 Pixel
有非常大的顏色的變化
那也許像這樣子的狀況
攝像頭根本沒有辦法抓到
所以它有把今天攝像頭的解析度
攝像頭本身解析度的能力的極限
也把它考慮進來
好 第三個有考慮的事情是
到底這個眼鏡能不能夠
真的被做出來的問題
他們有考慮到說 有某一些顏色
你可能在電腦裡面跟在真實的世界
看起來是會有差異的
某一些顏色
也許你要真的把它實現在物理的世界
真的把它印出來
它的顏色會偏掉
所以他們有考慮到說
今天在印製這個眼鏡的時候
不要使用那些
印製出來以後顏色會偏掉的顏色
會挑選一些印出來以後不會偏掉的顏色
所以你可以仔細去看一下這篇論文
它其實考慮了很多真實世界
在從這個三維的空間中
從三維的世界中
攻擊數位的世界的時候
會需要面對的真實問題
好 不是只有人臉辨識可以攻擊成功
我們知道說未來會有很多自駕車
自駕車會需要做車牌辨識
所以當然也有人對車牌辨識系統進行攻擊
所以有論文告訴我們說
你可以在這個 STOP 的 Sign 上面
貼一些貼紙
貼完這些貼紙以後
你的這個標誌的辨識系統
不管從什麼角度
遠的近的左邊右邊看這個 STOP Sign
它都會變成是速限 45 公里
它都變成不是停下來
而是另外一個交通號誌
但是有人
有人會覺得說
也許貼這種貼紙上去還是太招搖了
你隨便貼貼紙在路牌上面
大家都知道你要做 Attack 啦
所以隔天可能就被清掉了
所以太招搖了
所以有人製造了一種
比較不招搖的
非常隱密的攻擊方式
他直接把速限 35 的 3
拉長一點
如果沒有告訴你說
這個我特別拉長
你可能覺得這個字體本來就是這樣
但是當他把這個 3
這個特別拉長以後
這一個牌子
對於一個這個標誌的辨識系統來說
它就變成速限 85
這個是美國一個那個軟體安全公司做的啦
他們有放一個 Demo 的影片
在這個 Demo 的影片裡面呢
就是有人開著那個特斯拉的汽車
然後特斯拉的汽車會做那個號誌的辨識
然後這邊有一個人呢
舉著一個速限 35 的牌子
但這個牌子是有特別被攻擊過的
就是它的 3 呢
稍微長一點
本來特斯拉的車子看到速限 35
它的速限就沒有辦法超過 35
但是因為它實際上看到的
對於這個自駕車來說
它看到的牌子是速限 85
所以它就會加速
所以這個 Demo 是這樣子
As We Can See
（00：26：09）Is Holding Up Our Adversarial
35 Mile Per Hour Speed Limit Sign
With A Small Piece Of Black Tape
Is The Only Modification
We Start The Car Moving Forwards
And The （00：26：25）Camera
Immediately In Correctly Identifies
In 85 Mile An Hour Speed Limit Sign
Now We Engage Traffic Aware Cruise Control
And With Our Feet Entirely
Off The Gas And Brake Pedals
The Tesla Begins To Accelerate To 85 Miles Per Hour
We Cut It Short At （00：26：39）50 Miles
Per Hour For Safety Reasons.
對 好 所以像這樣的攻擊
在物理世界
也是有可能成功的
那攻擊其實還有很多
多樣的類型
就讓你見識一下人類的惡意啊
還有一種攻擊呢
叫做 Adversarial Reprogramming
什麼意思呢
它把原來的影像辨識系統
等於是放一個像殭屍一樣的東西去寄生它
讓它做它本來不想做的事情
大家知道說
舉例來說在那個最後生還者裡面啊
人被蟲草菌寄生以後
你還是有行動的能力
但是你會去攻擊其他人
做你本來不想做的事情
這個就是 Adversarial Reprogramming
好 那 Adversarial Reprogramming裡面
在右下角這篇論文裡面
他是怎麼做的呢
他想要做的事情是
他想做一個方塊的辨識系統
去數說圖片裡面有幾個方塊
1 個到 10 個
但他不想 Train 自己的模型
他想要寄生在某一個已有的
又 Train 在 ImageNet 的模型上面
那 ImageNet 的模型就它圖片
然後辨識說裡面有什麼樣的東西
什麼樣的動物 什麼樣的物品等等
然後呢
他希望說呢
他輸入一張圖片
這個圖片裡面如果有兩個方塊的時候
ImageNet 那個模型就要說
它看到 Goldfish
如果 3 個方塊
就看到 White Shark
如果 4 個方塊
就看到 Tiger Shark
以此類推
這樣他就可以操控這個 ImageNet
Train 出來的模型
做他本來不是訓練要做的事情
那怎麼做呢
你就把你要數方塊的圖片呢
嵌在這個雜訊的中間
所以這個是 4 個方塊的圖片
你希望丟到 ImageNet 裡面
它就輸出 Tiger Shark
這個是 10 個方塊的圖片
你希望丟到 ImageNet 的 Classifier 裡面
它就輸出 Ostrich
那你就把這個圖片外面呢
加一些雜訊
然後再把這個圖片呢
丟進 Image Classifier 裡面
它就會照你的操控
做一些它本來不是訓練來要做的事情
這個是 Adversarial Reprogramming
好 那還有一個
還有一種攻擊的方式啊
這個也是讓人驚嘆人類的惡意啊
就是在模型裡面開一個後門
到目前為止
我們的攻擊都是在測試的階段才展開
但是有沒有可能在訓練的階段就展開攻擊呢
舉例來說
假設我們要讓這一張圖片它被辨識錯誤
它是一個魚
但是你的 Image Classifier
要把它誤判為狗
到目前為止
我們都是在測試的階段
模型已經訓練好以後
才在圖片上面加入雜訊去騙過這個模型
但是有沒有可能攻擊
是從訓練的時候就已經展開了呢
有沒有可能
有人在你的訓練資料裡面加入一張圖片
這張圖片看起來沒有什麼問題
它的標註也沒有什麼問題
它並不是說
它加了很多魚的圖片
然後把魚的圖片都標註成狗
那這種攻擊是行不通的
因為有人去檢查你的訓練資料
就知道這個訓練資料有問題了嘛
所以你要在訓練階段就發起攻擊的時候
你要加的圖片是正常的圖片
而它的標註也都是正常的
一切看起來都沒有問題
但是拿這個樣子的資料去進行訓練的時候
訓練完的模型
只要看到這張圖片
它就會誤判為狗
有沒有可能做到這樣的事情
有沒有可能攻擊
從訓練的階段就開始了呢
你可以看一下右上角放的這個 Reference
看起來是有可能的
有可能在訓練資料裡面
加一些特別的
人看起來沒有問題
但實際上有問題的資料
讓模型訓練完以後
模型就開了一個後門
在測試的階段
它就會辨識錯誤
而且只會對某一張圖片辨識錯誤
對其他的圖片還是沒有問題的
所以你也不會覺得你的模型
訓練完以後有什麼不對的地方
而直到有人拿這張圖片來攻擊你的模型的時候
你才會發現這個模型
它是有被下毒的
它在訓練的時候就已經被開了後門
所以這個不得不讓人驚嘆人類的惡意啊
你想想看
假設這一種攻擊是有可能成功的話
未來你從網路上載什麼公開的資料集
你都要非常地小心啊
因為舉例來說
現在大家都可能會訓練人臉辨識的系統
人臉辨識的系統呢
在很多地方是真的有被使用的
那如果你今天的人臉辨識系統
是用一個公開的資料集來訓練
就某一天有某個人說
欸 我公開了一個到世界
到目前為止最大的人臉辨識的資料集
是免費的
然後呢 大家就開心地下載來用
那它裡面呢
就是有加某一張下過毒的有問題的圖片
但那個圖片也沒有人檢查了出來
然後你訓練完以後
大家也覺得說
嗯 這個資料集很好用
訓練出來的影像辨識系統
人臉辨識系統正確率也很高
但是它是有被開了後門的
這個影像辨識系統
只要看到某個人的圖片
就是釋出資料的那個人的照片
它就會把門打開這樣子
所以你要小心在網路上公開的資料集
搞不好裡面就有藏什麼怪東西
也說不定
如果這種開後門的方法
未來是可以 真的可以成功的話
那這是一個非常大的問題
不過你可以看一下這篇文章啦
看起來開後門要真的攻擊成功
還是有某一些限制的
並不是說隨便什麼模型
隨便什麼訓練方式
這種開後門的方法都可以攻擊成功
好 到目前為止
我們已經講了各式各樣的攻擊的方式
那接下來我們想要講一下防禦的方式
而那防禦呢
大致可以分為兩類
一種是被動防禦
一種是主動防禦
被動防禦是怎麼做的呢
被動防禦就是
你的模型是不動
訓練好模型
訓練好就訓練好了
就放在那邊 不要再去動它
但我們在模型前面加一個盾牌
加一個 Filter
我們期待說
本來我們的圖片加上 Attack 的 Signal
可以騙過來 Network
但是這個盾牌 這個 Filter
可以削減 Attack Signal 的威力
就是當圖片通過這個 Filter 的時候
一般的圖片不太會受到影響
但是 Attack 的 Signal
通過這個 Filter 以後
它就會失去它的威力
讓你的 Network 不會辨識錯誤
那有人就會想說
要製造什麼樣的 Filter
才可以達到這種效果呢
要製造什麼樣的 Filter
才能夠擋住你的訊號呢
其實你不需要把這個問題想得太複雜
非常簡單的做法
光是把圖片稍微做一點模糊化
可能就可以達到非常好的防禦效果了
舉例來說
我們剛才已經
我們之前已經看到說
上次看到過說這張圖片
加上了非常小的雜訊以後
影像辨識系統就覺得它是一個鍵盤
現在我們把這張圖片做一個非常輕微的模糊化
你可以明顯感覺說右邊這張圖片
有一點點模糊
但不是很嚴重
你還是可以看得出來這張圖片裡面有一隻貓
當我們做了這麼一點模糊化以後
再丟到同一個影像辨識系統
你就發現
辨識結果變成是正確了
本來是 Keyboard
現在變成 Tiger Cat
所以光是做模糊化這件事情
就可以非常有效地
擋住 Adversarial Attack
那為什麼呢
因為你可以想說
這個 Adversarial Attack
這個 Attack 的 Signal
其實只有某一個方向上的某一種攻擊的訊號
才能夠成功
並不是隨便 Sample 一個 Noise
都可以攻擊成功
我們之前已經看過說
你隨便 Sample 一個 Noise
並不會達成攻擊的效果
所以攻擊成功
會讓攻擊成功的訊號
它是非常特殊的
當你加上那個模糊化以後
那個攻擊成功的訊號就改變了
那它就失去攻擊的威力
但是它對原來的圖片影響甚小
你把原來的圖片做一點模糊化
其實不太會影響影像辨識的結果
當然這種模糊化的方法
它也是有一些副作用的
比如說本來完全沒有被攻擊的圖片
那 Machine 知道它是 Tiger Cat
但是我們把它稍微模糊化以後
機器現在辨識還是正確的
但是它的 Confidence 的分數就下降了
圖片變模糊以後
機器比較不確定
它看到的東西是什麼了
所以像這種模糊化的方法
你也不能夠把模糊這件事情做得太過頭
做得太過頭的話
它就會造成一些副作用
導致你原來正常的影像
也會辨識錯誤
其實像這樣子的被動防禦的方法
還有很多類似的做法
除了做模糊化以外
還有其他更精細的做法
舉例來說
有一系列的做法是
直接對影像做壓縮
再解壓縮
你知道你把一張圖片啊
存成 JPEG 檔以後
那個它就會失真嘛
那也許失真這一件事情
就可以讓被攻擊的圖片 失去它的
失去它的攻擊的威力
就可以讓攻擊的訊號
沒有那麼具有傷害性
所以有一系列的做法是
把影像做某種壓縮
那這種壓縮如果會失真的話
那可能攻擊的訊號受到的影響是比較大的
你就可以保護你的模型
還有另外一種方法
是基於 Generator 的方法
好 我們在作業裡面
大家都已經訓練過 Generator
那有一系列的做法是給一張圖片
這張圖片它可能有被攻擊過
可能沒有被攻擊過
那我們讓我們的 Generator
產生一張跟輸入一模一樣的圖片
也就是把輸入的圖片
用 Generator 重新畫過
重新產生過
那你可能會問說
欸 這個在作業裡面
我們的 Generator 只會亂生一些圖片啊
你根本沒辦法控制它生成出來的東西啊
有辦法控制 Generator 生成出來的東西
那這個不是今天的重點
我就把文獻留在這邊給大家參考
總之 Generator
我們有辦法控制它的輸出
我們要求 Generator 輸出一張圖片
這張圖片跟輸入給 Image Classifier 的圖片
越接近越好
那你可以想見說
假設有人攻擊了這張圖片
上面加了一個微小的雜訊是人看不到的
對 Generator 而言
它在訓練的時候
它從來沒有看過這些雜訊
它可能也無法產生
復現出這些非常小的雜訊
那這樣這些微小的雜訊就不見了
Generator 產生出來的圖片是沒有雜訊的
你就可以達到防禦的效果
好 但是這種 Passive 的 Defense 啊
這種被動的防禦啊
有一個非常大的弱點
雖然我們剛才在講的時候
雖然我們剛才在講這個模糊化的時候
說模糊化非常有效
但是模糊化這一種方法
只要一旦被別人知道你會做這件事情
它馬上就失去效用
為什麼
你可以完全把模糊化這件事情
想成是 Network 的第一層
所以模糊化這件事
等於就是在 Network 前面多加了一層啊
所以假設別人知道你的 Network 前面
多加這一層
把多加這一層放到攻擊的過程中
它就可以產生一個 Signal
是可以躲過模糊化這種防禦方式的
所以像這種被動的防禦
它既強大也不強大
它強大就是
假設人家不知道你有用這一招
它就非常有效
一旦人家知道你用什麼招數
那這種被動防禦的方法
就會瞬間失去效用
所以怎麼辦呢
還有一種再更強化被動防禦的方法
就是加上隨機性
怎麼做呢
就是你知道
就是不要怎麼樣才不會被別人猜中你的下一招
就是你自己都不知道自己的下一招是什麼
這個就是欲欺敵先瞞內的概念
你就在做這個 Defense 的時候啊
加上各種不同的 Defense 的方式
比如說在這篇文獻裡面 他們就說
哦 我們輸入的圖片
我們只要做一些小小的改變
就可以擋住 Attack 的訊號
但是我們改變的方式不能被別人知道
別人一知道
他就可以攻破你的防禦
所以怎麼辦呢
我們自己都不知道圖片會怎麼樣被改變
一張圖片進來以後
你可能把它放大
也可能把它縮小
任意改變它的大小
然後接下來呢
你把這個圖片呢
貼到某一個灰色的背景上
但貼的位置也是隨機的
你也事先也不知道
你會把這個圖片放在灰色背景哪個地方
再丟給你的影像辨識系統
也許透過這種隨機的防禦
就有辦法在
就有辦法擋住別人的攻擊
但這種隨機防禦也是有問題
你想想看
假設別人知道你的隨機的 Distribution 的話
他還是有可能攻破這種防禦的方式的
而且我們剛才有說過
Universal 的 Attacks 是有可能的
假設你各種隨機的可能性都已經被知道的話
那別人只要用 Universal Attacks
它找一個 Attack 的 Signal 可以攻破所有
所有圖片的變化方式的話
這樣子 Randomization 的方式
還是有可能被突破
好 那剛才講的是被動的防禦
那還有主動的防禦
主動的防禦是說
我們在訓練模型的時候
一開始啊
就要訓練一個比較不會被攻破的模型
一開始就要訓練一個比較 Robust
比較不會被攻破的模型
那這種訓練的方式叫做 Adversarial Training
那這個 Adversarial Training 是怎麼操作的呢
就是你有一些訓練資料
這個跟一般的 Training 是一樣的
你有 Image
這邊用 x 來表示
Image（00：40：49）Label 用 ŷ 來表示
然後呢
我們就拿我們的訓練資料來訓練一個模型
訓練完以後
接下來你在訓練的階段
就對這個模型進行攻擊
你把這邊訓練的資料
x1 到 xN 都拿出來
製造一些 Signal
讓這些圖片變得具有攻擊性
那被攻擊後的 Image
叫做 x Tilde
你把這邊 x1 到 xN
訓練資料裡面的每一張圖片
都拿出來進行攻擊
攻擊完以後
你再把這些被攻擊過後的圖片
標上正確的 Label
就你把 x1 變成 x1 Tilde 以後
你的 Machine 就會辨識錯誤
本來是個貓的圖片
它可能就辨識錯成鍵盤
但是你現在把那個辨識錯成鍵盤的圖片拿來
重新把它標成貓
因為你已經知道說 x1
它的 Label 就是貓嘛
所以就算它變成 x Tilde
它現在輸入影像辨識系統以後
輸入這個你訓練好的模型以後
輸出的 Label 變了
你也知道原來正確的 Label 是什麼
你就把原來正確的 Label 拿回來
所以現在就製造了一個新的訓練資料
叫 x′
在新的訓練資料裡面
每一筆資料都是有被攻擊過的
原來 x1 到 XN
變成 x1 Tilde 到 XN Tilde
但是你的 Label
ŷ1 到這邊
我發現這邊有寫錯了
這個不是 ŷy
這應該是 ŷN
這個應該是 N 才對
這應該是 N 才對
好 這個 ŷ1 到 ŷN
這個 ŷ1 到 ŷN
是一樣的
那你再把 x 跟 x′ 倒在一起
得到更多的訓練資料
再重新去訓練你的模型
所以這整個 Adversarial Training 的概念就是
我們先訓練好一個模型
然後看看這個模型呢
有沒有什麼漏洞
把漏洞找出來
然後接下來呢
再把漏洞填起來
就不斷地找漏洞
找到就把它填起來
這個就是 Adversarial Training 的精神
好 那這個方法啊
其實也可以看作是一種
Data Augmentation 的方法
因為我們產生了更多的圖片
X Tilde
那再把這些圖片加到訓練資料裡面
這個等於就是做了資料增強
做了 Data Augmentation 這件事
所以有人也會把 Adversarial Training
當做一個單純的資料增強的方式
就是像這樣子的方式
不是只在你的 Model 可能被攻擊的時候有用
有時候就算沒有人要攻擊你的模型
你也可以用這樣的方法產生更多的資料
然後再把更多的資料拿去做訓練
也可以讓你的模型
它的 Robotics 的能力更好
更不容易 Overfitting
所以就算是沒有人要攻擊你的模型
你也可以用 Adversarial Training
來強化你的模型
避免 Overfitting 的狀況
那這個 Process 啊
產生有問題的圖片
再重新訓練
這個 Process 啊 是可以反覆做的
你可以產生圖片 重新訓練
再產生圖片 再產生訓練
就不斷找出問題補起來
找出問題補起來
這個 Process 是可以反覆做多次
直到你開心為止
那像這樣 Adversarial Training
它其實有個非常大的問題就是
它不見得擋得住新的攻擊的方式
就假設我們今天在找 x Tilde 的時候
你用的是 Algorithm ABCD
然後接下來有人在實際攻擊的時候
他發明了一個 Algorithm F 去攻擊你的模型
往往就能成功
如果今天實際上攻擊你 Model 的方法
並沒有在 Adversarial Training 的時候被考慮過
那 Adversarial Training
也不見得能夠擋住新的 Attack 的 Algorithm
所以 Adversarial Training 還是有
還是有可能被攻破的
另外 Adversarial Training
還有一個比較大的問題就是
它需要非常大
比較多的運算資源
你想想看
本來一般在訓練模型的時候
走到這邊就結束了
你有訓練資料 訓練完模型就結束了
但是 Adversarial Training 它的問題是
首先你要花時間
找出這些 x Tilde
你的圖片有幾張
你可能就要找出多少張的 x Tilde
100 萬張圖片
你要找 100 萬個 x Tilde
光做這件事
可能就已經很花時間了
所以你會發現說
如果你的 Dataset 很大的時候
大家通常就不會想要做 Adversarial Training
（00：45：18）
所以 Adversarial Training
是一個比較吃運算資源的方法
那為了解決這個問題
有人發明了一個方法叫做
Adversarial Training For Free
這邊我們就不細講
有一些方法是做到 Adversarial Training 的效果
卻沒有 Adversarial Training 那麼大的
Computing 的 Intensity
那至於怎麼做到 Adversarial Training For Free
怎麼不在使用額外的計算的情況下
就達到 Adversarial Training 的效果
那這個把文獻放在這邊
留給大家參考
好 那到目前為止呢
我們就是告訴大家
有攻擊這件事情
攻擊非常容易成功
黑箱攻擊也是有可能成功的
然後跟大家介紹了幾種經典的 Defense 的方式
那目前攻擊跟防禦啊
它們都
這些方法仍然不斷地在演化
所以在國際會議會不斷看到
有新的攻擊方法被提出
有新的防禦方法被提出
它們仍然都在進化中
那不知道最後會是誰勝誰負
好 那這個是今天的現況
