好,那我們就開始來上課啦
然後我們現在要進入開始講圖像的生成
那因為我們下一個作業,作業六呢,就是圖像的生成
所以我們開始來講圖像的生成模型
那在開始之前呢,我們先講圖像生成有什麼特別的地方
圖像生成有什麼特別的地方呢?
大家都知道俗話說啊,一張圖勝過千言萬語
所以今天假設我們圖像生成的任務
是拿一段文字去生成一張圖
比如說你把一次奔跑的狗丟給Me Journey
然後牠就會生一張圖給你
但是一張圖是勝過千言萬語的
如果要把這一張圖描述成文字
它的資訊量是遠比一隻在奔跑的狗還要多的
比如說這隻狗奔跑的位置是在街道上,或者是這隻狗是一隻柯基
有很多資料是不在你做圖像生成的時候輸入的文字裡面的
所以你輸入的文字只是千言萬語的部分
在做圖像生成的時候,有很多部分不在人類輸入的文字裡面
機器需要進行大量的腦補才能夠生成好的圖像
那這次圖像生成,我覺得跟比如說生成文字比較不一樣的地方
當然生成文字輸入跟輸出也不是一對一的關係
但是很多文字生成的任務,比如說翻譯
你一旦給定了輸入的句子,你輸出的可能性其實是蠻有限的
但圖像不一樣,你給一個句子,它輸出的可能性有非常多不同的樣貌
那其實我覺得語音合成也是一樣,你給一個句子,它要合出什麼樣的聲音
男的、女的、老的、少的,還是要用什麼樣的口氣說話
生氣的還是高興的,有非常多不同的變化
所以我覺得語音合成跟圖像生成比較像
就是我們的輸入只是輸出的一小部分
機器需要進行大量的腦補才能夠產生正確的結果
那等一下要跟大家講說這個需要進行大量腦補這件事
造成模型的設計上有什麼特別的地方
好,那我們知道文字生成多採用Auto-regressive的方法
那之前跟大家講過說在生成的時候,兩種策略
Auto-regressive各個擊破,或者是Non-Auto-regressive一次到位的方法
那我們知道文字生成是這樣,給一個句子
不管你是像GVT一樣通通都是Decoder
還是你是一個Sequence to Sequence的Model
我們在這個課堂的錄影裡面,大家應該已經看過
Sequence to Sequence的Model了
反正概念是一樣的,給一個輸入一個句子
然後接下來你要輸出一個機率的分布
然後根據這個機率的分布去做Sample,取出一個文字
根據取出來的文字再產生新的機率分布
再取出新的文字,這樣就可以產生一個句子
產生一個回答的結果
那對於影像生成來說,其實直覺上你會覺得
應該也可以套用幾乎一模一樣的做法
一個影像生成的模型應該可以是一段文字
接下來它輸出的這個distribution
不是文字的distribution,而是顏色的distribution
比如說綠色產生的機率應該有多少
黃色產生的機率應該有多少
那這邊的單位可能就是用Pixel
通常是用Pixel像素來表示
也就是說現在要畫一隻奔跑的狗
那你第一個像素要放一個紅色像素的機率有多少
要放一個綠色像素的機率有多少
那我們知道說這個影像一個Pixel
它是由RGB三個顏色所組成
那RGB分別又是由一個0到255的數字所表示
所以照理說應該有256x256x256三種不同
這麼多種不同的顏色
不過你實際在做影像生成的時候
其實你也用不上那麼多的顏色
你可以選個比如說256色
其實往往就蠻足夠的
因為像小時候在畫畫的時候
那個蠟筆如果64色
你就已經畫得很高興了對不對
所以影像生成的時候
你其實沒有必要有256x256x256種顏色
那其實像這種用Auto-regressive
生成影像的方法過去也是有的
OpenAI其實也做過很類似的事情
那他們那時候如果沒記錯的話
他也只選256個顏色而已
那總之你決定第一個Pixel
要放每一個顏色的機率
然後根據這個機率做Sample
比如說Sample到綠色
就畫一個綠色的Pixel
然後接下來根據已經畫了綠色的Pixel這件事
還有已經輸入的文字
再產生新的分布
然後可能就決定畫紅色的Pixel
以此類推
那如果要畫一張256x256這樣子的解析度的影像
那他有65536個Pixel
那同樣的過程
就是重複25536次
你就可以畫一個解析度是256x256的圖像
就結束了
能不能這麼做呢?
能這麼做
所以其實OpenAI有一個影像版的GPT
那影像版的GPT
他就是先把一張圖片
本來圖片其實是二維的
拉直變成一排
變成一個sequence
然後接下來呢
就是直接當作一個language model的問題
直接把它當作文字
只是現在我們這個Token的數目
因為只有256色
所以Token的數目就是256個
然後呢
用一個Auto-regressive的model
跟GPT架構是一樣的
然後Trend下去
就結束了
然後他們就畫一些圖啊
那為了要顯示說這個圖呢
這個裡面的pixel都是一排一排生出來的
那他特別呢
做了這張動畫
告訴你說
這個圖呢
他就是一排一排一排的
把那個pixel生出來
這個叫做Raster Order
一排一排的生pixel的順序叫做Raster Order
這個方法其實也可以生成蠻高清的圖片
那我們之前有講過說
對於影像生成而言
這種方法太耗費時間了
所以在影像生成的話
今天你看到多數的模型採取的
都是一次到位的方法
也就是說
給一段文字
如果你要生一個影像
那在概念上
在概念上
你會做的事情是
我把每一個位置
要放什麼顏色的distribution
都先產生出來
然後再根據這些distribution
去做sample
但是這麼做有什麼樣的問題呢
因為我們剛才有講過說
一張圖勝過千言萬語
所以當你輸入
正在奔跑的狗的時候
牠的正確答案
不是只有一個
有很多種不同的可能性
在奔跑的狗
可能有大隻的
有小隻的
有各種不同的毛色
背景可能是在草原上
可能在沙地裡
可能在城市裡
有各種不同的背景
而且我這邊畫的圖啊
這邊圖都是Me Journey生的啦
這邊的圖
這個狗都是迎面朝你衝來
其實牠也可以有其他衝的方向
牠可以從左到右啊
或是遠離你啊
也都是在奔跑的狗
所以今天當你輸入一段文字的時候
正確答案不是一個
一般我們在做機器學習的時候
我們通常比較習慣是
輸入一個東西
有一個標準的正確答案
但是在影像生成這個問題
它不是這樣的一個問題
影像生成的問題是
有一個輸入
它的正確答案
是一個分佈
是一個distribution
在這個分佈內的
都可以說是正確答案
但是如果我們一個一個pixel
分開生成
分開去做sample
會發生什麼事呢
如果一個一個pixel
分開去做sample
每一個pixel的產生
都是各自獨立的
那就會變成
有些pixel是想要畫
往左跑的狗
有些pixel想要畫
往右跑的狗
有些pixel想要畫黑的狗
有些pixel想要畫
白的狗
然後最後全部湊起來
就會不成樣
那比如說我把這邊四張圖片
各切一塊拼在一起
那你就不知道
在畫些什麼東西
所以如果是一次到位的方法
你要pixel
這個獨立的各自生成
其實是會得到
非常差的結果的
所以怎麼辦呢
你會發現
今天影像生成的模型
都有一個共同的套路
這共同的套路是什麼呢
都不是直接給一段文字
就生出圖
他們都會有一個
額外的輸入
這個額外的輸入
是由一個normal distribution
或者你要用別的distribution
其實也是可以的
總之就是一個簡單的
你知道長什麼樣子的distribution
也有人會用這個
uniform distribution
反正只要你知道
這個distribution長什麼樣子
就可以了
你從一個
你知道怎麼做sample的
簡單的distribution
去做sample
sample出一個像量
那這邊我在畫
這個distribution的時候
可能會讓你誤以為說
這邊是從一個one dimensional
的distribution做sample
那我不是這個意思
就是說這個normal distribution
你要想成是一個
高微的normal distribution
然後從一個
高微的normal distribution
做sample
得到一個高微的像量
然後把這個像量
丟到影像生成的模型裡面
跟這個文字合力去產生
你最後的結果
所以你會發現說
不管你今天是
用什麼樣影像生成的模型
VAE、GAME、Full Face Model
或者是最近
最熱門的Diffusion Model
其實方法都是一樣的
你都不是只有拿文字去生影像
你都需要一個額外的輸入
你都需要從某一個
簡單的機率分佈裡面
去sample出一個東西
sample出來的這個東西
有點像是雜訊
因為通常從
你是從這個
normal distribution sample出來
所以sample出來
就是一個像是雜訊的東西
丟到影像生成模型裡面
你才能夠產生最終的結果
那這背後所代表的含義是什麼呢
我們現在如果把文字
用Y來描述
影像用X來描述
那正確答案的分佈
我們可以寫成P of X given Y
那這個P of X given Y
它顯然會非常的複雜
我們現在會想要去
如果我們可以成功
model P of X given Y
如果我們可以知道說
P of X given Y
長什麼樣子
我們從這個分佈裡面
去做sample
我們就可以把圖畫出來
但是問題是P of X given Y
它非常的複雜
其實在過去古早的時代
比如說在
甚至GAME都還沒有的
2014年以前
你知道這個要生圖
給文字生圖
不是今天人們才開始做的事情
一直都有人在
做這方面的研究
在更過去
還不知道用什麼
deep learning生圖的時候
那時候多數時候就假設說
比如說P of X given Y
可能是一個mixture of Gaussian
有多個Gaussian所組成的
然後我們把這個模型
看能不能夠認出來
然後從這個mixture of Gaussian去sample
你sample出來
那個圖都非常的模糊
跟今天就是完全不在
同一個量級上面
所以這個P of X given Y
它顯然非常的複雜
複雜到
它不是一個你用人腦
可以想出來的模型
它不是一個Gaussian distribution
所以怎麼辦呢
這邊的策略是
我們把normal distribution裡面
sample出來的vector
都對應到P of X given Y
裡面的每一個X
我們知道說
如果要畫一隻奔跑的狗
有這麼多不同可能的樣子
那我們把normal distribution裡面
sample出來的vector
一個一個對應到
可能畫的狗
這一個影像生成的模型
等於就是在產生這個對應關係
影像生成的模型
它做的工作
就是想辦法
把normal distribution裡面
sample出來的東西
對應到正確的
狗在奔跑的圖片
好
那所以接下來難的點就是
怎麼把這一個distribution
把它做一些扭曲
扭成P of X given Y的樣子呢
那這個其實就是
所有的影像生成的模型
都在解的問題
那所以這些影像生成的模型
和背後想攻克的問題
其實是一樣的
只是解法是不一樣的
好 那我們就很快地
速覽一下常用的
影像生成的模型
以下的說明都非常簡略
有很多東西
我就把它帶過
就不細講
那大家如果想知道
這些影像模型
真正背後的原理
請參見參考資料
好 那這邊呢
第一個會講的是VAE
那VAE Variational Autoencoder
它做的事情是什麼呢
我們今天想要從
Normal Distribution對應到影像
那怎麼做呢
我們期待有一個Decoder
這個Decoder呢
就吃Normal Distribution
Sample出來的Vector
作為輸入
它的輸出就應該是
一張正確的圖片
那我這邊是把文字的輸入省略啦
那希望大家知道我的意思
好 那但是怎麼訓練
這樣的Decoder呢
怎麼訓練一個向量
對應到一張圖片
這樣子的Decoder呢
你知道你今天要訓練一個Network
你就是要有成對的資料
才能訓練Network嘛
如果有人告訴你說
這邊每一張圖片
都應該對應到哪一個向量
那你就可以訓練這個Decoder
我們並不知道
Normal Distribution
Sample出來的Vector
跟這些狗正在奔跑的圖片
到底有什麼樣的關係
那怎麼辦呢
那另外一個東西叫Encoder
Encoder做的事
就是Decoder的相反
把一張圖片變成一個向量
那可是只有Encoder
跟只有Decoder都沒有辦法訓練
所以怎麼辦呢
把Encoder跟Decoder串起來
所以一張圖片
輸入Encoder變成一個向量
欸 請說
No
所以剛才那個同學是問說
在這個投影片上面
看起來是一個Unconditional的
我沒有把文字的部分放進去
那其實你也可以
把文字的部分放進去
所以如果今天是要用文字
生成影像的話
那這個Decoder呢
其實它應該要有一個文字的輸入
然後告訴它說
我們已經有用文字
限縮了我們可以生成的範圍
但就算已經有文字
限縮生成的範圍
你生成的範圍
仍然是一個機率的分布
仍然不是有一個標準的
正確的答案
那剛才講到說
好 我們就一個Encoder
輸一張圖片 產生一個向量
這個向量丟給Decoder
要還原回一樣的圖片
那Encoder跟Decoder是一起訓練的
要讓輸入跟輸出越接近越好
但是光只有這樣訓練是不夠的
因為如果光只有這樣訓練
這邊的這些向量的分布
不一定會是一個Normal Distribution
所以你要加一個額外的限制
強迫中間的向量
是Normal Distribution
所以這個就是VAE的概念
如果想要知道VAE更多背後的原理
請見過去上課的錄影
接下來講Flow-Based Model
等一下每一個模型
都只有一頁投影片而已
Flow-Based Model是怎麼做的呢
它跟剛才VAE反過來
我們先來想Encoder
我們能不能夠訓練一個Encoder
吃一張圖片
輸出就是一個向量
而這個向量的分布
希望它是一個Normal Distribution
如果能夠做到這件事
一個Encoder輸一張圖片
輸出的向量就是Normal Distribution
然後接下來我們再強迫這個Encoder
是一個Invertible的Function
你就可以直接把這個Encoder
當作Decoder來用
訓練的時候
它是吃圖片輸出向量
這個向量的分布是Normal Distribution
實際上你要畫圖的時候
要把這個Encoder反過來用
讓它可以輸入一個向量
然後就輸出一張圖片
那你第一個會問的問題是
怎麼強迫Encoder一定是Invertible的呢
我們現在是在訓練Neural Network
隨便訓練一個Neural Network
你怎麼知道它的Inverse
長什麼樣子呢
這個就是Flow-Based Model神奇的地方
它有刻意限制Network的架構
所以Flow-Based Model
不是你隨便兜Network架構
都可以當作Encoder來用的
它有刻意限制了Network的架構
讓你Train完以後
你馬上知道這個Encoder的Inverse
長什麼樣子
好 那這個細節呢
大家就再參考資料
那另外一件事情是
因為這個Encoder它必須是Invertible
意味著什麼
意味著輸出的那個Vector
你的Dimension要跟輸入的圖片一樣
所以我在這個投影片上
這個Encoder的輸出
看起來比輸入的圖片略小
那你要想像這個Encoder的輸出
跟輸入的圖片是一樣大的
如果你輸入的圖片是256x256的圖片
那你Encoder的輸出
就要是一個256x256的向量
或者是你要把它排成一個
256x256的圖片
只是裡面都是一堆
看起來像是噪音的
看不懂的東西
這樣也可以
為什麼輸入跟輸出的大小
一定是一樣
如果不一樣
你就沒有辦法保證
它是Invertible的
如果輸出的向量比輸入的圖片Dimension小
你就沒辦法保證它是Invertible
所以輸出的向量
必須要跟輸入的圖片
它們的大小是一樣的
所以這個就是Flow-based Model的精神
你可以再重複一次你的問題嗎
剛才我試著先描述一下我的認知
然後等一下再看看你有沒有問題
就是這個Encoder就是
輸入圖片輸出Distribution
輸出一個向量
然後這個向量的分佈是Normal Distribution
是說我們把很多圖片一起丟進去以後
它的分佈是Normal Distribution
這邊並不是輸出一個Distribution
然後這個是Invertible的
所以它可以逆向來用
這很神奇
它可以逆向來用
所以Train完以後
給它一個向量
從Normal Distribution Sample出來的
它會變成一張圖片
這樣回答到你的問題嗎
好的 謝謝
大家有問題都歡迎隨時打斷
大家有問題嗎
你們問我什麼問題
要不要提出來
一樣大的意思
就是說
一樣大的意思就是
我知道說這個圖畫得沒有很好
把輸出畫得小一點
就當作它離你遠一點
所以看起來小一點
這個圖如果它是256x256的圖片
這一個輸出的向量
輸出的東西
它也要有256x256的數字那麼多
然後那當然就是256x256的數字
你可以把它排成一個很長的向量
這樣也可以啦
這邊是把它排成256x256的一個矩陣
那為什麼一定要這樣
不這樣的話
你就沒有辦法保證這個function是invertible
它是invertible代表
它的這個input跟output的dimension是一樣大的
如果大家對這個floor-based model
真的這麼有興趣的話
就是詳盡的錄影
然後接下來我們來就講
Diffusion model吧
為什麼這學期一定要講Diffusion model呢
因為我們的作業6
我們從game改成Diffusion model
所以我們一定要來講一下Diffusion model
有必要這麼訝異嗎
就算是改成Diffusion model
還是有聲轟轟的這樣子
好 這個來講一下
等一下在下一段投影片
會更詳細的跟大家講
Diffusion model是怎麼做的
那在這一頁投影片
我們就只用一頁來講一下
Diffusion model的概念
Diffusion model的概念是什麼呢
我們就是把一張圖片
一直加雜訊 一直加雜訊
大到原來的圖看不出來是什麼
那這個圖看起來就像是
從normal distribution sample出來的一個雜訊一樣
那怎麼生圖片呢
生圖片的方法就是
你認一個Denoise的model
那實際上怎麼做
等一下都還會再講到
認一個Denoise的model
丟一個看起來像是
丟一個從normal distribution sample出來的vector
當作輸入
那它就去掉噪音 去掉噪音
你要的圖慢慢就產生出來的
就是這麼神奇
那如果你現在心裡充滿了困惑跟懷疑
也是很正常的
那我們等一下呢
會再詳細的講這個模型
好 那最後一個呢
就是大家都耳熟能詳的
Generative Adversarial Network
也就是GAN
那GAN呢
它是只認Decoder
它就沒有認Encoder了
那GAN怎麼認Decoder呢
那一開始GAN呢
你就多給它
你就給它一大堆的
從normal distribution sample出來的向量
那一開始呢
這個Decoder沒有經過訓練
所以它根本不知道
怎麼畫一張圖就輸出一些亂七八糟的東西
通常一開始的輸出
是比現在這個投影片上的還要差很多啦
通常一開始輸出的東西就是些雜訊
你根本不知道在畫些什麼
那接下來你會訓練一個Discriminator
這個Discriminator的工作啊
用白話來講就是
去分辨輸入的圖片
是Decoder產生出來的假的圖片
還是真正的圖片
如果我們今天把真正的圖片的分布
想成P of X
當然如果你是有文字輸入的話
這邊就要given Y啦
given一個文字的condition
不過這邊先把文字的condition省略掉
那這個真實圖片的分布是P of X
Decoder產生出來的圖片分布
是P' of X
這個Discriminator實際上在做的事情是什麼呢
其實這個Discriminator訓練的時候的loss
其實就代表了P of X跟P' of X
它們的相似的程度
那這個在直覺上其實也是蠻容易理解的
因為當Discriminator沒有辦法分辨
來自這兩個distribution的image的時候
代表這兩個distribution的image
非常的接近
如果你的Discriminator
它的performance很差的
它的分類的錯誤率是很高的
或是它的loss是很高的
就代表說P of X跟P' of X
非常的接近
Decoder要做的事情
就是想辦法調整它的參數
讓Discriminator做得越差越好
那這個就是Gan的概念
當Discriminator做得差了
就代表P' of X
這個Decoder生出來的圖片的分布
跟真正的圖片的分布有很大的差距
那這個就是Gan
講到這邊大家有問題想要問嗎
那你說為什麼要叫Decoder
然後不叫Generator
為了要跟前面講的VaE感覺比較一致
所以我就把它命名為Decoder
但是這真的只是名稱而已
我們也可以把它改成叫做Generator
希望這樣回答到你的問題
這只是名詞的不同而已
如果你想要知道Gan的話呢
過去花了很多很多時間講Gan
這個講的話可以大概講個10個小時左右
大家可以慢慢看
有一個Gan的系列的錄影
好那這張投影片呢
是一次速覽VaE Flow-based Model
跟Diffusion Model的差異
那它們的共通性就是
它們都有一個Encode的機制
一個Decode的機制
在VaE裡面
Encoder、Decoder都是Neural Network
都是類神經網路
都很複雜
那在Flow-based Model裡面
我們其實只認了Encoder
但是我們做了一些手腳
保證Encoder是Invertible的
所以我們的Decoder
其實就是Encoder的Inverse
所以在Flow-based Model裡面
只需要認Encoder
那Diffusion Model是反過來
其實它只有認Decoder的這個Module
它在Encode的時候
就沒有Encoder這個東西
它就是把圖片一直加雜訊
一直加雜訊
這個就是Encoder
當然你也可以把加雜訊這個步驟
想成是一個Encoder
只是這是一個不需要認的Encoder
它沒有參數
所以不需要Trend它
那產生這個雜訊以後
再做Denoise
做N次Denoise
把這個雜訊還原成圖片
那這個做N次Denoise的過程
其實就是Decoder
你可以想成
這每一次做Denoise
就是Decoder通過了一層
所以VaE、Flow-based Model
跟Diffusion Model
其實它們有非常多的共通性
好 那我知道說
今天大家看起來
就是說今天最強的那些模型
比如說Darli
都是用這個Diffusion
Stable Diffusion
都是用Diffusion Model做的
但是這些模型會這麼強
也不完全是Diffusion Model的功勞
其實等一下我們會講一下
概覽一下Stable Diffusion
那你會知道說Stable Diffusion裡面
其實是加了很多其他的東西
今天這些模型才會這麼的厲害
好 那這一頁投影片裡面沒有Gam
為什麼沒有Gam呢
因為我覺得Gam跟VaE
Flow-based Model跟Diffusion Model
它其實就是另外一個角度的思考
所以它跟VaE
Flow-based Model跟Diffusion Model
事實上是沒有互斥的
你永遠可以在你的Decoder後面
再接一個Discriminator
讓你的Decoder的Output
跟真實的圖片的分布
越接近越好
所以VaE可不可以加Gam呢
可以就有VaE Gam
這個是很古早的年代
15年的paper
從這個圖像就可以明顯看出
這是個VaE
然後這是個Gam
然後這邊這個Decoder呢
在這個VaE裡面叫做Decoder
在Gam裡面叫Generator
但其實是不同的名字
同樣的東西
所以VaE可以加上Gam
Flow-based Model可以加上Gam
Diffusion Model最近很紅
它也可以加上Gam
所以Gam算是另外一個外掛
它可以加到現有的
這個生成的模型上面
好,這個就是
影像常用生成模型速覽
