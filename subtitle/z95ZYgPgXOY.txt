臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://ai.ntu.edu.tw/
各位同學大家好，那我們來上課吧
那今天這一堂課，我們要講什麼呢？ 我們要講一個 policy gradient 的進階版
這個技術呢，叫做 Proximal Policy Optimization (PPO)
這個技術有多屌呢？這個技術呢， 它是 default reinforcement learning algorithm at OpenAI
是OpenAI 把這個方法， 當作他們 default reinforcement learning algorithm
所以今天假設你要 implement reinforcement learning，也許這是一個第一個你可以嘗試的方法，這是 PPO
今天這門課呢， 我們還是幫大家複習一下 Policy Gradient
這部分也許有點快，所以如果你有問題的話， 你就直接舉手打斷我
接下來，我們會講說，原來的 policy gradient， 是 On-Policy 的方法
那怎麼從 on-policy 的方法，變到 off-policy 的方法， 那等一下會講，on-policy 是什麼，off-policy 是什麼
最後，從 on-policy 變成 off-policy 以後， 再加一些 constrains，就變成了 PPO
那在講這個技術之前，我們先看一下， Deep mind 跟 OpenAI，釋出來的 demo
其實 deep mind 跟 OpenAI 各有一篇 PPO 的 paper
那是 Deep mind 先發了，如果我沒有記錯時間的話， 應該是在去年 7 月的月初，就發了 PPO 的 paper
那在他們的 paper 裡面，其實有， 他沒有說 PPO 不是他們 proposed
他們 cited 一個 reference，這個 reference 是什麼呢？
這個 reference 是 OpenAI 的某一個人， 我記得是 John Schulman
他在 Nice 的 tutorial 這樣， 他在他的 tutorial 裡面有提到 PPO，但他還沒有寫 paper
然後 deep mind 就先寫了 paper， 然後 proposed PPO 這個方法
然後在裡面是 cited Nice tutorial 影片的連結
然後過了數日之後，可能 OpenAI 就發現說被 deep mind 搶先發走了
他們也迅速的發了另外一片他們自己的 PPO 的 paper
我們看一下 deep mind 的 demo
這種讓機器人學走路的問題其實蠻難的， 可以學起來，其實蠻厲害的
這個是 OpenAI 的 demo，這個 demo 裡面， 它也一樣控制一個機器人，這個機器人會在球場上
去找紅色的球，吃紅色的球， 然後會有白色的球一直攻擊它
機器人被打倒以後，它會自己學著站起來， 然後它去找紅色的球
我覺得這個 demo 所 demo 的不只是 PPO， 它 demo 其實是人生你知道嗎？以下部分省略
那我們就來先複習一下 policy gradient，PPO 是 policy gradient 一個變形
所以我們先講 policy gradient
在 reinforcement learning 裡面呢有 3 個 components
一個 actor，一個 environment，一個 reward function
在我們作業 4-1 裡面呢，就是要做
讓機器玩 video game，那這個時候你 actor 做的事情
就是去操控，遊戲的搖桿， 比如說向左向右，開火，等等，
你的 environment 就是遊戲的主機， 負責控制遊戲的畫面
負責控制說，怪物要怎麼移動， 你現在要看到什麼畫面，等等
所謂的 reward function，就是決定，當你做什麼事情，發生什麼狀況的時候
你可以得到多少分數， 比如說殺一隻怪獸，得到20 分等等
那同樣的概念，用在圍棋上也是一樣
actor 就是 alpha Go，它要決定， 下哪一個位置，那你的 environment 呢
就是對手，你的 reward function 就是按照圍棋的規則， 贏就是得一分，輸就是負一分等等
那在 reinforcement 裡面啊，你要記得說 environment 跟 reward function，不是你可以控制的
environment 跟 reward function， 是在開始學習之前，就已經事先給定的
你唯一能做的事情，是調整你的 actor
調整你 actor 裡面的 policy
使得它可以得到最大的 reward，你可以調的只有 actor
environment 跟 reward function 是事先給定， 你是不能夠去動它的
那這個 actor 裡面，會有一個 policy， 這個 policy 決定了 actor 的行為，
那所謂的 policy 呢，就是給一個外界的輸入
然後它會輸出 actor 現在應該要執行的行為
那今天假設你是用 deep learning 的技術來做 reinforcement learning 的話
那你的 policy，policy 我們一般寫成 pi， policy 就是一個 network
那我們知道說，network 裡面，就有一堆參數， 我們用 theta 來代表 pi 的參數
今天你的這個 network，你的 policy 它是一個 network
這個 network 的 input 它就是現在 machine 看到的東西
如果讓 machine 打電玩的話， 那 machine 看到的東西，就是遊戲的畫面
當然讓 machine 看到什麼東西，會影響你現在 training
到底好不好 train
舉例來說，在玩遊戲的時候， 也許你覺得遊戲的畫面，前後是相關的
也許你覺得說，你應該讓你的 policy，看從遊戲初始
到現在這個時間點，所有畫面的總和， 你可能會覺得你要用到 RNN 來處理它
不過這樣子，你會比較難處理就是了，那要讓
你的 machine 你的 policy 看到什麼樣的畫面， 這個是你自己決定的
那等一下在講作業 4-1 的時候，助教會給你一些 tip， 給你參考，讓你知道說
給機器看到什麼樣的遊戲畫面，可能是比較有效的
那在 output 的部分， 輸出的就是今天機器要採取什麼樣的行為
這邊這個是具體的例子，你的 policy 就是一個 network
input 就是遊戲的畫面，那它通常就是由 pixels 所組成的
那 output 就是看看說現在有那些選項
是你可以去執行的， 那你的 output layer 就有幾個 neurons
假設你現在可以做的行為就是有 3 個， 那你的 output layer 就是有 3 個 neurons
每個 neuron 對應到一個可以採取的行為
好，那 input 一個東西以後呢
你的 network 就會給每一個可以採取的行為一個分數
接下來，你把這個分數，當作是機率
那你的 actor 就是看這個機率的分布
根據這個機率的分布，決定它要採取的行為
比如說 70% 會走 left，20% 走 right，10% 開火， 等等
那這個機率分布不同，你的 actor 採取的行為， 就會不一樣
那這是 policy 的部分，它就是一個 network
接下來用一個例子，具體的很快地說一下說， 今天你的 actor 是怎麼樣跟環境互動的
首先你的 actor 會看到一個遊戲畫面，這個遊戲畫面
我們就用 s1 來表示它，它代表遊戲初始的畫面
接下來你的 actor 看到這個遊戲的初始畫面以後
根據它內部的 network，根據它內部的 policy
它就會決定一個 action， 那假設它現在決定的 action 是向右，
那它決定完 action 以後，它就會得到一個 reward
代表它採取這個 action 以後，它會得到多少的分數，
那這邊我們把一開始的初始畫面，寫作 s1， 我們把第一次執行的動作叫做 a1
我們把第一次執行動作完以後得到的 reward，叫做 r1
那不同的文件，其實有不同的定義，有人會覺得說
這邊應該要叫做 r2，這個都可以，你自己看得懂就好
那接下來就看到新的遊戲畫面，你的
actor 決定一個的行為以後， 就會看到一個新的遊戲畫面，這邊是 s2
然後把這個 s2 輸入給 actor，這個 actor 決定要開火
然後它可能殺了一隻怪，就得到五分， 然後這個 process 就反覆的持續下去
直到今天走到某一個 timestamp
執行某一個 action，得到 reward 之後， 這個 environment 決定這個遊戲結束了
比如說，如果在這個遊戲裡面，你是控制綠色的船去殺怪，如果你被殺死的話
遊戲就結束，或是你把所有的怪都清空，遊戲就結束了
那一場遊戲，叫做一個 Episode
把這個遊戲裡面，所有得到的 reward
通通總合起來，就是 Total reward， 那這邊用大 R 來表示它，
那今天這個 actor 它存在的目的， 就是想辦法去 maximize 它可以得到的 reward，
那這邊是用圖像化的方式，來再跟大家說明一下， 你的 environment，actor，還有 reward 之間的關係，
首先，environment 其實它本身也是一個 function
連那個遊戲的主機，你也可以把它看作是一個 function
雖然它裡面不見得是 neural network，可能是 rule-based 的規則，但你可以把它看作是一個 function
那這個 function，一開始就先吐出一個 state
然後接下來呢，也就是遊戲的畫面
接下來你的 actor 看到這個遊戲畫面 s1 以後，它吐出 a1
然後接下來 environment，把這個 a1 當作它的輸入， 然後它再吐出 s2，吐出新的遊戲畫面
actor 看到新的遊戲畫面，又再決定新的行為 a2
然後 environment 再看到 a2，再吐出 s3， 那這個 process 就一直下去
直到 environment 覺得說應該要停止為止
在一場遊戲裡面，我們把 environment 輸出的 s
跟 actor 輸出的行為 a，把這個 s 跟 a 全部串起來， 叫做一個 Trajectory
每一個 trajectory，你可以計算它發生的機率
假設現在 actor 的參數已經被給定了話，就是 theta
根據這個 theta，你其實可以計算某一個 trajectory 發生的機率
你可以計算某一個回合，某一個 episode 裡面， 發生這樣子狀況的機率
怎麼算呢，某一個 trajectory tao， 在假設你 actor 的參數就是 theta 的情況下
某一個 trajectory tao，它的機率就是這樣算的
你先算說 environment 輸出 s1 的機率
再計算，根據 s1 執行 a1 的機率
是由你 policy 裡面的那個 network 參數 theta 所決定的，
它是一個機率，因為我們之前有講過說
你的 policy 的 network 它的 output 它其實是一個 distribution
那你的 actor 是根據這個 distribution 去做 sample， 決定現在實際上要採取的 action是哪一個
接下來你這個 environment，根據， 這邊畫是寫說根據 a1 產生 s2，那其實它是根據
a1 跟 s1 產生 s2，因為你想說，s2 跟 s1 還是有關係的，下一個遊戲畫面，跟前一個遊戲畫面
通常還是有關係的，至少要是連續的， 所以這邊是給定前一個遊戲畫面 s1
跟你現在 actor 採取的行為 a1，然後會產生 s2，
這件事情它可能是機率，也可能不是機率
這個是就取決於那個 environment，就是那個主機它內部設定是怎樣
看今天這個主機在決定，要輸出什麼樣的遊戲畫面的時候，有沒有機率，因為如果沒有機率的話
那這個遊戲的每次的行為都一樣，你只要找到一條 path，就可以過關了，這樣感覺是蠻無聊的
所以遊戲裡面，通常是還是有一些機率的
你做同樣的行為，給同樣的給前一個畫面， 下次產生的畫面其實不見得是一樣的，
Process 就反覆繼續下去，你就可以計算說
一個 trajectory s1,a1, s2, a2 它出現的機率有多大
這邊只是把這個機率，把它寫出來而已
那這個機率，取決於兩件事， 一部分是 environment 本身的行為
若 environment 的 function 它內部的參數或內部的規則長什麼樣子
那這個部分，就這一項 p of s(t+1) given st,at
代表的是 environment， 這個 environment 這一項通常你是無法控制它的
因為那個是人家寫好的，你不能控制它
你能控制的是這個， 你能控制的是 p theta of at given st，你就 given 一個 st
你的 actor 要採取什麼樣的行為， at 這件事，會取決於你 actor 的參數
你的 passed 參數 theta， 所以這部分是 actor 可以自己控制的
隨著 actor 的行為不同，每個同樣的 trajectory， 它就會有不同的出現的機率
我們說在 reinforcement learning 裡面
除了 environment 跟 actor 以外呢， 還有第三個角色，叫做 reward function
Reward function 做的事情就是， 根據在某一個 state 採取的某一個 action
決定說現在這個行為，可以得到多少的分數
它是一個 function，給它 s1，a1，它告訴你得到 r1， 給它 s2，a2，它告訴你得到 r2
我們把所有的小 r 都加起來，我們就得到了大 R
我們這邊寫做大 R of tao，代表說是
某一個 trajectory tao，在某一場遊戲裡面， 某一個 episode 裡面，我們會得到的大 R
那今天我們要做的事情就是調整 actor 內部的參數 theta
使得大 R 的值，越大越好
但是實際上 reward，它並不只是一個 scalar
reward 它其實是一個 random variable，這個大 R 其實是一個 random variable，為什麼呢？
因為你的 actor 本身，在給定同樣的 state 會做什麼樣的行為，這件事情是有隨機性的
你的 environment， 在給定同樣的 action 要採取什麼樣的 observation
要產生什麼樣的 observation，本身也是有隨機性的
所以這個大 R 其實是一個 random variable
你能夠計算的，是它的期望值
你能夠計算的是說，在給定某一組參數 theta 的情況下
我們會得到的這個大 R 的期望值，是多少
那這個期望值是怎麼算的呢？這期望值的算法就是， 窮舉所有可能的 trajectory
窮舉所有可能的 trajectory tao， 每一個 trajectory tao，它都有一個機率
比如說今天你的 theta 是一個很強的 model， 那它都不會死
那如果今天有一個 episode 是很快就死掉了， 它的機率就很小
如果有一個 episode 是都一直沒有死， 那它的機率就很大
那根據你的 theta， 你可以算出某一個 trajectory tao 出現的機率
接下來你計算這個 tao 的 total reward 是多少
把 total reward weighted by 這個 tao 出現的機率
summation over 所有的 tao，顯然就是期望值
顯然就是 given 某一個參數你會得到的期望值
或你會寫成這樣，從 p(theta) of tao 這個 distribution
sample 一個 trajectory tao，然後計算 R of tao 的期望值，就是你的 expected reward
那我們要做的事情，就是 maximize expected reward
怎麼 maximize expected reward 呢？我們用的就是 gradient ascent
因為我們是要讓它越大越好，所以不是 gradient decent，是 gradient ascent
所以跟 gradient decent 唯一不同的地方就只是
本來在 update 參數的時候，要減， 現在變成加，這 gradient ascent
然後這 gradient ascent 你就必須計算呢，R bar，這 expected reward，它的 gradient
R bar 的 gradient 怎麼計算呢？這跟我上周講那個 GAN 產生做 sequence generation 的式子，其實是一模一樣的
那所以這部分，我們會很快把它走過去
我們說，R bar 我們取一個 gradient，這裡面只有 p theta
是跟 theta 有關，所以 gradient 就放在 p theta 這個地方
那 R 這個 reward function 啊， 不需要是 differentiable，我們也可以解接下來的問題
舉例來說，如果是在 GAN 裡面， 你的這個 R 其實是一個 discriminator
它就算是沒有辦法微分，也無所謂， 你還是可以做接下來的運算
接下來要做的事情，這個大家可能都看過很多次了， 分子分母，上下同乘 p(theta) of tao
然後接下來我就會告訴你說，後面這一項， p(theta) of tao 的 gradient 除以 p(theta) of tao
其實就是這個 log p(theta) of tao，取 gradient
或者是你其實之後就可以直接背一個公式
就某一個 function f of x，你對它做 gradient 的話， 就等於 f of x 乘上 gradient log f of x
所以今天這邊有一個 gradient p(theta) of tao
帶進這個公式裡面呢，這邊應該變成 p(theta) of tao 乘上 gradient log p(theta) of tao
然後接下來呢，這邊又 summation over tao， 然後又有把這個 R 跟這個 log 這兩項啊
weighted by p(theta) of tao，
那既然有 weighted by p(theta) of tao，它們就可以被寫成這個 expected 的形式
也就是你從 p(theta) of tao 這個 distribution 裡面 sample tao 出來
去計算 R of tao 乘上 gradient log p(theta) of tao
然後把它對所有可能的 tao 呢， 做 summation，就是這個 expected value
然後接下來呢，這個 expected value 實際上你沒有辦法算，所以你是用 sample 的方式
來 sample 一大堆的 tao，你 sample 大 N 筆 tao， 然後每一筆呢，你都去計算它的這些 value
然後把它全部加起來，最後你就得到你的 gradient
你就可以去 update 你的參數， 你就可以去 update 你的 agent
那這邊呢，我們跳了一大步，這邊這個 p(theta) of tao，我們前面有講過 p(theta) of tao 是可以算的
那 p(theta) of tao 裡面有兩項，一項是來自於 environment，一項是來自於你的 agent
來自 environment 那一項，其實你根本就不能夠算它
你對它做 gradient 是沒有用的，因為它跟 theta 是完全沒有任何關係的，所以你不需要對它做 gradient
你真正會對它做 gradient 的， 只有 log p(theta) of at given st 而已
這個部分，其實你可以非常直觀的來理解它
也就是在你 sample 到的 data 裡面， 你 sample 到，在某一個 state st 要執行某一個 action at
那如果在某一個 state st，執行某一個 action at
最後導致整個 trajectory tao，就是這個 st 跟 at
它是在整個 trajectory tao 的裡面的某一個 state and action 的 pair
假設你在 st 執行 at，最後發現 tao 的 reward 是正的， 那你就要增加這一項的機率
你就要增加在 st 執行 at 的機率
反之，在 st 執行 at 會導致整個 trajectory 的 reward 變成負的
你就要減少這一項的機率，那這個概念就是怎麼簡單
那因為我們在作業裡面要實作，那這個怎麼實作呢？
你實作的方法就是這個樣子， 你用 gradient ascent 的方法
來 update 你的參數，所以你原來有一個參數 theta
你把你的 theta 加上你的 gradient 這一項，那當然前面要有個 learning rate
learning rate 其實也是要調的，你要用 ADAM、rmsprop 等等，還是要調一下
那這 gradient 這一項怎麼來呢？ gradient 這一項，就套下面這個公式，把它算出來
那在實際上做的時候，要套下面這個公式， 首先你要先收集一大堆的 s 跟 a 的 pair
你還要知道這些 s 跟 a，如果實際上在跟環境互動的時候
你會得到多少的 reward， 所以這些資料，你要去收集起來，
這些資料怎麼收集呢？ 你就要拿你的 agent，它的參數是 theta
去跟環境做互動， 也就是你拿你現在已經 train 好的那個 agent
先去跟環境玩一下，先去跟那個遊戲互動一下， 那互動完以後，你就會得到一大堆遊戲的紀錄
你會記錄說，今天先玩了第一場
在第一場遊戲裡面，我們在 state s1，採取 action a1，在 state s2，採取 action a2
那要記得說其實今天玩遊戲的時候，是有隨機性的
所以你的 agent 本身是有隨機性的，所以在同樣 state s1，不是每次都會採取 a1，所以你要記錄下來
在 state s1，採取 a1，在 state s2，採取 a2，然後最後呢
整場遊戲結束以後，得到的分數，是 R of tao(1)
那你會 sample 到另外一筆 data，也就是另外一場遊戲，在另外一場遊戲裡面
你在第一個 state 採取這個 action，在第二個 state 採取這個 action
在第二個遊戲畫面採取這個 action，然後你 sample 到的，你得到的 reward 是 R of tao(2)
你有了這些東西以後，你就去把這邊你 sample 到的東西
帶到這個 gradient 的式子裡面，把 gradient 算出來
也就是說你會做的事情是，把這邊的每一個 s 跟 a 的 pair
拿進來，算一下它的 log probability
你計算一下，在某一個 state，採取某一個 action 的 log probability，然後對它取 gradient
然後這個 gradient 前面會乘一個 weight，這個 weight 是什麼？這個 weight 就是這場遊戲的 reward
你有了這些以後，你就會去 update 你的 model， 你 update 完你的 model 以後
你回過頭來要重新再去收集你的 data
然後再去收集你的 data，再 update model...
那這邊要注意一下，一般 policy gradient，你 sample 的 data 就只會用一次
你把這些 data sample 起來，然後拿去 update 參數，這些 data 就丟掉了
再重新 sample data，才能夠再重新去 update 參數
等一下我們會解決這個問題
那接下來的就是實作的時候你會遇到的實作的一些細節
實際上這個東西到底是怎麼實作的呢？
因為到時候你要真的實作嘛，所以我們還是講一下
這個東西到底實際上在用這個 deep learning 的 framework implement 的時候
它是怎麼實作的呢，其實你的實作方法是這個樣子
你要把它想成你就是在做一個分類的問題
分類問題大家都會嘛，對不對， 現在我們電機營都已經教大家用 TensorFlow
implement MNIST classification， 理論上每個人都會做 classification
那在 classification 裡面就是 input 一個 image， 就是做 MNIST input 一個 image
然後 output 就是要決定說，是 10 個 class 裡面的哪一個
所以那要怎麼做 classification，當然要收集一堆 training data
你要有 input 跟 output 的 pair
那今天在 reinforcement learning 裡面，在實作的時候
你就把 state 當作是 classifier 的 input
你就當作你是要做 image classification 的 problem
只是現在的 class 不是說 image 裡面有什麼 objects
現在的 class 是說，看到這張 image 我們要採取什麼樣的行為，每一個行為就叫做一個 class
比如說第一個 class 叫做向左，第二個 class 叫做向右，第三個 class 叫做開火
那這些訓練的資料是從哪裡來的呢？
我們說你要做分類的問題，你要有 classified 的 input， 跟它正確的 output
這些訓練資料哪來呢？
這些訓練資料，就是從 sampling 的 process 來的
假設在 sampling 的 process 裡面，在某一個 state
你 sample 到你要採取 action a， 你就把這個 action a 當作是你的 ground truth
你在這個 state，你 sample 到要向左
本來向左這件事機率不一定是最高， 因為你是 sample，它不一定機率最高
假設你 sample 到向左，那接下來在 training 的時候
你叫告訴 machine 說，調整 network 的參數， 如果看到這個 state，你就向左
在一般的 classification 的 problem 裡面
其實你在 implement classification 的時候， 你的 objective function
你都會寫成，minimize cross entropy
那其實 minimize cross entropy 就是 maximize log likelihood
所以你今天在做 classification 的時候，你的 objective function
你要去 maximize 或是 minimize 的對象， 因為我們現在是 maximize likelihood
所以其實是 maximize， 你要 maximize 的對象，其實就長這樣子
像這種 lost function，你在 TensorFlow 裡面，你 even 不用手刻，它都會有現成的 function 就是了，
你就 call 個 function，它就會自動幫你算這樣子的東西
然後接下來呢，你就 apply 計算 gradient 這件事，那你就可以把 gradient 計算出來，這是一般的分類問題
那如果今天是 RL 的話，唯一不同的地方只是， 你要記得在你原來的 loss 前面
乘上一個 weight，這個 weight 是什麼？
這個weight 是，今天在這個 state，採取這個 action 的時候，你會得到的 reward
這個 reward 不是當場得到的 reward
而是整場遊戲的時候得到的 reward， 它並不是在 state s 採取 action a 的時候得到的 reward
而是說，今天在 state s 採取 action a 的這整場遊戲裡面，你最後得到的 total reward 這個大 R
你要把你的每一筆 training data，都 weighted by 這個大 R
然後接下來，你就交給 TensorFlow 或 pyTorch 去幫你算 gradient，然後就結束了
跟一般 classification 其實也沒太大的差別
這邊有一些通常實作的時候，你也許用得上的 tip， 一個就是你要 add 一個東西叫做 baseline
所謂的 add baseline 是什麼意思呢？
今天我們會遇到一個狀況是
我們說這個式子，它直覺上的含意就是，假設 given state s 採取 action a
會給你整場遊戲正面的 reward，那你就要增加它的機率
如果說今天在 state s 執行 action a，整場遊戲得到負的 reward，你就要減少這一項的機率
但是我們今天很容易遇到一個問題是， 很多遊戲裡面，它的 reward 總是正的
就是說最高都是 0，比如說打乒乓球
作業 4-1 是玩很簡單的就是碰的遊戲， 你的分數就是介於 0-21 分間
根本就不會得到負的分數，所以這個 R 總是正的
所以假設你直接套用這個式子， 你會發現說在 training 的時候
你告訴 model 說，今天不管是什麼 action
你都應該要把它的機率提升， 這樣聽起來好像有點怪怪的
在理想上，這麼做並不一定會有問題，為什麼呢？
因為今天雖然說 R 總是正的
但它正的量總是有大有小，你在玩乒乓球那個遊戲裡面，得到的 reward 總是正的
但它是介於 0-21 分之間，所以有時候你是得到 20 分， 有時候你採取某些 action 可能是得到 0 分
採取某些 action 可能是得到 20 分
假設你現在有 3 個 action a/b/c，可以執行，在某一個 state 有 3 個 action a/b/c，可以執行
根據這個式子，你要把這 3 項的分數， 機率，log probability 都拉高
但是它們前面 weight 的這個 R，是不一樣的
那麼前面 weight 的這個 R 是有大有小的，weight 小的
它上升的就少，weight 多的，它上升的就大一點
那因為今天這個 log probability，它是一個機率，所以，這三項的和，要是 0
所以上升的少的，在做完 normalize 以後， 它其實就是下降的，上升的多的，才會上升
那這個是一個理想上的狀況，但是實際上， 你千萬不要忘了，我們是在做 sampling
就本來這邊應該是一個 expectation， summation over 所有可能的 s 跟 a 的 pair
但是實際上你真正在學的時候，當然不可能是這麼做的，你只是 sample 了少量的 s 跟 a 的 pair 而已
所以因為我們今天做的是 sampling，有一些 action 你可能從來都沒有 sample 到
在某一個 state，雖然可以執行的 action 有 a/b/c，3 個
但你可能只 sample 到 action b，你可能只 sample 到 action c，你沒有 sample 到 action a
但現在所有 action 的 reward 都是正的，所以根據這個式子，今天它的每一項的機率都應該要上升
但現在你會遇到的問題是，因為 a 沒有被 sample 到
其它人的機率如果都要上升，那 a 的機率就下降， 所以，a 可能不是一個不好的 action
它只是沒被 sample 到， 也就是運氣不好沒有被 sample 到
但是只是因為它沒被 sample 到， 它的機率就會下降，那這個顯然是有問題的
要解決這個問題要怎麼辦呢？ 你會希望你的 reward 不要總是正的
為了解決望你的 reward 不要總是正的這個問題
你可以做的一個非常簡單的改變就是，把你的 reward 減掉一項叫做 b，這項 b 叫做 baseline
你減掉這項 b 以後，就可以讓 R-b 小括號這一項， 有正有負
所以今天如果你得到的 reward 這個 R of tao(n)， 這個 total reward 大於 b 的話
就讓他的機率上升，如果這個 total reward 小於 b， 就算它是負的，就算它是正的
因為遊戲裡面不可能有負的， 所以如果正的很小，也是不好的
所以你就要讓這一項的機率下降
如果今天 R of tao(n) 它小於 b 的話， 你就要讓這個 state 採取這個 action 的分數下降
那這個 b 怎麼設呢？你就隨便設， 你就自己想個方法來設
那一個最最簡單的做法就是， 你把那個 tao(n) 的值，取絕對值
喔不對，你把 tao(n) 的值，取 expectation， 算一下 tao(n) 的平均值
你就可以把它當作 b 來用，這是其中一種做法， 你永遠可以想想看你有沒有其它的做法
所以在實作上，你就是在 implement/training 的時候
你會不斷的把 R of tao 的分數，把它不斷的記錄下來
然後你會不斷的去計算 R of tao 的平均值， 然後你會把你的這個平均值，當作你的 b 來用
這樣就可以讓你在 training 的時候
你今天，這個 gradient log probability 乘上前面這一項， 是有正有負的，這個是第一個 tip
第二個 tip 我們在 machine learning 那一門課沒有講到，前面那的東西在 machine learning 講過了
今天我們要講在 machine learning 那一門課沒有講過的 tip，這個 tip 是這樣子
今天你應該要給每一個 action，合適的 credit
什麼意思呢，如果我們看今天下面這個式子的話
我們原來會做的事情是，今天在某一個 state
假設，你執行了某一個 action a
它得到的 reward ，它前面乘上的這一項，就是 (R of tao)-b
今天只要在同一個 Episode 裡面，在同一場遊戲裡面， 所有的 state 跟 a 的 pair
它都會 weighted by 同樣的 reward/term
這件事情顯然是不公平的，因為在同一場遊戲裡面
也許有些 action 是好的，也許有些 action 是不好的
那假設最終的結果，整場遊戲的結果是好的， 並不代表這個遊戲裡面每一個行為都是對的
若是整場遊戲結果不好， 但不代表遊戲裡面的所有行為都是錯的
所以我們其實希望，可以給每一個不同的 action， 前面都乘上不同的 weight
那這個每一個 action 的不同 weight， 它真正的反應了每一個 action，它到底是好還是不好
那這邊就是舉了一個例子說， 假設現在這個遊戲都很短，只會有 3-4 個互動
在 sa 這個 state 執行 a1 這件事，得到 5 分
在 sb 這個 state 執行 a2 這件事，得到 0 分
在 sc 這個 state 執行 a3 這件事，得到 -2 分
整場遊戲下來，你得到 +3 分，那今天你得到 +3 分
代表在 state sb 執行 action a2 是好的嗎？ 並不見得代表 state sb 執行 a2 是好的
因為這個正的分數，主要是來自於在一開始的時候 state sa 執行了 a1
也許跟在 state sb 執行 a2 是沒有關係的
也許在 state sb 執行 a2 反而是不好的， 因為它導致你接下來會進入 state sc 執行 a3 被扣分
所以今天整場遊戲得到的結果是好的， 並不代表每一個行為都是對的
如果按照我們剛才的講法，今天整場遊戲得到的分數是 3 分
那到時候在 training 的時候， 每一個 state 跟 action 的 pair，都會被乘上 +3
在理想的狀況下，這個問題，如果你 sample 夠多
就可以被解決，為什麼？因為假設你今天 sample 夠多
在 state sb 執行 a2 的這件事情，被 sample 到很多次
就某一場遊戲，在 state sb 執行 a2，你會得到 +3 分
但在另外一場遊戲，在 state sb 執行 a2，你卻得到了 -7 分
為什麼會得到 -7 分呢？
因為在 state sb 執行 a2 之前， 你在 state sa 執行 a2 得到 -5 分，那這 -5 分可能也不是
中間這一項的錯，這 -5 分這件事可能也不是在 sb 執行 a2 的錯，這兩件事情，可能是沒有關係的，因為它先發生了
這件事才發生，所以他們是沒有關係的
在 state sb 執行 a2，它可能造成問題只有
會在接下來 -2 分，而跟前面的 -5 分沒有關係的
但是假設我們今天 sample 到這項的次數夠多
把所有有發生這件事情的情況的分數通通都集合起來， 那可能不是一個問題
但現在的問題就是，我們 sample 的次數，是不夠多的
那在 sample 的次數，不夠多的情況下，你就需要想辦法，給每一個 state 跟 action pair 合理的 credit
你要給他合理的，你要讓大家知道它合理的 contribution，它實際上對這些分數的貢獻到底有多大
那怎麼給它一個合理的 contribution 呢？
一個做法是，我們今天在計算這個 pair，它真正的 reward 的時候
不把整場遊戲得到的 reward 全部加起來
我們只計算從這一個 action 執行以後
所得到的 reward，因為這場遊戲在執行這個 action 之前發生的事情
是跟執行這個 action 是沒有關係的， 前面的事情都已經發生了
那跟執行這個 action 是沒有關係的，所以在執行這個 action 之前
得到多少 reward 都不能算是這個 action 的功勞
跟這個 action 有關的東西， 只有在執行這個 action 以後發生的所有的 reward
把它總合起來，才是這個 action 它真正的 contribution
才比較可能是這個 action 它真正的 contribution
所以在這個例子裏面，在 state sb，執行 a2 這件事情
也許它真正會導致你得到的分數， 應該是 -2 分而不是 +3 分，因為前面的 +5 分
並不是執行 a2 的功勞
實際上執行 a2 以後，到遊戲結束前， 你只有被扣 2 分而已，所以它應該是 -2
那一樣的道理，今天執行 a2 實際上不應該是扣 7 分，因為前面扣 5 分，跟在 sb 這個 state 執行 a2 是沒有關係的
在 sb 這個 state 執行 a2，真正導致的問題只有你接下來
被扣兩分而已，所以也許在 sb 這個 state 執行 a2， 你真正會導致的結果只有扣兩分而已
那如果要把它寫成式子的話是什麼樣子呢？
你本來前面的 weight，是 R of tao，是整場遊戲的 reward 的總和
那現在改一下，怎麼改呢？
改成從某個時間 t 開始，假設這個 action 是在 t 這個時間點所執行的
從 t 這個時間點，一直到遊戲結束
所有 reward R 的總和，才真的代表這個 action，是好的，還是不好的
這樣大家了解我的意思嗎？
接下來再更進一步
我們會把比較未來的 reward，做一個 discount
為什麼我要把比較未來的 reward 做一個 discount 呢？
因為今天雖然我們說，在某一個時間點，執行某一個 action，會影響接下來所有的結果
有可能在某一個時間點執行的 action，接下來得到的 reward 都是這個 action 的功勞
但是在比較真實的情況下， 如果時間拖得越長，影響力就越小
就是今天我在第二個時間點執行某一個 action
哪我在第三個時間點得到 reward，那可能是再第二個時間點執行某個 action 的功勞
但是在 100 個 timestamp 之後，又得到 reward，那可能就不是在第二個時間點執行某一個 action 得到的功勞
所以我們實際上在做的時候，你會在你的 R 前面
乘上一個 term 叫做 gamma， 那 gamma 它是小於 1 的，它會設個 0.9 或 0.99
那如果你今天的 R，它是越之後的 time stamp
你這個 t prime 越大，這個 R 是越之後的 timestamp
它前面就乘上越多次的 gamma
就代表說現在在某一個 state st， 執行某一個 action at 的時候
真正它的 credit，其實是它之後，在執行這個 action 之後
所有 reward 的總和，而且你還要乘上 gamma， 這樣大家可以了解我的意思嗎？
或是要舉一個很具體的例子的話， 你就想成說，這是遊戲的第 1/2/3/4 回合
那你在遊戲的第二回合，的某一個 st 你執行 at， 它真正的 credit 到底是得到多分數呢？
它真正的 credit 得到的分數應該是，假設你這邊得到 +1 分
這邊得到 +3 分，這邊得到 -5 分
它的真正的 credit，應該是 1 加上有一個 discount 的 credit 叫做 gamma
乘上 3，再加上 gamma 的平方，乘上 -5
以下部分因為老師聲音不清楚省略
如果大家可以接受這樣子的話， 實際上你 implement 就是這麼 implement 的
那這個 b 呢，b 這個我們之後再講，它可以是 state-dependent 的
事實上 b 它通常是一個 network estimate 出來的
這還蠻複雜，它是一個 network 的 output， 這個我們之後再講
好，那把這個 R 減掉 b 這一項，這一項我們可以把它合起來
我們統稱為 advantage function， 我們這邊用 A 來代表 advantage function
那這個 advantage function 呢，它是 dependent on s and a，我們就是要計算的是說
在某一個 state s 採取某一個 action a 的時候， 你的 advantage function 有多大
然後這個 advantage function 它的上標是 theta， theta 是什麼意思呢？
因為你實際上在算這一項的時候， 你實際上在算這個 summation 的時候
你會需要有一個 interaction 的結果嘛，對不對，你會需要有一個 model 去跟環境做 interaction，你才知道你接下來得到的 reward 會有多少
而這個 theta 就是代表說，現在是用 theta 這個 model，跟環境去做 interaction
然後你才計算出這一項，從時間 t 開始到遊戲結束為止，所有 R 的 summation
把這一項減掉 b，然後這個就叫 advantage function
那它的意義就是，現在假設，我們在某一個 state st，執行某一個 action at
相較於其他可能的 action，它有多好
它真正在意的不是一個絕對的好，
而是說在同樣的 state 的時候
是採取某一個 action at，相較於其它的 action
它有多好，它是相對的好， 不是絕對好，那麼今天會減掉一個 b 嘛
減掉一個 baseline， 所以這個東西是相對的好，不是絕對的好
那這個 A 我們之後再講， 它通常可以是由一個 network estimate 出來的
那這個 network 叫做 critic， 我們講到 Actor-Critic 的方法的時候，再講這件事情
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
