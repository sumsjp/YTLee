okay well um thanks a lot for having me
today um I know um uh it's a little like
too late for too early here in this time
but it's really great for me to join
your event because um I'm really excited
about what your your team is doing but
at the same time I feel bad for not
being able to be there in person uh
because I really wanted to go there um
but anyway I I really appreciate uh for
hosting remotely um I hope this goes
well um today I will try to share some
maybe potential future directions that I
think in EUR speech codex but maybe with
a little more on uh emphasis on the
communication
applications so just wanted to introduce
um Neal speech and audio coding in
general in compar in comparison to the
uh traditional codex so back in the days
before we start to use machine learning
deep learning for coding uh this was
like the typical graph people looked at
for example there's this you know speech
codex um like the green line uh that are
standardized for speech communication
most of the time they work pretty well
in at low bit rates under 10k sometimes
um BPS but its performance usually
saturat r at around um
24K around this this was high bit rate
for speech codex and at the same time
there's this other codex called audio
codex that are
doing that is designed more to compress
the audio
signals um they work really well at high
bit rates and they sound great but um at
the same time at the low bit rate its
performance offer especially for speech
isn't as good so when we develop um you
know deep learning based codec or any
machine learning based one or data
driven uh methods for coding we want
that what we want it to be you know
better than these existing codex of
course although it's not as
straightforward as we think uh I can
maybe talk a little more about it but we
all know that it's not that easy um but
anyway if we can you know tryal Neal cod
in this way then they're really like
versatile in the sense that since it's
based on neural networks we can sort of
harmonize the you know neuron network
based codec with the other neuron
network based um application such as spe
mon ASR TTS or D llms because they are
really versatile uh and you know we can
sort of start to see the code as some
sort of a feature um that's very nice um
so
in order to do this we need to work on a
lot of limitations maybe for example the
notable issue would be complexity and
delay of this codex um because you know
if you think about this bits and audio
codex they are they do have their own
complexity of course but compared to the
neuro codex the complexity is really
incomparable so how do we handle that
and at the same time when you train the
the neuron network based codex um it's
not too straightforward to come up with
the loss function that makes sense to
you know human listening test for
example so that's another thing we need
to worry about and uh scalability is
another issue because you know these
codex for example speech and a codex
they are bit rate scalable so they can
sort of you know change the bit rate
depending on the uh uh bandwidth of the
network that's available at the moment
so things like that is sort of um you
know op Cas and challenges when develop
neural networks uh for coding especially
for communication
applications so if you think about audio
coding or speech coding um it is pretty
straightforward as a neuron network uh
you know before that back in the days
when people developed traditional codex
you know it consists of multiple sort of
blocks that people come up with and then
it's a combination of them for example
suppose you feed a uh Speech or audio
signal to your encoder then the encoder
consists of multiple modules in there
and then the encoder turns the input
signal into the B stream the code and
then it's either transmitted or stored
and then you know it's sort of
decomplexed by using the
decoder which also consists of multiple
modeles in there and then hopefully the
result after decoding is similar to the
input that's what we want want but uh
how do we actually judge the quality uh
back in the days people just listen to
the signal uh because people know that
um it's not always possible to rely on
some objective you know metrics there
anyway um so you know after listening to
the input and output of the codec and
then um they find something weird then
then uh the researchers or Engineers
they go back to the models that they
developed and then maybe adjust the
parameters in there or know switch the
model into different ones so there was
sort of a standard process when people
developed uh traditional codex and it
took 40 years basically to develop this
um you know so-call uh State ofthe
earthart communication codex
um but you know in the Deep learning
area era we can turn this into some Auto
encoder so basically you feed the input
signal to an encoder which now consists
of a multiple neon Network layers or
neon Network motors which is
parameterized by some you know um mod
parameters and then at the same time uh
it also produces some code and then you
know the code will be decoded by another
neuron Network decoder which will
produce the output and the input and
output can be compared of of course by
using subjective methods but you know at
the same time we can sort of come up
with the loss function which will
be back propagated to update the network
parameters so this process sounds like
an automated process and if you have
enough data we can sort of you know
develop this kind of system pretty
easily because differently from the uh
supervised method supervised learning
tasks we don't need to to labor the
signals the input and output are the
same thing so it's the auto incoder so
we don't we just have to dump a lot of
data to the network and then update the
model
accordingly so all encoders for audio
coding is not a due idea actually for
example this is like the drawing that I
did uh about uh 16 years ago um you know
I was thinking that for example the
spectrogram mdct or stft or whatever uh
that can be uh cons know if you just
select a few Spectra out of it then it
becomes a Matrix and then back in the
days Matrix factorization was U you know
very popular and easy to use so we can
sort of come up with the dimension
reduced version of that by doing some
Matrix low rank approximation basically
uh you can use PCA nmf or whatever you
know uh dictionary learning algorithms
to do this and then you can basically
recover the input Matrix the spectrogram
uh um by by using this you know a * B
kind of in a product um so this sounds
like uh a working system and I even
brought a paper by the way uh in 2008
this was me back in the days over here
um it it was like really fun to work on
this but I sort of gave up on this back
then because um I was able to see that
you know the Matrix factorization can do
some approximation but as for the bit
rate I wasn't able to compete with the
exist systems and you know it also had
this a lot of issues that I mentioned
like how to judge the subjective quality
of this and so on and then fast forward
now we can do neuron network based codex
which is uh sort of to me like not
multi-layered version of this all in
quers so you know we we are facing a lot
of different neuro codex these days uh
but from the communication codex
perspective these are sort of the you
know attributes that we expect expect
from a codc traditionally such as low
bit trate nobody likes a super high bit
rate for uh for from a codc uh but at
the same time when it's uh for speech
communication um the latency shouldn't
be long the algorithmic latency
shouldn't be too high because then you
know people will hear the delay when
they on the phone so it's not nice so
the delay has to the latency has to be
low enough so that people don't realize
it uh maybe 20 millisecond or something
like that um but at the same time we are
talking about running these codex
incoder and decoder on the device so
unless the device is super large uh
where it's power free enough to run gpus
uh we need to worry about the complexity
of the Codex um uh because otherwise
it's going to drain the battery uh it's
not just about spech ication by the way
because um the Codex audio codex can be
used for music streaming for example as
well so suppose you're on you're in the
airplane and then you're listening to
some music that you stored on your phone
you want to listen to it maybe for eight
hours or something um and what if the
dra uh battery drains after listening to
music for eight hours you don't want to
see that so complexity is another issue
and at the same time we want the decoded
signal um to be high quality or in other
words the uh the reconstructed signal
out of the codec decoded version should
be as similar as possible to the input
perceptually so that's another thing we
want and at the same time you know it
should be robust to the um you know uh
Network environment so if uh there is
any packet loss or something like that
then that has to be covered as well but
what else these days you know codex are
also used for some other Downstream
tasks uh as we might have seen today a
lot I'm sorry I missed most of them but
um what if we just want to use the code
as a representation feature for the
other down task that's another thing we
want to think about which is actually
something that the traditional codex are
missing and that's why we work on the
neural codex these days so you know
there are different kinds of you know
codex and many different uh CeX
developed uh uh very recently so if I
just roughly try to uh sort of
categorize them into different versions
so this one might be uh this kind might
be uh some models that are relying a lot
on the decoders complexity I'm not going
to go over all of them but uh most of
the time they don't care too much about
the quality of the code or how do how we
get the um code the incoder is really
simple in this case um but uh they can
do whatever funy things there like using
generative Motors or uh things like that
and eventually there's a trade-off
between quality and complexity and bit
rate in this case more balanced versions
are incoder decoder like Auto incoder
setup where the incoder is learning some
features uh in the meter and the decoder
is using the feater to recover the
original signal typically these are
based on the CNN uh encoder decoder
architectures um in that regard they are
more flexible structu wise and the
complex is more moderate because they
don't have to be too large uh and uh
it's versatile because the incoder
representations can be used for the
other Downstream task and then you know
the downstream task can use the decoder
to recover the original signal and then
there are more um
incoder uh heavy Motors um that are good
for Downstream apps although uh if you
want to think about this as a
communication codex they might be a
little too large to run of or in real
time for example um I just realized that
uh semantic Cod from my perspective in
this uh figure is a really unique one
because it is actually uh using some you
know diffusion M as a decoder so it can
be seen as a decoder having motor but it
cares a lot about the you know uh
feature quality as well so it's uh using
some proper decoder so it's a that's why
maybe it works really well um these are
sort of a conditions we want to think
about like you know back in the days
when we train of neuron Network codec
like psych Acoustics people use a lot of
this uh in traditional codecs and how
about regid learning or predictive
learning or generative model uh these
are sort of I think the key to uh
achieving higher performance and what if
we want to do some sort of um you know
uh analysis of mixture audio then we
need to do sort oparation and coding at
the same time I have a paper about this
as well actually how do we actually see
the foundational models as a as an
encoder or uh uh how about using langage
model on top of codex how about
personalizing the codec for some
personal you know speech communication
these are sort of uh really uh nice
things to think about as a Cod
development so one thing I would I'd
like to introduce today is um for
example predictive coding so suppose
that you have a code um we start from
some you know predictor maybe some r
model like Gru which fees some past
frames and then it wants to predict the
current frame out of it why do we care
about it for example um suppose you have
an encoder and instead of sending the
code to the decoder directly we actually
try to predict The Code by using the
previous code vectors in the past then
you know uh during the training time we
can sort of uh try to learn the wiid
your signal by comparing the prediction
out of the existing code then what the
means is that the residual signal now
can be quantized and then sent over to
the decoder instead of the code so code
here is by the way not quantized as is a
floting point code so why do I why do I
want to do that it's well known that the
regor signals are with the lower entropy
so you can reduce the bit rate uh even
further by doing so and then on
the in the in quarter side we are going
to uh actually just harmonize the
residual signal back again with the
prediction to recover the uh original
code Vector which is not perfect but
it's going to be a really nice
reconstruction so on the decoder side uh
we take the wiid your signal and then
merge that with another predictor which
the same predictor by the way uh running
on the decoder side and then it does
some harmonization of the regid signal
and the prediction recover the code and
it just keeps doing this and then we
finally use the code to uh uh run some
maybe vooder like LPC net um or any
other decoder to convert this into the
wave from signal so I wanted to
introduce the importance of doing
something in the latent Space by for
example doing predictive coding and
regor learning because that way we can
dramatically reduce the amount of
information we to send over to the uh
deoder by working on the reg signal and
then if you compare the this kind of a
predictive coding result uh with the
other uh uh model that are uh sort of
similar baselines uh we can see that the
performance is uh better although the
bit rate is lower so this is sort of um
how to say uh my emphasis that I wanted
to share today with uh with folks that
uh we can sort of try to you know playle
around with this later representation
because it's it's really nice and uh
important to do that for uh Speech
communication or actually any other
coding applications as well um but then
this make this makes us think about what
would be the best way to uh learn this
kind of lat
representation given what's happening
so my idea thinking was something like
this um we can assume that there is a
auto encoder like this one uh that takes
the input and then it converts the input
to some lat representation and then
there's a decoder that converts it back
to the original signal domain and this
encoder and decoder uh pair they don't
do any quantization by the way uh and
then they don't have to do some dramatic
Dimension reduction as well because this
is not a codec this is an auto encoder
it just learned the feature and then uh
since the feature is still you know
relatively High dimensional and then uh
it doesn't do any quantization the
Reconstruction must been near perfect
that's um something we can imagine as a
sort of idea Oro encoder and it's
usually this Auto encoder because uh it
didn't go through any quantization so
bit rate must be super high so it's you
know useless in that regard but at the
same time we can also think about the TP
ther you know newer codec uh such as U
you know encodec or DAC or whatever
other you know Auto incoder based codec
which Dodge the quantization in the
middle and hopefully it might have done
some Dimension reduction as well because
if you reduce the dimension uh the you
can sort of also reduce the bit rate
accordingly uh as well so it's very
typical to reduce Dimension and at the
same time apply quantization on it and
then you know that means is that we need
to train a corresponding decoder
accordingly so that the input and the
output are as similar as possible and
this is tricky because we want this H
the quantize H to be you know super low
dimensioner and super low bit rate so it
basically goes through a lot of uh
transformation and uh uh it's uh
injection of quantization error I would
say so my my proposal is like how do we
combine these two ideas because because
this is use for typical codec we want to
use this but the quality is not good and
this this one on the left is ideal but
we cannot use it because the bit rate is
too high so what if we just take the
incoder part of this um discrete codec
and then the decoder part of this
continuous codec and then somehow merge
them together what it means is that we
need to uh merge H and
z uh somehow and then they have to be in
the same representation which is
impossible because they're not in the
same lat space because they learned
differently one goes through
quantization and dimension reduction the
other one didn't go through anything so
we need to do some upsampling and
dequantization at the same time um how
do we do that uh we can just uh try to
do it but it's not that
easy though because you know just trying
to learn some upsampling and
quantization at the same time it just
sounds like a codec so we can get some
help from The General model by using
latent diffusion model that's why we
call this laif codec uh which is latent
diffusion codec and it I like it because
it sounds like uh French uh so basically
we convert this uh we we say we think
that there is a diffusion process that
converts this continuous code into some
noise and then the reverse diffusion
process will basically convert it back
to the uh original Space by conditioning
the the noising mod by using the
quantized code so if you do this
multiple times then this should sound
good so that's why it's called L codc
and then compared to the Baseline model
that we were using uh we were able to
see that um the you know uh I don't know
if you can see over there but the uh
user aing artifact of inod is gone and
it sounds much better uh I'm not going
to play the sound because uh it looks
like it's not audible from there uh but
we have a website for this so you can
check that out uh one thing I'd like to
introduce here is that for example you
know the left continuous codec now can
be uh with larger Dimension as well so
instead of applying the typical strides
a lot of strides at every layer we just
apply one stride in one layer that
actually sounds really well and we don't
we don't want to do this if this was a
typical codec because then the
dimensionality is too high and it's
going to increase the bit rate by a lot
but we don't care about it because you
know we can get the get this
representation for free by using the
generative process um and then if it did
the muser test and compared to in codc
it sounds much much better and DAC
actually we just uh use the public DAC
uh checkpoint for this which wasn't
trained on our data set so maybe that's
why it doesn't sound good but anyway
it's sounded better than DAC which
wasn't fine-tuned on our data by the way
so it's sort of unfair comparison um I
wanted to share some example here which
might not be Audible for you so I'm not
going to play but what's really funny
here is that uh if you use this codec
with some relatively low bit rate then
it starts to you know show some
hallucination Behavior so it flips the
phoneum from one to another this example
original uh word was soled serve like
serving the you know table at the
restaurant but uh with this uh knif
version it uses it f the phon and then
it sounds like serve instead of serve so
uh we had to do some kind of extra
training optimization tricks here and
there which we explained uh on our paper
in detail but we could sort of remove
that but I I thought it's really
interesting because this generative
process introduces hallucination which
is actually a deal breaker speech codc
um yeah and then I will move on to our
my other topic which is about
personalized speech codec which is
actually really interesting for me
because it's only possible using neuro
codex so let me try to explain this uh
from the manold learning perspective so
suppose that you have some sort of a
contaminated code which is off from the
original speech utterances is manifold
so the goal of the decoding job is to go
back onto this original manifold by
using some NE Network decoder but if the
decoder is compressed and small because
we care about complexity and everything
then the Reconstruction might not be as
detailed as the original manifold so the
recovered manifold will be some sort of
a smoother version which is not as good
as the original signal so that's why we
hear some you know uh a lot of uh
reconstruction uh noise if you do this
so the idea here is that if we focus on
a particular talker instead of everybody
um then we can construct the new
maniford that's sort of personer then uh
the the personer more subset of the
clean speech will be maybe you know
smoother as a manifold and maybe a
smaller uh decoding decoder can still do
a great job because the problem is now
simpler right so that's basically the
idea and then it means that the L power
moders can still work as a on the S
problem um so the idea is uh can be
represented in this way as a codec so we
feed the input signal to the speech
encoder and then um it goes through some
sort of a uh speaker encoder which
learns the speaker bding out of it and
then speak and beding will be uh
classified into different classes and
then at the same time the input signal
also goes through this uh the regular
utterance encoder which is the encoder
of the codec and then when we use the co
encoder we will choose the best encoder
for the speaker how do we choose that we
are going to use the classification
result that we just did over here and
then it means that each Speaker gets
assigned to the best encoder that's
specific for that poer and then you know
the bam is generated from the encoder
and then the B stream will be decoded on
the receiver side and same thing happens
we're going to use the personalized
decoder which is again uh using the
index information uh that's coming from
this classification so the reason why we
do this is that if we train if we pre
pre-train all the speaker specific
encoders and decoders then they are
going to be much smaller and works
really well for that particular person
so in that regard we can sort of bre
reduce the inference complexity by a lot
uh by only using one of the small
incoder and decoder pair and if you look
at the speaker embedding uh uh
visualization it's sort of if supposed
that you just uh group the speakers into
two groups then it's uh obvious that uh
it's following some gender information
so it means that it's it's doing some
semantic classification closing uh I
think as for the four speaker case it's
hard to see what's going on but I can
see that some clustering is happening in
the uh speaker embedding space so we're
going to rely on this um sort of speak
groups as a as a way to personalize the
codec and then finally uh as for the
Baseline by the way we're using appc net
I'm not going to di into the details
about LPC net uh but anyway we love LPC
net because it's lobit trate but at the
same time the uh the encoder is really
really simple it doesn't have a neural
encoder it's using uh codec 2 type of
very simple traditional code and then
the decoder is relatively heavier but
it's still very simple it's just couple
of Gru layers happening here and then uh
instead of um you know relying on the uh
sending the RPC residual as in the
typical speech codex it just synthesizes
the LPC resid which is uh kind of merged
with the LPC prediction so that it can
do better uh decoding and it works
really well uh given its super low
complexity um so if we apply this
personalization idea to the FPC net then
we do the same classification but the
incoder we fet doesn't have an encoder
so we just use the generic DSP encoder
and then we only personalize the decoder
part which is uh good enough for us if
you look at the performance um we can
first see
that the blue ipcn baselines um they the
bigger lpcn Baseline is better than the
smaller epet so if you do just
compession by reducing the motor size
the performance be great of course uh
but after personalization we can improve
the performance in both cases so this is
like really great because uh we don't we
don't have to increase the bit rate but
we just you know get the free uh
performance Improvement by doing the
personalization and what's interesting
here is that the small smaller model
gets better performance boost
by doing personalization and then
finally if I compare the small
personalized model and the large
Baseline which is not personalized the
performance is neck and neck so we can
say that um small personalized motor is
on part with our larger PC net and it
means that we can do some M compression
without losing the performance uh sound
quality of the codec so as a recap uh we
know that the space neural spech codecs
are very exper in terms of coding gain
and then all the codecs are getting
there these days uh but at the same time
we need to worry about deep learnings uh
typical computational complexity uh
because as uh Speech and audio coding
are typically used for realtime
communication or streaming application
so it has to be sort of uh low
complexity low delay and the you know
the we we still need to worry about
those typical issues as a codec and then
you know Foundation model speech
Foundation Motors audio Foundation
Motors they are good in the sense that
they are uh learning nice features but
they are particularly heavy than the uh
codc uh codc specific uh architectures
so that would be something to think
about uh as well um and then we we know
that predictive model regid learning and
generative models usually help uh codc
and then I also show that the
personalization is a New Concept and
then it helps a lot although traditional
codex cannot really use this kind of
concept so that's why it's sort of cool
to me and then um neural speech and AIO
Cod is versatile uh it can be combined
with other applications as you might
have seen today a lot uh and then from
the feature learning perspective it can
provide some discret Ty features uh for
the downam test but again uh if you want
to do this like in re time uh and on the
device that's uh we still have the same
typical uh challenges we need to resolve
yeah um uh yes kand at Google and I
recently uh wrote a like a small review
paper which is uh comparing the DSP
codex and the New Codex uh from this
communication perspective so uh if you
haven't checked it out please go ahead
and check out because um it's actually
pretty uh interesting comparisons there
happening and then um yeah thank you by
the way one advertisement these are the
students who are working on neuro codex
these days uh so far and um they're on
the job market so if you have any job
for them just let me know show me an
email thank you very
much thank you Professor M for the very
great
talk uh any question from the
audience
yeah okay hello M and H and um my
question is about the personalization
part so it's very interesting that you
show that the personalization can
improve the performance uh I have some
idea I wonder you have try or I want to
know your opinion so first of all
besides personalize for different
speakers it looks like you can also
personalize for different types of audio
so person you can have music decoder
speech decoder audio decoder this
probably one variation we can consider
and another thing probably we can also
consider is for different person like
can probably still share some parameter
they have some uh person specific
parameter but also some gen General
parameter in this way the whole model is
more like a mixture of expert actually
then you use the classifier to control
the miur of expert and uh this can be
another way to do personalization so I
want wondering whether you have any
comments or uh whether you have tried
any uh similar direction thank you yeah
thank you very much it's a very good
question actually um so I did this
actually for some other task earlier
before this I wrote several papers about
uh Speech anouncement personalized
speech anouncement uh and I tried uh
things that that are similar to what you
just said for example you know it's a
spech model but instead of personalizing
to the speaker it's uh kind of
customizing to different types of noise
so you could like with this similar
structure but the you know in the
mixture of local expert structure for
example the local experts are handling
different noise types for example so if
the surrounding noise is airplane noise
then you use this local expert and if
there's a background music then you use
this other local expert so that that's
something we can think about or you know
we can variate this by uh based on the
SNR of the input so we can do all sorts
of different things there and in that
regard yes uh we can do this for uh
Speech cod as well actually the we are
working on a uh uh audio version of this
uh by trying different music genre and
then turning this into a uh uh Mi local
expert version so that's uh sort of um
very good idea and uh we are working on
the music version and as for the um Vis
blocker expert idea by sharing some
features between speaker groups and so
on we haven't tried for this paper but
uh for the other you know speech enement
forance we sort of tried that uh and as
you say it works um one thing though
here is that
um I don't know if I should oh yeah so
we we did learn the speaker groups here
and uh instead I said it's
personalization and I might have uh
confused you by saying this it sounds
like uh everybody has his own his or her
own codec specialized for for that
speaker but actually what happened here
is that I grouped them into you know
speaker groups and then each group has a
specific uh codec so in that regard I'm
kind of kind of doing that but I'm not
really sharing the uh local experts
between the group groups so which is
something I can do in the future but
yeah they're actually really good ideas
yeah
thank you yeah thank you very much for
the insightful answers thank you
yeah yeah any other question from the
audience yeah I I have one question
actually I listen to the uh the codak
from height the quality is pretty good
um my question is that the diffusion
will bring very very good quality but
the latency is kind of uh a problem uh
what do you think for the future work
for that diffusion based yeah so the L
codc is actually the first codec that I
developed without worrying too much
about complexity before this I developed
some sort of very small CNN codex back
in the days um they are like really
really small uh it's like 05 million
parameters for example or even lower
than that so they didn't sound good they
I mean they found it maybe as good as
you know traditional codex but not
better than them because I was worried
too much about the complexity so with
this L codc I'm like okay let's give up
the complexity and then let's you know
try to make it um sound good and see
what happens like other people are doing
and then like you say it sounds good but
as is return since since it's a
diffusion model it has to go through
multiple you know reverse diffusion
sampling process um so I wouldn't say
this is good enough for as a
communication codec but I I didn't dive
into that part today but actually the
algorithm uh he came up with is a Midway
infilling which is uh actually using
much fewer number of steps um and we've
verified that we don't need even like
100 steps to do that so in that regard
although it's a diffusion model we can
there's a chance that we can even lower
the number of steps even I mean there
are better de learning people out there
they can try to reduce the number of
steps even further um so that although
it's a diffusion model if you don't have
to run it like too many times then it's
still not too complex and by the way
this might not be the algorithmic delay
I mean if the computer that's decoding
this is good enough then we can still do
the realtime processing um so it's not
actually introducing too much algorithm
delay I say uh although it's complexity
might be higher than the other Motors
that are not using diffusion Motors so
yeah I mean that's a that's a valid
concern that the diffusion model is not
good for communication codex but I
believe that there's a way we can even
uh lower the number of steps uh than
this thanks Pro Professor M and uh any
question for
audience if if there's no other question
probably I can ask another
one so my question is for the uh lot
diffusion codc uh is it fair to say this
C this cc is used use the diffusion
model as a decoder but I think there's
still some difference because besides
the diffusion model you also have a
fixed continuous decoder so my question
is how important it is for this
continuous decoder I mean if someone
directly Trend the diffusion model uh as
the decoder can they get the similar
performance to lot diffusion codex or
the whole design the diffusion model
plus a continuous decoder is very
important yeah well we when we first
developed this um he and I had a lot of
like discussion about this and you know
our idea was let's let's use diffusion
model as a decoder because otherwise
other people are going to do it anyway
let's do it quick and you know do this
um and of course the first idea was to
use the defusion model directly as a
decoder and then you know synthesize the
wave fire waveform uh directly but we
learned that um that way we can do some
I mean we then the decoder relies too
much on the diffusion modor because the
all the quality and everything should be
directly related to the diffusion modor
quality and maybe the diffusion motor
has to be large and then we need a lot
of steps to get the realistic uh
synthesized
audio so in that regard we felt that
doing diffusion the latent space as a
lat t diffusion motor is better because
you know compared to the raw waveform
still the you know V Space is much lower
dimensioner than than that and um at the
same time it's a it's a trade-off by the
way because the continuous de quter up
here is not perfect so the result is
although very good it's not perfect
reconstruction yet so there's a
trade-off but at the same time you know
we we figured that this way in this way
we we can sort of balance find the
balance between the you know the
Reliance on the generative model the
diffusion model and the complexity
related to that by off uh like
offloading the complex of the division
model to this continuous decoder which
is sort of deterministic once it's
trained so I think we don't have any
sort of subjective test that compared it
but uh with found that this is much
easier to to try the motor and then we
can get the nice performance without
increasing the complexity by too much
yeah I'm sorry I don't have any evidence
because it's just an old internal
comparisons but yeah this is this is
what what I think I'm pretty sure that
this is much better than the um warning
the diffusion motor directly for the to
synthesiz waveform
yeah thank than thank you very much for
the answer this is very reasonable thank
you yeah thank
you okay so I think due to time
limitation we still need to end the
session here let's thanks our speaker
again thanks Professor mja to stay up so
late to give the talk thank you very
much thank you it was my pleasure thank
you very much
