好 今天呢
我們要講 Batch 跟 Momentum
這兩個訓練的技巧
希望我們在剩下的30分鐘內 可以講完
好 第一個是 Batch
好多同學都問我說
為什麼我要用 Batch
為什麼 Training 的時候要用 Batch
那我本來第一週的時候
是不打算講這個東西的
因為我相信講了以後
大家就會有好多好多的問題
因為這好像不是一個
一般常理會想要採取的方式
但是因為助教的程式裡面有這一段
所以我決定先告訴大家
有 Batch 這一個東西
那 Batch 是怎麼做的呢
上次我們有講說
我們實際上在算微分的時候
你還不是真的對所有 Data 算出來的 L 作微分的
你是把所有的 Data 分成一個一個的 Batch
那有的人是叫做 Mini Batch 啦
那我這邊叫做 Batch
其實指的是一樣的東西
助教投影片裡面
是寫 Mini Batch
好 這邊每一個 Batch 的大小呢
就是大 B 一筆的資料
我們每次在 Update 參數的時候
我們是拿大 B 一筆資料出來
算個 Loss
算個 Gradient
Update 參數
拿另外B一筆資料
再算個 Loss
再算個 Gradient
再 Update 參數
以此類推
所以我們不會拿所有的資料一起去算出 Loss
我們只會拿一個 Batch 的資料
拿出來算 Loss
那我們說
所有的 Batch 看過一遍
叫做一個 Epoch
那事實上啊
你今天在做這些 Batch 的時候
在把你所有的資料分成一個一個 Batch 的時候
你會做一件事情叫做 Shuffle
Shuffle 有很多不同的做法
但一個常見的做法就是 我們
在每一個 Epoch 開始之前
會分一次 Batch
然後呢
每一個 Epoch 的 Batch 都不一樣
就是第一個 Epoch
我們分這樣子的 Batch
第二個 Epoch
會重新再分一次 Batch
所以哪些資料在同一個 Batch 裡面
每一個 Epoch 都不一樣的這件事情
叫做 Shuffle
那剛才助教程式裡面
其實也有提到 Shuffle 這件事情
好 再一個問題 就是
為什麼要用 Batch 呢
我們先解釋為什麼要用 Batch
再說 Batch 對 Training 帶來了什麼樣的幫助
那為什麼要用 Batch 呢
我們來比較左右兩邊這兩個 Case
那假設現在我們有20筆訓練資料
然後呢
左邊的 Case 就是沒有用 Batch
或者是說 我的 Batch Size
直接設的跟我訓練資料一樣多
這種狀況啊叫做 Full Batch
或者是 就是沒有用 Batch 的意思了
那右邊的 Case 就是
Batch Size 等於1
這是兩個最極端的狀況
我們先來看左邊的 Case
在左邊 Case 裡面
因為沒有用 Batch
我們有20筆訓練資料
我們的 Model 必須
我們的程式必須把20筆訓練資料都看完
才能夠計算 Loss
才能夠計算 Gradient
所以我們必須要把所有的 Example s
也就20筆 Example s 都看完以後
我們的參數才能夠 Update 一次
就假設開始的地方在這邊
把所有資料都看完以後
Update 參數就從這裡移動到這裡
好 那如果是一個 Batch 的話 會怎麼樣呢
如果是一個 Batch
如果 Batch Size 等於1的話
代表我們每次 Update 參數的時候
我們只要看一筆資料就好
Batch Size 等於1
代表我們只需要拿一筆資料出來算 Loss
我們就可以 Update 我們的參數
所以每次我們 Update 參數的時候
看一筆資料就好
所以我們開始的點在這邊
看一筆資料 就 Update 一次參數
再看一筆資料 就 Update 一次參數
如果今天總共有20筆資料的話 那在每
在一個 Epoch 裡面
我們的參數會 Update 20次
那不過
因為我們現在是只看一筆資料
就 Update 一次參數
所以用一筆資料算出來的 Loss
顯然是比較 Noisy 的
所以我們今天 Update 的方向
你會發現它是曲曲折折的
所以如果我們比較左邊跟右邊
哪一個比較好呢
他們有什麼差別呢
你會發現左邊這種方式啊
沒有用 Batch 的方式啊
它蓄力的時間比較長
還有它技能冷卻的時間比較長
你要把所有的資料都看過一遍
才能夠 Update 一次參數
而右邊的這個方法
Batch Size 等於1的時候
蓄力的時間比較短
每次看到一筆參數
每次看到一筆資料
你就會更新一次你的參數
所以今天假設有20筆資料
看完所有資料看過一遍
你已經更新了20次的參數
但是左邊這樣子的方法有一個優點
就是它這一步走的是穩的
那右邊這個方法它的缺點
就是它每一步走的是不穩的
那左邊跟右邊到底哪邊比較好呢
看起來左邊的方法跟右邊的方法
他們各自都有擅長跟不擅長的東西
左邊是蓄力時間長
但是威力比較大
右邊技能冷卻時間短
但是它是比較不準的
它是亂槍打鳥型的
到底誰比較好呢
看起來各自有各自的優缺點
但是你會覺得說
左邊的方法技能冷卻時間長
右邊的方法技能冷卻時間短
那只是你沒有考慮平行運算的問題
實際上考慮平行運算的話
左邊這個並不一定時間比較長
怎麼說呢
這邊呢 是真正的實驗結果了
事實上啊
比較大的 Batch Size
你要算 Loss
再進而算 Gradient
所需要的時間
不一定比小的 Batch Size 要花的時間長
那以下呢
是坐在一個叫做 MNIST 的（00：06：09）上面
MNIST 是手寫數字辨識的（00：06：13）
就機器要做的事情
就是給它一張圖片
然後判斷這張圖片
是0到9的哪一個數字
它要做數字的分類
那 MNIST 呢 是機器學習的果蠅
就是假設你今天
從來沒有做過機器學習的任務
一般大家第一個會嘗試的機器學習的任務
往往就是做 MNIST 做手寫數字辨識
那邊我們怎麼
這邊我們就是做了一個實驗
我們想要知道說
給機器一個 Batch
它要計算出 Gradient
進而 Update 參數
到底需要花多少的時間
這邊列出了 Batch Size 等於1 等於10
等於100 等於1000 所需要耗費的時間
你會發現說 Batch Size 啊 從1到1000
需要耗費的時間幾乎是一樣的
為什麼呢
你可能說 欸 直覺上有1000筆資料
那需要計算 Loss
然後計算 Gradient
花的時間不會是一筆資料的1000倍嗎
但是實際上並不是這樣的
因為在實際上做運算的時候
我們有 GPU
可以做平行運算
是因為你可以做平行運算的關係
這1000筆資料是平行處理的
所以1000筆資料所花的時間
並不是一筆資料的1000倍
好 當然 GPU 平行運算的能力還是有它的極限
當你的 Batch Size 真的非常非常巨大的時候
GPU 在跑完一個 Batch
計算出 Gradient 所花費的時間
還是會隨著 Batch Size 的增加
而逐漸增長
所以今天如果 Batch Size 是
Batch Size 是從1到1000
所需要的時間幾乎是一樣的
但是當你的 Batch Size 增加到 10000
乃至增加到60000的時候
你就會發現 GPU 要算完一個 Batch
把這個 Batch 裡面的資料都拿出來算 Loss
再進而算 Gradient
所要耗費的時間
確實有隨著 Batch Size 的增加而逐漸增長
但你會發現這邊用的是 V100
所以它挺厲害的
給它60000筆資料
一個 Batch 裡面
塞了60000筆資料
它在10秒鐘之內
也是把 Gradient 就算出來
而那這個 Batch Size 的大小跟時間的關係啊
其實每年都會做這個實驗啊
我特別把舊的投影片放在這邊了
如果你有興趣的話 可以看一下
可以看到什麼呢
可以看到這個時代的演進這樣
17年的時候用的是那個980啊
2015年的時候用的是那個760啊
然後980要跑什麼60000個 Batch
那要跑好幾分鐘才跑得完啊
現在只要10秒鐘就可以跑得完了
你可以看到這個時代的演進
好 所以 GPU 雖然有平行運算的能力
但它平行運算能力終究是有個極限
所以你 Batch Size 真的很大的時候
時間還是會增加的
但是因為有平行運算的能力
因此實際上啊
當你的 Batch Size 小的時候
你要跑完一個 Epoch
花的時間是大的 Batch Size
是比大的 Batch Size 還要多的
怎麼說呢
如果今天假設我們的訓練資料只有60000筆
那 Batch Size 設1
那你要60000個 Update 才能跑完一個 Epoch
如果今天是 Batch Size 等於1000
你要60個 Update 才能跑完一個 Epoch
假設今天一個 Batch Size 等於
一個 Batch Size 等於1000
要算 Gradient 的時間根本差不多
那60000次 Update
跟60次 Update 比起來
它的時間的差距量就非常可觀了
所以左邊這個圖是 Update 一次參數
拿一個 Batch 出來計算一個 Gradient
Update 一次參數所需要的時間
右邊這個圖是
跑完一個完整的 Epoch
需要花的時間
你會發現左邊的圖跟右邊的圖
它的趨勢正好是相反的
假設你 Batch Size 這個1
跑完一個 Epoch
你要 Update 60000次參數
它的時間是非常可觀的
但是假設你的 Batch Size 是1000
你只要跑60次
Update 60次參數就會跑完一個 Epoch
所以你跑完一個 Epoch
看完所有資料的時間
如果你的 Batch Size 設1000
其實是比較短的
Batch Size 設1000的時候
把所有的資料看過一遍
其實是比 Batch Size 設1 還要更快
所以如果我們看右邊這個圖的話
看完一個 Batch
把所有的資料看過一次這件事情
大的 Batch Size 反而是較有效率的
是不是跟你直覺想的不太一樣哦
在沒有考慮平行運算的時候
你覺得大的 Batch 比較慢
但實際上
在有考慮平行運算的時候
一個 Epoch 大的 Batch 花的時間反而是比較少的
好 所以這邊
我們如果要比較這個 Batch Size 大小的差異的話
看起來直接用技能時間冷卻的長短
並不是一個精確的描述
看起來在技能時間上面
大的 Batch 並沒有比較吃虧
甚至還佔到優勢了
好 所以事實上呢
這邊 Update 一次的時間
20筆資料 Update 一次的時間
跟這邊看一筆資料 Update 一次的時間
如果你用 GPU 的話
其實可能根本就是所以一樣的
所以大的 Batch
它的技能時間
它技能冷卻的時間
並沒有比較長
那所以這時候你可能就會說
欸 那個大的 Batch 的劣勢消失了
那難道它真的就
那這樣看起來大的 Batch 應該比較好囉
你不是說大的 Batch
這個 Update 比較穩定
小的 Batch
它的 Gradient 的方向比較 Noisy 嗎
那這樣看起來
大的 Batch 好像應該比較好哦
小的 Batch 應該比較差哦
因為現在大的 Batch 的劣勢已經
因為平行運算的時間被拿掉了
它好像只剩下優勢而已
優勢而已
那神奇的地方是 Noisy 的 Gradient
反而可以幫助 Training
這個也是跟直覺正好相反的
如果你今天拿不同的 Batch 來訓練你的模型
你可能會得到這樣子的結果
左邊是坐在 MNIST 上
右邊是坐在 CIFAR-10 上
不管是 MNIST 還是 CIFAR-10
都是影像辨識的問題
那橫軸啊
代表的是 Batch Size
從左到右越來越大
縱軸代表的是正確率
越上面正確率越高
當然正確率越高越好
而如果你今天看 Validation Acc 上的結果啊
如果你今天看
如果你今天看 Validation Acc 上的結果
會發現說
Batch Size 越大
Validation Acc 上的結果越差
但這個是 Overfitting 嗎
這個不是 Overfitting
因為如果你看你的 Training 的話
會發現說 Batch Size 越大
Training 的結果也是越差的
而我們現在用的是同一個模型哦
同一個模型
同一個 Network
照理說
它們可以表示的 Function 就是一模一樣的
但是神奇的事情是
大的 Batch Size
往往在 Training 的時候
會給你帶來比較差的結果
所以這個是什麼樣的問題
同樣的 Model
所以這個不是 Model Bias 的問題
這個是 Optimization 的問題
代表當你用大的 Batch Size 的時候
你的 Optimization 可能會有問題
小的 Batch Size
Optimization 的結果反而是比較好的
好 為什麼會這樣子呢
為什麼小的 Batch Size
在 Training Set 上會得到比較好的結果
為什麼 Noisy 的 Update
Noisy 的 Gradient 會在 Training 的時候
給我們比較好的結果呢
一個可能的解釋是這樣子的
假設你是 Full Batch
那你今天在 Update 你的參數的時候
你就是沿著一個 Loss Function 來 Update 參數
今天 Update 參數的時候走到一個 Local Minima
走到一個 Saddle Point
顯然就停下來了
Gradient 是零
如果你不特別去看（00：14：13）的話
那你用 Gradient Descent 的方法
你就沒有辦法再更新你的參數了
但是假如是 Small Batch 的話
假如你有用 Batch 的話
會發生什麼事呢
因為我們每次是挑一個 Batch 出來
算它的 Loss
所以等於是
等於你每一次 Update 你的參數的時候
你用的 Loss Function 都是越有差異的
你選到第一個 Batch 的時候
你是用 L1 來算你的 Gradient
你選到第二個 Batch 的時候
你是用 L2 來算你的 Gradient
假設你用 L1 算 Gradient 的時候
發現 Gradient 是零
卡住了
但 L2 它的 Function 跟 L1 又不一樣
所以 L1 卡住了
L2 就不一定會卡住啊
所以 L1 卡住了 沒關係
換下一個 Batch 來
L2 再算 Gradient
你還是有辦法 Training 你的 Model
還是有辦法讓你的 Loss 變小
所以今天這種 Noisy 的 Update 的方式
結果反而對 Training
其實是有幫助的
那這邊還有另外一個更神奇的事情啊
那這個神奇的事情是什麼呢
這個神奇的事情是
其實小的 Batch 也對 Testing 有幫助
假設我們今天在 Training 的時候
都不管是大的 Batch 還小的 Batch
都 Training 到一樣好
剛才的 Case 是
Training 的時候就已經 Training 不好了
那假設你有一些方法
你努力的調大的 Batch 的 Learning Rate
然後想辦法把大的 Batch
跟小的 Batch Training 得一樣好
結果你會發現小的 Batch
居然在 Testing 的時候會是比較好的
那以下這個實驗結果啊 是引用自
On Large-Batch Training For Deep Learning
Generalization Gap And Sharp Minima
這篇 Paper 的實驗結果
那這篇 Paper 裡面呢
作者 Train 了六個 Network 裡面有 CNN 的
有 Fully Connected Network 的
做在不同的 Cover 上
來代表說這個實驗是很泛用的
在很多不同的 Case 都觀察到一樣的結果
那它有小的 Batch
一個 Batch 裡面有256筆 Example
大的 Batch 就是那個 Data Set 乘 0.1
Data Set 乘 0.1
Data Set 有60000筆
那你就是一個 Batch 裡面有6000筆資料
好 然後他想辦法
在大的 Batch 跟小的 Batch
都 Train 到差不多的 Training 的 Accuracy
所以剛才我們看到的結果是
Batch Size 大的時候
Training Accuracy 就已經差掉了
這邊不是想辦法 Train 到大的 Batch 的時候
Training Accuracy 跟小的 Batch
其實是差不多的
但是就算是在 Training 的時候結果差不多
Testing 的時候你還是看到了
小的 Batch 居然比大的 Batch 差
Training 的時候都很好
Testing 的時候小的 Batch 差
那代表什麼
代表 Over Fitting
這個才是 Over Fitting 對不對
好 那為什麼會有這樣子的現象呢
在這篇文章裡面也給出了一個解釋
它的解釋是這個樣子的
假設這個是我們的 Training Loss
那在這個 Training Loss 上面呢
可能有很多個 Local Minima
有不只一個 Local Minima
那這些 Local Minima 它們的 Loss 都很低
它們 Loss 可能都趨近於 0
但是這個 Local Minima
還是有好 Minima 跟壞 Minima 之分
什麼叫做好 Minima 跟壞 Minima 呢
我們會認為
如果一個 Local Minima 它在一個峽谷裡面
它是壞的 Minima
然後它在一個平原上
它是好的 Minima
為什麼會有這樣的差異呢
因為假設現在 Training 跟 Testing 中間
有一個 Mismatch
Training 的 Loss 跟 Testing 的 Loss
它們那個 Function 不一樣
為什麼會不一樣呢
有可能是
本來你 Training 跟 Testing 的 Distribution
就不一樣
那也有可能是因為 Training 跟 Testing
你都是從 Sample 的 Data 算出來的
也許 Training 跟 Testing
Sample 到的 Data 不一樣
那所以它們算出來的 Loss
當然是有一點差距
那我們就假設說這個 Training 跟 Testing
它的差距就是把 Training 的 Loss
這個 Function 往右平移一點
這時候你會發現
對左邊這個 Minima 來說
對這個在一個盆地裡面的 Minima 來說
它的在 Training 跟 Testing 上面的結果
不會差太多
只差了一點點
但是對右邊這個在峽谷裡面的 Minima 來說
一差就可以天差地遠
它在這個 Training Set 上
算出來的 Loss 很低
但是因為 Training 跟 Testing 之間的不一樣
所以 Testing 的時候
哇 這個 Error Surface 一變
它算出來的 Loss 就變得很大
而很多人相信說這個大的 Batch Size
會讓我們傾向於走到峽谷裡面
而小的 Batch Size
傾向於讓我們走到盆地裡面
那他直覺上的想法是這樣
就是小的 Batch
它有很多的 Loss
它每次 Update 的方向都不太一樣
所以如果今天這個峽谷非常地窄
它可能一個不小心就跳出去了
因為每次 Update 的方向都不太一樣
它的 Update 的方向也就隨機性
所以一個很小的峽谷
沒有辦法困住小的 Batch
如果峽谷很小
它可能動一下就跳出去
之後停下來如果有一個非常寬的盆地
它才會停下來
那對於大的 Batch Size
反正它就是順著規定 Update
然後它就很有可能
走到一個比較小的峽谷裡面
但這只是一個解釋啦
那也不是每個人都相信這個解釋
那這個其實還是一個尚待可以研究的問題
好 那這邊就是比較了一下
大的 Batch 跟小的 Batch
好 左邊這個是第一個 Column 是小的 Batch
第二個 Column 是大的 Batch
如果在沒有平行運算的情況下
我們會覺得小的 Batch 比較有效
大的 Batch
一個 Batch 的時間要算得比較長
但是在有平行運算的情況下
小的 Batch 跟大的 Batch
其實運算的時間並沒有太大的差距
除非你的大的 Batch 那個大是真的非常大
才會顯示出差距來
但是一個 Epoch 需要的時間
小的 Batch 比較長
大的 Batch 反而是比較快的
所以從一個 Epoch 需要的時間來看
大的 Batch 其實是佔到優勢的
而小的 Batch
你會 Update 的方向比較 Noisy
大的 Batch Update 的方向比較穩定
但是 Noisy 的 Update 的方向
反而在 Optimization 的時候會佔到優勢
而且在 Testing 的時候也會佔到優勢
所以大的 Batch 跟小的 Batch
它們各自有它們擅長的地方
所以 Batch Size
變成另外一個 Hyperparameter
這個 Hyperparameter
是你需要去想過它的
這個也是一個
你需要去調整的 Hyperparameter
那講到這邊有人就會想說
那我們能不能夠魚與熊掌兼得呢
我們能不能夠截取大的 Batch 的優點
跟小的 Batch 的優點
我們用大的 Batch Size 來做訓練
用平行運算的能力來增加訓練的效率
但是訓練出來的結果同時又得到好的結果呢
又得到好的訓練結果呢
這是有可能的
有很多文章都在探討這個問題
那今天我們就不細講
我們把這些 Reference 列在這邊給大家參考
那你發現這些 Paper
往往它想要做的事情都是什麼
哇 76分鐘 Train BERT
15分鐘 Train ResNet
一分鐘 Train Imagenet 等等
這為什麼他們可以做到那麼快
就是因為他們 Batch Size 是真的開很大
比如說在第一篇 Paper 裡面
Batch Size 裡面有三萬筆 Example 這樣
Batch Size 開很大
Batch Size 開大 真的就可以算很快
你可以在很短的時間內看到大量的資料
那他們需要有一些特別的方法來解決
Batch Size 可能會帶來的劣勢
好 那最後想要跟大家下課前
想要跟大家分享的另外一個技術
是 Momentum
這也是另外一個
有可能可以對抗 Saddle Point
或 Local Minima 的技術
而這個 Momentum 是怎麼運作的呢
Momentum 的運作是這個樣子的
它的概念
你可以想像成在物理的世界裡面
假設 Error Surface 就是真正的斜坡
而我們的參數是一個球
你把球從斜坡上滾下來
如果今天是 Gradient Descent
它走到 Local Minima 就停住了
走到 Saddle Point 就停住了
但是在物理的世界裡面一個球會這樣子嗎
一個球如果從高處滾下來
從高處滾下來就算滾到 Saddle Point
如果有慣性
它從左邊滾下來
因為慣性的關係它還是會繼續往右走
甚至它走到一個 Local Minima
如果今天它的動量夠大的話
它還是會繼續往右走
甚至翻過這個小坡然後繼續往右走
那所以今天在物理的世界裡面
一個球從高處滾下來的時候
它並不會被 Saddle Point
或 Local Minima卡住
不一定會被 Saddle Point
或 Local Minima 卡住
我們有沒有辦法運用這樣子的概念
到 Gradient Descent 裡面呢
那這個就是我們等一下要講的
Momentum 這個技術
好 那我們先很快的複習一下
原來的 Gradient Descent 長得是什麼樣子
這個是 Vanilla 的 Gradient Descent
Vanilla 的意思就是一般的的意思
它直譯是香草的
但就其實是一般的
一般的 Gradient Descent 長什麼樣子呢
一般的 Gradient Descent 是說
我們有一個初始的參數叫做 θ0
我們計算一下 Gradient
然後計算完這個 Gradient 以後呢
我們往 Gradient 的反方向去 Update 參數
我們到了新的參數以後
再計算一次 Gradient
再往 Gradient 的反方向
再 Update 一次參數
到了新的位置以後再計算一次 Gradient
再往 Gradient 的反方向去 Update 參數
這個 Process 就一直這樣子下去
好 那如果加上 Momentum 的話
會是什麼樣子呢
加上 Momentum 以後
Gradient Descent 變成這個樣子
每一次我們在移動我們的參數的時候
我們不是只往 Gradient Descent
我們不是只往 Gradient 的反方向來移動參數
在一般的 Gradient Descent 裡面
我們都是往 Gradient 的反方向去移動參數
但現在不只往 Gradient 的反方向去移動參數
我們是 Gradient 的反方向
加上前一步移動的方向
兩者加起來的結果
去調整去到我們的參數
那具體說起來是這個樣子
一樣找一個初始的參數
然後我們假設潛在一開始的時候
前一步的變化量
前一步的參數的 Update 量呢
就設為 0
然後接下來在 θ0 的地方
你要計算 g0
計算 g0
計算 Gradient 的方向
然後接下來你要決定下一步要怎麼走
那我們說下一步要怎麼走呢
它是 Gradient 的方向加上前一步的方向
不過因為前一步正好是 0
現在是剛初始的時候所以前一步是 0
所以 Update 的方向
跟原來的 Gradient Descent 是一樣的
這沒有什麼有趣的地方
但從第二步開始
有加上 Momentum 以後就不太一樣了
從第二步開始
我們計算 g1
然後接下來我們 Update 的方向
不是 g1 的反方向
而是根據上一次 Update 方向
也就是 m1 減掉 g1
當做我們新的 Update 的方向
這邊寫成 m2
如果你看數學式子覺得有點模糊的話
那我們就看左邊這個圖
g1 告訴我們
Gradient 告訴我們要往這邊走
但是我們不是只聽 Gradient 的話
加上 Momentum 以後
我們不是只根據 Gradient 的反方向
來調整我們的參數
我們也會看前一次 Update 的方向
如果前一次說要往這個方向走
Gradient 說要往這個方向走
就把兩者相加起來
走兩者的折中
也就是往這一個方向走
所以我們就移動了 m2
走到 θ2 這個地方
接下來就反覆進行同樣的過程
在這個位置我們計算出 Gradient
但我們不是只根據 Gradient 反方向走
我們看前一步怎麼走
前一步走這個方向
走這個藍色虛線的方向
我們把藍色的虛線加紅色的虛線
前一步指示的方向跟 Gradient 指示的方向
當做我們下一步要移動的方向
以此類推
反覆進行同樣的操作
Gradient 在這個地方說要往這個方向走
然後前一步說要往這個方向走
兩者綜合起來就往左下走
所以這跟一般 Gradient Descent 不一樣
我們不是只看 Gradient 的方向來調整參數
我們還會考慮之前移動的方向來調整參數
好 那這邊的每一步的移動
我們都用 m 來表示
那這個 m 其實可以寫成之前所有算出來的
Gradient 的 Weighted Sum
怎麼說呢
從右邊的這個式子
其實就可以輕易的看出來
m0 我們把它設為 0
m1
m1 是 m0 減掉 g0
m0 為 0
所以 m1 就是 g0 乘上負的 η
m2 呢
m2 是 λ 乘上 m1
λ 就是另外一個參數
就跟 Learning Rate 一樣是要調的
就好像 η 是 Learning Rate 我們要調
λ 是另外一個參數
這個也是需要調的
m2 等於 λ 乘上 m1
減掉 η 乘上 g1
然後 m1 在哪裡呢
m1 在這邊
你把 m1 代進來
就知道說 m2
等於負的 λ 乘上 η 乘以 g0
減掉 η 乘上 g1
它是 g0 跟 g1 的 Weighted Sum
以此類推
所以你會發現說
現在這個加上 Momentum 以後
一個解讀是 Momentum 是
Gradient 的負反方向加上前一次移動的方向
那但另外一個解讀方式是
所謂的 Momentum
當加上 Momentum 的時候
我們 Update 的方向
不是只考慮現在的 Gradient
而是考慮過去所有 Gradient 的總合
好 那假設你這邊也沒有聽得很懂的話
那這邊有一個更簡單的例子
希望幫助你了解 Momentum 是怎麼回事
好 那我們從這個地方開始 Update 參數
根據 Gradient 的方向告訴我們
應該往右 Update 參數
那現在沒有前一次 Update 的方向
所以我們就完全按照 Gradient 給我們的指示
往右移動參數
好 那我們的參數
就往右移動了一點到這個地方
Gradient 變得很小
告訴我們往右移動
但是只有往右移動一點點
但前一步是往右移動的
我們把前一步的方向用虛線來表示
放在這個地方
我們把之前 Gradient 告訴我們要走的方向
跟前一步移動的方向加起來
得到往右走的方向
那再往右走 走到一個 Local Minima
照理說走到 Local Minima
一般 Gradient Descent 就無法向前走了
因為已經沒有這個 Gradient 的方向
那走到 Saddle Point 也一樣
沒有 Gradient 的方向已經無法向前走了
但沒有關係
如果有 Momentum 的話
你還是有辦法繼續走下去
因為 Momentum 不是只看 Gradient
Gradient 就算是 0
你還有前一步的方向
前一步的方向告訴我們向右走
我們就繼續向右走
甚至你走到這種地方
Gradient 告訴你應該要往左走了
但是假設你前一步的影響力
比 Gradient 要大的話
你還是有可能繼續往右走
甚至翻過一個小丘
搞不好就可以走到更好 Local Minima
這個就是 Momentum 有可能帶來的好處
好 那這個就是今天想要跟大家說的內容
