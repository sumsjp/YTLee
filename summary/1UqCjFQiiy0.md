### 小節歸納

#### 核心主題  
- 探討在回歸模型中使用梯度下降法（Gradient Descent）進行參數優化的過程及其挑戰。

#### 主要觀念  
1. 回歸模型的基本形式：\( y = b + w \cdot x \)，其中 \( b \) 和 \( w \) 是需要優化的參數。
2. 梯度下降法的基本原理：通過計算損失函數對參數的偏導數，逐步更新參數以最小化損失。
3. 損失函數的可視化：二維平面上的顏色表示不同的參數組合對應的損失值，最低點即爲最優解。

#### 問題原因  
1. 初始學習率（Learning Rate）設置不當導致梯度下降過程中的問題：
   - 學習率過小：迭代次數過多，仍無法接近最優解。
   - 學習率過大：參數更新過程中出現劇烈震蕩或發散，偏離最優解。

#### 解決方法  
1. 調整學習率大小：
   - 適當增大初始學習率，以加快收斂速度。
   - 避免過度調大學習率導致的發散問題。

#### 優化方式  
1. 引入自適應學習率調整方法（AdaGrad）：
   - 對每個參數 \( b \) 和 \( w \) 分別設置不同的學習率，使其能夠以不同的速率更新。
   - 通過動態調整學習率，使優化過程更加穩定和高效。

#### 結論  
- 梯度下降法在簡單回歸模型中的應用展示了參數初始化和學習率選擇的重要性。
- 對於複雜模型（如深度神經網絡），參數數量龐大且關係複雜，傳統的固定學習率難以滿足優化需求。
- 引入自適應優化算法（如AdaGrad、Adam等）是解決大規模參數優化問題的有效途徑。

### 關鍵字  
1. 梯度下降法（Gradient Descent）
2. 回歸模型
3. 學習率（Learning Rate）
4. 最佳化（Optimization）
5. 自適應學習率調整（AdaGrad）
6. 參數更新