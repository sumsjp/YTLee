### 核心主題
- **終身語言學習（Lifelong Language Learning）**：研究模型在學習多個NLP任務時，能夠逐步掌握而不導致災難性忘記的能力。
- **Lemmo 方法**：提出了一種基於語言模型的輕量級方法，用於解決終身學習中的災難性 forgetting 問題。


### 主要觀念
1. **終身學習的挑戰**：傳統機器學習算法在學習多個任務時易受災難性忘記影響。
2. **NLP 任務範圍**：涵蓋情感分類、語義角色標注、目標導向對話、問題解答和語義解析等多種任務。


### 問題原因
- **災難性 Forgetting**：模型在學習新任務後，舊任務的知識_recall 效果差。
- **數據生成需求**：傳統數據庫方法需要額外的數據生成模型，增加複雜度。


### 解決方法
1. **Lemmo 方法**：利用 Squad 格式轉換將多樣化的輸入輸出對統一為上下文、問題和答案格式。
2. **單一語言模型**: 一個模型用於任務解讀和假數據生成，降低複雜度。


### 優化方式
1. **任務特定令牌**：引入任務-specific令牌，在假數據生成時提高模型的目標識別能力。
2. **Squad 格式轉換**：將不同任務的數據格式化為統一的上下文、問題和答案格式，便於模型理解和生成。


### 結論
- **實驗結果**：Lemmo 方法在三個數據集上的表現優於先前的最先進方法，並接近多任務學習的上限。
- **未來方向**：進一步研究如何通過任務-specific令牌提高模型性能，特別是 задач數量增加時。