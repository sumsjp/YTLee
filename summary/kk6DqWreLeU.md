# 文章整理：強化學習中的價值函數與策略優化

## 小節一：核心主題
- 強化學習（Reinforcement Learning, RL）的核心目標是通過試錯來學習 optimal 策略。
- 價值函數（Value Function）在.rl中扮演關鍵角色，用於估計某個 states 的期望累積獎勵。

## 小節二：主要觀念
1. **價值函數的定義**：
   - 儀值函數 V(S) 表示從狀態 S 出發，按照策略 π 累積獎勵的期望值。
   - Q-價值函數 Q(S, A) 表示在狀態 S 下執行行動 A 後的期望累積獎勵。

2. **策略優化的目標**：
   - 策略 π 的目標是最大化每一步的即時獎勵，最終提升整體的累積獎勵。
   - 策略可以表示為條件機率分布，描述在每個狀態下採取各個行動的概率。

3. **樣本的重要影響**：
   - 強化學習的效果高度依賴於_sampling_ 的質量。良好的樣本能有效提升學習效果，反之則可能導致訓練失敗。

## 小節三：問題原因
1. **樣本不足的限制**：
   - 在某些情況下，特定狀態轉移（如 Sa 後面接 Sb）缺乏足夠的樣本，影響模型的學習能力。
   
2. **環境隨機性的挑戰**：
   - 環境中的不確定性導致價值函數 V(S) 代表的是期望值，而非固定的結果。這增加了估計的複雜性。

3. **行動分佈的理解不足**：
   - 理解_actor_ 的行動分佈至關重要。Actor 通過.softmax函數將行動分數轉換為機率分佈，根據此分佈進行抽樣。

## 小節四：解決方法
1. **增加樣本多樣性**：
   - 利用探索策略（如 ε-greedy）或 randomness 增加不同狀態轉移的樣本數量。
   
2. **價值函數的期望估計**：
   - 面對環境隨機性，價值函數 V(S) 使用期望值來捕獲所有可能結果的平均獎勵。

3. **_actor_ 分佈的建模**：
   - 將.Actor 看作條件機率分佈，在每個狀態下根據.softmax分佈進行行動抽樣，模擬人類的策略選擇。

## 小節五：優化方式
1. **Rainbow 方法**：
   - Rainbow 是一種知名的 DQN 變體，整合了七種改進技術，提升算法的穩定性和性能。
   
2. **經驗回放機制**：
   - 使用回放記憶庫儲存歷史樣本，隨機抽取進行訓練，增強樣本多樣性並降低短期相關性影響。

3. **軟最大值技術**：
   - 在.Actor 的行動分佈建模中使用Softmax函數，將分數轉換為溫和的機率分佈，平衡探索與利用。

## 小節六：結論
- 強化學習的成功依賴於價值函數的精準估計和Actor 分佈的有效建模。
- 樣本的多樣性和環境的不確定性是影響學習效果的重要因素。
- Rainbow 等先進算法為提升.rl性能提供了有效途徑，未來研究可進一步優化這些方法。